{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"skforecast \u00b6 Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...). Installation \u00b6 1 pip install skforecast Specific version: 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast@v0.1.9 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24 Dependencies \u00b6 1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24 statsmodels>=0.12.2 Features \u00b6 Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance Tutorials (spanish) \u00b6 Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python References \u00b6 Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance Licence \u00b6 joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Readme"},{"location":"index.html#skforecast","text":"Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...).","title":"skforecast"},{"location":"index.html#installation","text":"1 pip install skforecast Specific version: 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast@v0.1.9 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24","title":"Installation"},{"location":"index.html#dependencies","text":"1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24 statsmodels>=0.12.2","title":"Dependencies"},{"location":"index.html#features","text":"Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance","title":"Features"},{"location":"index.html#tutorials-spanish","text":"Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python","title":"Tutorials (spanish)"},{"location":"index.html#references","text":"Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance","title":"References"},{"location":"index.html#licence","text":"joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Licence"},{"location":"api/ForecasterAutoreg.html","text":"ForecasterAutoreg \u00b6 class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#forecasterautoreg","text":"class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoregCustom.html","text":"ForecasterAutoregCustom \u00b6 module skforecast.ForecasterAutoregCustom. ForecasterAutoregCustom Classes ForecasterAutoregCustom \u2014 This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#forecasterautoregcustom","text":"module skforecast.ForecasterAutoregCustom. ForecasterAutoregCustom Classes ForecasterAutoregCustom \u2014 This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregMultiOutput.html","text":"ForecasterAutoregMultiOutput \u00b6 module skforecast.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput Classes ForecasterAutoregMultiOutput \u2014 This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details.","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#forecasterautoregmultioutput","text":"module skforecast.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput Classes ForecasterAutoregMultiOutput \u2014 This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details.","title":"ForecasterAutoregMultiOutput"},{"location":"api/model_selection.html","text":"model_selection \u00b6 function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , set_out_sample_residuals=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection.html#model_selection","text":"function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , set_out_sample_residuals=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection_statsmodels.html","text":"model_selection_statsmodels \u00b6 function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax_statsmodels ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"api/model_selection_statsmodels.html#model_selection_statsmodels","text":"function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax_statsmodels ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"examples/examples.html","text":"Examples \u00b6 Use cases where skforecast library is used for forecasting. Forecasting series temporales con Python y Scikit-learn \u00b6 Forecasting de la demanda el\u00e9ctrica \u00b6 Forecasting de las visitas a una p\u00e1gina web \u00b6","title":"Forecasting web page traffic"},{"location":"examples/examples.html#examples","text":"Use cases where skforecast library is used for forecasting.","title":"Examples"},{"location":"examples/examples.html#forecasting-series-temporales-con-python-y-scikit-learn","text":"","title":"Forecasting series temporales con Python y Scikit-learn"},{"location":"examples/examples.html#forecasting-de-la-demanda-electrica","text":"","title":"Forecasting de la demanda el\u00e9ctrica"},{"location":"examples/examples.html#forecasting-de-las-visitas-a-una-pagina-web","text":"","title":"Forecasting de las visitas a una p\u00e1gina web"},{"location":"guides/autoregresive-forecaster-exogenous.html","text":"Recursive multi-step forecasting with exogenous variables \u00b6 ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i]. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :] Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: True Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 8 9 10 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] . values ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.965051 2005-08-01 1.014752 2005-09-01 1.140090 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.008474661390588431 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 0.1173852 , 0.0290203 , 0.02626445, -0.06322356, -0.04009523, -0.00921489, -0.04732691, -0.00662549, 0.01139862, 0.01485692, 0.15042588, 0.56962708, 0.01581026, -0.08791199, -0.06785373, 0.09607342, 0.21051962]) Extract training matrix \u00b6 1 2 3 4 X , y = forecaster . create_train_X_y ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values )","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#recursive-multi-step-forecasting-with-exogenous-variables","text":"ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i].","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster-exogenous.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :]","title":"Data"},{"location":"guides/autoregresive-forecaster-exogenous.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: True Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster-exogenous.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 8 9 10 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] . values ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.965051 2005-08-01 1.014752 2005-09-01 1.140090 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.008474661390588431","title":"Prediction"},{"location":"guides/autoregresive-forecaster-exogenous.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 0.1173852 , 0.0290203 , 0.02626445, -0.06322356, -0.04009523, -0.00921489, -0.04732691, -0.00662549, 0.01139862, 0.01485692, 0.15042588, 0.56962708, 0.01581026, -0.08791199, -0.06785373, 0.09607342, 0.21051962])","title":"Feature importance"},{"location":"guides/autoregresive-forecaster-exogenous.html#extract-training-matrix","text":"1 2 3 4 X , y = forecaster . create_train_X_y ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values )","title":"Extract training matrix"},{"location":"guides/autoregresive-forecaster.html","text":"Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Prediction \u00b6 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005 - 07 - 01 0.973131 2005 - 08 - 01 1.022154 2005 - 09 - 01 1.151334 Freq : MS , dtype : float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009918738501371805 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 1.58096176e-01, 6.18241513e-02, 6.44665806e-02, -2.41792429e-02, -2.60679572e-02, 7.04191008e-04, -4.28090339e-02, 4.87464352e-04, 1.66853207e-02, 1.00022527e-02, 1.62219885e-01, 6.15595305e-01, 2.85168042e-02, -7.31915864e-02, -5.38785052e-02]) Extract training matrix \u00b6 1 X , y = forecaster . create_train_X_y ( data_train )","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom .","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/autoregresive-forecaster.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster.html#prediction","text":"1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005 - 07 - 01 0.973131 2005 - 08 - 01 1.022154 2005 - 09 - 01 1.151334 Freq : MS , dtype : float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009918738501371805","title":"Prediction"},{"location":"guides/autoregresive-forecaster.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 1.58096176e-01, 6.18241513e-02, 6.44665806e-02, -2.41792429e-02, -2.60679572e-02, 7.04191008e-04, -4.28090339e-02, 4.87464352e-04, 1.66853207e-02, 1.00022527e-02, 1.62219885e-01, 6.15595305e-01, 2.85168042e-02, -7.31915864e-02, -5.38785052e-02])","title":"Feature importance"},{"location":"guides/autoregresive-forecaster.html#extract-training-matrix","text":"1 X , y = forecaster . create_train_X_y ( data_train )","title":"Extract training matrix"},{"location":"guides/cross-validation-backtest.html","text":"Cross validation and backtest \u00b6 Time series cross-validation The model is trained before making the predictions, in this way, the model use all the information available so far. It is a variation of the standar cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting It is a similar strategy to that of time series cross-validation but without retraining. After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster than time series cross-validation since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time. Libraries \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import cv_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import backtesting_forecaster_intervals from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) ax . legend (); Time series cross-validation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Cross-validation forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) cv_forecaster ( forecaster = forecaster , y = data , steps = 10 , metric = 'mean_squared_error' , initial_train_size = int ( len ( data ) * 0.5 ), allow_incomplete_fold = True , verbose = True ) 1 2 3 4 5 6 Number of folds: 11 Since `allow_incomplete_fold=True`, last fold only includes 2 observations instead of 10. Incomplete folds with few observations could overestimate or underestimate validation metrics. array([0.00422291, 0.00634336, 0.01092581, 0.00483358, 0.00746501, 0.0048392 , 0.01333912, 0.00597068, 0.00616372, 0.00881862, 0.00677257]) Backtest \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 + 1 # Last 3 months are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , verbose = True ) print ( f \"Error de backtest: { metric } \" ) 1 2 3 4 5 6 Number of observations used for training: 95 Number of observations used for testing: 109 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 9 observations Error de backtest: [0.01823166] 1 2 3 4 5 6 7 # Plot backtest predictions # ============================================================================== predictions_backtest = pd . Series ( data = predictions_backtest , index = data_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend (); Backtest with prediction intervals \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 + 1 # Last 3 months are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster_intervals ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) print ( f \"Error de backtest: { metric } \" ) 1 2 3 4 5 6 Number of observations used for training: 95 Number of observations used for testing: 109 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 9 observations Error de backtest: [0.01823166] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Plot backtest predictions # ============================================================================== predictions_backtest = pd . DataFrame ( data = predictions_backtest , index = data_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Cross validation and Backtest"},{"location":"guides/cross-validation-backtest.html#cross-validation-and-backtest","text":"Time series cross-validation The model is trained before making the predictions, in this way, the model use all the information available so far. It is a variation of the standar cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting It is a similar strategy to that of time series cross-validation but without retraining. After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster than time series cross-validation since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time.","title":"Cross validation and backtest"},{"location":"guides/cross-validation-backtest.html#libraries","text":"1 2 3 4 5 6 7 8 9 10 11 12 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import cv_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import backtesting_forecaster_intervals from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/cross-validation-backtest.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) ax . legend ();","title":"Data"},{"location":"guides/cross-validation-backtest.html#time-series-cross-validation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Cross-validation forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) cv_forecaster ( forecaster = forecaster , y = data , steps = 10 , metric = 'mean_squared_error' , initial_train_size = int ( len ( data ) * 0.5 ), allow_incomplete_fold = True , verbose = True ) 1 2 3 4 5 6 Number of folds: 11 Since `allow_incomplete_fold=True`, last fold only includes 2 observations instead of 10. Incomplete folds with few observations could overestimate or underestimate validation metrics. array([0.00422291, 0.00634336, 0.01092581, 0.00483358, 0.00746501, 0.0048392 , 0.01333912, 0.00597068, 0.00616372, 0.00881862, 0.00677257])","title":"Time series cross-validation"},{"location":"guides/cross-validation-backtest.html#backtest","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 + 1 # Last 3 months are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , verbose = True ) print ( f \"Error de backtest: { metric } \" ) 1 2 3 4 5 6 Number of observations used for training: 95 Number of observations used for testing: 109 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 9 observations Error de backtest: [0.01823166] 1 2 3 4 5 6 7 # Plot backtest predictions # ============================================================================== predictions_backtest = pd . Series ( data = predictions_backtest , index = data_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend ();","title":"Backtest"},{"location":"guides/cross-validation-backtest.html#backtest-with-prediction-intervals","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 + 1 # Last 3 months are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster_intervals ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) print ( f \"Error de backtest: { metric } \" ) 1 2 3 4 5 6 Number of observations used for training: 95 Number of observations used for testing: 109 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 9 observations Error de backtest: [0.01823166] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Plot backtest predictions # ============================================================================== predictions_backtest = pd . DataFrame ( data = predictions_backtest , index = data_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Backtest with prediction intervals"},{"location":"guides/custom-predictors.html","text":"Custom predictors for recursive multi-step forecasting \u00b6 It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 =======================ForecasterAutoregCustom======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with: create_predictors Window size: 20 Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Prediction \u00b6 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances () 1 2 3 array([0.53971953, 0.11909726, 0.04640365, 0.0241653 , 0.03056669, 0.01513909, 0.04288317, 0.012742 , 0.01893797, 0.10863873, 0.04170661]) Extract training matrix \u00b6 1 X , y = forecaster . create_train_X_y ( data_train )","title":"Custom predictors"},{"location":"guides/custom-predictors.html#custom-predictors-for-recursive-multi-step-forecasting","text":"It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors.","title":"Custom predictors for  recursive multi-step forecasting"},{"location":"guides/custom-predictors.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/custom-predictors.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/custom-predictors.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 =======================ForecasterAutoregCustom======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with: create_predictors Window size: 20 Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Create and train forecaster"},{"location":"guides/custom-predictors.html#prediction","text":"1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191","title":"Prediction"},{"location":"guides/custom-predictors.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances () 1 2 3 array([0.53971953, 0.11909726, 0.04640365, 0.0241653 , 0.03056669, 0.01513909, 0.04288317, 0.012742 , 0.01893797, 0.10863873, 0.04170661])","title":"Feature importance"},{"location":"guides/custom-predictors.html#extract-training-matrix","text":"1 X , y = forecaster . create_train_X_y ( data_train )","title":"Extract training matrix"},{"location":"guides/direct-multi-step-forecasting.html","text":"Direct multi-step forecaster \u00b6 ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = GradientBoostingRegressor (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 ============================ForecasterAutoregMultiOutput============================ Regressor: GradientBoostingRegressor() Steps: 36 Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False} Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.877073 2005-08-01 0.974353 2005-09-01 1.021718 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.006208145903529839 Feature importance \u00b6 Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances ( step = 1 ) 1 2 3 array([0.0070333 , 0.07330653, 0.03192484, 0.00284055, 0.00525716, 0.0039042 , 0.00717631, 0.00676659, 0.00587334, 0.00856639, 0.01275252, 0.81823596, 0.005138 , 0.00413031, 0.007094]) Extract training matrix \u00b6 Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , )","title":"Direct multi-step forecasting"},{"location":"guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","text":"ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model.","title":"Direct multi-step forecaster"},{"location":"guides/direct-multi-step-forecasting.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/direct-multi-step-forecasting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/direct-multi-step-forecasting.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = GradientBoostingRegressor (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 ============================ForecasterAutoregMultiOutput============================ Regressor: GradientBoostingRegressor() Steps: 36 Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}","title":"Create and train forecaster"},{"location":"guides/direct-multi-step-forecasting.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.877073 2005-08-01 0.974353 2005-09-01 1.021718 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.006208145903529839","title":"Prediction"},{"location":"guides/direct-multi-step-forecasting.html#feature-importance","text":"Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances ( step = 1 ) 1 2 3 array([0.0070333 , 0.07330653, 0.03192484, 0.00284055, 0.00525716, 0.0039042 , 0.00717631, 0.00676659, 0.00587334, 0.00856639, 0.01275252, 0.81823596, 0.005138 , 0.00413031, 0.007094])","title":"Feature importance"},{"location":"guides/direct-multi-step-forecasting.html#extract-training-matrix","text":"Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , )","title":"Extract training matrix"},{"location":"guides/introduction-forecasting.html","text":"Introduction to forcasting \u00b6 Time Series and forecasting \u00b6 A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable. Direct multi-step forecasting \u00b6 This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#introduction-to-forcasting","text":"","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#time-series-and-forecasting","text":"A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions.","title":"Time Series and forecasting"},{"location":"guides/introduction-forecasting.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable.","title":"Recursive multi-step forecasting"},{"location":"guides/introduction-forecasting.html#direct-multi-step-forecasting","text":"This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Direct multi-step forecasting"},{"location":"guides/prediction-intervals.html","text":"Prediction intervals \u00b6 Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Prediction \u00b6 1 2 3 4 5 6 7 8 9 10 11 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = steps , interval = [ 5 , 95 ], n_boot = 1000 ) # Add datetime index to predictions predictions = pd . DataFrame ( data = predictions , index = data_test . index ) predictions . head ( 4 ) 1 2 3 4 5 6 0 1 2 fecha 2005-07-01 0.973131 0.874545 1.074531 2005-08-01 1.022154 0.925574 1.118590 2005-09-01 1.151334 1.048968 1.255996 2005-10-01 1.206401 1.105658 1.311676 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions . iloc [:, 1 ], predictions . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#prediction-intervals","text":"","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/prediction-intervals.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/prediction-intervals.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}","title":"Create and train forecaster"},{"location":"guides/prediction-intervals.html#prediction","text":"1 2 3 4 5 6 7 8 9 10 11 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = steps , interval = [ 5 , 95 ], n_boot = 1000 ) # Add datetime index to predictions predictions = pd . DataFrame ( data = predictions , index = data_test . index ) predictions . head ( 4 ) 1 2 3 4 5 6 0 1 2 fecha 2005-07-01 0.973131 0.874545 1.074531 2005-08-01 1.022154 0.925574 1.118590 2005-09-01 1.151334 1.048968 1.255996 2005-10-01 1.206401 1.105658 1.311676 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions . iloc [:, 1 ], predictions . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction"},{"location":"guides/tuning-forecaster.html","text":"Tuning forecaster \u00b6 Skforecast library allows to combine grid search strategy with cross-validation and backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediccion performance. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] Grid search with time series cross-validation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 14:28:00,536 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.39it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.11s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:02<00:00, 1.08it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.09s/it] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:04<00:08, 4.16s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.22it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.40s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.32s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00, 1.42s/it] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:09<00:04, 4.91s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.18it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.23s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.01s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.27s/it] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.78s/it] 2021-08-13 14:28:14,897 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 4 5 6 7 8 9 10] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0265202 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0269665 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0280916 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0286928 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0295003 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0332516 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0338282 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0341536 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0365391 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.036623 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0393264 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.046249 1 skforecast 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: RandomForestRegressor(max_depth=10, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Grid search with backtesting \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'backtesting' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 16:28:52,777 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.42it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.36it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.80it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.33it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:01<00:03, 1.63s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.55it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.24it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.55it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.23it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:03<00:01, 1.68s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.31it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.30it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.64it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.21it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05<00:00, 1.68s/it] 2021-08-13 16:28:57,833 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 20] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0500237 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.0543223 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0568267 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0569292 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0575393 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0584062 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0586133 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0607635 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0618727 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0622848 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0625045 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.0642727 1 skforecast 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: RandomForestRegressor(max_depth=10, n_estimators=50, random_state=123) Lags: [ 1 2 3 20] Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#tuning-forecaster","text":"Skforecast library allows to combine grid search strategy with cross-validation and backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediccion performance.","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/tuning-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :]","title":"Data"},{"location":"guides/tuning-forecaster.html#grid-search-with-time-series-cross-validation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 14:28:00,536 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.39it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.11s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:02<00:00, 1.08it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.09s/it] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:04<00:08, 4.16s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.22it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.40s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.32s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00, 1.42s/it] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:09<00:04, 4.91s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.18it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.23s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.01s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.27s/it] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.78s/it] 2021-08-13 14:28:14,897 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 4 5 6 7 8 9 10] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0265202 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0269665 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0280916 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0286928 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0295003 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0332516 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0338282 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0341536 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0365391 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.036623 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0393264 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.046249 1 skforecast 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: RandomForestRegressor(max_depth=10, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Grid search with time series cross-validation"},{"location":"guides/tuning-forecaster.html#grid-search-with-backtesting","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'backtesting' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 16:28:52,777 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.42it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.36it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.80it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.33it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:01<00:03, 1.63s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.55it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.24it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.55it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.23it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:03<00:01, 1.68s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.31it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:00<00:00, 2.30it/s] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:01<00:00, 2.64it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.21it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05<00:00, 1.68s/it] 2021-08-13 16:28:57,833 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 20] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0500237 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.0543223 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0568267 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0569292 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0575393 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0584062 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0586133 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0607635 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0618727 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0622848 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0625045 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.0642727 1 skforecast 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: RandomForestRegressor(max_depth=10, n_estimators=50, random_state=123) Lags: [ 1 2 3 20] Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Grid search with backtesting"},{"location":"releases/releases.html","text":"Releases \u00b6 [0.3.0] - [2021-09-01] \u00b6 Added \u00b6 New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels Changed \u00b6 cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements Fixed \u00b6 [0.2.0] - [2021-08-26] \u00b6 Added \u00b6 Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Fixed \u00b6 [0.1.9] - 2121-07-27 \u00b6 Added \u00b6 Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog . Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric. Fixed \u00b6 Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster . [0.1.8.1] - 2021-05-17 \u00b6 Added \u00b6 set_out_sample_residuals method to store or update out of sample residuals used by predict_interval . Changed \u00b6 backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals . Fixed \u00b6 Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput . [0.1.8] - 2021-04-02 \u00b6 Added \u00b6 Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals. Changed \u00b6 Fixed \u00b6 [0.1.7] - 2021-03-19 \u00b6 Added \u00b6 Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors. Changed \u00b6 grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg . Fixed \u00b6 [0.1.6] - 2021-03-14 \u00b6 Added \u00b6 Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection . Changed \u00b6 Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster . Fixed \u00b6 [0.1.4] - 2021-02-15 \u00b6 Added \u00b6 Method get_coef to skforecast.ForecasterAutoreg . Changed \u00b6 Fixed \u00b6","title":"Releases"},{"location":"releases/releases.html#releases","text":"","title":"Releases"},{"location":"releases/releases.html#030-2021-09-01","text":"","title":"[0.3.0] - [2021-09-01]"},{"location":"releases/releases.html#added","text":"New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels","title":"Added"},{"location":"releases/releases.html#changed","text":"cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements","title":"Changed"},{"location":"releases/releases.html#fixed","text":"","title":"Fixed"},{"location":"releases/releases.html#020-2021-08-26","text":"","title":"[0.2.0] - [2021-08-26]"},{"location":"releases/releases.html#added_1","text":"Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing","title":"Added"},{"location":"releases/releases.html#changed_1","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput .","title":"Changed"},{"location":"releases/releases.html#fixed_1","text":"","title":"Fixed"},{"location":"releases/releases.html#019-2121-07-27","text":"","title":"[0.1.9] - 2121-07-27"},{"location":"releases/releases.html#added_2","text":"Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog .","title":"Added"},{"location":"releases/releases.html#changed_2","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.","title":"Changed"},{"location":"releases/releases.html#fixed_2","text":"Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster .","title":"Fixed"},{"location":"releases/releases.html#0181-2021-05-17","text":"","title":"[0.1.8.1] - 2021-05-17"},{"location":"releases/releases.html#added_3","text":"set_out_sample_residuals method to store or update out of sample residuals used by predict_interval .","title":"Added"},{"location":"releases/releases.html#changed_3","text":"backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals .","title":"Changed"},{"location":"releases/releases.html#fixed_3","text":"Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput .","title":"Fixed"},{"location":"releases/releases.html#018-2021-04-02","text":"","title":"[0.1.8] - 2021-04-02"},{"location":"releases/releases.html#added_4","text":"Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals.","title":"Added"},{"location":"releases/releases.html#changed_4","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_4","text":"","title":"Fixed"},{"location":"releases/releases.html#017-2021-03-19","text":"","title":"[0.1.7] - 2021-03-19"},{"location":"releases/releases.html#added_5","text":"Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors.","title":"Added"},{"location":"releases/releases.html#changed_5","text":"grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg .","title":"Changed"},{"location":"releases/releases.html#fixed_5","text":"","title":"Fixed"},{"location":"releases/releases.html#016-2021-03-14","text":"","title":"[0.1.6] - 2021-03-14"},{"location":"releases/releases.html#added_6","text":"Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection .","title":"Added"},{"location":"releases/releases.html#changed_6","text":"Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster .","title":"Changed"},{"location":"releases/releases.html#fixed_6","text":"","title":"Fixed"},{"location":"releases/releases.html#014-2021-02-15","text":"","title":"[0.1.4] - 2021-02-15"},{"location":"releases/releases.html#added_7","text":"Method get_coef to skforecast.ForecasterAutoreg .","title":"Added"},{"location":"releases/releases.html#changed_7","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_7","text":"","title":"Fixed"}]}