{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: to simplify models to make them easier to interpret, to reduce training time, to avoid the curse of dimensionality, to improve generalization by reducing overfitting (formally, variance reduction), and others.\n",
    "\n",
    "Skforecast is compatible with the [feature selection methods implemented in the scikit-learn](https://scikit-learn.org/stable/modules/feature_selection.html) library. There are several methods for feature selection, but the most common are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive feature elimination**\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination ([RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features, and the importance of each feature is obtained either by a specific attribute (such as `coef_`, `feature_importances_`) or by a `callable`. Then, the least important features are pruned from the current set of features. This procedure is repeated recursively on the pruned set until the desired number of features to select is eventually reached. [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) performs RFE in a cross-validation loop to find the optimal number of features.\n",
    "\n",
    "**Sequential Feature Selection**\n",
    "\n",
    "Sequential Feature Selection ([SFS](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)) can be either forward or backward, with the `direction` parameter controlling whether forward or backward SFS is used.\n",
    "\n",
    "+ **Forward-SFS** is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. It starts with zero features and finds the one that maximizes a cross-validated score when an estimator is trained on that single feature. Once this first feature is selected, the procedure is repeated, adding one new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter.\n",
    "\n",
    "+ **Backward-SFS** follows the same idea, but works in the opposite direction. Instead of starting with no features and greedily adding features, it starts with all features and greedily removes features from the set. \n",
    "\n",
    "In general, forward and backward selection do not produce equivalent results. Also, one can be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n",
    "\n",
    "SFS differs does not require the underlying model to expose a `coef_` or `feature_importances_` attribute. However, it may be slower compared to the other approaches, considering that more models have to be evaluated. For example in backward selection, the iteration going from $m$ features to $m - 1$ features using k-fold cross-validation requires fitting $m * k$ models to be evaluated.\n",
    "\n",
    "**Feature selection based on threshold (SelectFromModel)**\n",
    "\n",
    "[SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) can be used along with any estimator that has a `coef_` or `feature_importances_` attribute after fitting. Features are considered unimportant and removed, if the corresponding `coef_` or `feature_importances_` values are below the given `threshold` parameter. In addition to specifying the `threshold` numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are `'mean'`, `'median'` and float multiples of these, such as `'0.1*mean'`.\n",
    "\n",
    "This method is very fast compared to the others because it does not require any additional model training. However, it does not evaluate the impact of feature removal on the model. It is often used for an initial selection before applying another more computationally expensive feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: rgba(0,191,191,.1); padding-top: 0px; padding-bottom: 6px; border-radius: 8px; border-left: 8px solid #00bfa5; border-color: #00bfa5; padding-left: 10px; padding-right: 10px;\">\n",
    "\n",
    "<p class=\"title\">\n",
    "    <i style=\"font-size: 18px; color:#00bfa5;\"></i>\n",
    "    <b style=\"color: #00bfa5;\">&#128161 Tip</b>\n",
    "</p>\n",
    "\n",
    "Feature selection is a powerful tool for improving the performance of machine learning models. However, it is computationally expensive and can be time-consuming. Since the goal is to find the best subset of features, not the best model, it is not necessary to use the entire data set or a highly complex model. Instead, it is recommended to use a <b>small subset of the data and a simple model</b>. Once the best subset of features has been identified, the model can then be trained using the entire dataset and a more complex configuration.\n",
    "<br><br>\n",
    "For example, in this use case, the model is an <code>LGMBRegressor</code> with 900 trees and a maximum depth of 7. However, to find the best subset of features, only 100 trees and a maximum depth of 5 are used.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: rgba(255,145,0,.1); padding-top: 0px; padding-bottom: 6px; border-radius: 8px; border-left: 8px solid #ff9100; border-color: #ff9100; padding-left: 10px; padding-right: 10px\">\n",
    "\n",
    "<p class=\"title\">\n",
    "    <i style=\"font-size: 18px; color:#ff9100; border-color: #ff1744;\"></i>\n",
    "    <b style=\"color: #ff9100;\"> <span style=\"color: #ff9100;\">&#9888;</span> Warning</b>\n",
    "</p>\n",
    "\n",
    "The current version of skforecast only supports feature selection for forecasters of type <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> for univariate forecasts, and <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> for global forecast models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import select_features\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.model_selection_multiseries import select_features_multiseries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_sharing_extended_features\n",
      "------------------------------\n",
      "Hourly usage of the bike share system in the city of Washington D.C. during the\n",
      "years 2011 and 2012. In addition to the number of users per hour, the dataset\n",
      "was enriched by introducing supplementary features. Addition includes calendar-\n",
      "based variables (day of the week, hour of the day, month, etc.), indicators for\n",
      "sunlight, incorporation of rolling temperature averages, and the creation of\n",
      "polynomial features generated from variable pairs. All cyclic variables are\n",
      "encoded using sine and cosine functions to ensure accurate representation.\n",
      "Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\n",
      "https://doi.org/10.24432/C5W894.\n",
      "Shape of the dataset: (17352, 90)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>weather</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_of_year_sin</th>\n",
       "      <th>week_of_year_cos</th>\n",
       "      <th>week_day_sin</th>\n",
       "      <th>week_day_cos</th>\n",
       "      <th>hour_day_sin</th>\n",
       "      <th>hour_day_cos</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_roll_mean_1_day</th>\n",
       "      <th>temp_roll_mean_7_day</th>\n",
       "      <th>temp_roll_max_1_day</th>\n",
       "      <th>temp_roll_min_1_day</th>\n",
       "      <th>temp_roll_max_7_day</th>\n",
       "      <th>temp_roll_min_7_day</th>\n",
       "      <th>holiday_previous_day</th>\n",
       "      <th>holiday_next_day</th>\n",
       "      <th>temp</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-08 00:00:00</th>\n",
       "      <td>25.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.120537</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>...</td>\n",
       "      <td>8.063334</td>\n",
       "      <td>10.127976</td>\n",
       "      <td>9.02</td>\n",
       "      <td>6.56</td>\n",
       "      <td>18.86</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-08 01:00:00</th>\n",
       "      <td>16.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.120537</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>...</td>\n",
       "      <td>8.029166</td>\n",
       "      <td>10.113334</td>\n",
       "      <td>9.02</td>\n",
       "      <td>6.56</td>\n",
       "      <td>18.86</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-08 02:00:00</th>\n",
       "      <td>16.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.120537</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>...</td>\n",
       "      <td>7.995000</td>\n",
       "      <td>10.103572</td>\n",
       "      <td>9.02</td>\n",
       "      <td>6.56</td>\n",
       "      <td>18.86</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     users weather  month_sin  month_cos  week_of_year_sin  \\\n",
       "date_time                                                                    \n",
       "2011-01-08 00:00:00   25.0    mist        0.5   0.866025          0.120537   \n",
       "2011-01-08 01:00:00   16.0    mist        0.5   0.866025          0.120537   \n",
       "2011-01-08 02:00:00   16.0    mist        0.5   0.866025          0.120537   \n",
       "\n",
       "                     week_of_year_cos  week_day_sin  week_day_cos  \\\n",
       "date_time                                                           \n",
       "2011-01-08 00:00:00          0.992709     -0.781832       0.62349   \n",
       "2011-01-08 01:00:00          0.992709     -0.781832       0.62349   \n",
       "2011-01-08 02:00:00          0.992709     -0.781832       0.62349   \n",
       "\n",
       "                     hour_day_sin  hour_day_cos  ...  temp_roll_mean_1_day  \\\n",
       "date_time                                        ...                         \n",
       "2011-01-08 00:00:00      0.258819      0.965926  ...              8.063334   \n",
       "2011-01-08 01:00:00      0.500000      0.866025  ...              8.029166   \n",
       "2011-01-08 02:00:00      0.707107      0.707107  ...              7.995000   \n",
       "\n",
       "                     temp_roll_mean_7_day  temp_roll_max_1_day  \\\n",
       "date_time                                                        \n",
       "2011-01-08 00:00:00             10.127976                 9.02   \n",
       "2011-01-08 01:00:00             10.113334                 9.02   \n",
       "2011-01-08 02:00:00             10.103572                 9.02   \n",
       "\n",
       "                     temp_roll_min_1_day  temp_roll_max_7_day  \\\n",
       "date_time                                                       \n",
       "2011-01-08 00:00:00                 6.56                18.86   \n",
       "2011-01-08 01:00:00                 6.56                18.86   \n",
       "2011-01-08 02:00:00                 6.56                18.86   \n",
       "\n",
       "                     temp_roll_min_7_day  holiday_previous_day  \\\n",
       "date_time                                                        \n",
       "2011-01-08 00:00:00                 4.92                   0.0   \n",
       "2011-01-08 01:00:00                 4.92                   0.0   \n",
       "2011-01-08 02:00:00                 4.92                   0.0   \n",
       "\n",
       "                     holiday_next_day  temp  holiday  \n",
       "date_time                                             \n",
       "2011-01-08 00:00:00               0.0  7.38      0.0  \n",
       "2011-01-08 01:00:00               0.0  7.38      0.0  \n",
       "2011-01-08 02:00:00               0.0  7.38      0.0  \n",
       "\n",
       "[3 rows x 90 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "data = fetch_dataset(name=\"bike_sharing_extended_features\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection (reduce data size to speed up the example)\n",
    "# ==============================================================================\n",
    "data = data.drop(columns=\"weather\")\n",
    "data = data.loc[\"2012-01-01 00:00:00\":]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create forecaster\n",
    "\n",
    "A forecasting model is created to predict the number of users using the last 48 values (last two days) and the exogenous features available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                 regressor = LGBMRegressor(\n",
    "                                 n_estimators = 900,\n",
    "                                 random_state = 15926,\n",
    "                                 max_depth    = 7,\n",
    "                                 verbose      = -1\n",
    "                             ),\n",
    "                 lags      = 48,\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection with Recursive Feature Elimination (RFECV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select_features` function from the `skforecast.model_selection` module is used to select the best subset of features (autoregressive and exogenous variables). This function is compatible with the feature selection methods implemented in the scikit-learn library. The following parameters are available:\n",
    "\n",
    "- `forecaster`: Forecaster of type `ForecasterAutoreg` or `ForecasterAutoregCustom`.\n",
    "\n",
    "- `selector`: Feature selector from `sklearn.feature_selection`. For example, `RFE` or `RFECV`.\n",
    "\n",
    "- `y`: Target time series to which the feature selection will be applied.\n",
    "\n",
    "- `exog`: Exogenous variables.\n",
    "\n",
    "- `select_only`: Decide what type of features to include in the selection process. \n",
    "        \n",
    "    + If `'autoreg'`, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (`selected_exog`).\n",
    "\n",
    "    + If `'exog'`, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the output (`selected_autoreg`).\n",
    "\n",
    "    + If `None`, all features are evaluated by the selector.\n",
    "\n",
    "- `force_inclusion`: Features to force include in the final list of selected features.\n",
    "        \n",
    "    + If `list`, list of feature names to force include.\n",
    "    \n",
    "    + If `str`, regular expression to identify features to force include. For example, if `force_inclusion=\"^sun_\"`, all features that begin with \"sun_\" will be included in the final list of selected features.\n",
    "\n",
    "- `subsample`: Proportion of records to use for feature selection.\n",
    "\n",
    "- `random_state`: Sets a seed for the random subsample so that the subsampling process is always deterministic.\n",
    "\n",
    "- `verbose`: Print information about feature selection process.\n",
    "\n",
    "This function returns two `list`:\n",
    "\n",
    "- `selected_autoreg`: List of selected autoregressive features.\n",
    "\n",
    "- `selected_exog`: List of selected exogenous features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of autoregressive and exogenous features\n",
    "\n",
    "By default, the `select_features` function selects the best subset of autoregressive and exogenous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (RFECV)\n",
      "-------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 4356\n",
      "Number of features available: 136\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=88)\n",
      "Number of features selected: 39\n",
      "    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n",
      "    Exog    (n=12) : ['week_of_year_sin', 'hour_day_sin', 'hour_day_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'temp_roll_mean_1_day', 'temp']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (autoregressive and exog) with scikit-learn RFECV\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "selector = RFECV(\n",
    "    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n",
    ")\n",
    "\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster      = forecaster,\n",
    "    selector        = selector,\n",
    "    y               = data[\"users\"],\n",
    "    exog            = data.drop(columns=\"users\"),\n",
    "    select_only     = None,\n",
    "    force_inclusion = None,\n",
    "    subsample       = 0.5,\n",
    "    random_state    = 123,\n",
    "    verbose         = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection on a subset of features\n",
    "\n",
    "+ If `select_only = 'autoreg'`, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (`selected_exog`).\n",
    "\n",
    "+ If `select_only = 'exog'`, exogenous features are evaluated by the selector in the absence of autoregressive features. All autoregressive features are included in the output (`selected_autoreg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (RFECV)\n",
      "-------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 4356\n",
      "Number of features available: 48\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=88)\n",
      "Number of features selected: 28\n",
      "    Autoreg (n=28) : [1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 29, 32, 34, 35, 36, 42, 43, 48]\n",
      "    Exog    (n=88) : ['month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos', 'sunset_hour_sin', 'sunset_hour_cos', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_sin', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_sin__sunset_hour_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_month_cos__sunrise_hour_sin', 'poly_month_cos__sunrise_hour_cos', 'poly_month_cos__sunset_hour_sin', 'poly_month_cos__sunset_hour_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'poly_sunrise_hour_sin__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_sin', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunrise_hour_cos__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (only autoregressive) with scikit-learn RFECV\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "selector = RFECV(\n",
    "    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n",
    ")\n",
    "\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster  = forecaster,\n",
    "    selector    = selector,\n",
    "    y           = data[\"users\"],\n",
    "    exog        = data.drop(columns=\"users\"),\n",
    "    select_only = 'autoreg',\n",
    "    subsample   = 0.5,\n",
    "    verbose     = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all exogenous features are selected\n",
    "# ==============================================================================\n",
    "len(selected_exog) == data.drop(columns=\"users\").shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (RFECV)\n",
      "-------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 4356\n",
      "Number of features available: 88\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=88)\n",
      "Number of features selected: 66\n",
      "    Autoreg (n=48) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "    Exog    (n=66) : ['week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'temp', 'holiday']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (only exog) with scikit-learn RFECV\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "selector = RFECV(\n",
    "    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n",
    ")\n",
    "\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster  = forecaster,\n",
    "    selector    = selector,\n",
    "    y           = data[\"users\"],\n",
    "    exog        = data.drop(columns=\"users\"),\n",
    "    select_only = 'exog',\n",
    "    subsample   = 0.5,\n",
    "    verbose     = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all autoregressive features are selected\n",
    "# ==============================================================================\n",
    "len(selected_autoreg) == len(forecaster.lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force selection of specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `force_inclusion` argument can be used to force the selection of certain features. To illustrate this, a non-informative feature is added to the data set, `noise`. This feature contains no information about the target variable and therefore should not be selected by the feature selector. However, if we force the inclusion of this feature, it will be included in the final list of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add non-informative feature\n",
    "# ==============================================================================\n",
    "data['noise'] = np.random.normal(size=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (RFECV)\n",
      "-------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 4356\n",
      "Number of features available: 89\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=89)\n",
      "Number of features selected: 63\n",
      "    Autoreg (n=48) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "    Exog    (n=63) : ['week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'temp', 'holiday', 'noise']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (only exog) with scikit-learn RFECV\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "selector = RFECV(\n",
    "    estimator=regressor, step=1, cv=3, min_features_to_select=10, n_jobs=-1\n",
    ")\n",
    "\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster      = forecaster,\n",
    "    selector        = selector,\n",
    "    y               = data[\"users\"],\n",
    "    exog            = data.drop(columns=\"users\"),\n",
    "    select_only     = 'exog',\n",
    "    force_inclusion = [\"noise\"],\n",
    "    subsample       = 0.5,\n",
    "    verbose         = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if \"noise\" is in selected_exog\n",
    "# ==============================================================================\n",
    "\"noise\" in selected_exog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection with Sequential Feature Selection (SFS)\n",
    "\n",
    "Sequential Feature Selection is a robust method for selecting features, but it is computationally expensive. When the data set is very large, one way to reduce the computational cost is to use a single validation split to evaluate each candidate model instead of cross-validation (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (SequentialFeatureSelector)\n",
      "---------------------------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 1742\n",
      "Number of features available: 89\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=89)\n",
      "Number of features selected: 25\n",
      "    Autoreg (n=48) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "    Exog    (n=25) : ['week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunset_hour_cos', 'poly_month_sin__week_day_cos', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__sunrise_hour_cos', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_min_1_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (only exog) with scikit-learn SequentialFeatureSelector\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "selector = SequentialFeatureSelector(\n",
    "               estimator            = forecaster.regressor,\n",
    "               n_features_to_select = 25,\n",
    "               direction            = \"forward\",\n",
    "               cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n",
    "               scoring              = \"neg_mean_absolute_error\",\n",
    "               n_jobs               = -1,\n",
    "           )\n",
    "\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster   = forecaster,\n",
    "    selector     = selector,\n",
    "    y            = data[\"users\"],\n",
    "    exog         = data.drop(columns=\"users\"),\n",
    "    select_only  = 'exog',\n",
    "    subsample    = 0.2,\n",
    "    random_state = 123,\n",
    "    verbose      = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining feature selection methods can help speed up the process. An effective approach is to first use `SelectFromModel` to eliminate the less important features, and then use `SequentialFeatureSelector` to determine the best subset of features from this reduced list. This two-step method often improves efficiency by focusing on the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination (SelectFromModel)\n",
      "-----------------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 1742\n",
      "Number of features available: 137\n",
      "    Autoreg (n=48)\n",
      "    Exog    (n=89)\n",
      "Number of features selected: 62\n",
      "    Autoreg (n=44) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "    Exog    (n=18) : ['hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_day_sin', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunset_hour_sin', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp', 'noise']\n",
      "\n",
      "Recursive feature elimination (SequentialFeatureSelector)\n",
      "---------------------------------------------------------\n",
      "Total number of records available: 8712\n",
      "Total number of records used for feature selection: 1742\n",
      "Number of features available: 62\n",
      "    Autoreg (n=44)\n",
      "    Exog    (n=18)\n",
      "Number of features selected: 25\n",
      "    Autoreg (n=14) : [1, 2, 3, 10, 14, 24, 25, 26, 27, 33, 39, 42, 43, 48]\n",
      "    Exog    (n=11) : ['hour_day_sin', 'hour_day_cos', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'temp_roll_mean_7_day']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection (autoregressive and exog) with SelectFromModel + SequentialFeatureSelector\n",
    "# ==============================================================================\n",
    "regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n",
    "\n",
    "# Step 1: Select the 70% most important features with SelectFromModel\n",
    "selector_1 = SelectFromModel(\n",
    "                 estimator    = regressor,\n",
    "                 max_features = int(data.shape[1] * 0.7),\n",
    "                 threshold    = -np.inf\n",
    "             )\n",
    "selected_autoreg_1, selected_exog_1 = select_features(\n",
    "    forecaster  = forecaster,\n",
    "    selector    = selector_1,\n",
    "    y           = data[\"users\"],\n",
    "    exog        = data.drop(columns=\"users\"),\n",
    "    select_only = None,\n",
    "    subsample   = 0.2,\n",
    "    verbose     = True,\n",
    ")\n",
    "print(\"\")\n",
    "\n",
    "# Step 2: Select the 25 most important features with SequentialFeatureSelector\n",
    "forecaster.set_lags(lags=selected_autoreg_1)\n",
    "selector_2 = SequentialFeatureSelector(\n",
    "                 estimator            = regressor,\n",
    "                 n_features_to_select = 25,\n",
    "                 direction            = \"forward\",\n",
    "                 cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n",
    "                 scoring              = \"neg_mean_absolute_error\",\n",
    "                 n_jobs               = -1,\n",
    "             )\n",
    "selected_autoreg, selected_exog = select_features(\n",
    "    forecaster  = forecaster,\n",
    "    selector    = selector_2,\n",
    "    y           = data[\"users\"],\n",
    "    exog        = data[selected_exog_1],\n",
    "    select_only = None,\n",
    "    subsample   = 0.2,\n",
    "    verbose     = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Forecasting Models: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`select_features_multiseries` function from the `skforecast.model_selection_multiseries` module is used to select the best subset of features (autoregressive and exogenous variables). This function is compatible with the feature selection methods implemented in the scikit-learn library. The following parameters are available:\n",
    "\n",
    "- `forecaster`: Forecaster of type `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiSeriesCustom`.\n",
    "\n",
    "- `selector`: Feature selector from `sklearn.feature_selection`. For example, `RFE` or `RFECV`.\n",
    "\n",
    "- `series`: Target time series to which the feature selection will be applied.\n",
    "\n",
    "- `exog`: Exogenous variables.\n",
    "\n",
    "- `select_only`: Decide what type of features to include in the selection process. \n",
    "        \n",
    "    + If `'autoreg'`, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (`selected_exog`).\n",
    "\n",
    "    + If `'exog'`, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the output (`selected_autoreg`).\n",
    "\n",
    "    + If `None`, all features are evaluated by the selector.\n",
    "\n",
    "- `force_inclusion`: Features to force include in the final list of selected features.\n",
    "        \n",
    "    + If `list`, list of feature names to force include.\n",
    "    \n",
    "    + If `str`, regular expression to identify features to force include. For example, if `force_inclusion=\"^sun_\"`, all features that begin with \"sun_\" will be included in the final list of selected features.\n",
    "\n",
    "- `subsample`: Proportion of records to use for feature selection.\n",
    "\n",
    "- `random_state`: Sets a seed for the random subsample so that the subsampling process is always deterministic.\n",
    "\n",
    "- `verbose`: Print information about feature selection process.\n",
    "\n",
    "This function returns two `list`:\n",
    "\n",
    "- `selected_autoreg`: List of selected autoregressive features.\n",
    "\n",
    "- `selected_exog`: List of selected exogenous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = LGBMRegressor(\n",
    "                                 n_estimators = 900,\n",
    "                                 random_state = 15926,\n",
    "                                 max_depth    = 7,\n",
    "                                 verbose      = -1\n",
    "                             ),\n",
    "                 lags      = 48,\n",
    "             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de contenidos",
   "title_sidebar": "Tabla de contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.391px",
    "left": "1478px",
    "right": "20px",
    "top": "126px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f2f78aa07a2c9de4fc4e3d692008110cbee95f9e29d94fd62aa071e5108d1402"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
