{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to skforecast","text":""},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>Skforecast is a Python library for time series forecasting using machine learning models. It works with any regressor compatible with the scikit-learn API, including popular options like LightGBM, XGBoost, CatBoost, Keras, and many others.</p>"},{"location":"index.html#why-use-skforecast","title":"Why use skforecast?","text":"<p>Skforecast simplifies time series forecasting with machine learning by providing:</p> <ul> <li> Seamless integration with any scikit-learn compatible regressor (e.g., LightGBM, XGBoost, CatBoost, etc.).</li> <li> Flexible workflows that allow for both single and multi-series forecasting.</li> <li> Comprehensive tools for feature engineering, model selection, hyperparameter tuning, and more.</li> <li> Production-ready models with interpretability and validation methods for backtesting and realistic performance evaluation.</li> </ul> <p>Whether you're building quick prototypes or deploying models in production, skforecast ensures a fast, reliable, and scalable experience.</p>"},{"location":"index.html#get-involved","title":"Get Involved","text":"<p>We value your input! Here are a few ways you can participate:</p> <ul> <li>Report bugs and suggest new features on our GitHub Issues page.</li> <li>Contribute to the project by submitting code, adding new features, or improving the documentation.</li> <li>Share your feedback on LinkedIn to help spread the word about skforecast!</li> </ul> <p>Together, we can make time series forecasting accessible to everyone.</p>"},{"location":"index.html#installation-dependencies","title":"Installation &amp; Dependencies","text":"<p>To install the basic version of <code>skforecast</code> with core dependencies, run the following:</p> <pre><code>pip install skforecast\n</code></pre> <p>For more installation options, including dependencies and additional features, check out our Installation Guide.</p>"},{"location":"index.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRNN \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"index.html#features","title":"Features","text":"<p>Skforecast provides a set of key features designed to make time series forecasting with machine learning easy and efficient. For a detailed overview, see the User Guides.</p>"},{"location":"index.html#examples-and-tutorials","title":"Examples and tutorials","text":"<p>Explore our extensive list of examples and tutorials (English and Spanish) to get you started with skforecast. You can find them here.</p>"},{"location":"index.html#how-to-contribute","title":"How to contribute","text":"<p>Primarily, skforecast development consists of adding and creating new Forecasters, new validation strategies, or improving the performance of the current code. However, there are many other ways to contribute:</p> <ul> <li>Submit a bug report or feature request on GitHub Issues.</li> <li>Contribute a Jupyter notebook to our examples.</li> <li>Write unit or integration tests for our project.</li> <li>Answer questions on our issues, Stack Overflow, and elsewhere.</li> <li>Translate our documentation into another language.</li> <li>Write a blog post, tweet, or share our project with others.</li> </ul> <p>For more information on how to contribute to skforecast, see our Contribution Guide.</p> <p>Visit our authors section to meet all the contributors to skforecast.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use skforecast for a scientific publication, we would appreciate citations to the published software.</p> <p>Zenodo</p> <pre><code>Amat Rodrigo, Joaquin, &amp; Escobar Ortiz, Javier. (2024). skforecast (v0.13.0). Zenodo. https://doi.org/10.5281/zenodo.8382788\n</code></pre> <p>APA: <pre><code>Amat Rodrigo, J., &amp; Escobar Ortiz, J. (2024). skforecast (Version 0.13.0) [Computer software]. https://doi.org/10.5281/zenodo.8382788\n</code></pre></p> <p>BibTeX: <pre><code>@software{skforecast,\nauthor = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},\ntitle = {skforecast},\nversion = {0.13.0},\nmonth = {8},\nyear = {2024},\nlicense = {BSD-3-Clause},\nurl = {https://skforecast.org/},\ndoi = {10.5281/zenodo.8382788}\n}\n</code></pre></p>"},{"location":"index.html#publications-citing-skforecast","title":"Publications citing skforecast","text":"<ul> <li><p> Sanan, O., Sperling, J., Greene, D., &amp; Greer, R. (2024, April). Forecasting Weather and Energy Demand for Optimization of Renewable Energy and Energy Storage Systems for Water Desalination. In 2024 IEEE Conference on Technologies for Sustainability (SusTech) (pp. 175-182). IEEE. https://doi.org/10.1109/SusTech60925.2024.10553570 </p></li> <li><p> Bojer, A. K., Biru, B. H., Al-Quraishi, A. M. F., Debelee, T. G., Negera, W. G., Woldesillasie, F. F., &amp; Esubalew, S. Z. (2024). Machine learning and remote sensing based time series analysis for drought risk prediction in Borena Zone, Southwest Ethiopia. Journal of Arid Environments, 222, 105160. https://doi.org/10.1016/j.jaridenv.2024.105160 </p></li> <li><p> V. Negri, A. Mingotti, R. Tinarelli and L. Peretto, \"Comparison Between the Machine Learning and the Statistical Approach to the Forecasting of Voltage, Current, and Frequency,\" 2023 IEEE 13th International Workshop on Applied Measurements for Power Systems (AMPS), Bern, Switzerland, 2023, pp. 01-06, doi: 10.1109/AMPS59207.2023.10297192. https://doi.org/10.1109/AMPS59207.2023.10297192 </p></li> <li><p> Marcillo Vera, F., Rosado, R., Zambrano, P., Velastegui, J., Morales, G., Lagla, L., &amp; Herrera, A. (2024). Forecasting con Python, caso de estudio: visitas a las redes sociales en Ecuador con machine learning. CONECTIVIDAD, 5(2), 15-29. <li><p>OUKHOUYA, H., KADIRI, H., EL HIMDI, K., &amp; GUERBAZ, R. (2023). Forecasting International Stock Market Trends: XGBoost, LSTM, LSTM-XGBoost, and Backtesting XGBoost Models. Statistics, Optimization &amp; Information Computing, 12(1), 200-209. https://doi.org/10.19139/soic-2310-5070-1822</p> </li> <li><p>DUDZIK, S., &amp; Kowalczyk, B. (2023). Prognozowanie produkcji energii fotowoltaicznej z wykorzystaniem platformy NEXO i VRM Portal. Przeglad Elektrotechniczny, 2023(11). doi:10.15199/48.2023.11.41 </p> </li> <li><p>Polo J, Mart\u00edn-Chivelet N, Alonso-Abella M, Sanz-Saiz C, Cuenca J, de la Cruz M. Exploring the PV Power Forecasting at Building Fa\u00e7ades Using Gradient Boosting Methods. Energies. 2023; 16(3):1495. https://doi.org/10.3390/en16031495</p> </li> <li><p>Pop\u0142awski T, Dudzik S, Szel\u0105g P. Forecasting of Energy Balance in Prosumer Micro-Installations Using Machine Learning Models. Energies. 2023; 16(18):6726. https://doi.org/10.3390/en16186726</p> </li> <li><p>Harrou F, Sun Y, Taghezouit B, Dairi A. Artificial Intelligence Techniques for Solar Irradiance and PV Modeling and Forecasting. Energies. 2023; 16(18):6731. https://doi.org/10.3390/en16186731</p> </li> <li><p>Amara-Ouali, Y., Goude, Y., Doum\u00e8che, N., Veyret, P., Thomas, A., Hebenstreit, D., ... &amp; Phe-Neau, T. (2023). Forecasting Electric Vehicle Charging Station Occupancy: Smarter Mobility Data Challenge. arXiv preprint arXiv:2306.06142.</p> </li> <li><p>Emami, P., Sahu, A., &amp; Graf, P. (2023). BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. arXiv preprint arXiv:2307.00142.</p> </li> <li><p>Dang, HA., Dao, VD. (2023). Building Power Demand Forecasting Using Machine Learning: Application for an Office Building in Danang. In: Nguyen, D.C., Vu, N.P., Long, B.T., Puta, H., Sattler, KU. (eds) Advances in Engineering Research and Application. ICERA 2022. Lecture Notes in Networks and Systems, vol 602. Springer, Cham. https://doi.org/10.1007/978-3-031-22200-9_32</p> </li> <li><p>Morate del Moral, Iv\u00e1n (2023). Predici\u00f3n de llamadas realizadas a un Call Center. Proyecto Fin de Carrera / Trabajo Fin de Grado, E.T.S.I. de Sistemas Inform\u00e1ticos (UPM), Madrid.</p> </li> <li><p>Lopez Vega, A., &amp; Villanueva Vargas, R. A. (2022). Sistema para la automatizaci\u00f3n de procesos hospitalarios de control para pacientes para COVID-19 usando machine learning para el Centro de Salud San Fernando.</p> </li> <li><p>Garc\u00eda \u00c1lvarez, J. D. (2022). Modelo predictivo de rentabilidad de criptomonedas para un futuro cercano.</p> </li> <li><p>Chilet Vera, \u00c1. (2023). Elaboraci\u00f3n de un algoritmo predictivo para la reposici\u00f3n de hipoclorito en los dep\u00f3sitos mediante t\u00e9cnicas de Machine Learning (Doctoral dissertation, Universitat Polit\u00e8cnica de Val\u00e8ncia).</p> </li> <li><p>Bustinza Barrial, A. A., Bautista Abanto, A. M., Alva Alfaro, D. A., Villena Sotomayor, G. M., &amp; Trujillo Sabrera, J. M. (2022). Predicci\u00f3n de los valores de la demanda m\u00e1xima de energ\u00eda el\u00e9ctrica empleando t\u00e9cnicas de machine learning para la empresa Nexa Resources\u2013Cajamarquilla.</p> </li> <li><p>Morgado, K. Desarrollo de una t\u00e9cnica de gesti\u00f3n de activos para transformadores de distribuci\u00f3n basada en sistema de monitoreo (Doctoral dissertation, Universidad Nacional de Colombia).</p> </li> <li><p>Zafeiriou A., Chantzis G., Jonkaitis T., Fokaides P., Papadopoulos A., 2023, Smart Energy Strategy - A Comparative Study of Energy Consumption Forecasting Machine Learning Models, Chemical Engineering Transactions, 103, 691-696.</p> </li>"},{"location":"index.html#donating","title":"Donating","text":"<p>If you found skforecast useful, you can support us with a donation. Your contribution will help to continue developing and improving this project. Many thanks!  </p> <p> </p> <p></p>"},{"location":"index.html#license","title":"License","text":"<p>BSD-3-Clause License</p>"},{"location":"api/ForecasterAutoreg.html","title":"<code>ForecasterAutoreg</code>","text":""},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg","title":"<code>ForecasterAutoreg(regressor, lags=None, window_features=None, transformer_y=None, transformer_exog=None, weight_func=None, differentiation=None, fit_kwargs=None, binner_kwargs=None, forecaster_id=None)</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>`None`</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>`None`</code> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>KBinsDiscretizer</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. The <code>encode' argument is always set to 'ordinal'  and</code>dtype' to np.float64. New in version 0.12.0</p> <code>`None`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>differentiation</code> <code>int, default `None`</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.10.0</p> <code>binner</code> <code>KBinsDiscretizer</code> <p><code>KBinsDiscretizer</code> used to discretize residuals into k bins according  to the predicted values associated with each residual. New in version 0.12.0</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual. New in version 0.12.0</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>KBinsDiscretizer</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. The <code>encode' argument is always set to 'ordinal'  and</code>dtype' to np.float64. New in version 0.12.0</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window_</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. Only stored up to 200 values per bin. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale. New in version 0.12.0</p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. Only stored up to 200 values per bin. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. New in version 0.12.0</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Optional[Union[int, np.ndarray, list, range]] = None,\n    window_features: Optional[Union[object, list]] = None,\n    transformer_y: Optional[object] = None,\n    transformer_exog: Optional[object] = None,\n    weight_func: Optional[Callable] = None,\n    differentiation: Optional[int] = None,\n    fit_kwargs: Optional[dict] = None,\n    binner_kwargs: Optional[dict] = None,\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.transformer_y                      = transformer_y\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.source_code_weight_func            = None\n    self.differentiation                    = differentiation\n    self.differentiator                     = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.out_sample_residuals_              = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_by_bin_       = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_features, self.max_size_window_features, self.window_features_names = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and self.lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'encode': 'ordinal', 'strategy': 'quantile',\n            'subsample': 10000, 'random_state': 789654, 'dtype': np.float64\n        }\n    else:\n        self.binner_kwargs = binner_kwargs\n        self.binner_kwargs['encode'] = 'ordinal'\n        self.binner_kwargs['dtype'] = np.float64\n    self.binner = KBinsDiscretizer(**self.binner_kwargs).set_output(transform=\"default\")\n    self.binner_intervals_ = None\n\n    if self.differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                (f\"Argument `differentiation` must be an integer equal to or \"\n                 f\"greater than 1. Got {differentiation}.\")\n            )\n        self.window_size += self.differentiation\n        self.differentiator = TimeSeriesDifferentiator(order=self.differentiation)\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._repr_html_","title":"<code>_repr_html_()</code>","text":"<p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    if isinstance(self.regressor, Pipeline):\n        name_pipe_steps = tuple(name + \"__\" for name in self.regressor.named_steps.keys())\n        params = {key: value for key, value in self.regressor.get_params().items()\n                if key.startswith(name_pipe_steps)}\n    else:\n        params = self.regressor.get_params(deep=True)\n    params = str(params)\n\n    style = \"\"\"\n    &lt;style&gt;\n        .container {\n            font-family: 'Arial', sans-serif;\n            font-size: 0.9em;\n            color: #333;\n            border: 1px solid #ddd;\n            background-color: #fafafa;\n            padding: 5px 15px;\n            border-radius: 8px;\n            max-width: 600px;\n            #margin: auto;\n        }\n        .container h2 {\n            font-size: 1.2em;\n            color: #222;\n            border-bottom: 2px solid #ddd;\n            padding-bottom: 5px;\n            margin-bottom: 15px;\n        }\n        .container details {\n            margin: 10px 0;\n        }\n        .container summary {\n            font-weight: bold;\n            font-size: 1.1em;\n            cursor: pointer;\n            margin-bottom: 5px;\n            background-color: #f0f0f0;\n            padding: 5px;\n            border-radius: 5px;\n        }\n        .container summary:hover {\n        background-color: #e0e0e0;\n        }\n        .container ul {\n            font-family: 'Courier New', monospace;\n            list-style-type: none;\n            padding-left: 20px;\n            margin: 10px 0;\n        }\n        .container li {\n            margin: 5px 0;\n            font-family: 'Courier New', monospace;\n        }\n        .container li strong {\n            font-weight: bold;\n            color: #444;\n        }\n        .container li::before {\n            content: \"- \";\n            color: #666;\n        }\n    &lt;/style&gt;\n    \"\"\"\n\n    content = f\"\"\"\n    &lt;div class=\"container\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {self.regressor}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                 {self.exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterautoreg#forecasterautoreg.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    # Return the combined style and content\n    return style + content\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_lags","title":"<code>_create_lags(y, X_as_pandas=False, train_index=None)</code>","text":"<p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_data</code> is a pandas DataFrame.</p> <code>`False`</code> <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_data</code> when <code>X_as_pandas</code> is <code>True</code>.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, pandas DataFrame, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    X_as_pandas: bool = False,\n    train_index: Optional[pd.Index] = None\n) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    X_as_pandas : bool, default `False`\n        If `True`, the returned matrix `X_data` is a pandas DataFrame.\n    train_index : pandas Index, default `None`\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_data` when `X_as_pandas` is `True`.\n\n    Returns\n    -------\n    X_data : numpy ndarray, pandas DataFrame, None\n        Lagged values (predictors).\n    y_data : numpy ndarray\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    X_data = None\n    if self.lags is not None:\n        n_rows = len(y) - self.window_size\n        X_data = np.full(\n            shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n        )\n        for i, lag in enumerate(self.lags):\n            X_data[:, i] = y[self.window_size - lag: -lag]\n\n        if X_as_pandas:\n            X_data = pd.DataFrame(\n                         data    = X_data,\n                         columns = [f\"lag_{i}\" for i in self.lags],\n                         index   = train_index\n                     )\n\n    y_data = y[self.window_size:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_window_features","title":"<code>_create_window_features(y, train_index, X_as_pandas=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default `False`\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                (f\"The method `transform_batch` of {type(wf).__name__} \"\n                 f\"must return a pandas DataFrame.\")\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                (f\"The method `transform_batch` of {type(wf).__name__} \"\n                 f\"must return a DataFrame with the same number of rows as \"\n                 f\"the input time series - `window_size`: {len_train_index}.\")\n            )\n        if not (X_train_wf.index == train_index).all():\n            raise ValueError(\n                (f\"The method `transform_batch` of {type(wf).__name__} \"\n                 f\"must return a DataFrame with the same index as \"\n                 f\"the input time series - `window_size`.\")\n            )\n\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_train_X_y","title":"<code>_create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of the columns of the matrix created internally for training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated before the transformation.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values of the time series related to each row of `X_train`.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of the columns of the matrix created internally for training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n\n    \"\"\"\n\n    check_y(y=y)\n    y = input_to_frame(data=y, input_name='y')\n\n    if len(y) &lt;= self.window_size:\n        raise ValueError(\n            (f\"Length of `y` must be greater than the maximum window size \"\n             f\"needed by the forecaster.\\n\"\n             f\"    Length `y`: {len(y)}.\\n\"\n             f\"    Max window size: {self.window_size}.\\n\"\n             f\"    Lags window size: {self.max_lag}.\\n\"\n             f\"    Window features window size: {self.max_size_window_features}.\")\n        )\n\n    fit_transformer = False if self.is_fitted else True\n    y = transform_dataframe(\n            df                = y, \n            transformer       = self.transformer_y,\n            fit               = fit_transformer,\n            inverse_transform = False,\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if self.differentiation is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator.fit_transform(y_values)\n        else:\n            differentiator = clone(self.differentiator)\n            y_values = differentiator.fit_transform(y_values)\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    categorical_features = False\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        # TODO: Check if this check can be checked vs y_train. As happened\n        # in base on data (more data from y than exog, but can be aligned\n        # because of the window_size.\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n\n        exog_names_in_ = exog.columns.to_list()\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        categorical_features = (\n            exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n        )\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_train = []\n    X_train_features_names_out_ = []\n    train_index = y_index[self.window_size:]\n    X_as_pandas = True if categorical_features else False\n\n    X_train_lags, y_train = self._create_lags(\n        y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n    )\n    if X_train_lags is not None:\n        X_train.append(X_train_lags)\n        X_train_features_names_out_.extend([f\"lag_{i}\" for i in self.lags])\n\n    X_train_window_features_names_out_ = None\n    if self.window_features is not None:\n        n_diff = 0 if self.differentiation is None else self.differentiation\n        y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n        X_train_window_features, X_train_window_features_names_out_ = (\n            self._create_window_features(\n                y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n            )\n        )\n        X_train.extend(X_train_window_features)\n        X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train.\n        X_train_exog_names_out_ = exog.columns.to_list()\n        if X_as_pandas:\n            exog_to_train = exog.iloc[self.window_size:, ]\n            exog_to_train.index = train_index\n        else:\n            exog_to_train = exog.to_numpy()[self.window_size:, ]\n\n        X_train_features_names_out_.extend(X_train_exog_names_out_)\n        X_train.append(exog_to_train)\n\n    if X_as_pandas:\n        X_train = pd.concat(X_train, axis=1)\n        X_train.index = train_index\n    else:\n        X_train = pd.DataFrame(\n                      data    = np.concatenate(X_train, axis=1),\n                      index   = train_index,\n                      columns = X_train_features_names_out_\n                  )\n\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = train_index,\n                  name  = 'y'\n              )\n\n    return (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_\n    )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    output = self._create_train_X_y(y=y, exog=exog)\n\n    X_train = output[0]\n    y_train = output[1]\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._train_test_split_one_step_ahead","title":"<code>_train_test_split_one_step_ahead(y, initial_train_size, exog=None)</code>","text":"<p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_train</code>.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_test</code>.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    y: pd.Series,\n    initial_train_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same number\n        of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    \"\"\"\n\n    is_fitted = self.is_fitted\n    self.is_fitted = False\n    X_train, y_train, *_ = self._create_train_X_y(\n        y    = y.iloc[: initial_train_size],\n        exog = exog.iloc[: initial_train_size] if exog is not None else None\n    )\n\n    test_init = initial_train_size - self.window_size\n    self.is_fitted = True\n    X_test, y_test, *_ = self._create_train_X_y(\n        y    = y.iloc[test_init:],\n        exog = exog.iloc[test_init:] if exog is not None else None\n    )\n\n    self.is_fitted = is_fitted\n\n    return X_train, y_train, X_test, y_test\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n) -&gt; np.ndarray:\n    \"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.fit","title":"<code>fit(y, exog=None, store_last_window=True, store_in_sample_residuals=True, random_state=123)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>`True`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = True,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n    random_state : int, default `123`\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_\n    ) = self._create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n    self.X_train_features_names_out_ = X_train_features_names_out_\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type_ = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq_ = X_train.index.freqstr\n    else: \n        self.index_freq_ = X_train.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_names_in_ = exog_names_in_\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n        self._binning_in_sample_residuals(\n            y_true       = y_train.to_numpy(),\n            y_pred       = self.regressor.predict(X_train).ravel(),\n            random_state = random_state\n        )\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated. It\n    # also includes the values need to calculate the diferenctiation.\n    if store_last_window:\n        self.last_window_ = (\n            y.iloc[-self.window_size:]\n            .copy()\n            .to_frame(name=y.name if y.name is not None else 'y')\n        )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._binning_in_sample_residuals","title":"<code>_binning_in_sample_residuals(y_true, y_pred, random_state=123)</code>","text":"<p>Binning residuals according to the predicted value each residual is associated with. First a sklearn.preprocessing.KBinsDiscretizer is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>. Only up to 200 residuals are stored per bin.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Binning residuals according to the predicted value each residual is\n    associated with. First a sklearn.preprocessing.KBinsDiscretizer\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`. Only up to 200 residuals are stored per bin.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    random_state : int, default `123`\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    data = pd.DataFrame({'prediction': y_pred, 'residuals': (y_true - y_pred)})\n    data['bin'] = self.binner.fit_transform(y_pred.reshape(-1, 1)).astype(int)\n    self.in_sample_residuals_by_bin_ = (\n        data.groupby('bin')['residuals'].apply(np.array).to_dict()\n    )\n\n    # Only up to 200 residuals are stored per bin\n    for k, v in self.in_sample_residuals_by_bin_.items():\n        rng = np.random.default_rng(seed=random_state)\n        if len(v) &gt; 200:\n            sample = rng.choice(a=v, size=200, replace=False)\n            self.in_sample_residuals_by_bin_[k] = sample\n\n    self.in_sample_residuals_ = np.concatenate(list(\n        self.in_sample_residuals_by_bin_.values()\n    ))\n\n    self.binner_intervals_ = {\n        i: (\n            self.binner.bin_edges_[0][i],\n            (\n                self.binner.bin_edges_[0][i + 1]\n                if i + 1 &lt; len(self.binner.bin_edges_[0])\n                else None\n            ),\n        )\n        for i in range(len(self.binner.bin_edges_[0]) - 1)\n    }\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_predict_inputs","title":"<code>_create_predict_inputs(steps, last_window=None, exog=None, predict_boot=False, use_in_sample_residuals=True, use_binned_residuals=False, check_inputs=True)</code>","text":"<p>Create the inputs needed for the first iteration of the prediction  process. As this is a recursive process, the last window is updated at  each iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>predict_boot</code> <code>bool</code> <p>If <code>True</code>, residuals are returned to generate bootstrapping predictions.</p> <code>`False`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>last_window_values</code> <code>numpy ndarray</code> <p>Series predictors.</p> <code>exog_values</code> <code>numpy ndarray, default `None`</code> <p>Exogenous variable/s included as predictor/s.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    predict_boot: bool = False,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    check_inputs: bool = True\n) -&gt; Tuple[np.ndarray, Optional[np.ndarray], pd.Index]:\n    \"\"\"\n    Create the inputs needed for the first iteration of the prediction \n    process. As this is a recursive process, the last window is updated at \n    each iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    predict_boot : bool, default `False`\n        If `True`, residuals are returned to generate bootstrapping predictions.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Series predictors.\n    exog_values : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name  = type(self).__name__,\n            steps            = steps,\n            is_fitted        = self.is_fitted,\n            exog_in_         = self.exog_in_,\n            index_type_      = self.index_type_,\n            index_freq_      = self.index_freq_,\n            window_size      = self.window_size,\n            last_window      = last_window,\n            exog             = exog,\n            exog_type_in_    = self.exog_type_in_,\n            exog_names_in_   = self.exog_names_in_,\n            interval         = None,\n            max_steps        = None\n        )\n\n        if predict_boot and not use_in_sample_residuals:\n            if not use_binned_residuals and self.out_sample_residuals_ is None:\n                raise ValueError(\n                    (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                     \"`use_in_sample_residuals=True` or the \"\n                     \"`set_out_sample_residuals()` method before predicting.\")\n                )\n            if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                raise ValueError(\n                    (\"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\n                     \"`use_in_sample_residuals=True` or the \"\n                     \"`set_out_sample_residuals()` method before predicting.\")\n                )\n\n    last_window = last_window.iloc[-self.window_size:].copy()\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    last_window_values = transform_numpy(\n                             array             = last_window_values,\n                             transformer       = self.transformer_y,\n                             fit               = False,\n                             inverse_transform = False\n                         )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = exog.loc[:, self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    prediction_index = expand_index(\n                           index = last_window_index,\n                           steps = steps\n                       )\n\n    return last_window_values, exog_values, prediction_index\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._recursive_predict","title":"<code>_recursive_predict(steps, last_window_values, exog_values=None, residuals=None, use_binned_residuals=False, rng=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window_values</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog_values</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>residuals</code> <code>numpy ndarray</code> <p>Residuals used to generate bootstrapping predictions.</p> <code>`None`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <code>rng</code> <code>numpy Generator</code> <p>Random number generator used to select residuals in bootstrapping predictions when using binned residuals.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window_values: np.ndarray,\n    exog_values: Optional[np.ndarray] = None,\n    residuals: Optional[np.ndarray] = None,\n    use_binned_residuals: bool = False,\n    rng: Optional[np.random.Generator] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window_values : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog_values : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n    residuals : numpy ndarray, default `None`\n        Residuals used to generate bootstrapping predictions.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n    rng : numpy Generator, default `None`\n        Random number generator used to select residuals in bootstrapping\n        predictions when using binned residuals. \n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    n_lags = len(self.lags) if self.lags is not None else 0\n    n_window_features = (\n        len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n    )\n    n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n    X = np.full(\n        shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n    )\n    predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n    last_window = np.concatenate((last_window_values, predictions))\n\n    for i in range(steps):\n\n        if self.lags is not None:\n            X[:n_lags] = last_window[-self.lags - (steps - i)]\n        if self.window_features is not None:\n            X[n_lags:n_lags + n_window_features] = np.concatenate(\n                [wf.transform(last_window[i:-(steps - i)]) for wf in self.window_features]\n            )\n        if exog_values is not None:\n            X[n_lags + n_window_features:] = exog_values[i]\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n\n        if residuals is not None:\n            if use_binned_residuals:\n                predicted_bin = (\n                    int(self.binner.transform(pred.reshape(1, -1))[0, 0])\n                )\n                step_residual = rng.choice(a=residuals[predicted_bin], size=1)\n            else:\n                step_residual = residuals[i]\n\n            pred += step_residual\n\n        predictions[i] = pred[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window[-(steps - i)] = pred[0]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_predict_X","title":"<code>create_predict_X(steps, last_window=None, exog=None)</code>","text":"<p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive process, the predictors are created at each iteration of the prediction  process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead. As it is a recursive\n    process, the predictors are created at each iteration of the prediction \n    process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    last_window_values, exog_values, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog, check_inputs=False\n    )\n\n    X_predict = []\n    full_predictors = np.concatenate((last_window_values, predictions))\n\n    if self.lags is not None:\n        idx = np.arange(-steps, 0)[:, None] - self.lags\n        X_lags = full_predictors[idx + len(full_predictors)]\n        X_predict.append(X_lags)\n\n    if self.window_features is not None:\n        X_window_features = np.full(\n            shape      = (steps, len(self.X_train_window_features_names_out_)), \n            fill_value = np.nan, \n            order      = 'C',\n            dtype      = float\n        )\n        for i in range(steps):\n            X_window_features[i, :] = np.concatenate(\n                [wf.transform(full_predictors[i:-(steps - i)]) \n                 for wf in self.window_features]\n            )\n        X_predict.append(X_window_features)\n\n    if exog is not None:\n        X_predict.append(exog_values)\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(X_predict, axis=1),\n                    columns = self.X_train_features_names_out_,\n                    index   = prediction_index\n                )\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict","title":"<code>predict(steps, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    check_inputs: bool = True\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    last_window_values, exog_values, prediction_index  = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n    )\n\n    predictions = self._recursive_predict(\n                      steps              = steps,\n                      last_window_values = last_window_values,\n                      exog_values        = exog_values\n                  )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = prediction_index,\n                      name  = 'pred'\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=250, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index\n    ) = self._create_predict_inputs(\n        steps                   = steps, \n        last_window             = last_window, \n        exog                    = exog,\n        predict_boot            = True, \n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals\n    )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    rng = np.random.default_rng(seed=random_state)\n    if not use_binned_residuals:\n        sampled_residuals = rng.choice(\n                                a       = residuals,\n                                size    = (steps, n_boot),\n                                replace = True\n                            )\n\n    boot_columns = []\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           order      = 'F',\n                           dtype      = float\n                       )\n    for i in range(n_boot):\n\n        boot_columns.append(f\"pred_boot_{i}\")\n        boot_predictions[:, i] = self._recursive_predict(\n            steps                = steps,\n            last_window_values   = last_window_values,\n            exog_values          = exog_values,\n            residuals            = residuals_by_bin if use_binned_residuals else sampled_residuals[:, i],\n            use_binned_residuals = use_binned_residuals,\n            rng                  = rng\n        )\n\n    if self.differentiation is not None:\n        boot_predictions = (\n            self.differentiator.inverse_transform_next_window(boot_predictions)\n        )\n\n    if self.transformer_y:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_y,\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = prediction_index,\n                           columns = boot_columns\n                       )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, interval=[5, 95], n_boot=250, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False)</code>","text":"<p>Iterative process in which each prediction is used as a predictor for the next step, and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: list = [5, 95],\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Iterative process in which each prediction is used as a predictor\n    for the next step, and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )\n\n    predictions = self.predict(\n                      steps        = steps,\n                      last_window  = last_window,\n                      exog         = exog,\n                      check_inputs = False\n                  )\n\n    interval = np.array(interval) / 100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_quantiles","title":"<code>predict_quantiles(steps, last_window=None, exog=None, quantiles=[0.05, 0.5, 0.95], n_boot=250, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False)</code>","text":"<p>Calculate the specified quantiles for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  quantile is calculated for each step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>quantiles</code> <code>list</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>`[0.05, 0.5, 0.95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate quantiles.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot quantiles are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction quantiles. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    quantiles: list = [0.05, 0.5, 0.95],\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the specified quantiles for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    quantile is calculated for each step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, default `[0.05, 0.5, 0.95]`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate quantiles.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot quantiles are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction quantiles. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(quantiles=quantiles)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )\n\n    predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n    predictions.columns = [f'q_{q}' for q in quantiles]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_dist","title":"<code>predict_dist(steps, distribution, last_window=None, exog=None, n_boot=250, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. **New in version 0.12.0</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).  \n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n        **WARNING: This argument is newly introduced and requires special attention.\n        It is still experimental and may undergo changes.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps                   = steps,\n                       last_window             = last_window,\n                       exog                    = exog,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals,\n                       use_binned_residuals    = use_binned_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p == 'x'] + [\"loc\", \"scale\"]\n    param_values = np.apply_along_axis(\n                       lambda x: distribution.fit(x),\n                       axis = 1,\n                       arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and  <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_lags(\n    self, \n    lags: Optional[Union[int, np.ndarray, list, range]] = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `max_lag` and \n    `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: Optional[Union[object, list]] = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.window_features, self.max_size_window_features, self.window_features_names = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation   \n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, y_pred=None, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. If <code>y_pred</code> is provided, residuals are binned according to the predicted value they are associated with. If <code>y_pred</code> is <code>None</code>, residuals are stored without binning. Only up to 200 residuals are stored per bin.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>pandas Series, numpy ndarray</code> <p>Values of residuals. If <code>y_pred</code> is <code>None</code>, at most 1000 values are stored. If <code>y_pred</code> is not <code>None</code>, at most 200 * n_bins values are stored, where <code>n_bins</code> is the number of bins used in <code>self.binner</code>.</p> required <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Predicted values of the time series from which the residuals have been calculated. This argument is used to bin residuals according to the predicted values. <code>y_pred</code> and <code>residuals</code> must be of the same class (both pandas Series or both numpy ndarray) must have the same length and, if they are pandas Series, the same index. </p> <ul> <li>If <code>y_pred</code> is <code>None</code>, residuals are not binned.</li> <li>If affter binning, a bin has more than 200 residuals, only a random     sample of 200 residuals is stored.</li> <li>If affter binning, a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process. New in version 0.12.0</li> </ul> <code>`None`</code> <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the forecaster. Once the limit of 200 values per bin is reached, no more values are appended. If False, stored residuals are overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: Union[pd.Series, np.ndarray],\n    y_pred: Optional[Union[pd.Series, np.ndarray]] = None,\n    append: bool = True,\n    transform: bool = True,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. If `y_pred` is provided, residuals\n    are binned according to the predicted value they are associated with. If\n    `y_pred` is `None`, residuals are stored without binning. Only up to 200\n    residuals are stored per bin.\n\n    Parameters\n    ----------\n    residuals : pandas Series, numpy ndarray\n        Values of residuals. If `y_pred` is `None`, at most 1000 values are\n        stored. If `y_pred` is not `None`, at most 200 * n_bins values are\n        stored, where `n_bins` is the number of bins used in `self.binner`.\n    y_pred : pandas Series, numpy ndarray, default `None`\n        Predicted values of the time series from which the residuals have been\n        calculated. This argument is used to bin residuals according to the\n        predicted values. `y_pred` and `residuals` must be of the same class\n        (both pandas Series or both numpy ndarray) must have the same length\n        and, if they are pandas Series, the same index. \n\n        - If `y_pred` is `None`, residuals are not binned.\n        - If affter binning, a bin has more than 200 residuals, only a random\n            sample of 200 residuals is stored.\n        - If affter binning, a bin binning is empty, it is filled with a\n        random sample of residuals from other bins. This is done to ensure\n        that all bins have at least one residual and can be used in the\n        prediction process.\n        **New in version 0.12.0**\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. Once the limit of 200 values per bin is reached, no more values\n        are appended. If False, stored residuals are overwritten with the new\n        residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, (np.ndarray, pd.Series)):\n        raise TypeError(\n            (f\"`residuals` argument must be `numpy ndarray` or `pandas Series`, \"\n             f\"but found {type(residuals)}.\")\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series, type(None))):\n        raise TypeError(\n            (f\"`y_pred` argument must be `numpy ndarray`, `pandas Series` or `None`, \"\n             f\"but found {type(y_pred)}.\")\n        )\n\n    if y_pred is not None and len(residuals) != len(y_pred):\n        raise ValueError(\n            (f\"`residuals` and `y_pred` must have the same length, but found \"\n             f\"{len(residuals)} and {len(y_pred)}.\")\n        )\n\n    if isinstance(residuals, pd.Series) and isinstance(y_pred, pd.Series):\n        if not residuals.index.equals(y_pred.index):\n            raise ValueError(\n                (f\"`residuals` and `y_pred` must have the same index, but found \"\n                 f\"{residuals.index} and {y_pred.index}.\")\n            )\n\n    if y_pred is not None and not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if not isinstance(residuals, np.ndarray):\n        residuals = residuals.to_numpy()\n\n    if y_pred is not None and not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new residuals \"\n             f\"are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure that the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n        residuals = transform_numpy(\n                        array             = residuals,\n                        transformer       = self.transformer_y,\n                        fit               = False,\n                        inverse_transform = False\n                    )\n\n    if y_pred is None:\n        # Residuals are not binned.\n        if len(residuals) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals = rng.choice(a=residuals, size=1000, replace=False)\n        if append and self.out_sample_residuals_ is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals_))\n            if len(residuals) &lt; free_space:\n                residuals = np.concatenate((\n                                self.out_sample_residuals_,\n                                residuals\n                            ))\n            else:\n                residuals = np.concatenate((\n                                self.out_sample_residuals_,\n                                residuals[:free_space]\n                            ))\n        self.out_sample_residuals_ = residuals\n    else:\n        # Residuals are binned according to the predicted values.\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred.reshape(-1, 1)).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    free_space = max(0, 200 - len(self.out_sample_residuals_by_bin_[k]))\n                    if len(v) &lt; free_space:\n                        self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                            self.out_sample_residuals_by_bin_[k],\n                            v\n                        ))\n                    else:\n                        self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                            self.out_sample_residuals_by_bin_[k],\n                            v[:free_space]\n                        ))\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            rng = np.random.default_rng(seed=123)\n            if len(v) &gt; 200:\n                # Only up to 200 residuals are stored per bin\n                sample = rng.choice(a=v, size=200, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n\n        self.out_sample_residuals_ = np.concatenate(list(\n                                         self.out_sample_residuals_by_bin_.values()\n                                     ))\n\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n\n        empty_bins = [k for k, v in self.out_sample_residuals_by_bin_.items() \n                      if len(v) == 0]\n        if empty_bins:\n            warnings.warn(\n                (f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                 f\"No predicted values fall in the interval \"\n                 f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                 f\"Empty bins will be filled with a random sample of residuals from \"\n                 f\"the other bins.\")\n            )\n            for k in empty_bins:\n                rng = np.random.default_rng(seed=123)\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\n                                                           a       = self.out_sample_residuals_,\n                                                           size    = 200,\n                                                           replace = True\n                                                       )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances","title":"<code>get_feature_importances(sort_importance=True)</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    sort_importance: bool, default `True`\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_features_names_out_,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html","title":"<code>ForecasterAutoregDirect</code>","text":""},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect","title":"<code>ForecasterAutoregDirect(regressor, steps, lags=None, window_features=None, transformer_y=None, transformer_exog=None, weight_func=None, differentiation=None, fit_kwargs=None, n_jobs='auto', forecaster_id=None)</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive direct multi-step forecaster. A separate model is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>`None`</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>`None`</code> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>differentiation</code> <code>int, default `None`</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window_</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_direct_exog_names_out_</code> <code>list</code> <p>Same as <code>X_train_exog_names_out_</code> but using the direct format. The same  exogenous variable is repeated for each step.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>int, 'auto', default `'auto'`</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>binner</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>binner_intervals_</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>binner_kwargs</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>in_sample_residuals_by_bin_</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>out_sample_residuals_by_bin_</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> Notes <p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    steps: int,\n    lags: Optional[Union[int, np.ndarray, list, range]] = None,\n    window_features: Optional[Union[object, list]] = None,\n    transformer_y: Optional[object] = None,\n    transformer_exog: Optional[object] = None,\n    weight_func: Optional[Callable] = None,\n    differentiation: Optional[int] = None,\n    fit_kwargs: Optional[dict] = None,\n    n_jobs: Union[int, str] = 'auto',\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.steps                              = steps\n    self.transformer_y                      = transformer_y\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.source_code_weight_func            = None\n    self.differentiation                    = differentiation\n    self.differentiator                     = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n    self.binner                             = None\n    self.binner_intervals_                  = None\n    self.binner_kwargs                      = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_by_bin_       = None\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_features, self.max_size_window_features, self.window_features_names = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and self.lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n\n    if self.differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                (f\"Argument `differentiation` must be an integer equal to or \"\n                 f\"greater than 1. Got {differentiation}.\")\n            )\n        self.window_size += self.differentiation\n        self.differentiator = TimeSeriesDifferentiator(order=self.differentiation)\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals_ = None\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor_name  = type(self.regressor).__name__,\n                      )\n    else:\n        if not isinstance(n_jobs, int):\n            raise TypeError(\n                f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n            )\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._repr_html_","title":"<code>_repr_html_()</code>","text":"<p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    if isinstance(self.regressor, Pipeline):\n        name_pipe_steps = tuple(name + \"__\" for name in self.regressor.named_steps.keys())\n        params = {key: value for key, value in self.regressor.get_params().items()\n                if key.startswith(name_pipe_steps)}\n    else:\n        params = self.regressor.get_params(deep=True)\n    params = str(params)\n\n    style = \"\"\"\n    &lt;style&gt;\n        .container {\n            font-family: 'Arial', sans-serif;\n            font-size: 0.9em;\n            color: #333;\n            border: 1px solid #ddd;\n            background-color: #fafafa;\n            padding: 5px 15px;\n            border-radius: 8px;\n            max-width: 600px;\n            #margin: auto;\n        }\n        .container h2 {\n            font-size: 1.2em;\n            color: #222;\n            border-bottom: 2px solid #ddd;\n            padding-bottom: 5px;\n            margin-bottom: 15px;\n        }\n        .container details {\n            margin: 10px 0;\n        }\n        .container summary {\n            font-weight: bold;\n            font-size: 1.1em;\n            cursor: pointer;\n            margin-bottom: 5px;\n            background-color: #f0f0f0;\n            padding: 5px;\n            border-radius: 5px;\n        }\n        .container summary:hover {\n        background-color: #e0e0e0;\n        }\n        .container ul {\n            font-family: 'Courier New', monospace;\n            list-style-type: none;\n            padding-left: 20px;\n            margin: 10px 0;\n        }\n        .container li {\n            margin: 5px 0;\n            font-family: 'Courier New', monospace;\n        }\n        .container li strong {\n            font-weight: bold;\n            color: #444;\n        }\n        .container li::before {\n            content: \"- \";\n            color: #666;\n        }\n    &lt;/style&gt;\n    \"\"\"\n\n    content = f\"\"\"\n    &lt;div class=\"container\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {self.regressor}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Maximum steps to predict:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                 {self.exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterautoregdirect#forecasterautoregdirect.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/direct-multi-step-forecasting.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    # Return the combined style and content\n    return style + content\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_lags","title":"<code>_create_lags(y, X_as_pandas=False, train_index=None)</code>","text":"<p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_data</code> is a pandas DataFrame.</p> <code>`False`</code> <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_data</code> when <code>X_as_pandas</code> is <code>True</code>.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, pandas DataFrame, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    X_as_pandas: bool = False,\n    train_index: Optional[pd.Index] = None\n) -&gt; Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    X_as_pandas : bool, default `False`\n        If `True`, the returned matrix `X_data` is a pandas DataFrame.\n    train_index : pandas Index, default `None`\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_data` when `X_as_pandas` is `True`.\n\n    Returns\n    -------\n    X_data : numpy ndarray, pandas DataFrame, None\n        Lagged values (predictors).\n    y_data : numpy ndarray\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    X_data = None\n    n_rows = len(y) - self.window_size - (self.steps - 1)\n    if self.lags is not None:         \n        X_data = np.full(\n            shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n        )\n        for i, lag in enumerate(self.lags):\n            X_data[:, i] = y[self.window_size - lag : -(lag + self.steps - 1)] \n\n        if X_as_pandas:\n            X_data = pd.DataFrame(\n                         data    = X_data,\n                         columns = [f\"lag_{i}\" for i in self.lags],\n                         index   = train_index\n                     )\n\n    y_data = np.full(\n        shape=(self.steps, n_rows), fill_value=np.nan, order='C', dtype=float\n    )\n    for step in range(self.steps):\n        y_data[step, :] = y[self.window_size + step : self.window_size + step + n_rows]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_window_features","title":"<code>_create_window_features(y, train_index, X_as_pandas=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; Tuple[list, list]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default `False`\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                (f\"The method `transform_batch` of {type(wf).__name__} \"\n                 f\"must return a pandas DataFrame.\")\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                (f\"The method `transform_batch` of {type(wf).__name__} \"\n                 f\"must return a DataFrame with the same number of rows as \"\n                 f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\")\n            )\n        X_train_wf.index = train_index\n\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_train_X_y","title":"<code>_create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and  predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the <code>filter_train_X_y_for_step</code> method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of the columns of the matrix created internally for training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated before the transformation.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, dict, list, list, list, list, dict]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and \n    predictors needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the `filter_train_X_y_for_step` method.\n    y_train : dict\n        Values of the time series related to each row of `X_train`.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of the columns of the matrix created internally for training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n\n    \"\"\"\n\n    check_y(y=y)\n    y = input_to_frame(data=y, input_name='y')\n\n    if len(y) &lt; self.window_size + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `y` for training this forecaster is \"\n             f\"{self.window_size + self.steps}. Reduce the number of \"\n             f\"predicted steps, {self.steps}, or the maximum \"\n             f\"window_size, {self.window_size}, if no more data is available.\\n\"\n             f\"    Length `y`: {len(y)}.\\n\"\n             f\"    Max step : {self.steps}.\\n\"\n             f\"    Max window size: {self.window_size}.\\n\"\n             f\"    Lags window size: {self.max_lag}.\\n\"\n             f\"    Window features window size: {self.max_size_window_features}.\")\n        )\n\n    fit_transformer = False if self.is_fitted else True\n    y = transform_dataframe(\n            df                = y, \n            transformer       = self.transformer_y,\n            fit               = fit_transformer,\n            inverse_transform = False,\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if self.differentiation is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator.fit_transform(y_values)\n        else:\n            differentiator = clone(self.differentiator)\n            y_values = differentiator.fit_transform(y_values)\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    categorical_features = False\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        # TODO: Check if this check can be checked vs y_train. As happened\n        # in base on data (more data from y than exog, but can be aligned\n        # because of the window_size.\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.exog_in_ = True\n        exog_names_in_ = exog.columns.to_list()\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        categorical_features = (\n            exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n        )\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_train = []\n    X_train_features_names_out_ = []\n    train_index = y_index[self.window_size + (self.steps - 1):]\n    len_train_index = len(train_index)\n    X_as_pandas = True if categorical_features else False\n\n    X_train_lags, y_train = self._create_lags(\n        y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n    )\n    if X_train_lags is not None:\n        X_train.append(X_train_lags)\n        X_train_features_names_out_.extend([f\"lag_{i}\" for i in self.lags])\n\n    X_train_window_features_names_out_ = None\n    if self.window_features is not None:\n        n_diff = 0 if self.differentiation is None else self.differentiation\n        end_wf = None if self.steps == 1 else -(self.steps - 1)\n        y_window_features = pd.Series(\n            y_values[n_diff:end_wf], index=y_index[n_diff:end_wf]\n        )\n        X_train_window_features, X_train_window_features_names_out_ = (\n            self._create_window_features(\n                y           = y_window_features, \n                X_as_pandas = X_as_pandas, \n                train_index = train_index\n            )\n        )\n        X_train.extend(X_train_window_features)\n        X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train.\n        X_train_exog_names_out_ = exog.columns.to_list()\n        # TODO: See if can return direct cols names with exog_to_direct_numpy\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        )\n        exog_to_train = exog_to_train.iloc[-len_train_index:, :]\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_direct_exog_names_out_ = exog_to_train.columns.to_list()\n        if X_as_pandas:\n            exog_to_train.index = train_index\n        else:\n            exog_to_train = exog_to_train.to_numpy()\n\n        X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n        X_train.append(exog_to_train)\n\n    if X_as_pandas:\n        X_train = pd.concat(X_train, axis=1)\n        X_train.index = train_index\n    else:\n        X_train = pd.DataFrame(\n                      data    = np.concatenate(X_train, axis=1),\n                      index   = train_index,\n                      columns = X_train_features_names_out_\n                  )\n\n    y_train = {step: pd.Series(\n                         data  = y_train[step - 1, :], \n                         index = y_index[self.window_size + step - 1:][:len_train_index],\n                         name  = f\"y_step_{step}\"\n                     )\n               for step in range(1, self.steps + 1)}\n\n    return (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_\n    )\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and  predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the <code>filter_train_X_y_for_step</code> method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and \n    predictors needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the `filter_train_X_y_for_step` method.\n    y_train : dict\n        Values of the time series related to each row of `X_train`.\n\n    \"\"\"\n\n    output = self._create_train_X_y(y=y, exog=exog)\n\n    X_train = output[0]\n    y_train = output[1]\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict,\n    remove_suffix: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        Step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `create_train_X_y` method, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.exog_in_:\n        X_train_step = X_train\n    else:\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.window_features_names) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n        idx_columns_exog = (\n            np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1\n        )\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n                                for col_name in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._train_test_split_one_step_ahead","title":"<code>_train_test_split_one_step_ahead(y, initial_train_size, exog=None)</code>","text":"<p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_train</code>.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_test</code>.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    y: pd.Series,\n    initial_train_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same number\n        of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    \"\"\"\n\n    is_fitted = self.is_fitted\n    self.is_fitted = False\n    X_train, y_train, *_ = self._create_train_X_y(\n        y    = y.iloc[: initial_train_size],\n        exog = exog.iloc[: initial_train_size] if exog is not None else None\n    )\n\n    test_init = initial_train_size - self.window_size\n    self.is_fitted = True\n    X_test, y_test, *_ = self._create_train_X_y(\n        y    = y.iloc[test_init:],\n        exog = exog.iloc[test_init:] if exog is not None else None\n    )\n\n    self.is_fitted = is_fitted\n\n    return X_train, y_train, X_test, y_test\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>create_train_X_y</code> and <code>filter_train_X_y_for_step</code> methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n) -&gt; np.ndarray:\n    \"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `create_train_X_y` and `filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.fit","title":"<code>fit(y, exog=None, store_last_window=True, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>`True`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = True\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = {step: None for step in range(1, self.steps + 1)}\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n        \"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n        store_in_sample_residuals : bool\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor and in-sample residuals.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            residuals = (\n                (y_train_step - regressor.predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                residuals = rng.choice(\n                                a       = residuals, \n                                size    = 1000, \n                                replace = False\n                            )\n        else:\n            residuals = None\n\n        return step, regressor, residuals\n\n    # Here to `filter_train_X_y_for_step` to work\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor                 = copy(self.regressor),\n            X_train                   = X_train,\n            y_train                   = y_train,\n            step                      = step,\n            store_in_sample_residuals = store_in_sample_residuals\n        )\n        for step in range(1, self.steps + 1))\n    )\n\n    self.regressors_ = {step: regressor \n                        for step, regressor, _ in results_fit}\n\n    if store_in_sample_residuals:\n        self.in_sample_residuals_ = {step: residuals \n                                     for step, _, residuals in results_fit}\n\n    self.X_train_features_names_out_ = X_train_features_names_out_\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type_ = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq_ = X_train.index.freqstr\n    else: \n        self.index_freq_ = X_train.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_names_in_ = exog_names_in_\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    if store_last_window:\n        self.last_window_ = (\n            y.iloc[-self.window_size:]\n            .copy()\n            .to_frame(name=y.name if y.name is not None else 'y')\n        )\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_predict_inputs","title":"<code>_create_predict_inputs(steps=None, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Create the inputs needed for the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>Xs</code> <code>list</code> <p>List of numpy arrays with the predictors for each step.</p> <code>Xs_col_names</code> <code>list</code> <p>Names of the columns of the matrix created internally for prediction.</p> <code>steps</code> <code>list</code> <p>Steps to predict.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    check_inputs: bool = True\n) -&gt; Tuple[list, list, list, pd.Index]:\n    \"\"\"\n    Create the inputs needed for the prediction process.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    Xs : list\n        List of numpy arrays with the predictors for each step.\n    Xs_col_names : list\n        Names of the columns of the matrix created internally for prediction.\n    steps : list\n        Steps to predict.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    steps = prepare_steps_direct(\n                max_step = self.steps,\n                steps    = steps\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name = type(self).__name__,\n            steps           = steps,\n            is_fitted       = self.is_fitted,\n            exog_in_        = self.exog_in_,\n            index_type_     = self.index_type_,\n            index_freq_     = self.index_freq_,\n            window_size     = self.window_size,\n            last_window     = last_window,\n            exog            = exog,\n            exog_type_in_   = self.exog_type_in_,\n            exog_names_in_  = self.exog_names_in_,\n            interval        = None,\n            max_steps       = self.steps\n        )\n\n    last_window = last_window.iloc[-self.window_size:].copy()\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    last_window_values = transform_numpy(\n                             array             = last_window_values,\n                             transformer       = self.transformer_y,\n                             fit               = False,\n                             inverse_transform = False\n                         )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    X_autoreg = []\n    Xs_col_names = []\n    if self.lags is not None:\n        X_lags = last_window_values[-self.lags]\n        X_autoreg.append(X_lags)\n        Xs_col_names.extend([f\"lag_{i}\" for i in self.lags])\n\n    if self.window_features is not None:\n        X_window_features = np.concatenate(\n            [wf.transform(last_window_values) for wf in self.window_features]\n        )\n        X_autoreg.append(X_window_features)\n        Xs_col_names.extend(self.X_train_window_features_names_out_)\n\n    X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = exog.loc[:, self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n\n        n_exog = exog.shape[1]\n        Xs = [\n            np.concatenate(\n                [X_autoreg, \n                 exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1)],\n                axis=1\n            )\n            for step in steps\n        ]\n        Xs_col_names = Xs_col_names + self.X_train_exog_names_out_\n    else:\n        Xs = [X_autoreg] * len(steps)\n\n    prediction_index = expand_index(\n                           index = last_window_index,\n                           steps = max(steps)\n                       )[np.array(steps) - 1]\n    if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(\n        steps, np.arange(min(steps), max(steps) + 1)\n    ):\n        prediction_index.freq = last_window_index.freq\n\n    return Xs, Xs_col_names, steps, prediction_index\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_predict_X","title":"<code>create_predict_X(steps=None, last_window=None, exog=None)</code>","text":"<p>Create the predictors needed to predict <code>steps</code> ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog\n    )\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(Xs, axis=0), \n                    columns = Xs_col_names, \n                    index   = prediction_index\n                )\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict","title":"<code>predict(steps=None, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    check_inputs: bool = True\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    Xs, _, steps, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n    )\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = prediction_index,\n                      name  = 'pred'\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, use_binned_residuals=None)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if self.is_fitted:\n\n        steps = prepare_steps_direct(\n                    steps    = steps,\n                    max_step = self.steps\n                )\n\n        if use_in_sample_residuals:\n            if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.in_sample_residuals_` for steps: \"\n                     f\"{set(steps) - set(self.in_sample_residuals_.keys())}.\")\n                )\n            residuals = self.in_sample_residuals_\n        else:\n            if self.out_sample_residuals_ is None:\n                raise ValueError(\n                    (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                     \"`use_in_sample_residuals=True` or the \"\n                     \"`set_out_sample_residuals()` method before predicting.\")\n                )\n            else:\n                if not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(\n                        (f\"Not `forecaster.out_sample_residuals_` for steps: \"\n                         f\"{set(steps) - set(self.out_sample_residuals_.keys())}. \"\n                         f\"Use method `set_out_sample_residuals()`.\")\n                    )\n            residuals = self.out_sample_residuals_\n\n        check_residuals = (\n            \"forecaster.in_sample_residuals_\" if use_in_sample_residuals\n            else \"forecaster.out_sample_residuals_\"\n        )\n        for step in steps:\n            if residuals[step] is None:\n                raise ValueError(\n                    (f\"forecaster residuals for step {step} are `None`. \"\n                     f\"Check {check_residuals}.\")\n                )\n            elif (residuals[step] == None).any():\n                raise ValueError(\n                    (f\"forecaster residuals for step {step} contains `None` values. \"\n                     f\"Check {check_residuals}.\")\n                )\n\n    Xs, _, steps, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog\n    )\n\n    # Predictions must be transformed and differenced before adding residuals\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    boot_predictions = np.tile(predictions, (n_boot, 1)).T\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    rng = np.random.default_rng(seed=random_state)\n    for i, step in enumerate(steps):\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions[i, :] = boot_predictions[i, :] + sample_residuals\n\n    if self.differentiation is not None:\n        boot_predictions = (\n            self.differentiator.inverse_transform_next_window(boot_predictions)\n        )\n\n    if self.transformer_y:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_y,\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = prediction_index,\n                           columns = boot_columns\n                       )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, use_in_sample_residuals=True, use_binned_residuals=None)</code>","text":"<p>Bootstrapping based predicted intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: list = [5, 95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )\n\n    predictions = self.predict(\n                      steps        = steps,\n                      last_window  = last_window,\n                      exog         = exog,\n                      check_inputs = False\n                  )\n\n    interval = np.array(interval) / 100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_quantiles","title":"<code>predict_quantiles(steps=None, last_window=None, exog=None, quantiles=[0.05, 0.5, 0.95], n_boot=500, random_state=123, use_in_sample_residuals=True, use_binned_residuals=None)</code>","text":"<p>Bootstrapping based predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>quantiles</code> <code>list</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>`[0.05, 0.5, 0.95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate quantiles.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot quantiles are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction quantiles. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    quantiles: list = [0.05, 0.5, 0.95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted quantiles.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, default `[0.05, 0.5, 0.95]`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate quantiles.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot quantiles are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction quantiles. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(quantiles=quantiles)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )\n\n    predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n    predictions.columns = [f'q_{q}' for q in quantiles]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, use_binned_residuals=None)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>use_binned_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.Series] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    use_binned_residuals : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps                   = steps,\n                       last_window             = last_window,\n                       exog                    = exog,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p == 'x'] + [\"loc\", \"scale\"]\n    param_values = np.apply_along_axis(\n                       lambda x: distribution.fit(x),\n                       axis = 1,\n                       arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_lags","title":"<code>set_lags(lags=None)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and  <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_lags(\n    self, \n    lags: Optional[Union[int, np.ndarray, list, range]] = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `max_lag` and \n    `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_window_features","title":"<code>set_window_features(window_features=None)</code>","text":"<p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: Optional[Union[object, list]] = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.lags is None:\n        raise ValueError(\n            (\"At least one of the arguments `lags` or `window_features` \"\n             \"must be different from None. This is required to create the \"\n             \"predictors used in training the forecaster.\")\n        )\n\n    self.window_features, self.max_size_window_features, self.window_features_names = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation   \n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals_</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool = True,\n    transform: bool = True,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals_`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals_ is None:\n        self.out_sample_residuals_ = {step: None \n                                      for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals_.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"Only residuals of models (steps) \"\n             f\"{set(self.out_sample_residuals_.keys()).intersection(set(residuals.keys()))} \"\n             f\"are updated.\")\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals_.keys()}\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new \"\n             f\"residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_numpy(\n                                 array             = value,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals_[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals_[key]))\n            if len(value) &lt; free_space:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value\n                        ))\n            else:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals_[key] = value\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.get_feature_importances","title":"<code>get_feature_importances(step, sort_importance=True)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def get_feature_importances(\n    self, \n    step: int,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n    sort_importance: bool, default `True`\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f\"`step` must be an integer. Got {type(step)}.\"\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    idx_columns_lags = np.arange(len(self.lags))\n    if self.exog_in_:\n        idx_columns_exog = np.flatnonzero(\n                               [name.endswith(f\"step_{step}\") \n                                for name in self.X_train_features_names_out_]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterBaseline.html","title":"<code>ForecasterBaseline</code>","text":""},{"location":"api/ForecasterBaseline.html#skforecast.ForecasterBaseline.ForecasterEquivalentDate","title":"<code>ForecasterEquivalentDate</code>","text":""},{"location":"api/ForecasterBaseline.html#skforecast.ForecasterBaseline.ForecasterEquivalentDate.ForecasterEquivalentDate","title":"<code>ForecasterEquivalentDate(offset, n_offsets=1, agg_func=np.mean, forecaster_id=None)</code>","text":"<p>This forecaster predicts future values based on the most recent equivalent date. It also allows to aggregate multiple past values of the equivalent date using a function (e.g. mean, median, max, min, etc.). The equivalent date is calculated by moving back in time a specified number of steps (offset). The offset can be defined as an integer or as a pandas DateOffset. This approach is useful as a baseline, but it is a simplistic method and may not capture complex underlying patterns.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily,  <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of  valid dates. For example, Bday(2) can be used to move back two business  days. If the date does not start on a valid date, it is first moved to a  valid date. For example, if the date is a Saturday, it is moved to the  previous Friday. Then, the offset is applied. If the result is a non-valid  date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday.  For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> required <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>`1`</code> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>`np.mean`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily,  <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of  valid dates. For example, Bday(2) can be used to move back two business  days. If the date does not start on a valid date, it is first moved to a  valid date. For example, if the date is a Saturday, it is moved to the  previous Friday. Then, the offset is applied. If the result is a non-valid  date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday.  For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. If <code>offset</code> is greater than 1, the value at the equivalent dates is aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>window_size</code> <code>int</code> <p>Number of past values needed to include the last equivalent dates according to the <code>offset</code> and <code>n_offsets</code>.</p> <code>last_window_</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the past values needed to include the last equivalent date according the <code>offset</code> and <code>n_offsets</code>.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>regressor</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> Source code in <code>skforecast/ForecasterBaseline/ForecasterEquivalentDate.py</code> <pre><code>def __init__(\n    self,\n    offset: Union[int, pd.tseries.offsets.DateOffset],\n    n_offsets: int = 1,\n    agg_func: Callable = np.mean,\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.offset             = offset\n    self.n_offsets          = n_offsets\n    self.agg_func           = agg_func\n    self.last_window_       = None\n    self.index_type_        = None\n    self.index_freq_        = None\n    self.training_range_    = None\n    self.creation_date      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted          = False\n    self.fit_date           = None\n    self.skforecast_version = skforecast.__version__\n    self.python_version     = sys.version.split(\" \")[0]\n    self.forecaster_id      = forecaster_id\n    self.regressor          = None\n    self.differentiation    = None\n\n    if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n        raise TypeError(\n            (\"`offset` must be an integer greater than 0 or a \"\n             \"pandas.tseries.offsets. Find more information about offsets in \"\n             \"https://pandas.pydata.org/docs/reference/offset_frequency.html\")\n        )\n\n    self.window_size = self.offset * self.n_offsets\n</code></pre>"},{"location":"api/ForecasterBaseline.html#skforecast.ForecasterBaseline.ForecasterEquivalentDate.ForecasterEquivalentDate.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=None)</code>","text":"<p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>store_in_sample_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterBaseline/ForecasterEquivalentDate.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Any = None,\n    store_in_sample_residuals: Any = None\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n    store_in_sample_residuals : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        if not isinstance(y.index, pd.DatetimeIndex):\n            raise TypeError(\n                (\"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                 \"pandas DatetimeIndex with frequency.\")\n            )\n        elif y.index.freq is None:\n            raise TypeError(\n                (\"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                 \"pandas DatetimeIndex with frequency.\")\n            )\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_    = None\n    self.index_type_     = None\n    self.index_freq_     = None\n    self.training_range_ = None\n    self.is_fitted       = False\n\n    _, y_index = preprocess_y(y=y, return_values=False)\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        # Calculate the window_size in steps for compatibility with the\n        # check_predict_input function. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance\n        # between two dates may not be constant.\n        first_valid_index = (y_index[-1] - self.offset * self.n_offsets)\n\n        try:\n            window_size_idx_start = y_index.get_loc(first_valid_index)\n            window_size_idx_end = y_index.get_loc(y_index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                (f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                 f\"to the window size ({self.window_size}). This is because  \"\n                 f\"the offset ({self.offset}) is larger than the available \"\n                 f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                 f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                 f\"size of `y`.\")\n            )\n    else:\n        if len(y) &lt; self.window_size:\n            raise ValueError(\n                (f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                 f\"to the window size ({self.window_size}). This is because  \"\n                 f\"the offset ({self.offset}) is larger than the available \"\n                 f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                 f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                 f\"size of `y`.\")\n            )\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    self.index_freq_ = (\n        y_index.freqstr if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n    )\n\n    # The last time window of training data is stored so that equivalent\n    # dates are available when calling the `predict` method.\n    # Store the whole series to avoid errors when the offset is larger \n    # than the data available.\n    self.last_window_ = y.copy()\n</code></pre>"},{"location":"api/ForecasterBaseline.html#skforecast.ForecasterBaseline.ForecasterEquivalentDate.ForecasterEquivalentDate.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to  the offset. If <code>last_window = None</code>, the values stored in  <code>self.last_window_</code> are used and the predictions start immediately  after the training data.</p> <code>`None`</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterBaseline/ForecasterEquivalentDate.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    exog: Any = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Past values needed to select the last equivalent dates according to \n        the offset. If `last_window = None`, the values stored in \n        `self.last_window_` are used and the predictions start immediately \n        after the training data.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name = type(self).__name__,\n        steps           = steps,\n        is_fitted       = self.is_fitted,\n        exog_in_        = False,\n        index_type_     = self.index_type_,\n        index_freq_     = self.index_freq_,\n        window_size     = self.window_size,\n        last_window     = last_window\n    )\n\n    last_window = last_window.copy()\n\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    if isinstance(self.offset, int):\n\n        equivalent_indexes = np.tile(\n                                 np.arange(-self.offset, 0),\n                                 int(np.ceil(steps / self.offset))\n                             )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [equivalent_indexes - n * self.offset \n                                  for n in np.arange(self.n_offsets)]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                              self.agg_func,\n                              axis = 0,\n                              arr  = equivalent_values\n                          )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = expand_index(index=last_window_index, steps=steps),\n                          name  = 'pred'\n                      )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        predictions_index = expand_index(index=last_window_index, steps=steps)\n        max_allowed_date = last_window_index[-1]\n\n        # For every date in predictions_index, calculate the n offsets\n        offset_dates = []\n        for date in predictions_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.\n            reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n                                data    = equivalent_values,\n                                index   = predictions_index,\n                                columns = [f'offset_{i}' \n                                           for i in range(self.n_offsets)]\n                            )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                (f\"All equivalent values are missing. This is caused by using \"\n                 f\"an offset ({self.offset}) larger than the available data. \"\n                 f\"Try to decrease the size of the offset ({self.offset}), \"\n                 f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                 f\"size of `last_window`. In backtesting, this error may be \"\n                 f\"caused by using an `initial_train_size` too small.\")\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                (f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                 f\"are calculated with less than {self.n_offsets} n_offsets. \"\n                 f\"To avoid this, increase the `last_window` size or decrease \"\n                 f\"the number of n_offsets. The current configuration requires \" \n                 f\"a total offset of {self.offset * self.n_offsets}.\")\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.rename('pred')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html","title":"<code>ForecasterAutoregMultiSeries</code>","text":""},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries","title":"<code>ForecasterAutoregMultiSeries(regressor, lags, encoding='ordinal', transformer_series=None, transformer_exog=None, weight_func=None, series_weights=None, differentiation=None, dropna_from_series=False, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>encoding</code> <code>(str, None)</code> <p>Encoding used to identify the different series. </p> <ul> <li>If <code>'ordinal'</code>, a single column is created with integer values from 0  to n_series - 1. </li> <li>If <code>'ordinal_category'</code>, a single column is created with integer  values from 0 to n_series - 1 and the column is transformed into  pandas.category dtype so that it can be used as a categorical variable. </li> <li>If <code>'onehot'</code>, a binary column is created for each series.</li> <li>If None, no column is created to identify the series. Internally, the series are identified as an integer from 0 to n_series - 1, but no column is created in the training matrices. Changed to 'ordinal' in version 0.14.0</li> </ul> <code>`'ordinal'`</code> <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>`None`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>(Callable, dict)</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present in  <code>weight_func</code>.</li> </ul> <code>`None`</code> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.12.0</p> <code>`None`</code> <code>dropna_from_series</code> <code>bool</code> <p>Determine whether NaN detected in the training matrices will be dropped.</p> <ul> <li>If <code>True</code>, drop NaNs in X_train and same rows in y_train.</li> <li>If <code>False</code>, leave NaNs in X_train and warn the user. New in version 0.12.0</li> </ul> <code>`False`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series. </p> <ul> <li>If <code>'ordinal'</code>, a single column is created with integer values from 0  to n_series - 1. </li> <li>If <code>'ordinal_category'</code>, a single column is created with integer  values from 0 to n_series - 1 and the column is transformed into  pandas.category dtype so that it can be used as a categorical variable. </li> <li>If <code>'onehot'</code>, a binary column is created for each series.</li> <li>If None, no column is created to identify the series. Internally, the series are identified as an integer from 0 to n_series - 1, but no column is created in the training matrices. Changed to 'ordinal' in version 0.14.0</li> </ul> <code>encoder</code> <code>preprocessing</code> <p>Scikit-learn preprocessing encoder used to encode the series. New in version 0.12.0</p> <code>encoding_mapping_</code> <code>dict</code> <p>Mapping of the encoding used to identify the different series.</p> <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>(Callable, dict)</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>(str, dict)</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series.It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>differentiator_</code> <code>dict</code> <p>Dictionary with the <code>differentiator</code> for each series. It is created cloning the objects in <code>differentiator</code> and is used internally to avoid overwriting.</p> <code>dropna_from_series</code> <code>bool</code> <p>Determine whether NaN detected in the training matrices will be dropped.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. When using differentiation, the <code>window_size</code> is increased by the order of  differentiation so that the predictors can be created correctly.</p> <code>last_window_</code> <code>dict</code> <p>Last window of training data for each series. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>dict</code> <p>First and last values of index of the data used during training for each  series.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) provided by the user during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated before the transformation.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code> created internally for training. It can be different from <code>series_names_in_</code> if some series are dropped during the training process because of NaNs or  because they are not present in the training period.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> Notes <p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <ul> <li><code>series_weights</code> : controls the relative importance of each series. If a  series has twice as much weight as the others, the observations of that series  influence the training twice as much. The higher the weight of a series  relative to the others, the more the model will focus on trying to learn  that series.</li> <li><code>weight_func</code> : controls the relative importance of each observation  according to its index value. For example, a function that assigns a lower  weight to certain dates.</li> </ul> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Union[int, np.ndarray, list],\n    encoding: Optional[str] = 'ordinal',\n    transformer_series: Optional[Union[object, dict]] = None,\n    transformer_exog: Optional[object] = None,\n    weight_func: Optional[Union[Callable, dict]] = None,\n    series_weights: Optional[dict] = None,\n    differentiation: Optional[int] = None,\n    dropna_from_series: bool = False,\n    fit_kwargs: Optional[dict] = None,\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.regressor                   = copy(regressor)\n    self.encoding                    = encoding\n    self.encoder                     = None\n    self.encoding_mapping_           = {}\n    self.transformer_series          = transformer_series\n    self.transformer_series_         = None\n    self.transformer_exog            = transformer_exog\n    self.weight_func                 = weight_func\n    self.weight_func_                = None\n    self.source_code_weight_func     = None\n    self.series_weights              = series_weights\n    self.series_weights_             = None\n    self.differentiation             = differentiation\n    self.differentiator              = None\n    self.differentiator_             = None\n    self.dropna_from_series          = dropna_from_series\n    self.last_window_                = None\n    self.index_type_                 = None\n    self.index_freq_                 = None\n    self.training_range_             = None\n    self.series_names_in_            = None\n    self.exog_in_                    = False\n    self.exog_names_in_              = None\n    self.exog_type_in_               = None\n    self.exog_dtypes_in_             = None \n    self.X_train_series_names_in_    = None\n    self.X_train_exog_names_out_     = None\n    self.X_train_features_names_out_ = None\n    self.in_sample_residuals_        = None\n    self.out_sample_residuals_       = None\n    self.creation_date               = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                   = False\n    self.fit_date                    = None\n    self.skforecast_version          = skforecast.__version__\n    self.python_version              = sys.version.split(\" \")[0]\n    self.forecaster_id               = forecaster_id\n\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n        forecaster_name = type(self).__name__,\n        regressor       = regressor,\n        weight_func     = weight_func,\n        series_weights  = series_weights\n    )\n\n    if self.differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                (f\"Argument `differentiation` must be an integer equal to or \"\n                 f\"greater than 1. Got {differentiation}.\")\n            )\n        self.window_size += self.differentiation\n        self.differentiator = TimeSeriesDifferentiator(order=self.differentiation)\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    if self.encoding not in ['ordinal', 'ordinal_category', 'onehot', None]:\n        raise ValueError(\n            (f\"Argument `encoding` must be one of the following values: 'ordinal', \"\n             f\"'ordinal_category', 'onehot' or None. Got '{self.encoding}'.\")\n        )\n\n    if self.encoding == 'onehot':\n        self.encoder = OneHotEncoder(\n                           categories    = 'auto',\n                           sparse_output = False,\n                           drop          = None,\n                           dtype         = int\n                       ).set_output(transform='pandas')\n    else:\n        self.encoder = OrdinalEncoder(\n                           categories = 'auto',\n                           dtype      = int\n                       ).set_output(transform='pandas')\n\n    scaling_regressors = tuple(\n        member[1]\n        for member in inspect.getmembers(sklearn.linear_model, inspect.isclass)\n        + inspect.getmembers(sklearn.svm, inspect.isclass)\n    )\n\n    if self.transformer_series is None and isinstance(regressor, scaling_regressors):\n        warnings.warn(\n            (\"When using a linear model, it is recommended to use a transformer_series \"\n             \"to ensure all series are in the same scale. You can use, for example, a \"\n             \"`StandardScaler` from sklearn.preprocessing.\")\n        )\n\n    if isinstance(self.transformer_series, dict):\n        if self.encoding is None:\n            raise TypeError(\n                (\"When `encoding` is None, `transformer_series` must be a single \"\n                 \"transformer (not `dict`) as it is applied to all series.\")\n            )\n        if '_unknown_level' not in self.transformer_series.keys():\n            raise ValueError(\n                (\"If `transformer_series` is a `dict`, a transformer must be \"\n                 \"provided to transform series that do not exist during training. \"\n                 \"Add the key '_unknown_level' to `transformer_series`. \"\n                 \"For example: {'_unknown_level': your_transformer}.\")\n            )\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_lags","title":"<code>_create_lags(y, series_name)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <code>series_name</code> <code>str</code> <p>Name of the series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>. Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_lags(\n    self,\n    y: np.ndarray,\n    series_name: str\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n    series_name : str\n        Name of the series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`.\n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series '{series_name}', ({len(y)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag: -lag]\n\n    y_data = y[self.max_lag:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_train_X_y_single_series","title":"<code>_create_train_X_y_single_series(y, ignore_exog, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables. This method does not transform the exog variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>ignore_exog</code> <code>bool</code> <p>If <code>True</code>, <code>exog</code> is ignored.</p> required <code>exog</code> <code>pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train_lags</code> <code>pandas DataFrame</code> <p>Training values of lags. Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>X_train_exog</code> <code>pandas DataFrame</code> <p>Training values of exogenous variables. Shape: (len(y) - self.max_lag, len(exog.columns))</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag, )</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_train_X_y_single_series(\n    self,\n    y: pd.Series,\n    ignore_exog: bool,\n    exog: Optional[pd.DataFrame] = None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. This method does not transform the exog variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    ignore_exog : bool\n        If `True`, `exog` is ignored.\n    exog : pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    X_train_lags : pandas DataFrame\n        Training values of lags.\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    X_train_exog : pandas DataFrame\n        Training values of exogenous variables.\n        Shape: (len(y) - self.max_lag, len(exog.columns))\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    series_name = y.name\n    if self.encoding is None:\n        fit_transformer = False\n        transformer_series = self.transformer_series_['_unknown_level']\n    else:\n        fit_transformer = False if self.is_fitted else True\n        transformer_series = self.transformer_series_[series_name]\n\n    y = transform_series(\n            series            = y,\n            transformer       = transformer_series,\n            fit               = fit_transformer,\n            inverse_transform = False\n        )\n\n    y_values = y.to_numpy()\n    y_index = y.index\n\n    if self.differentiation is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator_[series_name].fit_transform(y_values)\n        else:\n            differentiator = clone(self.differentiator_[series_name])\n            y_values = differentiator.fit_transform(y_values)\n\n    X_train, y_train = self._create_lags(y=y_values, series_name=series_name)\n\n    X_train_lags = pd.DataFrame(\n                       data    = X_train,\n                       columns = [f\"lag_{i}\" for i in self.lags],\n                       index   = y_index[self.max_lag:]\n                   )\n    X_train_lags['_level_skforecast'] = series_name\n\n    if ignore_exog:\n        X_train_exog = None\n    else:\n        if exog is not None:\n            # The first `self.max_lag` positions have to be removed from exog\n            # since they are not in X_train.\n            X_train_exog = exog.iloc[self.max_lag:, ]\n        else:\n            X_train_exog = pd.DataFrame(\n                               data    = np.nan,\n                               columns = ['_dummy_exog_col_to_keep_shape'],\n                               index   = y_index[self.max_lag:]\n                           )\n\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = y_index[self.max_lag:],\n                  name  = 'y'\n              )\n\n    if self.differentiation is not None:\n        X_train_lags = X_train_lags.iloc[self.differentiation:]\n        y_train = y_train.iloc[self.differentiation:]\n        if X_train_exog is not None:\n            X_train_exog = X_train_exog.iloc[self.differentiation:]\n\n    return X_train_lags, X_train_exog, y_train\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_train_X_y","title":"<code>_create_train_X_y(series, exog=None, store_last_window=True)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. See Notes section for more details depending on the type of <code>series</code> and <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>store_last_window</code> <code>(bool, list)</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <ul> <li>If <code>True</code>, last window is stored for all series. </li> <li>If <code>list</code>, last window is stored for the series present in the list.</li> <li>If <code>False</code>, last window is not stored.</li> </ul> <code>`True`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> <code>series_indexes</code> <code>dict</code> <p>Dictionary with the index of each series.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) provided by the user during training.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code> created internally for training. It can be different from <code>series_names_in_</code> if some series are dropped during the training process because of NaNs or because they are not present in the training period.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated before the transformation.</p> <code>last_window_</code> <code>dict</code> <p>Last window of training data for each series. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> Notes <ul> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a pandas Series or  DataFrame, each exog is duplicated for each series. Exog must have the same index as <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a dict of pandas Series  or DataFrames. Each key in <code>exog</code> must be a column in <code>series</code> and the  values are the exog for each series. Exog must have the same index as  <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a dict of pandas Series, <code>exog</code> must be a dict of pandas Series or DataFrames. The keys in <code>series</code> and <code>exog</code> must be the same. All series and exog must have a pandas DatetimeIndex with the same  frequency.</li> </ul> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_train_X_y(\n    self,\n    series: Union[pd.DataFrame, dict],\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    store_last_window: Union[bool, list] = True,\n) -&gt; Tuple[pd.DataFrame, pd.Series, dict, list, list, list, list, dict, dict]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. See Notes section for more details depending on the type of\n    `series` and `exog`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    store_last_window : bool, list, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n\n        - If `True`, last window is stored for all series. \n        - If `list`, last window is stored for the series present in the list.\n        - If `False`, last window is not stored.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) provided by the user during training.\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train` created\n        internally for training. It can be different from `series_names_in_` if\n        some series are dropped during the training process because of NaNs or\n        because they are not present in the training period.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    last_window_ : dict\n        Last window of training data for each series. It stores the values \n        needed to predict the next `step` immediately after the training data.\n\n    Notes\n    -----\n    - If `series` is a pandas DataFrame and `exog` is a pandas Series or \n    DataFrame, each exog is duplicated for each series. Exog must have the\n    same index as `series` (type, length and frequency).\n    - If `series` is a pandas DataFrame and `exog` is a dict of pandas Series \n    or DataFrames. Each key in `exog` must be a column in `series` and the \n    values are the exog for each series. Exog must have the same index as \n    `series` (type, length and frequency).\n    - If `series` is a dict of pandas Series, `exog` must be a dict of pandas\n    Series or DataFrames. The keys in `series` and `exog` must be the same.\n    All series and exog must have a pandas DatetimeIndex with the same \n    frequency.\n\n    \"\"\"\n\n    series_dict, series_indexes = check_preprocess_series(series=series)\n    input_series_is_dict = isinstance(series, dict)\n    series_names_in_ = list(series_dict.keys())\n\n    if self.is_fitted and not set(series_names_in_).issubset(set(self.series_names_in_)):\n        raise ValueError(\n            (f\"Once the Forecaster has been trained, `series` must contain \"\n             f\"the same series names as those used during training:\\n\"\n             f\" Got      : {series_names_in_}\\n\"\n             f\" Expected : {self.series_names_in_}\")\n        )\n\n    exog_dict = {serie: None for serie in series_names_in_}\n    exog_names_in_ = None\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        exog_dict, exog_names_in_ = check_preprocess_exog_multiseries(\n                                        input_series_is_dict = input_series_is_dict,\n                                        series_indexes       = series_indexes,\n                                        series_names_in_     = series_names_in_,\n                                        exog                 = exog,\n                                        exog_dict            = exog_dict\n                                    )\n\n        if self.is_fitted:\n            if self.exog_names_in_ is None:\n                raise ValueError(\n                    (\"Once the Forecaster has been trained, `exog` must be `None` \"\n                     \"because no exogenous variables were added during training.\")\n                )\n            else:\n                if not set(exog_names_in_) == set(self.exog_names_in_):\n                    raise ValueError(\n                        (f\"Once the Forecaster has been trained, `exog` must contain \"\n                         f\"the same exogenous variables as those used during training:\\n\"\n                         f\" Got      : {exog_names_in_}\\n\"\n                         f\" Expected : {self.exog_names_in_}\")\n                    )\n\n    if not self.is_fitted:\n        self.transformer_series_ = initialize_transformer_series(\n                                       forecaster_name    = type(self).__name__,\n                                       series_names_in_   = series_names_in_,\n                                       encoding           = self.encoding,\n                                       transformer_series = self.transformer_series\n                                   )\n\n    if self.differentiation is None:\n        self.differentiator_ = {serie: None for serie in series_names_in_}\n    else:\n        if not self.is_fitted:\n            self.differentiator_ = {serie: clone(self.differentiator)\n                                    for serie in series_names_in_}\n\n    series_dict, exog_dict = align_series_and_exog_multiseries(\n                                 series_dict          = series_dict,\n                                 input_series_is_dict = input_series_is_dict,\n                                 exog_dict            = exog_dict\n                             )\n\n    if not self.is_fitted and self.transformer_series_['_unknown_level'] is not None:\n        self.transformer_series_['_unknown_level'].fit(\n            np.concatenate(list(series_dict.values())).reshape(-1, 1)\n        )\n\n    ignore_exog = True if exog is None else False\n    input_matrices = [\n        [series_dict[k], exog_dict[k], ignore_exog]\n         for k in series_dict.keys()\n    ]\n\n    X_train_lags_buffer = []\n    X_train_exog_buffer = []\n    y_train_buffer = []\n    for matrices in input_matrices:\n\n        X_train_lags, X_train_exog, y_train = (\n            self._create_train_X_y_single_series(\n                y           = matrices[0],\n                exog        = matrices[1],\n                ignore_exog = matrices[2],\n            )\n        )\n\n        X_train_lags_buffer.append(X_train_lags)\n        X_train_exog_buffer.append(X_train_exog)\n        y_train_buffer.append(y_train)\n\n    X_train = pd.concat(X_train_lags_buffer, axis=0)\n    y_train = pd.concat(y_train_buffer, axis=0)\n\n    if self.is_fitted:\n        encoded_values = self.encoder.transform(X_train[['_level_skforecast']])\n    else:\n        encoded_values = self.encoder.fit_transform(X_train[['_level_skforecast']])\n        for i, code in enumerate(self.encoder.categories_[0]):\n            self.encoding_mapping_[code] = i\n\n    X_train = pd.concat([\n                  X_train.drop(columns='_level_skforecast'),\n                  encoded_values\n              ], axis=1)\n\n    if self.encoding == 'onehot':\n        X_train.columns = X_train.columns.str.replace('_level_skforecast_', '')\n    elif self.encoding == 'ordinal_category':\n        X_train['_level_skforecast'] = (\n            X_train['_level_skforecast'].astype('category')\n        )\n\n    del encoded_values\n\n    exog_dtypes_in_ = None\n    if exog is not None:\n\n        X_train_exog = pd.concat(X_train_exog_buffer, axis=0)\n        if '_dummy_exog_col_to_keep_shape' in X_train_exog.columns:\n            X_train_exog = (\n                X_train_exog.drop(columns=['_dummy_exog_col_to_keep_shape'])\n            )\n\n        exog_names_in_ = X_train_exog.columns.to_list()\n        exog_dtypes_in_ = get_exog_dtypes(exog=X_train_exog)\n\n        fit_transformer = False if self.is_fitted else True\n        X_train_exog = transform_dataframe(\n                           df                = X_train_exog,\n                           transformer       = self.transformer_exog,\n                           fit               = fit_transformer,\n                           inverse_transform = False\n                       )\n\n        check_exog_dtypes(X_train_exog, call_check_exog=False)\n        if not (X_train_exog.index == X_train.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog` after transformation. \"\n                 \"They must be equal to ensure the correct alignment of values.\")\n            )\n\n        X_train_exog_names_out_ = X_train_exog.columns.to_list()\n        X_train = pd.concat([X_train, X_train_exog], axis=1)\n\n    if y_train.isnull().any():\n        mask = y_train.notna().to_numpy()\n        y_train = y_train.iloc[mask]\n        X_train = X_train.iloc[mask,]\n        warnings.warn(\n            (\"NaNs detected in `y_train`. They have been dropped because the \"\n             \"target variable cannot have NaN values. Same rows have been \"\n             \"dropped from `X_train` to maintain alignment. This is caused by \"\n             \"series with interspersed NaNs.\"),\n             MissingValuesWarning\n        )\n\n    if self.dropna_from_series:\n        if np.any(X_train.isnull().to_numpy()):\n            mask = X_train.notna().all(axis=1).to_numpy()\n            X_train = X_train.iloc[mask, ]\n            y_train = y_train.iloc[mask]\n            warnings.warn(\n                (\"NaNs detected in `X_train`. They have been dropped. If \"\n                 \"you want to keep them, set `forecaster.dropna_from_series = False`. \"\n                 \"Same rows have been removed from `y_train` to maintain alignment. \"\n                 \"This caused by series with interspersed NaNs.\"),\n                 MissingValuesWarning\n            )\n    else:\n        if np.any(X_train.isnull().to_numpy()):\n            warnings.warn(\n                (\"NaNs detected in `X_train`. Some regressors do not allow \"\n                 \"NaN values during training. If you want to drop them, \"\n                 \"set `forecaster.dropna_from_series = True`.\"),\n                 MissingValuesWarning\n            )\n\n    if X_train.empty:\n        raise ValueError(\n            (\"All samples have been removed due to NaNs. Set \"\n             \"`forecaster.dropna_from_series = False` or review `exog` values.\")\n        )\n\n    if self.encoding == 'onehot':\n        X_train_series_names_in_ = [\n            col for col in series_names_in_ if X_train[col].sum() &gt; 0\n        ]\n    else:\n        unique_levels = X_train['_level_skforecast'].unique()\n        X_train_series_names_in_ = [\n            k for k, v in self.encoding_mapping_.items()\n            if v in unique_levels\n        ]\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated.\n    last_window_ = None\n    if store_last_window:\n\n        series_to_store = (\n            X_train_series_names_in_ if store_last_window is True else store_last_window\n        )\n\n        series_not_in_series_dict = set(series_to_store) - set(X_train_series_names_in_)\n        if series_not_in_series_dict:\n            warnings.warn(\n                (f\"Series {series_not_in_series_dict} are not present in \"\n                 f\"`series`. No last window is stored for them.\"),\n                IgnoredArgumentWarning\n            )\n            series_to_store = [s for s in series_to_store \n                               if s not in series_not_in_series_dict]\n\n        if series_to_store:\n            last_window_ = {\n                k: v.iloc[-self.window_size:].copy()\n                for k, v in series_dict.items()\n                if k in series_to_store\n            }\n\n    return (\n        X_train,\n        y_train,\n        series_indexes,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_,\n        last_window_,\n    )\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_train_X_y","title":"<code>create_train_X_y(series, exog=None, suppress_warnings=False)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. See Notes section for more details depending on the type of <code>series</code> and <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the creation of the training matrices. See skforecast.exceptions.warn_skforecast_categories  for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> Notes <ul> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a pandas Series or  DataFrame, each exog is duplicated for each series. Exog must have the same index as <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a dict of pandas Series  or DataFrames. Each key in <code>exog</code> must be a column in <code>series</code> and the  values are the exog for each series. Exog must have the same index as  <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a dict of pandas Series, <code>exog</code>must be a dict of pandas Series or DataFrames. The keys in <code>series</code> and <code>exog</code> must be the same. All series and exog must have a pandas DatetimeIndex with the same  frequency.</li> </ul> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: Union[pd.DataFrame, dict],\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    suppress_warnings: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. See Notes section for more details depending on the type of\n    `series` and `exog`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the creation\n        of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n        for more information.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n\n    Notes\n    -----\n    - If `series` is a pandas DataFrame and `exog` is a pandas Series or \n    DataFrame, each exog is duplicated for each series. Exog must have the\n    same index as `series` (type, length and frequency).\n    - If `series` is a pandas DataFrame and `exog` is a dict of pandas Series \n    or DataFrames. Each key in `exog` must be a column in `series` and the \n    values are the exog for each series. Exog must have the same index as \n    `series` (type, length and frequency).\n    - If `series` is a dict of pandas Series, `exog`must be a dict of pandas\n    Series or DataFrames. The keys in `series` and `exog` must be the same.\n    All series and exog must have a pandas DatetimeIndex with the same \n    frequency.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    output = self._create_train_X_y(\n                 series            = series, \n                 exog              = exog, \n                 store_last_window = False\n             )\n\n    X_train = output[0]\n    y_train = output[1]\n\n    if self.encoding is None:\n        X_train = X_train.drop(columns='_level_skforecast')\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._train_test_split_one_step_ahead","title":"<code>_train_test_split_one_step_ahead(series, initial_train_size, exog=None)</code>","text":"<p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Test values (predictors).</p> <code>y_test</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_test</code>.</p> <code>X_train_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_train</code>.</p> <code>X_test_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_test</code>.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    series: Union[pd.DataFrame, dict],\n    initial_train_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Test values (predictors).\n    y_test : pandas Series\n        Values (target) of the time series related to each row of `X_test`.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n\n    \"\"\"\n\n    if isinstance(series, dict):\n        freqs = [s.index.freq for s in series.values() if s.index.freq is not None]\n        if not freqs:\n            raise ValueError(\"At least one series must have a frequency.\")\n        if not all(f == freqs[0] for f in freqs):\n            raise ValueError(\n                \"All series with frequency must have the same frequency.\"\n            )\n        min_index = min([v.index[0] for v in series.values()])\n        max_index = max([v.index[-1] for v in series.values()])\n        span_index = pd.date_range(start=min_index, end=max_index, freq=freqs[0])\n    else:\n        span_index = series.index\n\n    fold = [\n        [0, initial_train_size],\n        [initial_train_size - self.window_size, initial_train_size],\n        [initial_train_size - self.window_size, len(span_index)],\n        [0, 0],  # Dummy value\n        True\n    ]\n    data_fold = _extract_data_folds_multiseries(\n                    series             = series,\n                    folds              = [fold],\n                    span_index         = span_index,\n                    window_size        = self.window_size,\n                    exog               = exog,\n                    dropna_last_window = self.dropna_from_series,\n                    externally_fitted  = False\n                )\n    series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n    start_test_idx = initial_train_size - self.window_size\n    if isinstance(series, pd.DataFrame):\n        series_test = series.iloc[start_test_idx:, :]\n        series_test = series_test.loc[:, levels_last_window]\n        series_test = series_test.dropna(axis=1, how='all')\n    elif isinstance(series, dict):\n        start_test_date = span_index[start_test_idx]\n        series_test = {\n            k: v.loc[v.index &gt;= start_test_date]\n            for k, v in series.items()\n            if k in levels_last_window and not v.empty and not v.isna().all()\n        }\n\n    _is_fitted = self.is_fitted\n    _series_names_in_ = self.series_names_in_\n    _exog_names_in_ = self.exog_names_in_\n\n    self.is_fitted = False\n    X_train, y_train, _, series_names_in_, _, exog_names_in_, *_ = (\n        self._create_train_X_y(\n            series            = series_train,\n            exog              = exog_train,\n            store_last_window = False\n        )\n    )\n\n    self.series_names_in_ = series_names_in_\n    if exog is not None:\n        self.exog_names_in_ = exog_names_in_\n    self.is_fitted = True\n\n    X_test, y_test, *_ = self._create_train_X_y(\n                            series            = series_test,\n                            exog              = exog_test,\n                            store_last_window = False\n                         )\n    self.is_fitted = _is_fitted\n    self.series_names_in_ = _series_names_in_\n    self.exog_names_in_ = _exog_names_in_\n\n    if self.encoding in [\"ordinal\", \"ordinal_category\"]:\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train[[\"_level_skforecast\"]]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test[[\"_level_skforecast\"]]\n        ).ravel()\n    elif self.encoding == 'onehot':\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train.loc[:, self.encoding_mapping_.keys()]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test.loc[:, self.encoding_mapping_.keys()]\n        ).ravel()\n    else:\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train[[\"_level_skforecast\"]]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test[[\"_level_skforecast\"]]\n        ).ravel()\n        X_train = X_train.drop(columns=\"_level_skforecast\")\n        X_test = X_test.drop(columns=\"_level_skforecast\")\n\n    X_train_encoding = pd.Series(data=X_train_encoding, index=X_train.index).fillna(\n        \"_unknown_level\"\n    )\n    X_test_encoding = pd.Series(data=X_test_encoding, index=X_test.index).fillna(\n        \"_unknown_level\"\n    )\n\n    return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_sample_weights","title":"<code>create_sample_weights(series_names_in_, X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def create_sample_weights(\n    self,\n    series_names_in_: list,\n    X_train: pd.DataFrame\n) -&gt; np.ndarray:\n    \"\"\"\n    Crate weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    weights_series = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = set(series_names_in_) - set(self.series_weights.keys())\n        if series_not_in_series_weights:\n            warnings.warn(\n                (f\"{series_not_in_series_weights} not present in `series_weights`. \"\n                 f\"A weight of 1 is given to all their samples.\"),\n                 IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series_names_in_}\n        self.series_weights_.update(\n            (k, v)\n            for k, v in self.series_weights.items()\n            if k in self.series_weights_\n        )\n\n        if self.encoding == \"onehot\":\n            weights_series = [\n                np.repeat(self.series_weights_[serie], sum(X_train[serie]))\n                for serie in series_names_in_\n            ]\n        else:\n            weights_series = [\n                np.repeat(\n                    self.series_weights_[serie],\n                    sum(X_train[\"_level_skforecast\"] == self.encoding_mapping_[serie]),\n                )\n                for serie in series_names_in_\n            ]\n\n        weights_series = np.concatenate(weights_series)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {col: copy(self.weight_func)\n                                 for col in series_names_in_}\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = set(series_names_in_) - set(self.weight_func.keys())\n            if series_not_in_weight_func:\n                warnings.warn(\n                    (f\"{series_not_in_weight_func} not present in `weight_func`. \"\n                     f\"A weight of 1 is given to all their samples.\"),\n                     IgnoredArgumentWarning\n                )\n            self.weight_func_ = {col: lambda x: np.ones_like(x, dtype=float) \n                                 for col in series_names_in_}\n            self.weight_func_.update(\n                (k, v)\n                for k, v in self.weight_func.items()\n                if k in self.weight_func_\n            )\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            if self.encoding == \"onehot\":\n                idx = X_train.index[X_train[key] == 1.0]\n            else:\n                idx = X_train.index[X_train[\"_level_skforecast\"] == self.encoding_mapping_[key]]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if weights_series is not None:\n        weights = weights_series\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                (\"The resulting `weights` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.fit","title":"<code>fit(series, exog=None, store_last_window=True, store_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Training Forecaster. See Notes section for more details depending on  the type of <code>series</code> and <code>exog</code>.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>store_last_window</code> <code>(bool, list)</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <ul> <li>If <code>True</code>, last window is stored for all series. </li> <li>If <code>list</code>, last window is stored for the series present in the list.</li> <li>If <code>False</code>, last window is not stored.</li> </ul> <code>`True`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the training  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Type Description <code>None</code> Notes <ul> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a pandas Series or  DataFrame, each exog is duplicated for each series. Exog must have the same index as <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a pandas DataFrame and <code>exog</code> is a dict of pandas Series  or DataFrames. Each key in <code>exog</code> must be a column in <code>series</code> and the  values are the exog for each series. Exog must have the same index as  <code>series</code> (type, length and frequency).</li> <li>If <code>series</code> is a dict of pandas Series, <code>exog</code>must be a dict of pandas Series or DataFrames. The keys in <code>series</code> and <code>exog</code> must be the same. All series and exog must have a pandas DatetimeIndex with the same  frequency.</li> </ul> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def fit(\n    self,\n    series: Union[pd.DataFrame, dict],\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    store_last_window: Union[bool, list] = True,\n    store_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster. See Notes section for more details depending on \n    the type of `series` and `exog`.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    store_last_window : bool, list, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n\n        - If `True`, last window is stored for all series. \n        - If `list`, last window is stored for the series present in the list.\n        - If `False`, last window is not stored.\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the training \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    - If `series` is a pandas DataFrame and `exog` is a pandas Series or \n    DataFrame, each exog is duplicated for each series. Exog must have the\n    same index as `series` (type, length and frequency).\n    - If `series` is a pandas DataFrame and `exog` is a dict of pandas Series \n    or DataFrames. Each key in `exog` must be a column in `series` and the \n    values are the exog for each series. Exog must have the same index as \n    `series` (type, length and frequency).\n    - If `series` is a dict of pandas Series, `exog`must be a dict of pandas\n    Series or DataFrames. The keys in `series` and `exog` must be the same.\n    All series and exog must have a pandas DatetimeIndex with the same \n    frequency.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                = None\n    self.index_type_                 = None\n    self.index_freq_                 = None\n    self.training_range_             = None\n    self.series_names_in_            = None\n    self.exog_in_                    = False\n    self.exog_names_in_              = None\n    self.exog_type_in_               = None\n    self.exog_dtypes_in_             = None\n    self.X_train_series_names_in_    = None\n    self.X_train_exog_names_out_     = None\n    self.X_train_features_names_out_ = None\n    self.in_sample_residuals_        = None\n    self.is_fitted                   = False\n    self.fit_date                    = None\n\n    (\n        X_train,\n        y_train,\n        series_indexes,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_,\n        last_window_\n    ) = self._create_train_X_y(\n            series=series, exog=exog, store_last_window=store_last_window\n    )\n\n    sample_weight = self.create_sample_weights(\n                        series_names_in_ = series_names_in_,\n                        X_train          = X_train\n                    )\n\n    X_train_regressor = X_train if self.encoding is not None else X_train.drop(columns='_level_skforecast')\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train_regressor,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train_regressor, y=y_train, **self.fit_kwargs)\n\n    self.series_names_in_ = series_names_in_\n    self.X_train_series_names_in_ = X_train_series_names_in_\n    self.X_train_features_names_out_ = X_train_regressor.columns.to_list()\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = {k: v[[0, -1]] for k, v in series_indexes.items()}\n    self.index_type_ = type(series_indexes[series_names_in_[0]])\n    if isinstance(series_indexes[series_names_in_[0]], pd.DatetimeIndex):\n        self.index_freq_ = series_indexes[series_names_in_[0]].freqstr\n    else:\n        self.index_freq_ = series_indexes[series_names_in_[0]].step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    in_sample_residuals_ = {}\n    if store_in_sample_residuals:\n\n        residuals = (y_train - self.regressor.predict(X_train_regressor)).to_numpy()\n\n        rng = np.random.default_rng(seed=123)\n        if self.encoding is not None:\n            for col in X_train_series_names_in_:\n                if self.encoding == 'onehot':\n                    mask = X_train[col].to_numpy() == 1.\n                else:\n                    encoded_value = self.encoding_mapping_[col]\n                    mask = X_train['_level_skforecast'].to_numpy() == encoded_value\n\n                residuals_col = residuals[mask]\n                if len(residuals_col) &gt; 1000:\n                    residuals_col = rng.choice(\n                                        a       = residuals_col,\n                                        size    = 1000,\n                                        replace = False\n                                    )\n                in_sample_residuals_[col] = residuals_col\n\n        if len(residuals) &gt; 1000:\n            in_sample_residuals_['_unknown_level'] = rng.choice(\n                                                        a       = residuals,\n                                                        size    = 1000,\n                                                        replace = False\n                                                    )\n        else:\n            in_sample_residuals_['_unknown_level'] = residuals\n    else:\n        if self.encoding is not None:\n            for col in X_train_series_names_in_:\n                in_sample_residuals_[col] = None\n        in_sample_residuals_['_unknown_level'] = None\n\n    self.in_sample_residuals_ = in_sample_residuals_\n\n    if store_last_window:\n        self.last_window_ = last_window_\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_predict_inputs","title":"<code>_create_predict_inputs(steps, levels=None, last_window=None, exog=None, predict_boot=False, use_in_sample_residuals=True, check_inputs=True)</code>","text":"<p>Create inputs needed for the first iteration of the prediction process.  Since it is a recursive process, last window is updated at each  iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>predict_boot</code> <code>bool</code> <p>If <code>True</code>, residuals are returned to generate bootstrapping predictions.</p> <code>`False`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> <code>exog_values_dict</code> <code>dict</code> <p>Exogenous variable/s included as predictor/s for each series in  each step. The keys are the steps and the values are numpy arrays where each column is an exog and each row a series (level).</p> <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> <code>residuals</code> <code>(dict, None)</code> <p>Residuals used to generate bootstrapping predictions for each level  in the form <code>{level: residuals}</code>. If <code>predict_boot = False</code>,  <code>residuals</code> is <code>None</code>.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    predict_boot: bool = False,\n    use_in_sample_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; Tuple[pd.DataFrame, dict, list, pd.Index, Optional[dict]]:\n    \"\"\"\n    Create inputs needed for the first iteration of the prediction process. \n    Since it is a recursive process, last window is updated at each \n    iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    predict_boot : bool, default `False`\n        If `True`, residuals are returned to generate bootstrapping predictions.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog_values_dict : dict\n        Exogenous variable/s included as predictor/s for each series in \n        each step. The keys are the steps and the values are numpy arrays\n        where each column is an exog and each row a series (level).\n    levels : list\n        Names of the series (levels) to be predicted.\n    prediction_index : pandas Index\n        Index of the predictions.\n    residuals : dict, None\n        Residuals used to generate bootstrapping predictions for each level \n        in the form `{level: residuals}`. If `predict_boot = False`, \n        `residuals` is `None`.\n\n    \"\"\"\n\n    levels, input_levels_is_list = prepare_levels_multiseries(\n        X_train_series_names_in_=self.X_train_series_names_in_, levels=levels\n    )\n\n    if self.is_fitted and last_window is None:\n        levels, last_window = preprocess_levels_self_last_window_multiseries(\n                                  levels               = levels,\n                                  input_levels_is_list = input_levels_is_list,\n                                  last_window_         = self.last_window_\n                              )\n\n    if self.is_fitted and predict_boot:\n        residuals = prepare_residuals_multiseries(\n                        levels                  = levels,\n                        use_in_sample_residuals = use_in_sample_residuals,\n                        encoding                = self.encoding,\n                        in_sample_residuals_    = self.in_sample_residuals_,\n                        out_sample_residuals_   = self.out_sample_residuals_\n                    )\n    else:\n        residuals = None\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name  = type(self).__name__,\n            steps            = steps,\n            is_fitted        = self.is_fitted,\n            exog_in_         = self.exog_in_,\n            index_type_      = self.index_type_,\n            index_freq_      = self.index_freq_,\n            window_size      = self.window_size,\n            last_window      = last_window,\n            last_window_exog = None,\n            exog             = exog,\n            exog_type_in_    = self.exog_type_in_,\n            exog_names_in_   = self.exog_names_in_,\n            interval         = None,\n            alpha            = None,\n            max_steps        = None,\n            levels           = levels,\n            series_names_in_ = self.series_names_in_,\n            encoding         = self.encoding\n        )\n\n    last_window = last_window.iloc[\n        -self.window_size :, last_window.columns.get_indexer(levels)\n    ].copy()\n    _, last_window_index = preprocess_last_window(\n                               last_window   = last_window,\n                               return_values = False\n                           )\n    prediction_index = expand_index(\n                           index = last_window_index,\n                           steps = steps\n                       )\n\n    if exog is not None:\n        if isinstance(exog, dict):\n            # Empty dataframe to be filled with the exog values of each level\n            empty_exog = pd.DataFrame(\n                             data  = {col: pd.Series(dtype=dtype)\n                                      for col, dtype in self.exog_dtypes_in_.items()},\n                             index = prediction_index\n                         )\n        else:\n            if isinstance(exog, pd.Series):\n                exog = exog.to_frame()\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    exog_values_all_levels = []\n    for level in levels:\n        last_window_level = last_window[level].to_numpy()\n        last_window_level = transform_numpy(\n            array             = last_window_level,\n            transformer       = self.transformer_series_.get(level, self.transformer_series_['_unknown_level']),\n            fit               = False,\n            inverse_transform = False\n        )\n\n        if self.differentiation is not None:\n            if level not in self.differentiator_.keys():\n                self.differentiator_[level] = clone(self.differentiator)\n            last_window_level = self.differentiator_[level].fit_transform(last_window_level)\n\n        last_window[level] = last_window_level\n\n        if isinstance(exog, dict):\n            # Fill the empty dataframe with the exog values of each level\n            # and transform them if necessary\n            exog_values = exog.get(level, None)\n            if exog_values is not None:\n                if isinstance(exog_values, pd.Series):\n                    exog_values = exog_values.to_frame()\n\n                exog_values = empty_exog.fillna(exog_values)\n                exog_values = transform_dataframe(\n                                  df                = exog_values,\n                                  transformer       = self.transformer_exog,\n                                  fit               = False,\n                                  inverse_transform = False\n                              )\n\n                check_exog_dtypes(\n                    exog      = exog_values,\n                    series_id = f\"`exog` for series '{level}'\"\n                )\n                exog_values = exog_values.to_numpy()\n            else:\n                exog_values = empty_exog.to_numpy(copy=True)\n\n        exog_values_all_levels.append(exog_values)\n\n    if exog is not None:\n        # Exog is transformed into a dict where each key is a step and each value\n        # is a numpy array where each column is an exog and each row a series\n        exog_values_all_levels = np.concatenate(exog_values_all_levels)\n        exog_values_dict = {}\n        for i in range(steps):\n            exog_values_dict[i + 1] = exog_values_all_levels[i::steps, :]\n    else:\n        exog_values_dict = None\n\n    return last_window, exog_values_dict, levels, prediction_index, residuals\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._recursive_predict","title":"<code>_recursive_predict(steps, levels, last_window, exog_values_dict=None, residuals=None)</code>","text":"<p>Predict n steps for one or multiple levels. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>list</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the features (lags) needed in the first iteration of the prediction (t + 1).</p> required <code>exog_values_dict</code> <code>dict</code> <p>Exogenous variable/s included as predictor/s for each series in  each step. The keys are the steps and the values are numpy arrays where each column is an exog and each row a series (level).</p> <code>`None`</code> <code>residuals</code> <code>numpy ndarray</code> <p>Residuals used to generate bootstrapping predictions in the form (steps, levels).</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    levels: list,\n    last_window: pd.DataFrame,\n    exog_values_dict: Optional[dict] = None,\n    residuals: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict n steps for one or multiple levels. It is an iterative process\n    in which, each prediction, is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : list\n        Time series to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the features (lags) needed in the\n        first iteration of the prediction (t + 1).\n    exog_values_dict : dict, default `None`\n        Exogenous variable/s included as predictor/s for each series in \n        each step. The keys are the steps and the values are numpy arrays\n        where each column is an exog and each row a series (level).\n    residuals : numpy ndarray, default `None`\n        Residuals used to generate bootstrapping predictions in the form\n        (steps, levels).\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    n_levels = len(levels)\n    lags_shape = len(self.lags)\n    exog_shape = len(self.X_train_exog_names_out_) if exog_values_dict is not None else 0\n\n    if self.encoding is not None:\n        if self.encoding == \"onehot\":\n            levels_encoded = np.zeros(\n                (n_levels, len(self.X_train_series_names_in_)), dtype=float\n            )\n            for i, level in enumerate(levels):\n                if level in self.X_train_series_names_in_:\n                    levels_encoded[i, self.X_train_series_names_in_.index(level)] = 1.\n        else:\n            levels_encoded = np.array(\n                [self.encoding_mapping_.get(level, None) for level in levels],\n                dtype=\"float64\"\n            ).reshape(-1, 1)\n        levels_encoded_shape = levels_encoded.shape[1]\n    else:\n        levels_encoded_shape = 0\n\n    features_shape = lags_shape + levels_encoded_shape + exog_shape\n    features = np.full(shape=(n_levels, features_shape), fill_value=np.nan, dtype=float)\n    if self.encoding is not None:\n        features[:, lags_shape : lags_shape + levels_encoded_shape] = levels_encoded\n\n    predictions = np.full(shape=(steps, n_levels), fill_value=np.nan, dtype=float)\n    last_window = np.concatenate((last_window.to_numpy(), predictions), axis=0)\n\n    for i in range(steps):\n\n        step = i + 1\n        features[:, :lags_shape] = last_window[-self.lags - (steps - i), :].transpose()\n        if exog_values_dict is not None:\n            features[:, -exog_shape:] = exog_values_dict[step]\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            pred = self.regressor.predict(features)\n\n        if residuals is not None:\n            pred += residuals[i, :]\n\n        predictions[i, :] = pred \n\n        # Update `last_window` values. The first position is discarded and\n        # the new prediction is added at the end.\n        last_window[-(steps - i), :] = pred\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_predict_X","title":"<code>create_predict_X(steps, levels=None, last_window=None, exog=None, suppress_warnings=False)</code>","text":"<p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive process, the predictors are created at each iteration of the prediction  process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_predict_dict</code> <code>dict</code> <p>Dict in the form <code>{level: X_predict}</code> with the predictors for each  step and series. The index is the same as the prediction index.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    suppress_warnings: bool = False\n) -&gt; dict:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead. As it is a recursive\n    process, the predictors are created at each iteration of the prediction \n    process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    X_predict_dict : dict\n        Dict in the form `{level: X_predict}` with the predictors for each \n        step and series. The index is the same as the prediction index.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    predictions = self.predict(\n                      steps             = steps,\n                      levels            = levels,\n                      last_window       = last_window,\n                      exog              = exog,\n                      suppress_warnings = suppress_warnings\n                  )\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index,\n        _\n    ) = self._create_predict_inputs(\n        steps        = steps,\n        levels       = levels,\n        last_window  = last_window,\n        exog         = exog,\n        check_inputs = False\n    )\n\n    X_predict_dict = {}\n    idx_lags = np.arange(-steps, 0)[:, None] - self.lags\n    len_X_train_series_names_in_ = len(self.X_train_series_names_in_)\n    exog_shape = len(self.X_train_exog_names_out_) if exog is not None else 0\n    for i, level in enumerate(levels):\n\n        X_predict_list = []\n\n        full_predictors = np.concatenate(\n            (last_window[level].to_numpy(), predictions[level].to_numpy())\n        )\n        X_predict_list.append(full_predictors[idx_lags + len(full_predictors)])\n\n        if self.encoding is not None:\n            if self.encoding == 'onehot':\n                level_encoded = np.zeros(shape=(1, len_X_train_series_names_in_), dtype=float)\n                level_encoded[0][self.X_train_series_names_in_.index(level)] = 1.\n            else:\n                level_encoded = np.array([self.encoding_mapping_.get(level, None)], dtype='float64')\n\n            level_encoded = np.tile(level_encoded, (steps, 1))\n            X_predict_list.append(level_encoded)\n\n        if exog is not None:\n            exog_cols = np.full(shape=(steps, exog_shape), fill_value=np.nan, dtype=float)\n            for j in range(steps):\n                step = j + 1\n                exog_cols[j, :] = exog_values_dict[step][i, :]\n            X_predict_list.append(exog_cols)\n\n        X_predict_dict[level] = pd.DataFrame(\n                                    data    = np.concatenate(X_predict_list, axis=1),\n                                    columns = self.X_train_features_names_out_,\n                                    index   = prediction_index\n                                )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_predict_dict\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict","title":"<code>predict(steps, levels=None, last_window=None, exog=None, suppress_warnings=False, check_inputs=True)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Only levels whose last window ends at the same datetime index can be predicted together.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values, one column for each level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step. Only levels whose last window\n    ends at the same datetime index can be predicted together.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values, one column for each level.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index,\n        _\n    ) = self._create_predict_inputs(\n        steps        = steps,\n        levels       = levels,\n        last_window  = last_window,\n        exog         = exog,\n        check_inputs = check_inputs\n    )\n\n    predictions = self._recursive_predict(\n                      steps            = steps,\n                      levels           = levels,\n                      last_window      = last_window,\n                      exog_values_dict = exog_values_dict\n                  )\n\n    for i, level in enumerate(levels):\n        if self.differentiation is not None:\n            predictions[:, i] = (\n                self\n                .differentiator_[level]\n                .inverse_transform_next_window(predictions[:, i])\n            )\n\n        predictions[:, i] = transform_numpy(\n            array             = predictions[:, i],\n            transformer       = self.transformer_series_.get(level, self.transformer_series_['_unknown_level']),\n            fit               = False,\n            inverse_transform = True\n        )\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      index   = prediction_index,\n                      columns = levels\n                  )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  Only levels whose last window ends at the same datetime index can be  predicted together. See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>dict</code> <p>Predictions generated by bootstrapping for each level.</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; dict:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    Only levels whose last window ends at the same datetime index can be \n    predicted together. See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    boot_predictions : dict\n        Predictions generated by bootstrapping for each level.\n        {level: pandas DataFrame, shape (steps, n_boot)}\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index,\n        residuals\n    ) = self._create_predict_inputs(\n        steps                   = steps,\n        levels                  = levels,\n        last_window             = last_window,\n        exog                    = exog,\n        predict_boot            = True,\n        use_in_sample_residuals = use_in_sample_residuals\n    )\n\n    n_levels = len(levels)\n    rng = np.random.default_rng(seed=random_state)\n    sample_residuals = np.full(\n                           shape      = (steps, n_boot, n_levels),\n                           fill_value = np.nan,\n                           dtype      = float\n                       )\n    for i, level in enumerate(levels):\n        sample_residuals[:, :, i] = rng.choice(\n                                        a       = residuals[level],\n                                        size    = (steps, n_boot),\n                                        replace = True\n                                    )\n\n    boot_columns = []\n    boot_predictions_full = np.full(\n                                shape      = (steps, n_levels, n_boot),\n                                fill_value = np.nan,\n                                dtype      = float\n                            )\n    for i in range(n_boot):\n\n        boot_columns.append(f\"pred_boot_{i}\")\n        boot_predictions_full[:, :, i] = self._recursive_predict(\n            steps            = steps,\n            levels           = levels,\n            last_window      = last_window,\n            exog_values_dict = exog_values_dict,\n            residuals        = sample_residuals[:, i, :]\n        )\n\n    boot_predictions = {}\n    for i, level in enumerate(levels):\n\n        if self.differentiation is not None:\n            boot_predictions_full[:, i, :] = (\n                self.differentiator_[level].inverse_transform_next_window(boot_predictions_full[:, i, :])\n            )\n\n        transformer_level = self.transformer_series_.get(level, self.transformer_series_['_unknown_level'])\n        if transformer_level is not None:\n            boot_predictions_full[:, i, :] = np.apply_along_axis(\n                func1d            = transform_numpy,\n                axis              = 0,\n                arr               = boot_predictions_full[:, i, :],\n                transformer       = transformer_level,\n                fit               = False,\n                inverse_transform = True\n            )\n\n        boot_predictions[level] = pd.DataFrame(\n                                      data    = boot_predictions_full[:, i, :],\n                                      index   = prediction_index,\n                                      columns = boot_columns\n                                  )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_interval","title":"<code>predict_interval(steps, levels=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction  intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>level: predictions.</li> <li>level_lower_bound: lower bound of the interval.</li> <li>level_upper_bound: upper bound of the interval.</li> </ul> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: list = [5, 95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Iterative process in which, each prediction, is used as a predictor\n    for the next step and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction \n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - level: predictions.\n        - level_lower_bound: lower bound of the interval.\n        - level_upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(interval=interval)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           levels                  = levels,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           suppress_warnings       = suppress_warnings\n                       )\n\n    preds = self.predict(\n                steps             = steps,\n                levels            = levels,\n                last_window       = last_window,\n                exog              = exog,\n                suppress_warnings = suppress_warnings,\n                check_inputs      = False\n            )\n\n    interval = np.array(interval) / 100\n    predictions = []\n\n    for level in preds.columns:\n        preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n        preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n        predictions.append(preds[level])\n        predictions.append(preds_interval)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_quantiles","title":"<code>predict_quantiles(steps, levels=None, last_window=None, exog=None, quantiles=[0.05, 0.5, 0.95], n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Calculate the specified quantiles for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  quantile is calculated for each step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>quantiles</code> <code>list</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>`[0.05, 0.5, 0.95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate quantiles.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot quantiles are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create quantiles. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    quantiles: list = [0.05, 0.5, 0.95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the specified quantiles for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    quantile is calculated for each step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, default `[0.05, 0.5, 0.95]`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate quantiles.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot quantiles are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create quantiles. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(quantiles=quantiles)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           levels                  = levels,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           suppress_warnings       = suppress_warnings\n                       )\n\n    predictions = []\n\n    for level in boot_predictions.keys():\n        preds_quantiles = boot_predictions[level].quantile(q=quantiles, axis=1).transpose()\n        preds_quantiles.columns = [f'{level}_q_{q}' for q in quantiles]\n        predictions.append(preds_quantiles)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_dist","title":"<code>predict_dist(steps, distribution, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats. For example scipy.stats.norm.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step and level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats. For example scipy.stats.norm.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step and level.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    boot_samples = self.predict_bootstrapping(\n                       steps                   = steps,\n                       levels                  = levels,\n                       last_window             = last_window,\n                       exog                    = exog,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals,\n                       suppress_warnings       = suppress_warnings\n                   )\n\n    param_names = [\n        p for p in inspect.signature(distribution._pdf).parameters if not p == \"x\"\n    ] + [\"loc\", \"scale\"]\n    predictions = []\n\n    for level in boot_samples.keys():\n        param_values = np.apply_along_axis(\n            lambda x: distribution.fit(x), axis=1, arr=boot_samples[level]\n        )\n        level_param_names = [f'{level}_{p}' for p in param_names]\n\n        pred_level = pd.DataFrame(\n                         data    = param_values,\n                         columns = level_param_names,\n                         index   = boot_samples[level].index\n                     )\n\n        predictions.append(pred_level)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and  <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `max_lag` and \n    `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(self.lags)\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each level in the form {level: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored. Keys must be the same as <code>levels</code>.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals_</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_series.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict,\n    append: bool = True,\n    transform: bool = True,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each level in the\n        form {level: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored. Keys must be the same as `levels`.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals_`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_series.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{level: residuals}`. \"\n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.encoding is None:\n        self.out_sample_residuals_ = {'_unknown_level': None}\n        if list(residuals.keys()) != ['_unknown_level']:\n            warnings.warn(\n                (\"As `encoding` is set to `None`, no distinction between levels \"\n                 \"is made. All residuals are stored in the '_unknown_level' key.\"),\n                 UnknownLevelWarning\n            )\n            residuals = [v for v in residuals.values() if v is not None]\n            if residuals:\n                residuals = np.concatenate(residuals)\n            residuals = {'_unknown_level': residuals}\n\n    else:\n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {level: None for level in self.series_names_in_}\n\n        if not set(self.out_sample_residuals_.keys()).issubset(set(residuals.keys())):\n            warnings.warn(\n                (\n                    f\"Only residuals of levels \" \n                    f\"{set(self.out_sample_residuals_.keys()).intersection(set(residuals.keys()))} \"\n                    f\"are updated.\"\n                ), IgnoredArgumentWarning\n            )\n        residuals = {\n            k: v \n            for k, v in residuals.items() \n            if k in self.out_sample_residuals_.keys() and k != '_unknown_level'\n        }\n\n    for level, value in residuals.items():\n\n        residuals_level = value\n        transformer_level = self.transformer_series_[level]\n        level_str = f\"level '{level}'\" if self.encoding is not None else 'all levels'\n\n        if not transform and transformer_level is not None:\n            warnings.warn(\n                (f\"Argument `transform` is set to `False` but forecaster was \"\n                 f\"trained using a transformer {transformer_level} \"\n                 f\"for {level_str}. Ensure that the new residuals are \"\n                 f\"already transformed or set `transform=True`.\")\n            )\n\n        if transform and self.transformer_series_ and transformer_level:\n            warnings.warn(\n                (f\"Residuals will be transformed using the same transformer used \"\n                 f\"when training the forecaster for {level_str} : \"\n                 f\"({transformer_level}). Ensure that the new \"\n                 f\"residuals are on the same scale as the original time series.\")\n            )\n            residuals_level = transform_numpy(\n                array             = residuals_level,\n                transformer       = transformer_level,\n                fit               = False,\n                inverse_transform = False\n            )\n\n        if len(residuals_level) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n\n        if append and self.out_sample_residuals_[level] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals_[level]))\n            if len(residuals_level) &lt; free_space:\n                residuals_level = np.hstack((\n                                      self.out_sample_residuals_[level],\n                                      residuals_level\n                                  ))\n            else:\n                residuals_level = np.hstack((\n                                      self.out_sample_residuals_[level],\n                                      residuals_level[:free_space]\n                                  ))\n\n        self.out_sample_residuals_[level] = residuals_level\n\n    if self.encoding is not None:\n        residuals_unknown_level = [\n            v for k, v in self.out_sample_residuals_.items() \n            if v is not None and k != '_unknown_level'\n        ]\n        if residuals_unknown_level:\n            residuals_unknown_level = np.concatenate(residuals_unknown_level)\n            if len(residuals_unknown_level) &gt; 1000:\n                rng = np.random.default_rng(seed=random_state)\n                residuals_unknown_level = rng.choice(\n                                              a       = residuals_unknown_level,\n                                              size    = 1000,\n                                              replace = False\n                                          )\n        else:\n            residuals_unknown_level = None\n\n        self.out_sample_residuals_['_unknown_level'] = residuals_unknown_level\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.get_feature_importances","title":"<code>get_feature_importances(sort_importance=True)</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    sort_importance: bool, default `True`\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_features_names_out_,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html","title":"<code>ForecasterAutoregMultiVariate</code>","text":""},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate","title":"<code>ForecasterAutoregMultiVariate(regressor, level, steps, lags, transformer_series=StandardScaler(), transformer_exog=None, weight_func=None, fit_kwargs=None, n_jobs='auto', forecaster_id=None)</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive multivariate direct multi-step forecaster. A separate model  is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>level</code> <code>str</code> <p>Name of the time series to be predicted.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value must be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series. {'series_column_name': lags}.</li> </ul> required <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>`sklearn.preprocessing.StandardScaler`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray, dict</code> <p>Lags used as predictors.</p> <code>lags_</code> <code>dict</code> <p>Dictionary containing the lags of each series. Created from <code>lags</code> and  used internally.</p> <code>transformer_series</code> <code>transformer (preprocessor), dict, default `None`</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. When using differentiation, the <code>window_size</code> is increased by the order of  differentiation so that the predictors can be created correctly.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict <code>steps</code> immediately after the training data.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series added to <code>X_train</code> when creating the training matrices with <code>_create_train_X_y</code> method. It is a subset of  <code>series_names_in_</code>.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>int, 'auto', default `'auto'`</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the fuction skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>dropna_from_series</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>encoding</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiator</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> Notes <p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    level: str,\n    steps: int,\n    lags: Union[int, np.ndarray, list, dict],\n    transformer_series: Optional[Union[object, dict]] = StandardScaler(),\n    transformer_exog: Optional[object] = None,\n    weight_func: Optional[Callable] = None,\n    fit_kwargs: Optional[dict] = None,\n    n_jobs: Union[int, str] = 'auto',\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.regressor                      = copy(regressor)\n    self.level                          = level\n    self.steps                          = steps\n    self.transformer_series             = transformer_series\n    self.transformer_series_            = None\n    self.transformer_exog               = transformer_exog\n    self.weight_func                    = weight_func\n    self.source_code_weight_func        = None\n    self.max_lag                        = None\n    self.window_size                    = None\n    self.last_window_                   = None\n    self.index_type_                    = None\n    self.index_freq_                    = None\n    self.training_range_                = None\n    self.series_names_in_               = None\n    self.exog_in_                       = False\n    self.exog_names_in_                 = None\n    self.exog_type_in_                  = None\n    self.exog_dtypes_in_                = None\n    self.X_train_series_names_in_       = None\n    self.X_train_exog_names_out_        = None\n    self.X_train_direct_exog_names_out_ = None\n    self.X_train_features_names_out_    = None\n    self.creation_date                  = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                      = False\n    self.fit_date                       = None\n    self.skforecast_version             = skforecast.__version__\n    self.python_version                 = sys.version.split(\" \")[0]\n    self.forecaster_id                  = forecaster_id\n    self.dropna_from_series             = False  # Ignored in this forecaster\n    self.encoding                       = None   # Ignored in this forecaster\n    self.differentiation                = None   # Ignored in this forecaster\n    self.differentiator                 = None   # Ignored in this forecaster\n\n    if not isinstance(level, str):\n        raise TypeError(\n            f\"`level` argument must be a str. Got {type(level)}.\"\n        )\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        list_max_lags = []\n        for key in lags:\n            if lags[key] is None:\n                self.lags[key] = None\n            else:\n                self.lags[key], max_lag = initialize_lags(\n                    forecaster_name = type(self).__name__,\n                    lags            = lags[key]\n                )\n                list_max_lags.append(max_lag)\n        self.max_lag = max(list_max_lags)\n    else:\n        self.lags, self.max_lag = initialize_lags(\n            forecaster_name = type(self).__name__, \n            lags            = lags\n        )\n\n    self.lags_ = self.lags\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals_ = None\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor_name  = type(self.regressor).__name__,\n                      )\n    else:\n        if not isinstance(n_jobs, int):\n            raise TypeError(\n                f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n            )\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._create_lags","title":"<code>_create_lags(y, lags, return_data='both')</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <code>lags</code> <code>numpy ndarray</code> <p>lags to create.</p> required <code>return_data</code> <code>str</code> <p>Specifies which data to return. Options are 'X', 'y', 'both'.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors). Will be None if  <code>return_data</code> is 'y'. Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the values of the time series related to each  row of <code>X_data</code> for each step. Will be None if <code>return_data</code> is  'X'. Shape: (len(self.steps), samples - max(self.lags))</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    lags: np.ndarray,\n    return_data: str = 'both'\n) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n    \"\"\"\n    Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n    lags : numpy ndarray\n        lags to create.\n    return_data : str, default 'both'\n        Specifies which data to return. Options are 'X', 'y', 'both'.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). Will be None if \n        `return_data` is 'y'. Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        2d numpy ndarray with the values of the time series related to each \n        row of `X_data` for each step. Will be None if `return_data` is \n        'X'. Shape: (len(self.steps), samples - max(self.lags))\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - (self.steps - 1)  # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series minus the number of steps ({len(y) - (self.steps - 1)}).\")\n        )\n\n    if return_data == 'y':\n        X_data = None\n    else:\n        X_data = np.full(shape=(n_splits, len(lags)), fill_value=np.nan, dtype=float)\n        for i, lag in enumerate(lags):\n            X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n\n    if return_data == 'X':\n        y_data = None\n    else:       \n        y_data = np.full(shape=(self.steps, n_splits), fill_value=np.nan, dtype=float)\n        for step in range(self.steps):\n            y_data[step, ] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._create_train_X_y","title":"<code>_create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method. Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)</p> <code>y_train</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step of the form {step: y_step_[i]}. Shape of each series: (len(y) - self.max_lag, )</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) provided by the user during training.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series added to <code>X_train</code> when creating the training matrices with <code>_create_train_X_y</code> method. It is a subset of  <code>series_names_in_</code>.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables included in the training matrices.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated before the transformation.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def _create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, dict, list, list, list, list, dict]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n        Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)\n    y_train : dict\n        Values (target) of the time series related to each row of `X_train` \n        for each step of the form {step: y_step_[i]}.\n        Shape of each series: (len(y) - self.max_lag, )\n    series_names_in_ : list\n        Names of the series (levels) provided by the user during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    exog_names_in_ : list\n        Names of the exogenous variables included in the training matrices.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n        )\n\n    series_names_in_ = list(series.columns)\n\n    if self.level not in series_names_in_:\n        raise ValueError(\n            (f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n             f\"  Forecaster `level` : {self.level}.\\n\"\n             f\"  `series` columns   : {series_names_in_}.\")\n        )\n\n    if isinstance(self.lags, dict):\n        self.lags_ = self.lags\n        lags_keys = list(self.lags_.keys())\n        if lags_keys != series_names_in_:\n            raise ValueError(\n                (f\"When `lags` parameter is a `dict`, its keys must be the \"\n                 f\"same as `series` column names.\\n\"\n                 f\"  Lags keys        : {lags_keys}.\\n\"\n                 f\"  `series` columns : {series_names_in_}.\")\n            )\n    else:\n        self.lags_ = {serie: self.lags for serie in series_names_in_}\n\n    # If col is not level and has lags, create 'X' if no lags don't include\n    # If col is level, create 'both' (`X` and `y`)\n    cols_to_create_lags = {\n        col: ('both' if col == self.level else 'X')\n        for col in series_names_in_\n        if col == self.level or self.lags_.get(col) is not None\n    }\n\n    # Adjust 'level' in case self.lags_[level] is None\n    if self.lags_.get(self.level) is None:\n        cols_to_create_lags[self.level] = 'y'\n\n    # Update series_names_in_ with the columns that will be used during training\n    series_names_in_ = list(cols_to_create_lags.keys())\n    # X_train_series_names_in_ include series that will be added to X_train\n    X_train_series_names_in_ = [col for col in series_names_in_ \n                                if cols_to_create_lags[col] in ['X', 'both']]\n\n    if len(series) &lt; self.max_lag + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `series` for training this forecaster is \"\n             f\"{self.max_lag + self.steps}. Got {len(series)}. Reduce the \"\n             f\"number of predicted steps, {self.steps}, or the maximum \"\n             f\"lag, {self.max_lag}, if no more data is available.\")\n        )\n\n    fit_transformer = False\n    if not self.is_fitted:\n        fit_transformer = True\n        self.transformer_series_ = initialize_transformer_series(\n                                       forecaster_name    = type(self).__name__,\n                                       series_names_in_   = series_names_in_,\n                                       transformer_series = self.transformer_series\n                                   )\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog_names_in_ = exog.columns.to_list()\n        if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of \"\n                 f\"the series (column names of series).\\n\"\n                 f\"  `series` columns : {series_names_in_}.\\n\"\n                 f\"  `exog`   columns : {exog_names_in_}.\")\n            )\n\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.exog_in_ = True\n\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog.index[:len(series)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\") \n            )\n\n    X_train_list = []\n    X_train_features_names_out_ = []\n    for col, return_data in cols_to_create_lags.items():\n\n        y = series[col]\n        check_y(y=y, series_id=f\"Column '{col}'\")\n        y = transform_series(\n                series            = y,\n                transformer       = self.transformer_series_[col],\n                fit               = fit_transformer,\n                inverse_transform = False\n            )\n        y_values, y_index = preprocess_y(y=y)\n\n        X_train_values, y_train_values = self._create_lags(\n                                             y           = y_values,\n                                             lags        = self.lags_[col],\n                                             return_data = return_data\n                                         )\n\n        if X_train_values is not None:\n            X_train_list.append(X_train_values)\n            X_train_features_names_out_.extend(\n                [f\"{col}_lag_{lag}\" for lag in self.lags_[col]]\n            )\n        if col == self.level:\n            y_train = y_train_values\n\n    X_train = pd.DataFrame(\n                  data    = np.hstack(X_train_list),\n                  columns = X_train_features_names_out_,\n                  index   = y_index[self.max_lag + (self.steps - 1):]\n              )\n\n    if exog is not None:\n        # Transform exog to match direct format\n        # The first `self.max_lag` positions have to be removed from X_exog\n        # since they are not in X_lags.\n        X_train_exog_names_out_ = exog.columns.to_list()\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        ).iloc[-X_train.shape[0]:, :]\n        exog_to_train.index = exog_index[-X_train.shape[0]:]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n        # Need X_train_direct_exog_names_out_ here for filter_train_X_y_for_step \n        # to work without fitting\n        if not self.is_fitted:\n            self.X_train_direct_exog_names_out_ = exog_to_train.columns.to_list()\n\n    y_train = {\n        step: pd.Series(\n                  data  = y_train[step - 1], \n                  index = y_index[self.max_lag + step - 1:][:len(y_train[0])],\n                  name  = f\"{self.level}_step_{step}\"\n              )\n        for step in range(1, self.steps + 1)\n    }    \n\n    return (\n        X_train,\n        y_train,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_\n    )\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_train_X_y","title":"<code>create_train_X_y(series, exog=None, suppress_warnings=False)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the creation of the training matrices. See skforecast.exceptions.warn_skforecast_categories  for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method. Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)</p> <code>y_train</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step of the form {step: y_step_[i]}. Shape of each series: (len(y) - self.max_lag, )</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    suppress_warnings: bool = False\n) -&gt; Tuple[pd.DataFrame, dict, list, list, list]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the creation\n        of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n        for more information.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n        Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)\n    y_train : dict\n        Values (target) of the time series related to each row of `X_train` \n        for each step of the form {step: y_step_[i]}.\n        Shape of each series: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    output = self._create_train_X_y(\n                 series = series, \n                 exog   = exog\n             )\n\n    X_train = output[0]\n    y_train = output[1]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>_create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>_create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>_create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict,\n    remove_suffix: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `_create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `_create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `_create_train_X_y` method, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.exog_in_:\n        X_train_step = X_train\n    else:\n        len_columns_lags = len(list(\n            chain(*[v for v in self.lags_.values() if v is not None])\n        ))\n        idx_columns_lags = np.arange(len_columns_lags)\n        n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n        idx_columns_exog = (\n            np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_lags[-1] + 1 \n        )\n        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [col.replace(f\"_step_{step}\", \"\")\n                                for col in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._train_test_split_one_step_ahead","title":"<code>_train_test_split_one_step_ahead(series, initial_train_size, exog=None)</code>","text":"<p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors)</p> <code>y_train</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Test values (predictors)</p> <code>y_test</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_test</code>  for each step.</p> <code>X_train_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_train</code>.</p> <code>X_test_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_test</code>.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    series: pd.DataFrame,\n    initial_train_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors)\n    y_train : dict\n        Values (target) of the time series related to each row of `X_train` \n        for each step.\n    X_test : pandas DataFrame\n        Test values (predictors)\n    y_test : dict\n        Values (target) of the time series related to each row of `X_test` \n        for each step.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n\n    \"\"\"\n\n    span_index = series.index\n\n    fold = [\n        [0, initial_train_size],\n        [initial_train_size - self.window_size, initial_train_size],\n        [initial_train_size - self.window_size, len(span_index)],\n        [0, 0],  # Dummy value\n        True\n    ]\n    data_fold = _extract_data_folds_multiseries(\n                    series             = series,\n                    folds              = [fold],\n                    span_index         = span_index,\n                    window_size        = self.window_size,\n                    exog               = exog,\n                    dropna_last_window = self.dropna_from_series,\n                    externally_fitted  = False\n                )\n    series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n    start_test_idx = initial_train_size - self.window_size\n    series_test = series.iloc[start_test_idx:, :]\n    series_test = series_test.loc[:, levels_last_window]\n    series_test = series_test.dropna(axis=1, how='all')\n\n    _is_fitted = self.is_fitted\n    _series_names_in_ = self.series_names_in_\n    _exog_names_in_ = self.exog_names_in_\n\n    self.is_fitted = False\n    X_train, y_train, series_names_in_, _, exog_names_in_, _, _ = (\n        self._create_train_X_y(\n            series = series_train,\n            exog   = exog_train,\n        )\n    )\n    self.series_names_in_ = series_names_in_\n    if exog is not None:\n        self.exog_names_in_ = exog_names_in_\n    self.is_fitted = True\n\n    X_test, y_test, *_ = self._create_train_X_y(\n                            series = series_test,\n                            exog   = exog_test,\n                         )\n    self.is_fitted = _is_fitted\n    self.series_names_in_ = _series_names_in_\n    self.exog_names_in_ = _exog_names_in_\n\n    X_train_encoding = pd.Series(self.level, index=X_train.index)\n    X_test_encoding = pd.Series(self.level, index=X_test.index)\n\n    return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>. </p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>_create_train_X_y</code> and filter_train_X_y_for_step` methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame\n) -&gt; np.ndarray:\n    \"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`. \n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.fit","title":"<code>fit(series, exog=None, store_last_window=True, store_in_sample_residuals=True, suppress_warnings=False)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>`True`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the training  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_last_window : bool, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the training \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                = None\n    self.index_type_                 = None\n    self.index_freq_                 = None\n    self.training_range_             = None\n    self.series_names_in_            = None\n    self.exog_in_                    = False\n    self.exog_names_in_              = None\n    self.exog_type_in_               = None\n    self.exog_dtypes_in_             = None\n    self.X_train_series_names_in_    = None\n    self.X_train_exog_names_out_     = None\n    self.X_train_features_names_out_ = None\n    self.in_sample_residuals_        = {step: None for step in range(1, self.steps + 1)}\n    self.is_fitted                   = False\n    self.fit_date                    = None\n\n    (\n        X_train,\n        y_train,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_\n    ) = self._create_train_X_y(series=series, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n        \"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n        store_in_sample_residuals : bool\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor and in-sample residuals.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            residuals = (\n                (y_train_step - regressor.predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                residuals = rng.choice(\n                                a       = residuals, \n                                size    = 1000, \n                                replace = False\n                            )\n        else:\n            residuals = None\n\n        return step, regressor, residuals\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor                 = copy(self.regressor),\n            X_train                   = X_train,\n            y_train                   = y_train,\n            step                      = step,\n            store_in_sample_residuals = store_in_sample_residuals\n        )\n        for step in range(1, self.steps + 1))\n    )\n\n    self.regressors_ = {step: regressor \n                        for step, regressor, _ in results_fit}\n\n    if store_in_sample_residuals:\n        self.in_sample_residuals_ = {step: residuals \n                                     for step, _, residuals in results_fit}\n\n    self.series_names_in_ = series_names_in_\n    self.X_train_series_names_in_ = X_train_series_names_in_\n    self.X_train_features_names_out_ = X_train.columns.to_list()\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = preprocess_y(\n                               y = series[self.level],\n                               return_values = False\n                           )[1][[0, -1]]\n    self.index_type_ = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq_ = X_train.index.freqstr\n    else: \n        self.index_freq_ = X_train.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    if store_last_window:\n        self.last_window_ = series.iloc[-self.max_lag:, ][\n            self.X_train_series_names_in_\n        ].copy()\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._create_predict_inputs","title":"<code>_create_predict_inputs(steps=None, last_window=None, exog=None, check_inputs=True)</code>","text":"<p>Create the inputs needed for the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>Xs</code> <code>list</code> <p>List of numpy arrays with the predictors for each step.</p> <code>Xs_col_names</code> <code>list</code> <p>Names of the columns of the matrix created internally for prediction.</p> <code>steps</code> <code>list</code> <p>Steps to predict.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    check_inputs: bool = True\n) -&gt; Tuple[list, list, list, pd.Index]:\n    \"\"\"\n    Create the inputs needed for the prediction process.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    Xs : list\n        List of numpy arrays with the predictors for each step.\n    Xs_col_names : list\n        Names of the columns of the matrix created internally for prediction.\n    steps : list\n        Steps to predict.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    steps = prepare_steps_direct(\n                steps    = steps,\n                max_step = self.steps\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name  = type(self).__name__,\n            steps            = steps,\n            is_fitted        = self.is_fitted,\n            exog_in_         = self.exog_in_,\n            index_type_      = self.index_type_,\n            index_freq_      = self.index_freq_,\n            window_size      = self.window_size,\n            last_window      = last_window,\n            exog             = exog,\n            exog_type_in_    = self.exog_type_in_,\n            exog_names_in_   = self.exog_names_in_,\n            interval         = None,\n            max_steps        = self.steps,\n            series_names_in_ = self.X_train_series_names_in_\n        )\n\n    last_window = last_window.iloc[-self.window_size:, ][self.X_train_series_names_in_].copy()\n\n    Xs_col_names = []\n    X_lags = np.array([[]], dtype=float)\n    for serie in self.X_train_series_names_in_:\n        last_window_serie = transform_numpy(\n                                array             = last_window[serie].to_numpy(),\n                                transformer       = self.transformer_series_[serie],\n                                fit               = False,\n                                inverse_transform = False\n                            )       \n\n        Xs_col_names.extend([f\"{serie}_lag_{lag}\" for lag in self.lags_[serie]])\n        X_lags = np.hstack(\n                     [X_lags, last_window_serie[-self.lags_[serie]].reshape(1, -1)]\n                 )\n\n    _, last_window_index = preprocess_last_window(\n                               last_window   = last_window[self.X_train_series_names_in_[0]],\n                               return_values = False\n                           )\n\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = exog.loc[:, self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n\n        n_exog = exog.shape[1]\n        Xs = [\n            np.hstack(\n                [X_lags, \n                 exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1)]\n            )\n            for step in steps\n        ]\n        Xs_col_names = Xs_col_names + exog.columns.to_list()\n    else:\n        Xs = [X_lags] * len(steps)\n\n    prediction_index = expand_index(\n                           index = last_window_index,\n                           steps = max(steps)\n                       )[np.array(steps) - 1]\n    if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(\n        steps, np.arange(min(steps), max(steps) + 1)\n    ):\n        prediction_index.freq = last_window_index.freq\n\n    return Xs, Xs_col_names, steps, prediction_index\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_predict_X","title":"<code>create_predict_X(steps=None, last_window=None, exog=None, suppress_warnings=False)</code>","text":"<p>Create the predictors needed to predict <code>steps</code> ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog\n    )\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(Xs, axis=0), \n                    columns = Xs_col_names, \n                    index   = prediction_index\n                )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict","title":"<code>predict(steps=None, last_window=None, exog=None, suppress_warnings=False, check_inputs=True, levels=None)</code>","text":"<p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>`True`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default `True`\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    Xs, _, steps, prediction_index = self._create_predict_inputs(\n        steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n    )\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      columns = [self.level],\n                      index   = prediction_index\n                  )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False, levels=None)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.     \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.               \n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if self.is_fitted:\n\n        steps = prepare_steps_direct(\n                    steps    = steps,\n                    max_step = self.steps\n                )\n\n        if use_in_sample_residuals:\n            if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.in_sample_residuals_` for steps: \"\n                     f\"{set(steps) - set(self.in_sample_residuals_.keys())}.\")\n                )\n            residuals = self.in_sample_residuals_\n        else:\n            if self.out_sample_residuals_ is None:\n                raise ValueError(\n                    (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                     \"`use_in_sample_residuals=True` or the \"\n                     \"`set_out_sample_residuals()` method before predicting.\")\n                )\n            else:\n                if not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(\n                        (f\"Not `forecaster.out_sample_residuals_` for steps: \"\n                         f\"{set(steps) - set(self.out_sample_residuals_.keys())}. \"\n                         f\"Use method `set_out_sample_residuals()`.\")\n                    )\n            residuals = self.out_sample_residuals_\n\n        check_residuals = (\n            'forecaster.in_sample_residuals_' if use_in_sample_residuals\n            else 'forecaster.out_sample_residuals_'\n        )\n        for step in steps:\n            if residuals[step] is None:\n                raise ValueError(\n                    (f\"forecaster residuals for step {step} are `None`. \"\n                     f\"Check {check_residuals}.\")\n                )\n            elif (any(element is None for element in residuals[step]) or\n                  np.any(np.isnan(residuals[step]))):\n                raise ValueError(\n                    (f\"forecaster residuals for step {step} contains `None` \"\n                     f\"or `NaNs` values. Check {check_residuals}.\")\n                )\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog \n                  )\n\n    # Predictions must be in the transformed scale before adding residuals\n    boot_predictions = transform_numpy(\n                           array             = predictions.to_numpy().ravel(),\n                           transformer       = self.transformer_series_[self.level],\n                           fit               = False,\n                           inverse_transform = False\n                       )\n    boot_predictions = np.tile(boot_predictions, (n_boot, 1)).T\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    rng = np.random.default_rng(seed=random_state)\n    for i, step in enumerate(steps):\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions[i, :] = boot_predictions[i, :] + sample_residuals\n\n    if self.transformer_series_[self.level]:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_series_[self.level],\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = predictions.index,\n                           columns = boot_columns\n                       )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False, levels=None)</code>","text":"<p>Bootstrapping based predicted intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: list = [5, 95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(interval=interval)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )\n\n    predictions = self.predict(\n                      steps        = steps,\n                      last_window  = last_window,\n                      exog         = exog,\n                      check_inputs = False\n                  )\n\n    interval = np.array(interval) / 100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = [f'{self.level}_lower_bound', f'{self.level}_upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_quantiles","title":"<code>predict_quantiles(steps=None, last_window=None, exog=None, quantiles=[0.05, 0.5, 0.95], n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False, levels=None)</code>","text":"<p>Bootstrapping based predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>quantiles</code> <code>list</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>`[0.05, 0.5, 0.95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate quantiles.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot quantiles are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create quantiles. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> Notes <p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2<sup>nd</sup> ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    quantiles: list = [0.05, 0.5, 0.95],\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted quantiles.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, default `[0.05, 0.5, 0.95]`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate quantiles.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot quantiles are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create quantiles. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(quantiles=quantiles)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )\n\n    predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n    predictions.columns = [f'{self.level}_q_{q}' for q in quantiles]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, use_in_sample_residuals=True, suppress_warnings=False, levels=None)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate predictions.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot predictions are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions. If <code>False</code>, out of sample  residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate predictions.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot predictions are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. If `False`, out of sample \n        residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    boot_samples = self.predict_bootstrapping(\n                       steps                   = steps,\n                       last_window             = last_window,\n                       exog                    = exog,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters \n                   if not p == 'x'] + [\"loc\", \"scale\"]\n    param_values = np.apply_along_axis(\n                       lambda x: distribution.fit(x),\n                       axis = 1,\n                       arr  = boot_samples\n                   )\n\n    level_param_names = [f'{self.level}_{p}' for p in param_names]\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = level_param_names,\n                      index   = boot_samples.index\n                  )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and  <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series.  {'series_column_name': lags}.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, np.ndarray, list, dict]\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `max_lag` and \n    `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, dict\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. \n        {'series_column_name': lags}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        list_max_lags = []\n        for key in lags:\n            if lags[key] is None:\n                self.lags[key] = None\n            else:\n                self.lags[key], max_lag = initialize_lags(\n                    forecaster_name = type(self).__name__,\n                    lags            = lags[key]\n                )\n                list_max_lags.append(max_lag)\n        self.max_lag = max(list_max_lags)\n    else:\n        self.lags, self.max_lag = initialize_lags(\n            forecaster_name = type(self).__name__, \n            lags            = lags\n        )\n\n    self.lags_ = self.lags\n    self.window_size = self.max_lag\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals_</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool = True,\n    transform: bool = True,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals_`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals_ is None:\n        self.out_sample_residuals_ = {step: None \n                                      for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals_.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"Only residuals of models (steps) \"\n             f\"{set(self.out_sample_residuals_.keys()).intersection(set(residuals.keys()))} \"\n             f\"are updated.\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value for key, value in residuals.items()\n                 if key in self.out_sample_residuals_.keys()}\n\n    if not transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_series_[self.level]}. Ensure \"\n             f\"that the new residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used when \"\n             f\"training the forecaster ({self.transformer_series_[self.level]}). Ensure \"\n             f\"the new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_numpy(\n                                 array             = value,\n                                 transformer       = self.transformer_series_[self.level],\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals_[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals_[key]))\n            if len(value) &lt; free_space:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value\n                        ))\n            else:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals_[key] = value\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.get_feature_importances","title":"<code>get_feature_importances(step, sort_importance=True)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def get_feature_importances(\n    self,\n    step: int,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n    sort_importance: bool, default `True`\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f\"`step` must be an integer. Got {type(step)}.\"\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    len_columns_lags = len(list(\n        chain(*[v for v in self.lags_.values() if v is not None])\n    ))\n    idx_columns_lags = np.arange(len_columns_lags)\n    if self.exog_in_:\n        idx_columns_exog = np.flatnonzero(\n                               [name.endswith(f\"step_{step}\")\n                                for name in self.X_train_features_names_out_]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterRnn.html","title":"<code>ForecasterRnn</code>","text":""},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn","title":"<code>ForecasterRnn(regressor, levels, lags='auto', steps='auto', transformer_series=MinMaxScaler(feature_range=(0, 1)), weight_func=None, fit_kwargs={}, forecaster_id=None, n_jobs=None, transformer_exog=None)</code>","text":"<p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the Keras API into a Keras RNN multi-serie multi-step forecaster. A unique model is created to forecast all time steps and series. Keras enables workflows on top of  either JAX, TensorFlow, or PyTorch. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the Keras API</code> <p>An instance of a regressor or pipeline compatible with the Keras API.</p> required <code>levels</code> <code>(str, list)</code> <p>Name of one or more time series to be predicted. This determine the series the forecaster will be handling. If <code>None</code>, all series used during training will be available for prediction.</p> required <code>lags</code> <code>(int, list, str)</code> <p>Lags used as predictors. If 'auto', lags used are from 1 to N, where N is extracted from the input layer <code>self.regressor.layers[0].input_shape[0][1]</code>.</p> <code>`'auto'`</code> <code>transformer_series</code> <code>(object, dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. Transformation is applied to each <code>series</code> before training the forecaster. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series.</li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>`sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <code>steps</code> <code>(int, list, str)</code> <p>Steps to be predicted. If 'auto', steps used are from 1 to N, where N is extracted from the output layer <code>self.regressor.layers[-1].output_shape[1]</code>.</p> <code>`'auto'`</code> <code>lags</code> <code>Optional[Union[int, list, str]]</code> <p>Not used, present here for API consistency by convention.</p> <code>'auto'</code> <code>transformer_exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>weight_func</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>n_jobs</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the Keras API</code> <p>An instance of a regressor or pipeline compatible with the Keras API. An instance of this regressor is trained for each step. All of them are stored in <code>self.regressors_</code>.</p> <code>levels</code> <code>(str, list)</code> <p>Name of one or more time series to be predicted. This determine the series the forecaster will be handling. If <code>None</code>, all series used during training will be available for prediction.</p> <code>steps</code> <code>numpy ndarray</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_series</code> <code>(object, dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. Transformation is applied to each <code>series</code> before training the forecaster. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors.</p> <code>last_window_</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code> is used, the dtypes are calculated after the transformation.</p> <code>X_train_dim_names_</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> <code>y_train_dim_names_</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals()</code> method to set values.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>history</code> <code>dict</code> <p>Dictionary with the history of the training of each step. It is created internally to avoid overwriting.</p> <code>dropna_from_series</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>encoding</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiator</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    levels: Union[str, list],\n    lags: Optional[Union[int, list, str]] = \"auto\",\n    steps: Optional[Union[int, list, str]] = \"auto\",\n    transformer_series: Optional[Union[object, dict]] = MinMaxScaler(\n        feature_range=(0, 1)\n    ),\n    weight_func: Optional[Callable] = None,\n    fit_kwargs: Optional[dict] = {},\n    forecaster_id: Optional[Union[str, int]] = None,\n    n_jobs: Any = None,\n    transformer_exog: Any = None\n) -&gt; None:\n    self.levels = None\n    self.transformer_series = transformer_series\n    self.transformer_series_ = None\n    self.transformer_exog = None\n    self.weight_func = weight_func\n    self.source_code_weight_func = None\n    self.max_lag = None\n    self.window_size = None\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.exog_in_ = False\n    self.exog_type_in_ = None\n    self.exog_dtypes_in_ = None\n    self.exog_names_in_ = None\n    self.series_names_in_ = None\n    self.X_train_dim_names_ = None\n    self.y_train_dim_names_ = None\n    self.is_fitted = False\n    self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.fit_date = None\n    self.skforecast_version = skforecast.__version__\n    self.python_version = sys.version.split(\" \")[0]\n    self.forecaster_id = forecaster_id\n    self.history = None  # TODO: Change to history_ as come from fit method?\n    self.dropna_from_series = False  # Ignored in this forecaster\n    self.encoding = None   # Ignored in this forecaster\n    self.differentiation = None   # Ignored in this forecaster\n    self.differentiator = None   # Ignored in this forecaster\n\n    # Infer parameters from the model\n    self.regressor = regressor  # TODO: Create copy of regressor copy(regressor)\n    layer_init = self.regressor.layers[0]\n\n    if lags == \"auto\":\n        if keras.__version__ &lt; \"3.0\":\n            self.lags = np.arange(layer_init.input_shape[0][1]) + 1\n        else:\n            self.lags = np.arange(layer_init.output.shape[1]) + 1\n\n        warnings.warn(\n            \"Setting `lags` = 'auto'. `lags` are inferred from the regressor \" \n            \"architecture. Avoid the warning with lags=lags.\"\n        )\n    elif isinstance(lags, int):\n        self.lags = np.arange(lags) + 1\n    elif isinstance(lags, list):\n        self.lags = np.array(lags)\n    else:\n        raise TypeError(\n            f\"`lags` argument must be an int, list or 'auto'. Got {type(lags)}.\"\n        )\n\n    self.max_lag = np.max(self.lags)\n    self.window_size = self.max_lag\n\n    layer_end = self.regressor.layers[-1]\n\n    try:\n        if keras.__version__ &lt; \"3.0\":\n            self.series = layer_end.output_shape[-1]\n        else:\n            self.series = layer_end.output.shape[-1]\n    # if does not work, break the and raise an error the input shape should\n    # be shape=(lags, n_series))\n    except:\n        raise TypeError(\n            \"Input shape of the regressor should be Input(shape=(lags, n_series)).\"\n        )\n\n    if steps == \"auto\":\n        if keras.__version__ &lt; \"3.0\":\n            self.steps = np.arange(layer_end.output_shape[1]) + 1\n        else:\n            self.steps = np.arange(layer_end.output.shape[1]) + 1\n        warnings.warn(\n            \"`steps` default value = 'auto'. `steps` inferred from regressor \"\n            \"architecture. Avoid the warning with steps=steps.\"\n        )\n    elif isinstance(steps, int):\n        self.steps = np.arange(steps) + 1\n    elif isinstance(steps, list):\n        self.steps = np.array(steps)\n    else:\n        raise TypeError(\n            f\"`steps` argument must be an int, list or 'auto'. Got {type(steps)}.\"\n        )\n\n    self.max_step = np.max(self.steps)\n    if keras.__version__ &lt; \"3.0\":\n        self.outputs = layer_end.output_shape[-1]\n    else:\n        self.outputs = layer_end.output.shape[-1]\n\n    if not isinstance(levels, (list, str, type(None))):\n        raise TypeError(\n            f\"`levels` argument must be a string, list or. Got {type(levels)}.\"\n        )\n\n    if isinstance(levels, str):\n        self.levels = [levels]\n    elif isinstance(levels, list):\n        self.levels = levels\n    else:\n        raise TypeError(\n            f\"`levels` argument must be a string or a list. Got {type(levels)}.\"\n        )\n\n    self.series_val = None\n    if \"series_val\" in fit_kwargs:\n        self.series_val = fit_kwargs[\"series_val\"]\n        fit_kwargs.pop(\"series_val\")\n\n    self.fit_kwargs = check_select_fit_kwargs(\n        regressor=self.regressor, fit_kwargs=fit_kwargs\n    )\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 3d array (X) and a 3d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>3d numpy ndarray with the lagged values (predictors). Shape: (samples - max(lags), len(lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>3d numpy ndarray with the values of the time series related to each row of <code>X_data</code> for each step. Shape: (len(max_step), samples - max(lags))</p> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def _create_lags(\n    self,\n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Transforms a 1d array into a 3d array (X) and a 3d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        3d numpy ndarray with the lagged values (predictors).\n        Shape: (samples - max(lags), len(lags))\n    y_data : numpy ndarray\n        3d numpy ndarray with the values of the time series related to each\n        row of `X_data` for each step.\n        Shape: (len(max_step), samples - max(lags))\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - self.max_step + 1  # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (\n                f\"The maximum lag ({self.max_lag}) must be less than the length \"\n                f\"of the series minus the maximum of steps ({len(y)-self.max_step}).\"\n            )\n        )\n\n    X_data = np.full(\n        shape=(n_splits, (self.max_lag)), fill_value=np.nan, dtype=float\n    )\n    for i, lag in enumerate(range(self.max_lag - 1, -1, -1)):\n        X_data[:, i] = y[self.max_lag - lag - 1 : -(lag + self.max_step)]\n\n    y_data = np.full(\n        shape=(n_splits, self.max_step), fill_value=np.nan, dtype=float\n    )\n    for step in range(self.max_step):\n        y_data[:, step] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    # Get lags index\n    X_data = X_data[:, self.lags - 1]\n\n    # Get steps index\n    y_data = y_data[:, self.steps-1]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices. The resulting multi-dimensional matrices contain the target variable and predictors needed to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention. This type of forecaster does not allow exogenous variables.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>ndarray</code> <p>Training values (predictors) for each step. The resulting array has 3 dimensions: (time_points, n_lags, n_series)</p> <code>y_train</code> <code>ndarray</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. The resulting array has 3 dimensions: (time_points, n_steps, n_levels)</p> <code>dimension_names</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def create_train_X_y(\n    self, series: pd.DataFrame, exog: Any = None\n) -&gt; Tuple[np.ndarray, np.ndarray, dict]:\n    \"\"\"\n    Create training matrices. The resulting multi-dimensional matrices contain\n    the target variable and predictors needed to train the model.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : Ignored\n        Not used, present here for API consistency by convention. This type of\n        forecaster does not allow exogenous variables.\n\n    Returns\n    -------\n    X_train : np.ndarray\n        Training values (predictors) for each step. The resulting array has\n        3 dimensions: (time_points, n_lags, n_series)\n    y_train : np.ndarray\n        Values (target) of the time series related to each row of `X_train`.\n        The resulting array has 3 dimensions: (time_points, n_steps, n_levels)\n    dimension_names : dict\n        Labels for the multi-dimensional arrays created internally for training.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    series_names_in_ = list(series.columns)\n\n    if not set(self.levels).issubset(set(series.columns)):\n        raise ValueError(\n            (\n                f\"`levels` defined when initializing the forecaster must be included \"\n                f\"in `series` used for trainng. {set(self.levels) - set(series.columns)} \"\n                f\"not found.\"\n            )\n        )\n\n    if len(series) &lt; self.max_lag + self.max_step:\n        raise ValueError(\n            (\n                f\"Minimum length of `series` for training this forecaster is \"\n                f\"{self.max_lag + self.max_step}. Got {len(series)}. Reduce the \"\n                f\"number of predicted steps, {self.max_step}, or the maximum \"\n                f\"lag, {self.max_lag}, if no more data is available.\"\n            )\n        )\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {\n            serie: clone(self.transformer_series) for serie in series_names_in_\n        }\n    else:\n        self.transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v)\n            for k, v in deepcopy(self.transformer_series).items()\n            if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = set(series.columns) - set(\n            self.transformer_series.keys()\n        )\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (\n                    f\"{series_not_in_transformer_series} not present in \"\n                    f\"`transformer_series`. No transformation is applied to \"\n                    f\"these series.\"\n                ),\n                IgnoredArgumentWarning,\n            )\n\n    # Step 1: Create lags for all columns\n    X_train = []\n    y_train = []\n\n    for i, serie in enumerate(series.columns):\n        x = series[serie]\n        check_y(y=x)\n        x = transform_series(\n            series=x,\n            transformer=self.transformer_series_[serie],\n            fit=True,\n            inverse_transform=False,\n        )\n        X, _ = self._create_lags(x)\n        X_train.append(X)\n\n    for i, serie in enumerate(self.levels):\n        y = series[serie]\n        check_y(y=y)\n        y = transform_series(\n            series=y,\n            transformer=self.transformer_series_[serie],\n            fit=True,\n            inverse_transform=False,\n        )\n\n        _, y = self._create_lags(y)\n        y_train.append(y)\n\n    X_train = np.stack(X_train, axis=2)\n    y_train = np.stack(y_train, axis=2)\n\n    train_index = series.index.to_list()[\n        self.max_lag : (len(series.index.to_list()) - self.max_step + 1)\n    ]\n    dimension_names = {\n        \"X_train\": {\n            0: train_index,\n            1: [\"lag_\" + str(l) for l in self.lags],\n            2: series.columns.to_list(),\n        },\n        \"y_train\": {\n            0: train_index,\n            1: [\"step_\" + str(l) for l in self.steps],\n            2: self.levels,\n        },\n    }\n\n    return X_train, y_train, dimension_names\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.fit","title":"<code>fit(series, store_in_sample_residuals=True, exog=None, suppress_warnings=False, store_last_window='Ignored')</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>`True`</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>store_last_window</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>'Ignored'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    store_in_sample_residuals: bool = True,\n    exog: Any = None,\n    suppress_warnings: bool = False,\n    store_last_window: str = \"Ignored\",\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor\n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    store_last_window : Ignored\n        Not used, present here for API consistency by convention.\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.last_window_ = None\n    self.exog_in_ = None\n    self.exog_type_in_ = None\n    self.exog_dtypes_in_ = None\n    self.exog_names_in_ = None\n    self.series_names_in_ = None\n    self.X_train_dim_names_ = None\n    self.y_train_dim_names_ = None\n    self.in_sample_residuals_ = None\n    self.is_fitted = False\n    self.training_range_ = None\n\n    self.series_names_in_ = list(series.columns)\n\n    X_train, y_train, X_train_dim_names_ = self.create_train_X_y(series=series)\n    self.X_train_dim_names_ = X_train_dim_names_[\"X_train\"]\n    self.y_train_dim_names_ = X_train_dim_names_[\"y_train\"]\n\n    if self.series_val is not None:\n        X_val, y_val, _ = self.create_train_X_y(series=self.series_val)\n        history = self.regressor.fit(\n            x=X_train, y=y_train, validation_data=(X_val, y_val), **self.fit_kwargs\n        )\n    else:\n        history = self.regressor.fit(x=X_train, y=y_train, **self.fit_kwargs)\n\n    self.history = history.history\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    _, y_index = preprocess_y(y=series[self.levels], return_values=False)\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    if isinstance(y_index, pd.DatetimeIndex):\n        self.index_freq_ = y_index.freqstr\n    else:\n        self.index_freq_ = y_index.step\n\n    self.last_window_ = series.iloc[-self.max_lag :].copy()\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.predict","title":"<code>predict(steps=None, levels=None, last_window=None, exog=None, suppress_warnings=False)</code>","text":"<p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at initialization.</li> </ul> <code>`None`</code> <code>levels</code> <code>(str, list)</code> <p>Name of one or more time series to be predicted. It must be included in <code>levels</code> defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the fitting  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]] = None,\n    levels: Optional[Union[str, list]] = None,\n    last_window: Optional[pd.DataFrame] = None,\n    exog: Any = None,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the\n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list\n        are predicted.\n        - If `None`: As many steps are predicted as were defined at\n        initialization.\n    levels : str, list, default `None`\n        Name of one or more time series to be predicted. It must be included\n        in `levels` defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the\n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the fitting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if levels is None:\n        levels = self.levels\n    elif isinstance(levels, str):\n        levels = [levels]\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        if isinstance(self.steps, int):\n            steps = list(np.arange(self.steps) + 1)\n        elif isinstance(self.steps, (list, np.ndarray)):\n            steps = list(np.array(self.steps))\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (\n                    f\"`steps` argument must be an int, a list of ints or `None`. \"\n                    f\"Got {type(steps)}.\"\n                )\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name=type(self).__name__,\n        steps=steps,\n        is_fitted=self.is_fitted,\n        exog_in_=self.exog_in_,\n        index_type_=self.index_type_,\n        index_freq_=self.index_freq_,\n        window_size=self.window_size,\n        last_window=last_window,\n        exog=None,\n        exog_type_in_=None,\n        exog_names_in_=None,\n        interval=None,\n        max_steps=self.max_step,\n        levels=levels,\n        levels_forecaster=self.levels,\n        series_names_in_=self.series_names_in_,\n    )\n\n    last_window = last_window.iloc[-self.window_size :,].copy()\n\n    for serie_name in self.series_names_in_:\n        last_window_serie = transform_series(\n            series=last_window[serie_name],\n            transformer=self.transformer_series_[serie_name],\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_values, last_window_index = preprocess_last_window(\n            last_window=last_window_serie\n        )\n        last_window.loc[:, serie_name] = last_window_values\n\n    X = np.reshape(last_window.to_numpy(), (1, self.max_lag, last_window.shape[1]))\n    predictions = self.regressor.predict(X, verbose=0)\n    predictions_reshaped = np.reshape(\n        predictions, (predictions.shape[1], predictions.shape[2])\n    )\n\n    # if len(self.levels) == 1:\n    #     predictions_reshaped = np.reshape(predictions, (predictions.shape[1], 1))\n    # else:\n    #     predictions_reshaped = np.reshape(\n    #         predictions, (predictions.shape[1], predictions.shape[2])\n    #     )\n    idx = expand_index(index=last_window_index, steps=max(steps))\n\n    predictions = pd.DataFrame(\n        data=predictions_reshaped[np.array(steps) - 1],\n        columns=self.levels,\n        index=idx[np.array(steps) - 1],\n    )\n    predictions = predictions[levels]\n\n    for serie in levels:\n        x = predictions[serie]\n        check_y(y=x)\n        x = transform_series(\n            series=x,\n            transformer=self.transformer_series_[serie],\n            fit=False,\n            inverse_transform=True,\n        )\n        predictions.loc[:, serie] = x\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.plot_history","title":"<code>plot_history(ax=None, **fig_kw)</code>","text":"<p>Plots the training and validation loss curves from the given history object stores in the ForecasterRnn.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() internally.</p> <code>`None`</code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def plot_history(\n    self, ax: matplotlib.axes.Axes = None, **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Plots the training and validation loss curves from the given history object stores\n    in the ForecasterRnn.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes, default `None`\n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()\n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots().\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n    else:\n        fig = ax.get_figure()\n\n    # Setting up the plot style\n\n    if self.history is None:\n        raise ValueError(\"ForecasterRnn has not been fitted yet.\")\n\n    # Plotting training loss\n    ax.plot(\n        range(1, len(self.history[\"loss\"]) + 1),\n        self.history[\"loss\"],\n        color=\"b\",\n        label=\"Training Loss\",\n    )\n\n    # Plotting validation loss\n    if \"val_loss\" in self.history:\n        ax.plot(\n            range(1, len(self.history[\"val_loss\"]) + 1),\n            self.history[\"val_loss\"],\n            color=\"r\",\n            label=\"Validation Loss\",\n        )\n\n    # Labeling the axes and adding a title\n    ax.set_xlabel(\"Epochs\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_title(\"Training and Validation Loss\")\n\n    # Adding a legend\n    ax.legend()\n\n    # Displaying grid for better readability\n    ax.grid(True, linestyle=\"--\", alpha=0.7)\n\n    # Setting x-axis ticks to integers only\n    ax.set_xticks(range(1, len(self.history[\"loss\"]) + 1))\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:  # TODO testear\n    \"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same\n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.reset_states()\n    self.regressor.compile(**params)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def set_fit_kwargs(\n    self,\n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit`\n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.ForecasterRnn.ForecasterRnn.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Not used, present here for API consistency by convention.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterRnn/ForecasterRnn.py</code> <pre><code>def set_lags(\n    self, \n    lags: Any\n) -&gt; None:\n    \"\"\"\n    Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.ForecasterRnn.utils.create_and_compile_model","title":"<code>create_and_compile_model(series, lags, steps, levels=None, recurrent_layer='LSTM', recurrent_units=100, dense_units=64, activation='relu', optimizer=Adam(learning_rate=0.01), loss=MeanSquaredError(), compile_kwargs={})</code>","text":"<p>Creates a neural network model for time series prediction with flexible recurrent layers.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Input time series.</p> required <code>lags</code> <code>(int, list)</code> <p>Number of lagged time steps to consider in the input, or a list of  specific lag indices.</p> required <code>steps</code> <code>(int, list)</code> <p>Number of steps to predict into the future, or a list of specific step  indices.</p> required <code>levels</code> <code>(str, int, list)</code> <p>Number of output levels (features) to predict, or a list of specific  level indices. If None, defaults to the number of input series.</p> <code>`None`</code> <code>recurrent_layer</code> <code>str</code> <p>Type of recurrent layer to be used ('LSTM' or 'RNN').</p> <code>`'LSTM'`</code> <code>recurrent_units</code> <code>(int, list)</code> <p>Number of units in the recurrent layer(s). Can be an integer or a  list of integers for multiple layers.</p> <code>`100`</code> <code>dense_units</code> <code>(int, list)</code> <p>List of integers representing the number of units in each dense layer.</p> <code>`64`</code> <code>activation</code> <code>str</code> <p>Activation function for the recurrent and dense layers.</p> <code>`'relu'`</code> <code>optimizer</code> <code>object</code> <p>Optimization algorithm and learning rate.</p> <code>`Adam(learning_rate=0.01)`</code> <code>loss</code> <code>object</code> <p>Loss function for model training.</p> <code>`MeanSquaredError()`</code> <code>compile_kwargs</code> <code>dict</code> <p>Additional arguments for model compilation.</p> <code>`{}` </code> <p>Returns:</p> Name Type Description <code>model</code> <code>Model</code> <p>Compiled neural network model.</p> Source code in <code>skforecast/ForecasterRnn/utils.py</code> <pre><code>def create_and_compile_model(\n    series: pd.DataFrame,\n    lags: Union[int, list],\n    steps: Union[int, list],\n    levels: Optional[Union[str, int, list]]=None,\n    recurrent_layer: str=\"LSTM\",\n    recurrent_units: Union[int, list]=100,\n    dense_units: Union[int, list]=64,\n    activation: str=\"relu\",\n    optimizer: object=Adam(learning_rate=0.01),\n    loss: object=MeanSquaredError(),\n    compile_kwargs: dict={},\n) -&gt; keras.models.Model:\n    \"\"\"\n    Creates a neural network model for time series prediction with flexible recurrent layers.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Input time series.\n    lags : int, list\n        Number of lagged time steps to consider in the input, or a list of \n        specific lag indices.\n    steps : int, list\n        Number of steps to predict into the future, or a list of specific step \n        indices.\n    levels : str, int, list, default `None`\n        Number of output levels (features) to predict, or a list of specific \n        level indices. If None, defaults to the number of input series.\n    recurrent_layer : str, default `'LSTM'`\n        Type of recurrent layer to be used ('LSTM' or 'RNN').\n    recurrent_units : int, list, default `100`\n        Number of units in the recurrent layer(s). Can be an integer or a \n        list of integers for multiple layers.\n    dense_units : int, list, default `64`\n        List of integers representing the number of units in each dense layer.\n    activation : str, default `'relu'`\n        Activation function for the recurrent and dense layers.\n    optimizer : object, default `Adam(learning_rate=0.01)`\n        Optimization algorithm and learning rate.\n    loss : object, default `MeanSquaredError()`\n        Loss function for model training.\n    compile_kwargs : dict, default `{}` \n        Additional arguments for model compilation.\n\n    Returns\n    -------\n    model : keras.models.Model\n        Compiled neural network model.\n\n    \"\"\"\n\n    if keras.__version__ &gt; \"3\":\n        print(f\"keras version: {keras.__version__}\")\n        print(f\"Using backend: {keras.backend.backend()}\")\n        if keras.backend.backend() == \"tensorflow\":\n            import tensorflow\n            print(f\"tensorflow version: {tensorflow.__version__}\")\n        elif keras.backend.backend() == \"torch\":\n            import torch\n            print(f\"torch version: {torch.__version__}\")\n        elif keras.backend.backend() == \"jax\":\n            import jax\n            print(f\"jax version: {jax.__version__}\")\n        else:\n            print(\"Backend not recognized\")\n\n    err_msg = f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(err_msg)\n\n    n_series = series.shape[1]\n\n    # Dense units must be a list, None or int\n    if not isinstance(dense_units, (list, int, type(None))):\n        raise TypeError(\n            f\"`dense_units` argument must be a list or int. Got {type(dense_units)}.\"\n        )\n    if isinstance(dense_units, int):\n        dense_units = [dense_units]\n\n    # Recurrent units must be a list or int\n    if not isinstance(recurrent_units, (list, int)):\n        raise TypeError(\n            f\"`recurrent_units` argument must be a list or int. Got {type(recurrent_units)}.\"\n        )\n    if isinstance(recurrent_units, int):\n        recurrent_units = [recurrent_units]\n\n    # Lags, steps and levels must be int or list\n    if not isinstance(lags, (int, list)):\n        raise TypeError(f\"`lags` argument must be a list or int. Got {type(lags)}.\")\n    if not isinstance(steps, (int, list)):\n        raise TypeError(f\"`steps` argument must be a list or int. Got {type(steps)}.\")\n    if not isinstance(levels, (str, int, list, type(None))):\n        raise TypeError(\n            f\"`levels` argument must be a string, list or int. Got {type(levels)}.\"\n        )\n\n    if isinstance(lags, list):\n        lags = len(lags)\n    if isinstance(steps, list):\n        steps = len(steps)\n    if isinstance(levels, list):\n        levels = len(levels)\n    elif isinstance(levels, (str)):\n        levels = 1\n    elif isinstance(levels, type(None)):\n        levels = series.shape[1]\n    elif isinstance(levels, int):\n        pass\n    else:\n        raise TypeError(\n            f\"`levels` argument must be a string, list or int. Got {type(levels)}.\"\n        )\n\n    input_layer = Input(shape=(lags, n_series))\n    x = input_layer\n\n    # Dynamically create multiple recurrent layers if recurrent_units is a list\n    if isinstance(recurrent_units, list):\n        for units in recurrent_units[:-1]:  # All layers except the last one\n            if recurrent_layer == \"LSTM\":\n                x = LSTM(units, activation=activation, return_sequences=True)(x)\n            elif recurrent_layer == \"RNN\":\n                x = SimpleRNN(units, activation=activation, return_sequences=True)(x)\n            else:\n                raise ValueError(f\"Invalid recurrent layer: {recurrent_layer}\")\n        # Last layer without return_sequences\n        if recurrent_layer == \"LSTM\":\n            x = LSTM(recurrent_units[-1], activation=activation)(x)\n        elif recurrent_layer == \"RNN\":\n            x = SimpleRNN(recurrent_units[-1], activation=activation)(x)\n        else:\n            raise ValueError(f\"Invalid recurrent layer: {recurrent_layer}\")\n    else:\n        # Single recurrent layer\n        if recurrent_layer == \"LSTM\":\n            x = LSTM(recurrent_units, activation=activation)(x)\n        elif recurrent_layer == \"RNN\":\n            x = SimpleRNN(recurrent_units, activation=activation)(x)\n        else:\n            raise ValueError(f\"Invalid recurrent layer: {recurrent_layer}\")\n\n    # Dense layers\n    if dense_units is not None:\n        for nn in dense_units:\n            x = Dense(nn, activation=activation)(x)\n\n    # Output layer\n    x = Dense(levels * steps, activation=\"linear\")(x)\n    # model = Model(inputs=input_layer, outputs=x)\n    output_layer = keras.layers.Reshape((steps, levels))(x)\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n    # Compile the model if optimizer, loss or compile_kwargs are passed\n    if optimizer is not None or loss is not None or compile_kwargs:\n        # give more priority to the parameters passed in the function check if the \n        # parameters passes in compile_kwargs include optimizer and loss if so, \n        # delete them from compile_kwargs and raise a warning\n        if \"optimizer\" in compile_kwargs.keys():\n            compile_kwargs.pop(\"optimizer\")\n            warnings.warn(\"`optimizer` passed in `compile_kwargs`. Ignoring it.\")\n        if \"loss\" in compile_kwargs.keys():\n            compile_kwargs.pop(\"loss\")\n            warnings.warn(\"`loss` passed in `compile_kwargs`. Ignoring it.\")\n\n        model.compile(optimizer=optimizer, loss=loss, **compile_kwargs)\n\n    return model\n</code></pre>"},{"location":"api/ForecasterSarimax.html","title":"<code>ForecasterSarimax</code>","text":""},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax","title":"<code>ForecasterSarimax(regressor, transformer_y=None, transformer_exog=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>This class turns Sarimax model from the skforecast library into a Forecaster  compatible with the skforecast API. New in version 0.10.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>Sarimax</code> <p>A Sarimax model instance from skforecast.</p> required <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.  When using the skforecast Sarimax model, the fit kwargs should be passed  using the model parameter <code>sm_fit_kwargs</code> and not this one.</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>Sarimax</code> <p>A Sarimax model instance from skforecast.</p> <code>params</code> <code>dict</code> <p>Parameters of the sarimax model.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>window_size</code> <code>int</code> <p>Not used, present here for API consistency by convention.</p> <code>last_window_</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>extended_index_</code> <code>pandas Index</code> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_. Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    transformer_y: Optional[object] = None,\n    transformer_exog: Optional[object] = None,\n    fit_kwargs: Optional[dict] = None,\n    forecaster_id: Optional[Union[str, int]] = None\n) -&gt; None:\n\n    self.regressor               = copy(regressor)\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.window_size             = 1\n    self.last_window_            = None\n    self.extended_index_         = None\n    self.index_type_             = None\n    self.index_freq_             = None\n    self.training_range_         = None\n    self.exog_in_                = False\n    self.exog_names_in_          = None\n    self.exog_type_in_           = None\n    self.exog_dtypes_in_         = None\n    self.X_train_exog_names_out_ = None\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted               = False\n    self.fit_date                = None\n    self.skforecast_version      = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(self.regressor, skforecast.Sarimax.Sarimax):\n        raise TypeError(\n            (f\"`regressor` must be an instance of type \"\n             f\"`skforecast.Sarimax.Sarimax`. Got '{type(regressor)}'.\")\n        )\n\n    self.params = self.regressor.get_params(deep=True)\n\n    if fit_kwargs:\n        warnings.warn(\n            (\"When using the skforecast Sarimax model, the fit kwargs should \"\n                \"be passed using the model parameter `sm_fit_kwargs`.\"),\n                IgnoredArgumentWarning\n        )\n    self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.fit","title":"<code>fit(y, exog=None, store_last_window=True, suppress_warnings=False)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>`False`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    store_last_window: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default `True`\n        Whether or not to store the last window (`last_window_`) of training data.\n    suppress_warnings : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_y(y=y)\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        check_exog(exog=exog)\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_            = None\n    self.extended_index_         = None\n    self.index_type_             = None\n    self.index_freq_             = None\n    self.training_range_         = None\n    self.exog_in_                = False\n    self.exog_names_in_          = None\n    self.exog_type_in_           = None\n    self.exog_dtypes_in_         = None\n    self.X_train_exog_names_out_ = None\n    self.in_sample_residuals_    = None\n    self.is_fitted               = False\n    self.fit_date                = None\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n        self.exog_names_in_ = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = True,\n                   inverse_transform = False\n               )\n        self.X_train_exog_names_out_ = exog.columns.to_list()\n\n    if suppress_warnings:\n        warnings.filterwarnings(\"ignore\")\n\n    self.regressor.fit(y=y, exog=exog)\n\n    if suppress_warnings:\n        warnings.filterwarnings(\"default\")\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y.index[[0, -1]]\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else: \n        self.index_freq_ = y.index.step\n\n    if store_last_window:\n        self.last_window_ = y.copy()\n\n    self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index.copy()\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax._create_predict_inputs","title":"<code>_create_predict_inputs(steps, last_window=None, last_window_exog=None, exog=None)</code>","text":"<p>Create inputs needed for the first iteration of the prediction process.  Since it is a recursive process, last window is updated at each  iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>last_window</code> <code>pandas Series</code> <p>Series predictors.</p> <code>last_window_exog</code> <code>pandas DataFrame, default `None`</code> <p>Values of the exogenous variables aligned with <code>last_window</code>.</p> <code>exog</code> <code>pandas DataFrame, default `None`</code> <p>Exogenous variable/s included as predictor/s.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; Tuple[\n        pd.Series,\n        Optional[pd.DataFrame],\n        Optional[pd.DataFrame],\n    ]:\n    \"\"\"\n    Create inputs needed for the first iteration of the prediction process. \n    Since it is a recursive process, last window is updated at each \n    iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    last_window : pandas Series\n        Series predictors.\n    last_window_exog : pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`.\n    exog : pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append when using \n    # self.last_window. It already has it stored.\n    last_window_check = last_window if last_window is not None else self.last_window_\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        is_fitted        = self.is_fitted,\n        exog_in_         = self.exog_in_,\n        index_type_      = self.index_type_,\n        index_freq_      = self.index_freq_,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_type_in_    = self.exog_type_in_,\n        exog_names_in_   = self.exog_names_in_,\n        interval         = None,\n        alpha            = None\n    )\n\n    # If not last_window is provided, last_window needs to be None\n    if last_window is not None:\n        last_window = last_window.copy()\n\n    # When last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            (\"To make predictions unrelated to the original data, both \"\n             \"`last_window` and `last_window_exog` must be provided.\")\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.exog_in_:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. To make predictions \"\n             \"unrelated to the original data, same variable/s must be provided \"\n             \"using `last_window_exog`.\")\n        )\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # Check index append values\n        expected_index = expand_index(index=self.extended_index_, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                (f\"To make predictions unrelated to the original data, `last_window` \"\n                 f\"has to start at the end of the index seen by the forecaster.\\n\"\n                 f\"    Series last index         : {self.extended_index_[-1]}.\\n\"\n                 f\"    Expected index            : {expected_index}.\\n\"\n                 f\"    `last_window` index start : {last_window.index[0]}.\")\n            )\n\n        last_window = transform_series(\n                          series            = last_window,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # Transform last_window_exog\n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    (f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                     f\"has to start at the end of the index seen by the forecaster.\\n\"\n                     f\"    Series last index              : {self.extended_index_[-1]}.\\n\"\n                     f\"    Expected index                 : {expected_index}.\\n\"\n                     f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\")\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    return last_window, last_window_exog, exog\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict","title":"<code>predict(steps, last_window=None, last_window_exog=None, exog=None)</code>","text":"<p>Forecast future values.</p> <p>Generate predictions (forecasts) n steps in the future. Note that if  exogenous variables were used in the model fit, they will be expected  for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n) -&gt; pd.Series:\n    \"\"\"\n    Forecast future values.\n\n    Generate predictions (forecasts) n steps in the future. Note that if \n    exogenous variables were used in the model fit, they will be expected \n    for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index_.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    last_window, last_window_exog, exog = self._create_predict_inputs(\n                                              steps            = steps,\n                                              last_window      = last_window,\n                                              last_window_exog = last_window_exog,\n                                              exog             = exog,\n                                          )\n\n    if last_window is not None:\n        self.regressor.append(\n            y     = last_window,\n            exog  = last_window_exog,\n            refit = False\n        )\n        self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index\n\n    # Get following n steps predictions\n    predictions = self.regressor.predict(\n                      steps = steps,\n                      exog  = exog\n                  ).iloc[:, 0]\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n    predictions.name = 'pred'\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict_interval","title":"<code>predict_interval(steps, last_window=None, last_window_exog=None, exog=None, alpha=0.05, interval=None)</code>","text":"<p>Forecast future values and their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only need when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>`0.05`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series] = None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    alpha: float = 0.05,\n    interval: list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Forecast future values and their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index_.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        need when `last_window` is not None and the forecaster has been\n        trained including exogenous variables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    # If interval and alpha take alpha, if interval transform to alpha\n    if alpha is None:\n        if 100 - interval[1] != interval[0]:\n            raise ValueError(\n                (f\"When using `interval` in ForecasterSarimax, it must be symmetrical. \"\n                 f\"For example, interval of 95% should be as `interval = [2.5, 97.5]`. \"\n                 f\"Got {interval}.\")\n            )\n        alpha = 2 * (100 - interval[1]) / 100\n\n    last_window, last_window_exog, exog = self._create_predict_inputs(\n                                              steps            = steps,\n                                              last_window      = last_window,\n                                              last_window_exog = last_window_exog,\n                                              exog             = exog,\n                                          )\n\n    if last_window is not None:\n        self.regressor.append(\n            y     = last_window,\n            exog  = last_window_exog,\n            refit = False\n        )\n        self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index\n\n    # Get following n steps predictions with intervals\n    predictions = self.regressor.predict(\n                      steps = steps,\n                      exog  = exog,\n                      return_conf_int = True,\n                      alpha = alpha\n                  )\n\n    if self.transformer_y:\n        predictions = predictions.apply(lambda col: transform_series(\n                          series            = col,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      ))\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the model stored in the forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    warnings.warn(\n        (\"When using the skforecast Sarimax model, the fit kwargs should \"\n         \"be passed using the model parameter `sm_fit_kwargs`.\"),\n         IgnoredArgumentWarning\n    )\n    self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_feature_importances","title":"<code>get_feature_importances(sort_importance=True)</code>","text":"<p>Return feature importances of the regressor stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n\n    Parameters\n    ----------\n    sort_importance: bool, default `True`\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    feature_importances = self.regressor.params().to_frame().reset_index()\n    feature_importances.columns = ['feature', 'importance']\n\n    if sort_importance:\n        feature_importances = feature_importances.sort_values(\n                                  by='importance', ascending=False\n                              )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_info_criteria","title":"<code>get_info_criteria(criteria='aic', method='standard')</code>","text":"<p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>`'aic'`</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>`'standard'`</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def get_info_criteria(\n    self, \n    criteria: str = 'aic', \n    method: str = 'standard'\n) -&gt; float:\n    \"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default `'aic'`\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default `'standard'`\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            (\"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n             \"and 'hqic'.\")\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            (\"Invalid value for `method`. Valid options are 'standard' and \"\n             \"'lutkepohl'.\")\n        )\n\n    metric = self.regressor.get_info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.summary","title":"<code>summary()</code>","text":"<p>Show forecaster information.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/Sarimax.html","title":"<code>Sarimax</code>","text":""},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax","title":"<code>Sarimax(order=(1, 0, 0), seasonal_order=(0, 0, 0, 0), trend=None, measurement_error=False, time_varying_regression=False, mle_regression=True, simple_differencing=False, enforce_stationarity=True, enforce_invertibility=True, hamilton_representation=False, concentrate_scale=False, trend_offset=1, use_exact_diffuse=False, dates=None, freq=None, missing='none', validate_specification=True, method='lbfgs', maxiter=50, start_params=None, disp=False, sm_init_kwargs={}, sm_fit_kwargs={}, sm_predict_kwargs={})</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A universal scikit-learn style wrapper for statsmodels SARIMAX.</p> <p>This class wraps the statsmodels.tsa.statespace.sarimax.SARIMAX model to  follow the scikit-learn style. The following docstring is based on the  statmodels documentation and it is highly recommended to visit their site  for the best level of detail.</p> <p>https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.html</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters. </p> <ul> <li><code>d</code> must be an integer indicating the integration order of the process.</li> <li><code>p</code> and <code>q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include.</li> </ul> <code>`(1, 0, 0)`</code> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity. </p> <ul> <li><code>D</code> must be an integer indicating the integration order of the process.</li> <li><code>P</code> and <code>Q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include. </li> <li><code>s</code> is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data.</li> </ul> <code>`(0, 0, 0, 0)`</code> <code>trend</code> <code>str</code> <p>Parameter controlling the deterministic trend polynomial <code>A(t)</code>.</p> <ul> <li><code>'c'</code> indicates a constant (i.e. a degree zero component of the  trend polynomial).</li> <li><code>'t'</code> indicates a linear trend with time.</li> <li><code>'ct'</code> indicates both, <code>'c'</code> and <code>'t'</code>. </li> <li>Can also be specified as an iterable defining the non-zero polynomial  exponents to include, in increasing order. For example, <code>[1,1,0,1]</code>  denotes <code>a + b*t + ct^3</code>.</li> </ul> <code>`None`</code> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>`False`</code> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>`False`</code> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>`True`</code> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation. </p> <ul> <li>If <code>True</code>, differencing is performed prior to estimation, which  discards the first <code>s*D + d</code> initial rows but results in a smaller  state-space formulation. </li> <li>If <code>False</code>, the full SARIMAX model is put in state-space form so  that all datapoints can be used in estimation.</li> </ul> <code>`False`</code> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>`True`</code> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>`True`</code> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>`False`</code> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>`False`</code> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values. Default is 1, so that if <code>trend='t'</code> the trend is equal to 1, 2, ..., nobs. Typically is only set when the model created by extending a previous dataset.</p> <code>`1`</code> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states. Default is <code>False</code> (in which case approximate diffuse initialization is used).</p> <code>`False`</code> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used, and it  can be chosen from among the following strings:</p> <ul> <li><code>'newton'</code> for Newton-Raphson</li> <li><code>'nm'</code> for Nelder-Mead</li> <li><code>'bfgs'</code> for Broyden-Fletcher-Goldfarb-Shanno (BFGS)</li> <li><code>'lbfgs'</code> for limited-memory BFGS with optional box constraints</li> <li><code>'powell'</code> for modified Powell`s method</li> <li><code>'cg'</code> for conjugate gradient</li> <li><code>'ncg'</code> for Newton-conjugate gradient</li> <li><code>'basinhopping'</code> for global basin-hopping solver</li> </ul> <code>`'lbfgs'`</code> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>`50`</code> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.  If <code>None</code>, the default is given by regressor.start_params.</p> <code>`None`</code> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>`False`</code> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>`{}`</code> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model. The statsmodels SARIMAX.fit parameters  <code>method</code>, <code>max_iter</code>, <code>start_params</code> and <code>disp</code> have been moved to the  initialization of this model and will have priority over those provided  by the user using via <code>sm_fit_kwargs</code>.</p> <code>`{}` </code> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>`{}`</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters.</p> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity.</p> <code>trend</code> <code>str</code> <p>Deterministic trend polynomial <code>A(t)</code>.</p> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation.</p> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values.</p> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states.</p> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used.</p> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.</p> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model.</p> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>_sarimax_params</code> <code>dict</code> <p>Parameters of this model that can be set with the <code>set_params</code> method.</p> <code>output_type</code> <code>str</code> <p>Format of the object returned by the predict method. This is set  automatically according to the type of <code>y</code> used in the fit method to  train the model, <code>'numpy'</code> or <code>'pandas'</code>.</p> <code>sarimax</code> <code>object</code> <p>The statsmodels.tsa.statespace.sarimax.SARIMAX object created.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>sarimax_res</code> <code>object</code> <p>The resulting statsmodels.tsa.statespace.sarimax.SARIMAXResults object  created by statsmodels after fitting the SARIMAX model.</p> <code>training_index</code> <code>pandas Index</code> <p>Index of the training series as long as it is a pandas Series or Dataframe.</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>def __init__(\n    self,\n    order: tuple = (1, 0, 0),\n    seasonal_order: tuple = (0, 0, 0, 0),\n    trend: str = None,\n    measurement_error: bool = False,\n    time_varying_regression: bool = False,\n    mle_regression: bool = True,\n    simple_differencing: bool = False,\n    enforce_stationarity: bool = True,\n    enforce_invertibility: bool = True,\n    hamilton_representation: bool = False,\n    concentrate_scale: bool = False,\n    trend_offset: int = 1,\n    use_exact_diffuse: bool = False,\n    dates = None,\n    freq = None,\n    missing = 'none',\n    validate_specification: bool = True,\n    method: str = 'lbfgs',\n    maxiter: int = 50,\n    start_params: np.ndarray = None,\n    disp: bool = False,\n    sm_init_kwargs: dict = {},\n    sm_fit_kwargs: dict = {},\n    sm_predict_kwargs: dict = {}\n) -&gt; None:\n\n    self.order                   = order\n    self.seasonal_order          = seasonal_order\n    self.trend                   = trend\n    self.measurement_error       = measurement_error\n    self.time_varying_regression = time_varying_regression\n    self.mle_regression          = mle_regression\n    self.simple_differencing     = simple_differencing\n    self.enforce_stationarity    = enforce_stationarity\n    self.enforce_invertibility   = enforce_invertibility\n    self.hamilton_representation = hamilton_representation\n    self.concentrate_scale       = concentrate_scale\n    self.trend_offset            = trend_offset\n    self.use_exact_diffuse       = use_exact_diffuse\n    self.dates                   = dates\n    self.freq                    = freq\n    self.missing                 = missing\n    self.validate_specification  = validate_specification\n    self.method                  = method\n    self.maxiter                 = maxiter\n    self.start_params            = start_params\n    self.disp                    = disp\n\n    # Create the dictionaries with the additional statsmodels parameters to be  \n    # used during the init, fit and predict methods. Note that the statsmodels \n    # SARIMAX.fit parameters `method`, `max_iter`, `start_params` and `disp` \n    # have been moved to the initialization of this model and will have \n    # priority over those provided by the user using via `sm_fit_kwargs`.\n    self.sm_init_kwargs    = sm_init_kwargs\n    self.sm_fit_kwargs     = sm_fit_kwargs\n    self.sm_predict_kwargs = sm_predict_kwargs\n\n    # Params that can be set with the `set_params` method\n    _, _, _, _sarimax_params = inspect.getargvalues(inspect.currentframe())\n    _sarimax_params.pop(\"self\")\n    self._sarimax_params = _sarimax_params\n\n    self._consolidate_kwargs()\n\n    # Create Results Attributes \n    self.output_type    = None\n    self.sarimax        = None\n    self.fitted         = False\n    self.sarimax_res    = None\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax._consolidate_kwargs","title":"<code>_consolidate_kwargs()</code>","text":"<p>Create the dictionaries to be used during the init, fit, and predict methods. Note that the parameters in this model's initialization take precedence  over those provided by the user using via the statsmodels kwargs dicts.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>def _consolidate_kwargs(\n    self\n) -&gt; None:\n    \"\"\"\n    Create the dictionaries to be used during the init, fit, and predict methods.\n    Note that the parameters in this model's initialization take precedence \n    over those provided by the user using via the statsmodels kwargs dicts.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # statsmodels.tsa.statespace.SARIMAX parameters\n    _init_kwargs = self.sm_init_kwargs.copy()\n    _init_kwargs.update({\n       'order': self.order,\n       'seasonal_order': self.seasonal_order,\n       'trend': self.trend,\n       'measurement_error': self.measurement_error,\n       'time_varying_regression': self.time_varying_regression,\n       'mle_regression': self.mle_regression,\n       'simple_differencing': self.simple_differencing,\n       'enforce_stationarity': self.enforce_stationarity,\n       'enforce_invertibility': self.enforce_invertibility,\n       'hamilton_representation': self.hamilton_representation,\n       'concentrate_scale': self.concentrate_scale,\n       'trend_offset': self.trend_offset,\n       'use_exact_diffuse': self.use_exact_diffuse,\n       'dates': self.dates,\n       'freq': self.freq,\n       'missing': self.missing,\n       'validate_specification': self.validate_specification\n    })\n    self._init_kwargs = _init_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAX.fit parameters\n    _fit_kwargs = self.sm_fit_kwargs.copy()\n    _fit_kwargs.update({\n       'method': self.method,\n       'maxiter': self.maxiter,\n       'start_params': self.start_params,\n       'disp': self.disp,\n    })        \n    self._fit_kwargs = _fit_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAXResults.get_forecast parameters\n    self._predict_kwargs = self.sm_predict_kwargs.copy()\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax._create_sarimax","title":"<code>_create_sarimax(endog, exog=None)</code>","text":"<p>A helper method to create a new statsmodel SARIMAX model.</p> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized can be added with the <code>init_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>endog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The endogenous variable.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The exogenous variables.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>def _create_sarimax(\n    self,\n    endog: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None\n) -&gt; None:\n    \"\"\"\n    A helper method to create a new statsmodel SARIMAX model.\n\n    Additional keyword arguments to pass to the statsmodels SARIMAX model \n    when it is initialized can be added with the `init_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    endog : numpy ndarray, pandas Series, pandas DataFrame\n        The endogenous variable.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        The exogenous variables.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.sarimax = SARIMAX(endog=endog, exog=exog, **self._init_kwargs)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.fit","title":"<code>fit(y, exog=None)</code>","text":"<p>Fit the model to the data.</p> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model can be added with the <code>fit_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>def fit(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None\n) -&gt; None:\n    \"\"\"\n    Fit the model to the data.\n\n    Additional keyword arguments to pass to the `fit` method of the\n    statsmodels SARIMAX model can be added with the `fit_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        Training time series.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n\n    self.output_type = 'numpy' if isinstance(y, np.ndarray) else 'pandas'\n\n    self._create_sarimax(endog=y, exog=exog)\n    self.sarimax_res = self.sarimax.fit(**self._fit_kwargs)\n    self.fitted = True\n\n    if self.output_type == 'pandas':\n        self.training_index = y.index\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.predict","title":"<code>predict(steps, exog=None, return_conf_int=False, alpha=0.05)</code>","text":"<p>Forecast future values and, if desired, their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAX model can be added with the <code>predict_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Value of the exogenous variable/s for the next steps. The number of  observations needed is the number of steps to predict.</p> <code>`None`</code> <code>return_conf_int</code> <code>bool</code> <p>Whether to get the confidence intervals of the forecasts.</p> <code>`False`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>`0.05`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray, pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval. The  output type is the same as the type of <code>y</code> used in the fit method.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval. (if <code>return_conf_int</code>)</li> <li>upper_bound: upper bound of the interval. (if <code>return_conf_int</code>)</li> </ul> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef predict(\n    self,\n    steps: int,\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None, \n    return_conf_int: bool = False,\n    alpha: float = 0.05\n) -&gt; Union[np.ndarray, pd.DataFrame]:\n    \"\"\"\n    Forecast future values and, if desired, their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    Additional keyword arguments to pass to the `get_forecast` method of the\n    statsmodels SARIMAX model can be added with the `predict_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        Value of the exogenous variable/s for the next steps. The number of \n        observations needed is the number of steps to predict. \n    return_conf_int : bool, default `False`\n        Whether to get the confidence intervals of the forecasts.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n\n    Returns\n    -------\n    predictions : numpy ndarray, pandas DataFrame\n        Values predicted by the forecaster and their estimated interval. The \n        output type is the same as the type of `y` used in the fit method.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval. (if `return_conf_int`)\n        - upper_bound: upper bound of the interval. (if `return_conf_int`)\n\n    \"\"\"\n\n    # This is done because statsmodels doesn't allow `exog` length greater than\n    # the number of steps\n    if exog is not None and len(exog) &gt; steps:\n        warnings.warn(\n            (f\"When predicting using exogenous variables, the `exog` parameter \"\n             f\"must have the same length as the number of predicted steps. Since \"\n             f\"len(exog) &gt; steps, only the first {steps} observations are used.\")\n        )\n        exog = exog[:steps]\n\n    predictions = self.sarimax_res.get_forecast(\n                      steps = steps,\n                      exog  = exog,\n                      **self._predict_kwargs\n                  )\n\n    if not return_conf_int:\n        predictions = predictions.predicted_mean\n        if self.output_type == 'pandas':\n            predictions = predictions.rename(\"pred\").to_frame()\n    else:\n        if self.output_type == 'numpy':\n            predictions = np.column_stack(\n                              [predictions.predicted_mean,\n                               predictions.conf_int(alpha=alpha)]\n                          )\n        else:\n            predictions = pd.concat((\n                              predictions.predicted_mean,\n                              predictions.conf_int(alpha=alpha)),\n                              axis = 1\n                          )\n            predictions.columns = ['pred', 'lower_bound', 'upper_bound']\n\n    return predictions\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.append","title":"<code>append(y, exog=None, refit=False, copy_initialization=False, **kwargs)</code>","text":"<p>Recreate the results object with new data appended to the original data.</p> <p>Creates a new result object applied to a dataset that is created by  appending new data to the end of the model's original data. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, based on the combined dataset.</p> <code>`False`</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>`False`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> and <code>exog</code> arguments to this method must be formatted in the same  way (e.g. Pandas Series versus Numpy array) as were the <code>y</code> and <code>exog</code>  arrays passed to the original model.</p> <p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of <code>y</code>. For any other kind of  dataset, see the apply method.</p> <p>This method will apply filtering to all of the original data as well as  to the new data. To apply filtering only to the new data (which can be  much faster if the original dataset is large), see the extend method.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef append(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Recreate the results object with new data appended to the original data.\n\n    Creates a new result object applied to a dataset that is created by \n    appending new data to the end of the model's original data. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the parameters, based on the combined dataset.\n    copy_initialization : bool, default `False`\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` and `exog` arguments to this method must be formatted in the same \n    way (e.g. Pandas Series versus Numpy array) as were the `y` and `exog` \n    arrays passed to the original model.\n\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of `y`. For any other kind of \n    dataset, see the apply method.\n\n    This method will apply filtering to all of the original data as well as \n    to the new data. To apply filtering only to the new data (which can be \n    much faster if the original dataset is large), see the extend method.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.append(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.apply","title":"<code>apply(y, exog=None, refit=False, copy_initialization=False, **kwargs)</code>","text":"<p>Apply the fitted parameters to new data unrelated to the original data.</p> <p>Creates a new result object using the current fitted parameters, applied  to a completely new dataset that is assumed to be unrelated to the model's original data. The new results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, using the new dataset.</p> <code>`False`</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>`False`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> argument to this method should consist of new observations that  are not necessarily related to the original model's <code>y</code> dataset. For  observations that continue that original dataset by follow directly after  its last element, see the append and extend methods.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef apply(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Apply the fitted parameters to new data unrelated to the original data.\n\n    Creates a new result object using the current fitted parameters, applied \n    to a completely new dataset that is assumed to be unrelated to the model's\n    original data. The new results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the parameters, using the new dataset.\n    copy_initialization : bool, default `False`\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    are not necessarily related to the original model's `y` dataset. For \n    observations that continue that original dataset by follow directly after \n    its last element, see the append and extend methods.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.apply(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.extend","title":"<code>extend(y, exog=None, **kwargs)</code>","text":"<p>Recreate the results object for new data that extends the original data.</p> <p>Creates a new result object applied to a new dataset that is assumed to  follow directly from the end of the model's original data. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of the model's original <code>y</code>  array. For any other kind of dataset, see the apply method.</p> <p>This method will apply filtering only to the new data provided by the <code>y</code>  argument, which can be much faster than re-filtering the entire dataset.  However, the returned results object will only have results for the new  data. To retrieve results for both the new data and the original data,  see the append method.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef extend(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Recreate the results object for new data that extends the original data.\n\n    Creates a new result object applied to a new dataset that is assumed to \n    follow directly from the end of the model's original data. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of the model's original `y` \n    array. For any other kind of dataset, see the apply method.\n\n    This method will apply filtering only to the new data provided by the `y` \n    argument, which can be much faster than re-filtering the entire dataset. \n    However, the returned results object will only have results for the new \n    data. To retrieve results for both the new data and the original data, \n    see the append method.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend\n\n    \"\"\"\n\n    self.sarimax_res = self.sarimax_res.extend(\n                           endog = y,\n                           exog  = exog,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set new values to the parameters of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>def set_params(\n    self, \n    **params: dict\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the regressor.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    params = {k: v for k, v in params.items() if k in self._sarimax_params}\n    for key, value in params.items():\n        setattr(self, key, value)\n\n    self._consolidate_kwargs()\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.params","title":"<code>params()</code>","text":"<p>Get the parameters of the model. The order of variables is the trend coefficients, the <code>k_exog</code> exogenous coefficients, the <code>k_ar</code> AR  coefficients, and finally the <code>k_ma</code> MA coefficients.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>numpy ndarray, pandas Series</code> <p>The parameters of the model.</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef params(\n    self\n) -&gt; Union[np.ndarray, pd.Series]:\n    \"\"\"\n    Get the parameters of the model. The order of variables is the trend\n    coefficients, the `k_exog` exogenous coefficients, the `k_ar` AR \n    coefficients, and finally the `k_ma` MA coefficients.\n\n    Returns\n    -------\n    params : numpy ndarray, pandas Series\n        The parameters of the model.\n\n    \"\"\"\n\n    return self.sarimax_res.params\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.summary","title":"<code>summary(alpha=0.05, start=None)</code>","text":"<p>Get a summary of the SARIMAXResults object.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>`0.05`</code> <code>start</code> <code>int</code> <p>Integer of the start observation.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>Summary instance</code> <p>This holds the summary table and text, which can be printed or  converted to various output formats.</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef summary(\n    self,\n    alpha: float = 0.05,\n    start: int = None\n) -&gt; object:\n    \"\"\"\n    Get a summary of the SARIMAXResults object.\n\n    Parameters\n    ----------\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n    start : int, default `None`\n        Integer of the start observation.\n\n    Returns\n    -------\n    summary : Summary instance\n        This holds the summary table and text, which can be printed or \n        converted to various output formats.\n\n    \"\"\"\n\n    return self.sarimax_res.summary(alpha=alpha, start=start)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.get_info_criteria","title":"<code>get_info_criteria(criteria='aic', method='standard')</code>","text":"<p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>`'aic'`</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>`'standard'`</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast/Sarimax/Sarimax.py</code> <pre><code>@_check_fitted\ndef get_info_criteria(\n    self,\n    criteria: str = 'aic',\n    method: str = 'standard'\n) -&gt; float:\n    \"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default `'aic'`\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default `'standard'`\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            (\"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n             \"and 'hqic'.\")\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            (\"Invalid value for `method`. Valid options are 'standard' and \"\n             \"'lutkepohl'.\")\n        )\n\n    metric = self.sarimax_res.info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/datasets.html","title":"<code>Datasets</code>","text":""},{"location":"api/datasets.html#skforecast.datasets.fetch_dataset","title":"<code>fetch_dataset(name, version='latest', raw=False, kwargs_read_csv={}, verbose=True)</code>","text":"<p>Fetch a dataset from the skforecast-datasets repository.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to fetch.</p> required <code>version</code> <code>str</code> <p>Version of the dataset to fetch. If 'latest', the lastest version will be  fetched (the one in the main branch). For a list of available versions,  see the repository branches.</p> <code>'latest'</code> <code>raw</code> <code>bool</code> <p>If True, the raw dataset is fetched. If False, the preprocessed dataset  is fetched. The preprocessing consists of setting the column with the  date/time as index and converting the index to datetime. A frequency is  also set to the index.</p> <code>False</code> <code>kwargs_read_csv</code> <code>dict</code> <p>Kwargs to pass to pandas <code>read_csv</code> function.</p> <code>{}</code> <code>verbose</code> <code>bool</code> <p>If True, print information about the dataset.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Dataset.</p> Source code in <code>skforecast/datasets/datasets.py</code> <pre><code>def fetch_dataset(\n    name: str,\n    version: str = 'latest',\n    raw: bool = False,\n    kwargs_read_csv: dict = {},\n    verbose: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch a dataset from the skforecast-datasets repository.\n\n    Parameters\n    ----------\n    name: str\n        Name of the dataset to fetch.\n    version: str, int, default `'latest'`\n        Version of the dataset to fetch. If 'latest', the lastest version will be \n        fetched (the one in the main branch). For a list of available versions, \n        see the repository branches.\n    raw: bool, default `False`\n        If True, the raw dataset is fetched. If False, the preprocessed dataset \n        is fetched. The preprocessing consists of setting the column with the \n        date/time as index and converting the index to datetime. A frequency is \n        also set to the index.\n    kwargs_read_csv: dict, default `{}`\n        Kwargs to pass to pandas `read_csv` function.\n    verbose: bool, default `True`\n        If True, print information about the dataset.\n\n    Returns\n    -------\n    df: pandas DataFrame\n        Dataset.\n\n    \"\"\"\n\n    version = 'main' if version == 'latest' else f'{version}'\n\n    datasets = {\n        'h2o': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/h2o.csv'\n            ),\n            'sep': ',',\n            'index_col': 'fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'description': (\n                'Monthly expenditure ($AUD) on corticosteroid drugs that the '\n                'Australian health system had between 1991 and 2008. '\n            ),\n            'source': (\n                'Hyndman R (2023). fpp3: Data for Forecasting: Principles and Practice'\n                '(3rd Edition). http://pkg.robjhyndman.com/fpp3package/,'\n                'https://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.'\n            )\n        },\n        'h2o_exog': {\n            'url': (\n                f\"https://raw.githubusercontent.com/JoaquinAmatRodrigo/\"\n                f\"skforecast-datasets/{version}/data/h2o_exog.csv\"\n            ),\n            'sep': ',',\n            'index_col': 'fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'description': (\n                'Monthly expenditure ($AUD) on corticosteroid drugs that the '\n                'Australian health system had between 1991 and 2008. Two additional '\n                'variables (exog_1, exog_2) are simulated.'\n            ),\n            'source': (\n                \"Hyndman R (2023). fpp3: Data for Forecasting: Principles and Practice \"\n                \"(3rd Edition). http://pkg.robjhyndman.com/fpp3package/, \"\n                \"https://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\"\n            )\n        },\n        'fuel_consumption': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/consumos-combustibles-mensual.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'description': (\n                'Monthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.'\n            ),\n            'source': (\n                'Obtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos '\n                'Petrol\u00edferos and Corporaci\u00f3n de Derecho P\u00fablico tutelada por el '\n                'Ministerio para la Transici\u00f3n Ecol\u00f3gica y el Reto Demogr\u00e1fico. '\n                'https://www.cores.es/es/estadisticas'\n            )\n        },\n        'items_sales': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/simulated_items_sales.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': 'Simulated time series for the sales of 3 different items.',\n            'source': 'Simulated data.'\n        },\n        'air_quality_valencia': {\n            'url': (\n                f\"https://raw.githubusercontent.com/JoaquinAmatRodrigo/\"\n                f\"skforecast-datasets/{version}/data/air_quality_valencia.csv\"\n            ),\n            'sep': ',',\n            'index_col': 'datetime',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'description': (\n                'Hourly measures of several air chemical pollutant (pm2.5, co, no, '\n                'no2, pm10, nox, o3, so2) at Valencia city.'\n            ),\n            'source': (\n                \" Red de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, \"\n                \"46250054-Val\u00e8ncia - Centre, \"\n                \"https://mediambient.gva.es/es/web/calidad-ambiental/datos-historicos.\"\n            )\n        },\n        'website_visits': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/visitas_por_dia_web_cienciadedatos.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': '1D',\n            'description': (\n                'Daily visits to the cienciadedatos.net website registered with the '\n                'google analytics service.'\n            ),\n            'source': (\n                \"Amat Rodrigo, J. (2021). cienciadedatos.net (1.0.0). Zenodo. \"\n                \"https://doi.org/10.5281/zenodo.10006330\"\n            )\n        },\n        'bike_sharing': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/bike_sharing_dataset_clean.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'description': (\n                'Hourly usage of the bike share system in the city of Washington D.C. '\n                'during the years 2011 and 2012. In addition to the number of users per '\n                'hour, information about weather conditions and holidays is available.'\n            ),\n            'source': (\n                \"Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning \"\n                \"Repository. https://doi.org/10.24432/C5W894.\"\n            )\n        },\n        'bike_sharing_extended_features': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/bike_sharing_extended_features.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'description': (\n                'Hourly usage of the bike share system in the city of Washington D.C. '\n                'during the years 2011 and 2012. In addition to the number of users per '\n                'hour, the dataset was enriched by introducing supplementary features. '\n                'Addition includes calendar-based variables (day of the week, hour of '\n                'the day, month, etc.), indicators for sunlight, incorporation of '\n                'rolling temperature averages, and the creation of polynomial features '\n                'generated from variable pairs. All cyclic variables are encoded using '\n                'sine and cosine functions to ensure accurate representation.'\n            ),\n            'source': (\n                \"Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning \"\n                \"Repository. https://doi.org/10.24432/C5W894.\"\n            )\n        },\n        'australia_tourism': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/australia_tourism.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d',\n            'freq': 'Q',\n            'description': (\n                \"Quarterly overnight trips (in thousands) from 1998 Q1 to 2016 Q4 \"\n                \"across Australia. The tourism regions are formed through the \"\n                \"aggregation of Statistical Local Areas (SLAs) which are defined by \"\n                \"the various State and Territory tourism authorities according to \"\n                \"their research and marketing needs.\"\n            ),\n            'source': (\n                \"Wang, E, D Cook, and RJ Hyndman (2020). A new tidy data structure to \"\n                \"support exploration and modeling of temporal data, Journal of \"\n                \"Computational and Graphical Statistics, 29:3, 466-478, \"\n                \"doi:10.1080/10618600.2019.1695624.\"\n            )\n        },\n        'uk_daily_flights': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/uk_daily_flights.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Date',\n            'date_format': '%d/%m/%Y',\n            'freq': 'D',\n            'description': 'Daily number of flights in UK from 02/01/2019 to 23/01/2022.',\n            'source': (\n                'Experimental statistics published as part of the Economic activity and '\n                'social change in the UK, real-time indicators release, Published 27 '\n                'January 2022. Daily flight numbers are available in the dashboard '\n                'provided by the European Organisation for the Safety of Air Navigation '\n                '(EUROCONTROL). '\n                'https://www.ons.gov.uk/economy/economicoutputandproductivity/output/'\n                'bulletins/economicactivityandsocialchangeintheukrealtimeindicators/latest'\n            )\n        },\n        'wikipedia_visits': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/wikipedia_visits.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': (\n                'Log daily page views for the Wikipedia page for Peyton Manning. '\n                'Scraped data using the Wikipediatrend package in R.'\n            ),\n            'source': (\n                'https://github.com/facebook/prophet/blob/main/examples/'\n                'example_wp_log_peyton_manning.csv'\n            )\n        },\n        'vic_electricity': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/vic_electricity.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Time',\n            'date_format': '%Y-%m-%dT%H:%M:%SZ',\n            'freq': '30min',\n            'description': 'Half-hourly electricity demand for Victoria, Australia',\n            'source': (\n                \"O'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse \"\n                \"Datasets for 'tsibble'. https://tsibbledata.tidyverts.org/, \"\n                \"https://github.com/tidyverts/tsibbledata/. \"\n                \"https://tsibbledata.tidyverts.org/reference/vic_elec.html\"\n            )\n        },\n        'store_sales': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/store_sales.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': (\n                'This dataset contains 913,000 sales transactions from 2013-01-01 to '\n                '2017-12-31 for 50 products (SKU) in 10 stores.'\n            ),\n            'source': (\n                'The original data was obtained from: inversion. (2018). Store Item '\n                'Demand Forecasting Challenge. Kaggle. '\n                'https://kaggle.com/competitions/demand-forecasting-kernels-only'\n            )\n        },\n        'bicimad': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/bicimad_users.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': (\n                'This dataset contains the daily users of the bicycle rental '\n                'service (BiciMad) in the city of Madrid (Spain) from 2014-06-23 '\n                'to 2022-09-30.'\n            ),\n            'source': (\n                'The original data was obtained from: Portal de datos abiertos '\n                'del Ayuntamiento de Madrid https://datos.madrid.es/portal/site/egob'\n            )\n        },\n        'm4_hourly': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/m4_hourly.parquet'\n            ),\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'description': \"Time series with hourly frequency from the M4 competition.\",\n            'source': (\n                \"Monash Time Series Forecasting Repository  \"\n                \"(https://zenodo.org/communities/forecasting) Godahewa, R., Bergmeir, \"\n                \"C., Webb, G. I., Hyndman, R. J., &amp; Montero-Manso, P. (2021). Monash \"\n                \"Time Series Forecasting Archive. In Neural Information Processing \"\n                \"Systems Track on Datasets and Benchmarks. \\n\"\n                \"Raw data, available in .tsf format, has been converted to Pandas \"\n                \"format using the code provided by the authors in \"\n                \"https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py \\n\"\n                \"The category of each time series has been included in the dataset. This \"\n                \"information has been obtainded from the Kaggle competition page: \"\n                \"https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset\"\n            )\n        },\n        'm4_daily': {\n            'url': (\n                f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n                f'skforecast-datasets/{version}/data/m4_daily.parquet'\n            ),\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'D',\n            'description': \"Time series with daily frequency from the M4 competition.\",\n            'source': (\n                \"Monash Time Series Forecasting Repository  \"\n                \"(https://zenodo.org/communities/forecasting) Godahewa, R., Bergmeir, \"\n                \"C., Webb, G. I., Hyndman, R. J., &amp; Montero-Manso, P. (2021). Monash \"\n                \"Time Series Forecasting Archive. In Neural Information Processing \"\n                \"Systems Track on Datasets and Benchmarks. \\n\"\n                \"Raw data, available in .tsf format, has been converted to Pandas \"\n                \"format using the code provided by the authors in \"\n                \"https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py \\n\"\n                \"The category of each time series has been included in the dataset. This \"\n                \"information has been obtainded from the Kaggle competition page: \"\n                \"https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset\"\n            )\n        },\n        'ashrae_daily': {\n            'url': 'https://drive.google.com/file/d/1fMsYjfhrFLmeFjKG3jenXjDa5s984ThC/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': (\n                \"Daily energy consumption data from the ASHRAE competition with \"\n                \"building metadata and weather data.\"\n            ),\n            'source': (\n                \"Kaggle competition Addison Howard, Chris Balbach, Clayton Miller, \"\n                \"Jeff Haberl, Krishnan Gowri, Sohier Dane. (2019). ASHRAE - Great Energy \"\n                \"Predictor III. Kaggle. https://www.kaggle.com/c/ashrae-energy-prediction/overview\"\n            )\n        },\n        'bdg2_daily': {\n            'url': 'https://drive.google.com/file/d/1KHYopzclKvS1F6Gt6GoJWKnxiuZ2aqen/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'description': (\n                \"Daily energy consumption data from the The Building Data Genome Project 2 \"\n                \"with building metadata and weather data. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        },\n        'bdg2_hourly': {\n            'url': 'https://drive.google.com/file/d/1I2i5mZJ82Cl_SHPTaWJmLoaXnntdCgh7/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'D',\n            'description': (\n                \"Hourly energy consumption data from the The Building Data Genome Project 2 \"\n                \"with building metadata and weather data. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        }\n    }\n\n    if name not in datasets.keys():\n        raise ValueError(\n            f\"Dataset '{name}' not found. Available datasets are: {list(datasets.keys())}\"\n        )\n\n    url = datasets[name]['url']\n\n    if url.endswith('.csv'):\n        try:\n            sep = datasets[name]['sep']\n            df = pd.read_csv(url, sep=sep, **kwargs_read_csv)\n        except:\n            raise ValueError(\n                f\"Error reading dataset '{name}' from {url}. Try to version = 'latest'\"\n            )\n\n    if url.endswith('.parquet'):\n        try:\n            df = pd.read_parquet(url)\n        except:\n            raise ValueError(\n                f\"Error reading dataset '{name}' from {url}. Try to version = 'latest'\"\n            )\n\n    if url.startswith('https://drive.google.com'):\n        file_id = url.split('/')[-2]\n        url = 'https://drive.google.com/uc?id=' + file_id\n        df = pd.read_parquet(url)\n\n    if not raw:\n        try:\n            index_col = datasets[name]['index_col']\n            freq = datasets[name]['freq']\n            if freq == 'H' and pd.__version__ &gt;= '2.2.0':\n                freq = \"h\"\n            date_format = datasets[name]['date_format']\n            df = df.set_index(index_col)\n            df.index = pd.to_datetime(df.index, format=date_format)\n            df = df.asfreq(freq)\n            df = df.sort_index()\n        except:\n            pass\n\n    if verbose:\n        print(name)\n        print('-' * len(name))\n        description = textwrap.fill(datasets[name]['description'], width=80)\n        source = textwrap.fill(datasets[name]['source'], width=80)\n        print(description)\n        print(source)\n        print(f\"Shape of the dataset: {df.shape}\")\n\n    return df\n</code></pre>"},{"location":"api/datasets.html#skforecast.datasets.load_demo_dataset","title":"<code>load_demo_dataset(version='latest')</code>","text":"<p>Load demo data set with monthly expenditure ($AUD) on corticosteroid drugs that the Australian health system had between 1991 and 2008. Obtained from the book: Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos. Index is set to datetime with monthly frequency and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>Version of the dataset to fetch. If 'latest', the lastest version will be fetched (the one in the main branch). For a list of available versions, see the repository branches.</p> <code>'latest'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas Series</code> <p>Dataset.</p> Source code in <code>skforecast/datasets/datasets.py</code> <pre><code>def load_demo_dataset(version: str = 'latest') -&gt; pd.Series:\n    \"\"\"\n    Load demo data set with monthly expenditure ($AUD) on corticosteroid drugs that\n    the Australian health system had between 1991 and 2008. Obtained from the book:\n    Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos.\n    Index is set to datetime with monthly frequency and sorted.\n\n    Parameters\n    ----------\n    version: str, default `'latest'`\n        Version of the dataset to fetch. If 'latest', the lastest version will be\n        fetched (the one in the main branch). For a list of available versions,\n        see the repository branches.\n\n    Returns\n    -------\n    df: pandas Series\n        Dataset.\n\n    \"\"\"\n\n    version = 'main' if version == 'latest' else f'{version}'\n\n    url = (\n        f'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/{version}/'\n        'data/h2o.csv'\n    )\n\n    df = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d')\n    df = df.set_index('datetime')\n    df = df.asfreq('MS')\n    df = df['y']\n    df = df.sort_index()\n\n    return df\n</code></pre>"},{"location":"api/exceptions.html","title":"<code>exceptions</code>","text":""},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingValuesWarning","title":"<code>MissingValuesWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to indicate that there are missing values in the data. This  warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingExogWarning","title":"<code>MissingExogWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTypeWarning","title":"<code>DataTypeWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that a large number of models will be trained and the the process may take a while to run.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that an argument is ignored when using a method  or a function.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.SkforecastVersionWarning","title":"<code>SkforecastVersionWarning(message)</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that the skforecast version installed in the  environment differs from the version used to initialize the forecaster.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/feature_selection.html","title":"<code>feature_selection</code>","text":""},{"location":"api/feature_selection.html#skforecast.feature_selection.feature_selection.select_features","title":"<code>select_features(forecaster, selector, y, exog=None, select_only=None, force_inclusion=None, subsample=0.5, random_state=123, verbose=True)</code>","text":"<p>Feature selection using any of the sklearn.feature_selection module selectors  (such as <code>RFECV</code>, <code>SelectFromModel</code>, etc.). Two groups of features are evaluated: autoregressive features (lags and window features) and exogenous features. By default, the selection process is performed on both sets of features at the same time, so that the most relevant autoregressive and exogenous features are selected. However, using the <code>select_only</code> argument, the selection process can focus only on the autoregressive or exogenous features without taking into account the other features. Therefore, all other features will remain in the model.  It is also possible to force the inclusion of certain features in the final list of selected features using the <code>force_inclusion</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoreg, ForecasterAutoregDirect)</code> <p>Forecaster model. If forecaster is a ForecasterAutoregDirect, the selector will only be applied to the features of the first step.</p> required <code>selector</code> <code>object</code> <p>A feature selector from sklearn.feature_selection.</p> required <code>y</code> <code>pandas Series, pandas DataFrame</code> <p>Target time series to which the feature selection will be applied.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>select_only</code> <code>str</code> <p>Decide what type of features to include in the selection process. </p> <ul> <li>If <code>'autoreg'</code>, only autoregressive features (lags and window features) are evaluated by the selector. All exogenous features are included in the output (<code>selected_exog</code>).</li> <li>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included  in the output (<code>selected_autoreg</code>).</li> <li>If <code>None</code>, all features are evaluated by the selector.</li> </ul> <code>`None`</code> <code>force_inclusion</code> <code>(list, str)</code> <p>Features to force include in the final list of selected features.</p> <ul> <li>If <code>list</code>, list of feature names to force include.</li> <li>If <code>str</code>, regular expression to identify features to force include.  For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin  with \"sun_\" will be included in the final list of selected features.</li> </ul> <code>`None`</code> <code>subsample</code> <code>(int, float)</code> <p>Proportion of records to use for feature selection.</p> <code>`0.5`</code> <code>random_state</code> <code>int</code> <p>Sets a seed for the random subsample so that the subsampling process  is always deterministic.</p> <code>`123`</code> <code>verbose</code> <code>bool</code> <p>Print information about feature selection process.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>selected_autoreg</code> <code>list</code> <p>List of selected autoregressive features.</p> <code>selected_exog</code> <code>list</code> <p>List of selected exogenous features.</p> Source code in <code>skforecast/feature_selection/feature_selection.py</code> <pre><code>def select_features(\n    forecaster: object,\n    selector: object,\n    y: Union[pd.Series, pd.DataFrame],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    select_only: Optional[str] = None,\n    force_inclusion: Optional[Union[list, str]] = None,\n    subsample: Union[int, float] = 0.5,\n    random_state: int = 123,\n    verbose: bool = True\n) -&gt; Union[list, list]:\n    \"\"\"\n    Feature selection using any of the sklearn.feature_selection module selectors \n    (such as `RFECV`, `SelectFromModel`, etc.). Two groups of features are\n    evaluated: autoregressive features (lags and window features) and exogenous\n    features. By default, the selection process is performed on both sets of features\n    at the same time, so that the most relevant autoregressive and exogenous features\n    are selected. However, using the `select_only` argument, the selection process\n    can focus only on the autoregressive or exogenous features without taking into\n    account the other features. Therefore, all other features will remain in the model. \n    It is also possible to force the inclusion of certain features in the final list\n    of selected features using the `force_inclusion` parameter.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregDirect\n        Forecaster model. If forecaster is a ForecasterAutoregDirect, the\n        selector will only be applied to the features of the first step.\n    selector : object\n        A feature selector from sklearn.feature_selection.\n    y : pandas Series, pandas DataFrame\n        Target time series to which the feature selection will be applied.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    select_only : str, default `None`\n        Decide what type of features to include in the selection process. \n\n        - If `'autoreg'`, only autoregressive features (lags and window features)\n        are evaluated by the selector. All exogenous features are included in the\n        output (`selected_exog`).\n        - If `'exog'`, only exogenous features are evaluated without the presence\n        of autoregressive features. All autoregressive features are included \n        in the output (`selected_autoreg`).\n        - If `None`, all features are evaluated by the selector.\n    force_inclusion : list, str, default `None`\n        Features to force include in the final list of selected features.\n\n        - If `list`, list of feature names to force include.\n        - If `str`, regular expression to identify features to force include. \n        For example, if `force_inclusion=\"^sun_\"`, all features that begin \n        with \"sun_\" will be included in the final list of selected features.\n    subsample : int, float, default `0.5`\n        Proportion of records to use for feature selection.\n    random_state : int, default `123`\n        Sets a seed for the random subsample so that the subsampling process \n        is always deterministic.\n    verbose : bool, default `True`\n        Print information about feature selection process.\n\n    Returns\n    -------\n    selected_autoreg : list\n        List of selected autoregressive features.\n    selected_exog : list\n        List of selected exogenous features.\n\n    \"\"\"\n\n    valid_forecasters = ['ForecasterAutoreg', 'ForecasterAutoregDirect']\n\n    if type(forecaster).__name__ not in valid_forecasters:\n        raise TypeError(\n            f\"`forecaster` must be one of the following classes: {valid_forecasters}.\"\n        )\n\n    if select_only not in ['autoreg', 'exog', None]:\n        raise ValueError(\n            \"`select_only` must be one of the following values: 'autoreg', 'exog', None.\"\n        )\n\n    if subsample &lt;= 0 or subsample &gt; 1:\n        raise ValueError(\n            \"`subsample` must be a number greater than 0 and less than or equal to 1.\"\n        )\n\n    forecaster = deepcopy(forecaster)\n    forecaster.is_fitted = False\n    X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n    if type(forecaster).__name__ == 'ForecasterAutoregDirect':\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step          = 1,\n                               X_train       = X_train,\n                               y_train       = y_train,\n                               remove_suffix = True\n                           )\n    autoreg_cols = []\n    if forecaster.lags is not None:\n        autoreg_cols.extend([f\"lag_{lag}\" for lag in forecaster.lags])\n    if forecaster.window_features is not None:\n        autoreg_cols.extend(forecaster.window_features_names)\n\n    exog_cols = [col for col in X_train.columns if col not in autoreg_cols]\n\n    forced_autoreg = []\n    forced_exog = []\n    if force_inclusion is not None:\n        if isinstance(force_inclusion, list):\n            forced_autoreg = [col for col in force_inclusion if col in autoreg_cols]\n            forced_exog = [col for col in force_inclusion if col in exog_cols]\n        elif isinstance(force_inclusion, str):\n            forced_autoreg = [col for col in autoreg_cols if re.match(force_inclusion, col)]\n            forced_exog = [col for col in exog_cols if re.match(force_inclusion, col)]\n\n    if select_only == 'autoreg':\n        X_train = X_train.drop(columns=exog_cols)\n    elif select_only == 'exog':\n        X_train = X_train.drop(columns=autoreg_cols)\n\n    if isinstance(subsample, float):\n        subsample = int(len(X_train) * subsample)\n\n    rng = np.random.default_rng(seed=random_state)\n    sample = rng.choice(X_train.index, size=subsample, replace=False)\n    X_train_sample = X_train.loc[sample, :]\n    y_train_sample = y_train.loc[sample]\n    selector.fit(X_train_sample, y_train_sample)\n    selected_features = selector.get_feature_names_out()\n\n    if select_only == 'exog':\n        selected_autoreg = autoreg_cols\n    else:\n        selected_autoreg = [\n            feature\n            for feature in selected_features\n            if feature in autoreg_cols\n        ]\n\n    if select_only == 'autoreg':\n        selected_exog = exog_cols\n    else:\n        selected_exog = [\n            feature\n            for feature in selected_features\n            if feature in exog_cols\n        ]\n\n    if force_inclusion is not None: \n        if select_only != 'autoreg':\n            forced_exog_not_selected = set(forced_exog) - set(selected_features)\n            selected_exog.extend(forced_exog_not_selected)\n            selected_exog.sort(key=exog_cols.index)\n        if select_only != 'exog':\n            forced_autoreg_not_selected = set(forced_autoreg) - set(selected_features)\n            selected_autoreg.extend(forced_autoreg_not_selected)\n            selected_autoreg.sort(key=autoreg_cols.index)\n\n    if len(selected_autoreg) == 0:\n        warnings.warn(\n            (\"No autoregressive features have been selected. Since a Forecaster \"\n             \"cannot be created without them, be sure to include at least one \"\n             \"using the `force_inclusion` parameter.\")\n        )\n    else:\n        selected_autoreg = [\n            int(feature.replace('lag_', '')) if feature.startswith('lag_') else feature\n            for feature in selected_autoreg\n        ]\n\n    if verbose:\n        print(f\"Recursive feature elimination ({selector.__class__.__name__})\")\n        print(\"--------------------------------\" + \"-\" * len(selector.__class__.__name__))\n        print(f\"Total number of records available: {X_train.shape[0]}\")\n        print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n        print(f\"Number of features available: {X_train.shape[1]}\") \n        print(f\"    Autoreg (n={len(autoreg_cols)})\")\n        print(f\"    Exog    (n={len(exog_cols)})\")\n        print(f\"Number of features selected: {len(selected_features)}\")\n        print(f\"    Autoreg (n={len(selected_autoreg)}) : {selected_autoreg}\")\n        print(f\"    Exog    (n={len(selected_exog)}) : {selected_exog}\")\n\n    return selected_autoreg, selected_exog\n</code></pre>"},{"location":"api/feature_selection.html#skforecast.feature_selection.feature_selection.select_features_multiseries","title":"<code>select_features_multiseries(forecaster, selector, series, exog=None, select_only=None, force_inclusion=None, subsample=0.5, random_state=123, verbose=True)</code>","text":"<p>Feature selection using any of the sklearn.feature_selection module selectors  (such as <code>RFECV</code>, <code>SelectFromModel</code>, etc.). Two groups of features are evaluated: autoregressive features and exogenous features. By default, the  selection process is performed on both sets of features at the same time,  so that the most relevant autoregressive and exogenous features are selected.  However, using the <code>select_only</code> argument, the selection process can focus  only on the autoregressive or exogenous features without taking into account  the other features. Therefore, all other features will remain in the model.  It is also possible to force the inclusion of certain features in the final  list of selected features using the <code>force_inclusion</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries</code> <p>Forecaster model.</p> required <code>selector</code> <code>object</code> <p>A feature selector from sklearn.feature_selection.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Target time series to which the feature selection will be applied.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>select_only</code> <code>str</code> <p>Decide what type of features to include in the selection process. </p> <ul> <li>If <code>'autoreg'</code>, only autoregressive features (lags or custom  predictors) are evaluated by the selector. All exogenous features are  included in the output (<code>selected_exog</code>).</li> <li>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included  in the output (<code>selected_autoreg</code>).</li> <li>If <code>None</code>, all features are evaluated by the selector.</li> </ul> <code>`None`</code> <code>force_inclusion</code> <code>(list, str)</code> <p>Features to force include in the final list of selected features.</p> <ul> <li>If <code>list</code>, list of feature names to force include.</li> <li>If <code>str</code>, regular expression to identify features to force include.  For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin  with \"sun_\" will be included in the final list of selected features.</li> </ul> <code>`None`</code> <code>subsample</code> <code>(int, float)</code> <p>Proportion of records to use for feature selection.</p> <code>`0.5`</code> <code>random_state</code> <code>int</code> <p>Sets a seed for the random subsample so that the subsampling process  is always deterministic.</p> <code>`123`</code> <code>verbose</code> <code>bool</code> <p>Print information about feature selection process.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>selected_autoreg</code> <code>list</code> <p>List of selected autoregressive features.</p> <code>selected_exog</code> <code>list</code> <p>List of selected exogenous features.</p> Source code in <code>skforecast/feature_selection/feature_selection.py</code> <pre><code>def select_features_multiseries(\n    forecaster: object,\n    selector: object,\n    series: Union[pd.DataFrame, dict],\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    select_only: Optional[str] = None,\n    force_inclusion: Optional[Union[list, str]] = None,\n    subsample: Union[int, float] = 0.5,\n    random_state: int = 123,\n    verbose: bool = True,\n) -&gt; Union[list, list]:\n    \"\"\"\n    Feature selection using any of the sklearn.feature_selection module selectors \n    (such as `RFECV`, `SelectFromModel`, etc.). Two groups of features are\n    evaluated: autoregressive features and exogenous features. By default, the \n    selection process is performed on both sets of features at the same time, \n    so that the most relevant autoregressive and exogenous features are selected. \n    However, using the `select_only` argument, the selection process can focus \n    only on the autoregressive or exogenous features without taking into account \n    the other features. Therefore, all other features will remain in the model. \n    It is also possible to force the inclusion of certain features in the final \n    list of selected features using the `force_inclusion` parameter.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries\n        Forecaster model.\n    selector : object\n        A feature selector from sklearn.feature_selection.\n    series : pandas DataFrame\n        Target time series to which the feature selection will be applied.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    select_only : str, default `None`\n        Decide what type of features to include in the selection process. \n\n        - If `'autoreg'`, only autoregressive features (lags or custom \n        predictors) are evaluated by the selector. All exogenous features are \n        included in the output (`selected_exog`).\n        - If `'exog'`, only exogenous features are evaluated without the presence\n        of autoregressive features. All autoregressive features are included \n        in the output (`selected_autoreg`).\n        - If `None`, all features are evaluated by the selector.\n    force_inclusion : list, str, default `None`\n        Features to force include in the final list of selected features.\n\n        - If `list`, list of feature names to force include.\n        - If `str`, regular expression to identify features to force include. \n        For example, if `force_inclusion=\"^sun_\"`, all features that begin \n        with \"sun_\" will be included in the final list of selected features.\n    subsample : int, float, default `0.5`\n        Proportion of records to use for feature selection.\n    random_state : int, default `123`\n        Sets a seed for the random subsample so that the subsampling process \n        is always deterministic.\n    verbose : bool, default `True`\n        Print information about feature selection process.\n\n    Returns\n    -------\n    selected_autoreg : list\n        List of selected autoregressive features.\n    selected_exog : list\n        List of selected exogenous features.\n\n    \"\"\"\n\n    valid_forecasters = [\n        'ForecasterAutoregMultiSeries'\n    ]\n\n    if type(forecaster).__name__ not in valid_forecasters:\n        raise TypeError(\n            f\"`forecaster` must be one of the following classes: {valid_forecasters}.\"\n        )\n\n    if select_only not in ['autoreg', 'exog', None]:\n        raise ValueError(\n            \"`select_only` must be one of the following values: 'autoreg', 'exog', None.\"\n        )\n\n    if subsample &lt;= 0 or subsample &gt; 1:\n        raise ValueError(\n            \"`subsample` must be a number greater than 0 and less than or equal to 1.\"\n        )\n\n    forecaster = deepcopy(forecaster)\n    forecaster.fitted = False\n    output = forecaster._create_train_X_y(series=series, exog=exog)\n    X_train = output[0]\n    y_train = output[1]\n    series_col_names = output[3]\n\n    if forecaster.encoding == 'onehot':\n        encoding_cols = series_col_names\n    else:\n        encoding_cols = ['_level_skforecast']\n\n    if hasattr(forecaster, 'lags'):\n        autoreg_cols = [f\"lag_{lag}\" for lag in forecaster.lags]\n    else:\n        if forecaster.name_predictors is not None:\n            autoreg_cols = forecaster.name_predictors\n        else:\n            autoreg_cols = [\n                col\n                for col in X_train.columns\n                if re.match(r'^custom_predictor_\\d+', col)\n            ]\n    exog_cols = [\n        col\n        for col in X_train.columns\n        if col not in autoreg_cols and col not in encoding_cols\n    ]\n\n    forced_autoreg = []\n    forced_exog = []\n    if force_inclusion is not None:\n        if isinstance(force_inclusion, list):\n            forced_autoreg = [col for col in force_inclusion if col in autoreg_cols]\n            forced_exog = [col for col in force_inclusion if col in exog_cols]\n        elif isinstance(force_inclusion, str):\n            forced_autoreg = [col for col in autoreg_cols if re.match(force_inclusion, col)]\n            forced_exog = [col for col in exog_cols if re.match(force_inclusion, col)]\n\n    if select_only == 'autoreg':\n        X_train = X_train.drop(columns=exog_cols + encoding_cols)\n    elif select_only == 'exog':\n        X_train = X_train.drop(columns=autoreg_cols + encoding_cols)\n    else:\n        X_train = X_train.drop(columns=encoding_cols)\n\n    if isinstance(subsample, float):\n        subsample = int(len(X_train) * subsample)\n\n    rng = np.random.default_rng(seed=random_state)\n    sample = rng.choice(X_train.index, size=subsample, replace=False)\n    X_train_sample = X_train.loc[sample, :]\n    y_train_sample = y_train.loc[sample]\n    selector.fit(X_train_sample, y_train_sample)\n    selected_features = selector.get_feature_names_out()\n\n    if select_only == 'exog':\n        selected_autoreg = autoreg_cols\n    else:\n        selected_autoreg = [\n            feature\n            for feature in selected_features\n            if feature in autoreg_cols\n        ]\n\n    if select_only == 'autoreg':\n        selected_exog = exog_cols\n    else:\n        selected_exog = [\n            feature\n            for feature in selected_features\n            if feature in exog_cols\n        ]\n\n    if force_inclusion is not None: \n        if select_only != 'autoreg':\n            forced_exog_not_selected = set(forced_exog) - set(selected_features)\n            selected_exog.extend(forced_exog_not_selected)\n            selected_exog.sort(key=exog_cols.index)\n        if select_only != 'exog':\n            forced_autoreg_not_selected = set(forced_autoreg) - set(selected_features)\n            selected_autoreg.extend(forced_autoreg_not_selected)\n            selected_autoreg.sort(key=autoreg_cols.index)\n\n    if len(selected_autoreg) == 0:\n        warnings.warn(\n            (\"No autoregressive features have been selected. Since a Forecaster \"\n             \"cannot be created without them, be sure to include at least one \"\n             \"using the `force_inclusion` parameter.\")\n        )\n    else:\n        if hasattr(forecaster, 'lags'):\n            selected_autoreg = [int(feature.replace('lag_', '')) \n                                for feature in selected_autoreg] \n\n    if verbose:\n        print(f\"Recursive feature elimination ({selector.__class__.__name__})\")\n        print(\"--------------------------------\" + \"-\" * len(selector.__class__.__name__))\n        print(f\"Total number of records available: {X_train.shape[0]}\")\n        print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n        print(f\"Number of features available: {len(autoreg_cols) + len(exog_cols)}\") \n        print(f\"    Autoreg (n={len(autoreg_cols)})\")\n        print(f\"    Exog    (n={len(exog_cols)})\")\n        print(f\"Number of features selected: {len(selected_features)}\")\n        print(f\"    Autoreg (n={len(selected_autoreg)}) : {selected_autoreg}\")\n        print(f\"    Exog    (n={len(selected_exog)}) : {selected_exog}\")\n\n    return selected_autoreg, selected_exog\n</code></pre>"},{"location":"api/metrics.html","title":"<code>Metrics</code>","text":""},{"location":"api/metrics.html#skforecast.metrics.add_y_train_argument","title":"<code>add_y_train_argument(func)</code>","text":"<p>Add <code>y_train</code> argument to a function if it is not already present.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to which the argument is added.</p> required <p>Returns:</p> Name Type Description <code>wrapper</code> <code>callable</code> <p>Function with <code>y_train</code> argument added.</p> Source code in <code>skforecast/metrics/metrics.py</code> <pre><code>def add_y_train_argument(func: Callable) -&gt; Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n\n    \"\"\"\n\n    sig = inspect.signature(func)\n\n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n\n    wrapper.__signature__ = new_sig\n\n    return wrapper\n</code></pre>"},{"location":"api/metrics.html#skforecast.metrics.mean_absolute_scaled_error","title":"<code>mean_absolute_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Mean Absolute Scaled Error (MASE)</p> <p>MASE is a scale-independent error metric that measures the accuracy of a forecast. It is the mean absolute error of the forecast divided by the mean absolute error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list, pandas Series, numpy ndarray</code> <p>True values of the target variable in the training set. If <code>list</code>, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Name Type Description <code>mase</code> <code>float</code> <p>MASE value.</p> Source code in <code>skforecast/metrics/metrics.py</code> <pre><code>def mean_absolute_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -&gt; float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n\n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n</code></pre>"},{"location":"api/metrics.html#skforecast.metrics.root_mean_squared_scaled_error","title":"<code>root_mean_squared_scaled_error(y_true, y_pred, y_train)</code>","text":"<p>Root Mean Squared Scaled Error (RMSSE)</p> <p>RMSSE is a scale-independent error metric that measures the accuracy of a forecast. It is the root mean squared error of the forecast divided by the root mean squared error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list, pandas Series, numpy ndarray</code> <p>True values of the target variable in the training set. If list, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Name Type Description <code>rmsse</code> <code>float</code> <p>RMSSE value.</p> Source code in <code>skforecast/metrics/metrics.py</code> <pre><code>def root_mean_squared_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -&gt; float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n\n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n\n    return rmsse\n</code></pre>"},{"location":"api/model_selection.html","title":"<code>model_selection</code>","text":""},{"location":"api/model_selection.html#skforecast.model_selection._validation.backtesting_forecaster","title":"<code>backtesting_forecaster(forecaster, y, cv, metric, exog=None, interval=None, n_boot=250, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False, n_jobs='auto', verbose=False, show_progress=True)</code>","text":"<p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoreg, ForecasterAutoregDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>. If <code>None</code>, no intervals are estimated.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals. If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values.</p> <code>`False`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>metric_values</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection/_validation.py</code> <pre><code>def backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: Optional[list] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    forecaters_allowed = [\n        'ForecasterAutoreg', \n        'ForecasterAutoregDirect',\n        'ForecasterEquivalentDate'\n    ]\n\n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(\n            (f\"`forecaster` must be of type {forecaters_allowed}, for all other types of \"\n             f\" forecasters use the functions available in the other `model_selection` \"\n             f\"modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster              = forecaster,\n        cv                      = cv,\n        y                       = y,\n        metric                  = metric,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        n_jobs                  = n_jobs,\n        show_progress           = show_progress\n    )\n\n    if type(forecaster).__name__ == 'ForecasterAutoregDirect' and \\\n       forecaster.steps &lt; cv.steps + cv.gap:\n        raise ValueError(\n            (f\"When using a ForecasterAutoregDirect, the combination of steps \"\n             f\"+ gap ({cv.steps + cv.gap}) cannot be greater than the `steps` parameter \"\n             f\"declared when the forecaster is initialized ({forecaster.steps}).\")\n        )\n\n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster              = forecaster,\n        y                       = y,\n        cv                      = cv,\n        metric                  = metric,\n        exog                    = exog,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.grid_search_forecaster","title":"<code>grid_search_forecaster(forecaster, y, cv, param_grid, metric, exog=None, lags_grid=None, return_best=True, n_jobs='auto', verbose=True, show_progress=True, output_file=None)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoreg, ForecasterAutoregDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>`None`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection/_search.py</code> <pre><code>def grid_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: Union[TimeSeriesFold, OneStepAheadFold],\n    param_grid: dict,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    lags_grid: Optional[Union[list, dict]] = None,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        param_grid            = param_grid,\n        metric                = metric,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.random_search_forecaster","title":"<code>random_search_forecaster(forecaster, y, cv, param_distributions, metric, exog=None, lags_grid=None, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True, output_file=None)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoreg, ForecasterAutoregDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>`None`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection/_search.py</code> <pre><code>def random_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: Union[TimeSeriesFold, OneStepAheadFold],\n    param_distributions: dict,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    lags_grid: Optional[Union[list, dict]] = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        param_grid            = param_grid,\n        metric                = metric,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.bayesian_search_forecaster","title":"<code>bayesian_search_forecaster(forecaster, y, cv, search_space, metric, exog=None, n_trials=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True, output_file=None, kwargs_create_study={}, kwargs_study_optimize={})</code>","text":"<p>Bayesian optimization for a Forecaster object using time series backtesting and  optuna library.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoreg, ForecasterAutoregDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>search_space</code> <code>Callable(optuna)</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the sampling for reproducible output. When a new sampler  is passed in <code>kwargs_create_study</code>, the seed must be set within the  sampler. For example <code>{'sampler': TPESampler(seed=145)}</code>.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <code>kwargs_create_study</code> <code>dict</code> <p>Keyword arguments (key, value mappings) to pass to optuna.create_study(). If default, the direction is set to 'minimize' and a TPESampler(seed=123)  sampler is used during optimization.</p> <code>`{}`</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Other keyword arguments (key, value mappings) to pass to study.optimize().</p> <code>`{}`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> <code>best_trial</code> <code>optuna object</code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast/model_selection/_search.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: Union[TimeSeriesFold, OneStepAheadFold],\n    search_space: Callable,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    output_file: Optional[str] = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {}\n) -&gt; Tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian optimization for a Forecaster object using time series backtesting and \n    optuna library.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    search_space : Callable (optuna)\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    n_trials : int, default `10`\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default `123`\n        Sets a seed to the sampling for reproducible output. When a new sampler \n        is passed in `kwargs_create_study`, the seed must be set within the \n        sampler. For example `{'sampler': TPESampler(seed=145)}`.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n    kwargs_create_study : dict, default `{}`\n        Keyword arguments (key, value mappings) to pass to optuna.create_study().\n        If default, the direction is set to 'minimize' and a TPESampler(seed=123) \n        sampler is used during optimization.\n    kwargs_study_optimize : dict, default `{}`\n        Other keyword arguments (key, value mappings) to pass to study.optimize().\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n    best_trial : optuna object\n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            (f\"`exog` must have same number of samples as `y`. \"\n             f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n        )\n\n    results, best_trial = _bayesian_search_optuna(\n                                forecaster            = forecaster,\n                                y                     = y,\n                                cv                    = cv,\n                                exog                  = exog,\n                                search_space          = search_space,\n                                metric                = metric,\n                                n_trials              = n_trials,\n                                random_state          = random_state,\n                                return_best           = return_best,\n                                n_jobs                = n_jobs,\n                                verbose               = verbose,\n                                show_progress         = show_progress,\n                                output_file           = output_file,\n                                kwargs_create_study   = kwargs_create_study,\n                                kwargs_study_optimize = kwargs_study_optimize\n                          )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold","title":"<code>BaseFold(steps=None, initial_train_size=None, window_size=None, differentiation=None, refit=False, fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, return_all_indexes=False, verbose=True)</code>","text":"<p>Base class for all Fold classes in skforecast. All fold classes should specify all the parameters that can be set at the class level in their <code>__init__</code>.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>`None`</code> <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>`None`</code> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <code>`False`</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9.</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete.</p> <code>`True`</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>`False`</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>`True`</code> <p>Attributes:</p> Name Type Description <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def __init__(\n    self,\n    steps: Optional[int] = None,\n    initial_train_size: Optional[int] = None,\n    window_size: Optional[int] = None,\n    differentiation: Optional[int] = None,\n    refit: Union[bool, int] = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None:\n\n    self._validate_params(\n        cv_name               = type(self).__name__,\n        steps                 = steps,\n        initial_train_size    = initial_train_size,\n        window_size           = window_size,\n        differentiation       = differentiation,\n        refit                 = refit,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        skip_folds            = skip_folds,\n        allow_incomplete_fold = allow_incomplete_fold,\n        return_all_indexes    = return_all_indexes,\n        verbose               = verbose\n    )\n\n    self.steps                 = steps\n    self.initial_train_size    = initial_train_size\n    self.window_size           = window_size\n    self.differentiation       = differentiation\n    self.refit                 = refit\n    self.fixed_train_size      = fixed_train_size\n    self.gap                   = gap\n    self.skip_folds            = skip_folds\n    self.allow_incomplete_fold = allow_incomplete_fold\n    self.return_all_indexes    = return_all_indexes\n    self.verbose               = verbose\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold._validate_params","title":"<code>_validate_params(cv_name, steps=None, initial_train_size=None, window_size=None, differentiation=None, refit=False, fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, return_all_indexes=False, verbose=True)</code>","text":"<p>Validate all input parameters to ensure correctness.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def _validate_params(\n    self,\n    cv_name: str,\n    steps: Optional[int] = None,\n    initial_train_size: Optional[int] = None,\n    window_size: Optional[int] = None,\n    differentiation: Optional[int] = None,\n    refit: Union[bool, int] = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None: \n    \"\"\"\n    Validate all input parameters to ensure correctness.\n    \"\"\"\n\n    if cv_name == \"TimeSeriesFold\":\n        if not isinstance(steps, (int, np.integer)) or steps &lt; 1:\n            raise ValueError(\n                f\"`steps` must be an integer greater than 0. Got {steps}.\"\n            )\n        if not isinstance(initial_train_size, (int, np.integer, type(None))):\n            raise ValueError(\n                f\"`initial_train_size` must be an integer or None. Got {initial_train_size}.\"\n            )\n        if not isinstance(refit, (bool, int, np.integer)):\n            raise TypeError(\n                f\"`refit` must be a boolean or an integer greater than 0. Got {refit}.\"\n            )\n        if not isinstance(fixed_train_size, bool):\n            raise TypeError(\n                (f\"`fixed_train_size` must be a boolean: `True`, `False`. \"\n                 f\"Got {fixed_train_size}.\")\n            )\n        if not isinstance(gap, (int, np.integer)) or gap &lt; 0:\n            raise ValueError(\n                f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n            )\n        if skip_folds is not None:\n            if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                raise TypeError(\n                    (f\"`skip_folds` must be an integer greater than 0, a list of \"\n                     f\"integers or `None`. Got {type(skip_folds)}.\")\n                )\n            if isinstance(skip_folds, (int, np.integer)) and skip_folds &lt; 1:\n                raise ValueError(\n                    (f\"`skip_folds` must be an integer greater than 0, a list of \"\n                     f\"integers or `None`. Got {skip_folds}.\")\n                )\n            if isinstance(skip_folds, list) and any([x &lt; 1 for x in skip_folds]):\n                raise ValueError(\n                    (f\"`skip_folds` list must contain integers greater than or \"\n                     f\"equal to 1. The first fold is always needed to train the \"\n                    f\"forecaster. Got {skip_folds}.\")\n                ) \n        if not isinstance(allow_incomplete_fold, bool):\n            raise TypeError(\n                (f\"`allow_incomplete_fold` must be a boolean: `True`, `False`. \"\n                f\"Got {allow_incomplete_fold}.\")\n            )\n\n    if cv_name == \"OneStepAheadFold\":\n        if not isinstance(initial_train_size, (int, np.integer)):\n            raise ValueError(\n                f\"`initial_train_size` must be an integer. Got {initial_train_size}.\"\n            )\n\n    if (\n        not isinstance(window_size, (int, np.integer, type(None)))\n        or window_size is not None\n        and window_size &lt; 1\n    ):\n        raise ValueError(\n            f\"`window_size` must be an integer greater than 0. Got {window_size}.\"\n        )\n\n    if not isinstance(return_all_indexes, bool):\n        raise TypeError(\n            (f\"`return_all_indexes` must be a boolean: `True`, `False`. \"\n             f\"Got {return_all_indexes}.\")\n        )\n    if differentiation is not None:\n        if not isinstance(differentiation, (int, np.integer)) or differentiation &lt; 0:\n            raise ValueError(\n                (f\"`differentiation` must be None or an integer greater than or \"\n                 f\"equal to 0. Got {differentiation}.\")\n            )\n    if not isinstance(verbose, bool):\n        raise TypeError(\n            (f\"`verbose` must be a boolean: `True`, `False`. \"\n             f\"Got {verbose}.\")\n        )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold._extract_index","title":"<code>_extract_index(X)</code>","text":"<p>Extracts and returns the index from the input data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame, pandas Index, dict</code> <p>Time series data or index to split.</p> required <p>Returns:</p> Name Type Description <code>idx</code> <code>pandas Index</code> <p>Index extracted from the input data.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def _extract_index(\n    self,\n    X: Union[pd.Series, pd.DataFrame, pd.Index, dict]\n) -&gt; pd.Index:\n    \"\"\"\n    Extracts and returns the index from the input data X.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame, pandas Index, dict\n        Time series data or index to split.\n\n    Returns\n    -------\n    idx : pandas Index\n        Index extracted from the input data.\n\n    \"\"\"\n\n    if isinstance(X, (pd.Series, pd.DataFrame)):\n        idx = X.index\n    elif isinstance(X, dict):\n        freqs = [s.index.freq for s in X.values() if s.index.freq is not None]\n        if not freqs:\n            raise ValueError(\"At least one series must have a frequency.\")\n        if not all(f == freqs[0] for f in freqs):\n            raise ValueError(\n                \"All series with frequency must have the same frequency.\"\n            )\n        min_idx = min([v.index[0] for v in X.values()])\n        max_idx = max([v.index[-1] for v in X.values()])\n        idx = pd.date_range(start=min_idx, end=max_idx, freq=freqs[0])\n    else:\n        idx = X\n\n    return idx\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.set_params","title":"<code>set_params(params)</code>","text":"<p>Set the parameters of the TimeSeriesFold object. Before overwriting the current parameters, the input parameters are validated to ensure correctness.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary with the parameters to set.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set the parameters of the TimeSeriesFold object. Before overwriting the\n    current parameters, the input parameters are validated to ensure correctness.\n\n    Parameters\n    ----------\n    params : dict\n        Dictionary with the parameters to set.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(params, dict):\n        raise ValueError(\n            f\"`params` must be a dictionary. Got {type(params)}.\"\n        )\n\n    current_params = deepcopy(vars(self))\n    unknown_params = set(params.keys()) - set(current_params.keys())\n    if unknown_params:\n        warnings.warn(\n            f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n            IgnoredArgumentWarning\n        )\n    updated_params = {'cv_name': type(self).__name__, **current_params, **params}\n    self._validate_params(**updated_params)\n    for key, value in updated_params.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold","title":"<code>TimeSeriesFold(steps, initial_train_size=None, window_size=None, differentiation=None, refit=False, fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, return_all_indexes=False, verbose=True)</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds.  When used within a backtesting or hyperparameter search, the arguments 'initial_train_size', 'window_size' and 'differentiation' are not required as they are automatically set by the backtesting or hyperparameter search functions.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> required <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>`None`</code> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <code>`False`</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9.</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete.</p> <code>`True`</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>`False`</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>`True`</code> <p>Attributes:</p> Name Type Description <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Notes <p>Returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is <code>X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code>, the  <code>initial_train_size = 3</code>, <code>window_size = 2</code>, <code>steps = 4</code>, and <code>gap = 1</code>, the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].</p> <p>The first list <code>[0, 3]</code> indicates that the training set goes from the first to the third observation. The second list <code>[1, 3]</code> indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list <code>[3, 8]</code> indicates that the test set goes from the fourth to the eighth observation. The fourth list <code>[4, 8]</code> indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean <code>False</code> indicates that the forecaster should not be trained in this fold.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def __init__(\n    self,\n    steps: int,\n    initial_train_size: Optional[int] = None,\n    window_size: Optional[int] = None,\n    differentiation: Optional[int] = None,\n    refit: Union[bool, int] = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None:\n\n    super().__init__(\n        steps                 = steps,\n        initial_train_size    = initial_train_size,\n        window_size           = window_size,\n        differentiation       = differentiation,\n        refit                 = refit,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        skip_folds            = skip_folds,\n        allow_incomplete_fold = allow_incomplete_fold,\n        return_all_indexes    = return_all_indexes,\n        verbose               = verbose\n    )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold.split","title":"<code>split(X, externally_fitted=False, as_pandas=False)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame, pandas Index, dict</code> <p>Time series data or index to split.</p> required <code>externally_fitted</code> <code>bool</code> <p>If True, the forecaster is assumed to be already fitted so no training is done in the first fold.</p> <code>`False`</code> <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>folds</code> <code>list, pandas DataFrame</code> <p>A list of lists containing the indices (position) for for each fold. Each list contains 4 lists and a boolean with the following information:</p> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> <li>[last_window_start, last_window_end]: list with the start and end positions of the last window seen by the forecaster during training. The last window is used to generate the lags use as predictors. If <code>differentiation</code> is included, the interval is extended as many observations as the differentiation order. If the argument <code>window_size</code> is <code>None</code>, this list is empty.</li> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> <li>[test_start_with_gap, test_end_with_gap]: list with the start and end positions of the test set including the gap. The gap is the number of observations between the end of the training set and the start of the test set.</li> <li>fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.</li> </ul> <p>It is important to note that the returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def split(\n    self,\n    X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n    externally_fitted: bool = False,\n    as_pandas: bool = False\n) -&gt; Union[list, pd.DataFrame]:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame, pandas Index, dict\n        Time series data or index to split.\n    externally_fitted : bool, default `False`\n        If True, the forecaster is assumed to be already fitted so no training is\n        done in the first fold.\n    as_pandas : bool, default `False`\n        If True, the folds are returned as a DataFrame. This is useful to visualize\n        the folds in a more interpretable way.\n\n    Returns\n    -------\n    folds : list, pandas DataFrame\n        A list of lists containing the indices (position) for for each fold. Each list\n        contains 4 lists and a boolean with the following information:\n\n        - [train_start, train_end]: list with the start and end positions of the\n        training set.\n        - [last_window_start, last_window_end]: list with the start and end positions\n        of the last window seen by the forecaster during training. The last window\n        is used to generate the lags use as predictors. If `differentiation` is\n        included, the interval is extended as many observations as the\n        differentiation order. If the argument `window_size` is `None`, this list is\n        empty.\n        - [test_start, test_end]: list with the start and end positions of the test\n        set. These are the observations used to evaluate the forecaster.\n        - [test_start_with_gap, test_end_with_gap]: list with the start and end\n        positions of the test set including the gap. The gap is the number of\n        observations between the end of the training set and the start of the test\n        set.\n        - fit_forecaster: boolean indicating whether the forecaster should be fitted\n        in this fold.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n        'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n        'test_end_with_gap', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            (f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n             f\"Got {type(X)}.\")\n        )\n\n    if self.window_size is None:\n        warnings.warn(\n            \"Last window cannot be calculated because `window_size` is None.\"\n        )\n\n    index = self._extract_index(X)\n    idx = range(len(X))\n    folds = []\n    i = 0\n    last_fold_excluded = False\n\n    if len(index) &lt; self.initial_train_size + self.steps:\n        raise ValueError(\n            (f\"The time series must have at least `initial_train_size + steps` \"\n             f\"observations. Got {len(index)} observations.\")\n        )\n\n    while self.initial_train_size + (i * self.steps) + self.gap &lt; len(index):\n\n        if self.refit:\n            # If `fixed_train_size` the train size doesn't increase but moves by \n            # `steps` positions in each iteration. If `False`, the train size\n            # increases by `steps` in each iteration.\n            train_iloc_start = i * (self.steps) if self.fixed_train_size else 0\n            train_iloc_end = self.initial_train_size + i * (self.steps)\n            test_iloc_start = train_iloc_end\n        else:\n            # The train size doesn't increase and doesn't move.\n            train_iloc_start = 0\n            train_iloc_end = self.initial_train_size\n            test_iloc_start = self.initial_train_size + i * (self.steps)\n\n        if self.window_size is not None:\n            last_window_iloc_start = test_iloc_start - self.window_size\n        test_iloc_end = test_iloc_start + self.gap + self.steps\n\n        partitions = [\n            idx[train_iloc_start : train_iloc_end],\n            idx[last_window_iloc_start : test_iloc_start] if self.window_size is not None else [],\n            idx[test_iloc_start : test_iloc_end],\n            idx[test_iloc_start + self.gap : test_iloc_end]\n        ]\n        folds.append(partitions)\n        i += 1\n\n    if not self.allow_incomplete_fold and len(folds[-1][3]) &lt; self.steps:\n        folds = folds[:-1]\n        last_fold_excluded = True\n\n    # Replace partitions inside folds with length 0 with `None`\n    folds = [\n        [partition if len(partition) &gt; 0 else None for partition in fold] \n        for fold in folds\n    ]\n\n    # Create a flag to know whether to train the forecaster\n    if self.refit == 0:\n        self.refit = False\n\n    if isinstance(self.refit, bool):\n        fit_forecaster = [self.refit] * len(folds)\n        fit_forecaster[0] = True\n    else:\n        fit_forecaster = [False] * len(folds)\n        for i in range(0, len(fit_forecaster), self.refit): \n            fit_forecaster[i] = True\n\n    for i in range(len(folds)): \n        folds[i].append(fit_forecaster[i])\n        if fit_forecaster[i] is False:\n            folds[i][0] = folds[i - 1][0]\n\n    index_to_skip = []\n    if self.skip_folds is not None:\n        if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n            index_to_keep = np.arange(0, len(folds), self.skip_folds)\n            index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n            index_to_skip = [int(x) for x in index_to_skip]  # Required since numpy 2.0\n        if isinstance(self.skip_folds, list):\n            index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]        \n\n    if self.verbose:\n        self._print_info(\n            index = index,\n            folds = folds,\n            externally_fitted = externally_fitted,\n            last_fold_excluded = last_fold_excluded,\n            index_to_skip = index_to_skip\n        )\n\n    folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n    if not self.return_all_indexes:\n        # +1 to prevent iloc pandas from deleting the last observation\n        folds = [\n            [[fold[0][0], fold[0][-1] + 1], \n             [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [],\n             [fold[2][0], fold[2][-1] + 1],\n             [fold[3][0], fold[3][-1] + 1],\n             fold[4]] \n            for fold in folds\n        ]\n\n    if as_pandas:\n        if self.window_size is None:\n            for fold in folds:\n                fold[1] = [None, None]\n\n        if not self.return_all_indexes:\n            folds = pd.DataFrame(\n                data = [list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds],\n                columns = [\n                    'train_start',\n                    'train_end',\n                    'last_window_start',\n                    'last_window_end',\n                    'test_start',\n                    'test_end',\n                    'test_start_with_gap',\n                    'test_end_with_gap',\n                    'fit_forecaster'\n                ],\n            )\n        else:\n            folds = pd.DataFrame(\n                data = folds,\n                columns = [\n                    'train_index',\n                    'last_window_index',\n                    'test_index',\n                    'test_index_with_gap',\n                    'fit_forecaster'\n                ],\n            )\n        folds.insert(0, 'fold', range(len(folds)))\n\n    return folds\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold._print_info","title":"<code>_print_info(index, folds, externally_fitted, last_fold_excluded, index_to_skip)</code>","text":"<p>Print information about folds.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def _print_info(\n    self,\n    index: pd.Index,\n    folds: list,\n    externally_fitted: bool,\n    last_fold_excluded: bool,\n    index_to_skip: list,\n) -&gt; None:\n    \"\"\"\n    Print information about folds.\n    \"\"\"\n\n    print(\"Information of folds\")\n    print(\"--------------------\")\n    if externally_fitted:\n        print(\n            f\"An already trained forecaster is to be used. Window size: \"\n            f\"{self.window_size}\"\n        )\n    else:\n        if self.differentiation is None:\n            print(\n                f\"Number of observations used for initial training: \"\n                f\"{self.initial_train_size}\"\n            )\n        else:\n            print(\n                f\"Number of observations used for initial training: \"\n                f\"{self.initial_train_size - self.differentiation}\"\n            )\n            print(f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n            )\n    print(\n        f\"Number of observations used for backtesting: \"\n        f\"{len(index) - self.initial_train_size}\"\n    )\n    print(f\"    Number of folds: {len(folds)}\")\n    print(\n        f\"    Number skipped folds: \"\n        f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n    )\n    print(f\"    Number of steps per fold: {self.steps}\")\n    print(\n        f\"    Number of steps to exclude from the end of each train set \"\n        f\"before test (gap): {self.gap}\"\n    )\n    if last_fold_excluded:\n        print(\"    Last fold has been excluded because it was incomplete.\")\n    if len(folds[-1][3]) &lt; self.steps:\n        print(f\"    Last fold only includes {len(folds[-1][3])} observations.\")\n    print(\"\")\n\n    if self.differentiation is None:\n        differentiation = 0\n    else:\n        differentiation = self.differentiation\n\n    for i, fold in enumerate(folds):\n        is_fold_skipped   = i in index_to_skip\n        has_training      = fold[-1] if i != 0 else True\n        training_start    = (\n            index[fold[0][0] + differentiation] if fold[0] is not None else None\n        )\n        training_end      = index[fold[0][-1]] if fold[0] is not None else None\n        training_length   = (\n            len(fold[0]) - differentiation if fold[0] is not None else 0\n        )\n        validation_start  = index[fold[3][0]]\n        validation_end    = index[fold[3][-1]]\n        validation_length = len(fold[3])\n\n        print(f\"Fold: {i}\")\n        if is_fold_skipped:\n            print(\"    Fold skipped\")\n        elif not externally_fitted and has_training:\n            print(\n                f\"    Training:   {training_start} -- {training_end}  \"\n                f\"(n={training_length})\"\n            )\n            print(\n                f\"    Validation: {validation_start} -- {validation_end}  \"\n                f\"(n={validation_length})\"\n            )\n        else:\n            print(\"    Training:   No training in this fold\")\n            print(\n                f\"    Validation: {validation_start} -- {validation_end}  \"\n                f\"(n={validation_length})\"\n            )\n\n    print(\"\")\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold","title":"<code>OneStepAheadFold(initial_train_size, window_size=None, differentiation=None, return_all_indexes=False, verbose=True)</code>","text":"<p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds for one-step-ahead forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> required <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>`None`</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>`False`</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>`True`</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>steps</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>fixed_train_size</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>gap</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>skip_folds</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>allow_incomplete_fold</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>refit</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def __init__(\n    self,\n    initial_train_size: int,\n    window_size: Optional[int] = None,\n    differentiation: Optional[int] = None,\n    return_all_indexes: bool = False,\n    verbose: bool = True,\n) -&gt; None:\n\n    super().__init__(\n        initial_train_size = initial_train_size,\n        window_size        = window_size,\n        differentiation    = differentiation,\n        return_all_indexes = return_all_indexes,\n        verbose            = verbose\n    )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold.split","title":"<code>split(X, as_pandas=False, externally_fitted=None)</code>","text":"<p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, DataFrame, Index, or dictionary</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way.</p> <code>`False`</code> <code>externally_fitted</code> <code>Any</code> <p>This argument is not used in this class. It is included for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fold</code> <code>list, pandas DataFrame</code> <p>A list of lists containing the indices (position) for for each fold. Each list contains 2 lists the following information:</p> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> </ul> <p>It is important to note that the returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def split(\n    self,\n    X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n    as_pandas: bool = False,\n    externally_fitted: Any = None\n) -&gt; Union[list, pd.DataFrame]:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Parameters\n    ----------\n    X : pandas Series, DataFrame, Index, or dictionary\n        Time series data or index to split.\n    as_pandas : bool, default `False`\n        If True, the folds are returned as a DataFrame. This is useful to visualize\n        the folds in a more interpretable way.\n    externally_fitted : Any\n        This argument is not used in this class. It is included for API consistency.\n\n    Returns\n    -------\n    fold : list, pandas DataFrame\n        A list of lists containing the indices (position) for for each fold. Each list\n        contains 2 lists the following information:\n\n        - [train_start, train_end]: list with the start and end positions of the\n        training set.\n        - [test_start, test_end]: list with the start and end positions of the test\n        set. These are the observations used to evaluate the forecaster.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n\n    \"\"\"\n\n    index = self._extract_index(X)\n    fold = [\n        [0, self.initial_train_size],\n        [self.initial_train_size, len(X)],\n        True\n    ]\n\n    if self.verbose:\n        self._print_info(\n            index = index,\n            fold = fold,\n        )\n\n    if self.return_all_indexes:\n        fold = [\n            [range(fold[0][0], fold[0][1])],\n            [range(fold[1][0], fold[1][1])],\n            fold[2]\n        ]\n\n    if as_pandas:\n        if not self.return_all_indexes:\n            fold = pd.DataFrame(\n                data = [list(itertools.chain(*fold[:-1])) + [fold[-1]]],\n                columns = [\n                    'train_start',\n                    'train_end',\n                    'test_start',\n                    'test_end',\n                    'fit_forecaster'\n                ],\n            )\n        else:\n            fold = pd.DataFrame(\n                data = [fold],\n                columns = [\n                    'train_index',\n                    'test_index',\n                    'fit_forecaster'\n                ],\n            )\n        fold.insert(0, 'fold', range(len(fold)))\n\n    return fold\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold._print_info","title":"<code>_print_info(index, fold)</code>","text":"<p>Print information about folds.</p> Source code in <code>skforecast/model_selection/_split.py</code> <pre><code>def _print_info(\n    self,\n    index: pd.Index,\n    fold: list,\n) -&gt; None:\n    \"\"\"\n    Print information about folds.\n    \"\"\"\n\n    print(\"Information of folds\")\n    print(\"--------------------\")\n    print(\n        f\"Number of observations used for initial training: \"\n        f\"{self.initial_train_size}\"\n    )\n    print(\n        f\"Number of observations in test: \"\n        f\"{len(index) - self.initial_train_size}\"\n    )\n\n    if self.differentiation is None:\n        differentiation = 0\n    else:\n        differentiation = self.differentiation\n\n    training_start = index[fold[0][0] + differentiation]\n    training_end = index[fold[0][-1]]\n    training_length = self.initial_train_size\n    test_start  = index[fold[1][0]]\n    test_end    = index[fold[1][-1] - 1]\n    test_length = len(index) - self.initial_train_size\n\n    print(\n        f\"Training : {training_start} -- {training_end} (n={training_length})\"\n    )\n    print(\n        f\"Test     : {test_start} -- {test_end} (n={test_length})\"\n    )\n    print(\"\")\n</code></pre>"},{"location":"api/model_selection_multiseries.html","title":"<code>model_selection_multiseries</code>","text":""},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.backtesting_forecaster_multiseries","title":"<code>backtesting_forecaster_multiseries(forecaster, series, steps, metric, initial_train_size, fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, levels=None, add_aggregated_metric=True, exog=None, refit=False, interval=None, n_boot=500, random_state=123, use_in_sample_residuals=True, n_jobs='auto', verbose=False, show_progress=True, suppress_warnings=False)</code>","text":"<p>Backtesting for multi-series and multivariate forecasters.</p> <ul> <li>If <code>refit</code> is <code>False</code>, the model will be trained only once using the  <code>initial_train_size</code> first observations. </li> <li>If <code>refit</code> is <code>True</code>, the model is trained on each iteration, increasing the training set. </li> <li>If <code>refit</code> is an <code>integer</code>, the model will be trained every that number  of iterations.</li> <li>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is <code>None</code>, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</li> </ul> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate, ForecasterRnn)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> <code>`None`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>add_aggregated_metric</code> <code>bool</code> <p>If <code>True</code>, and multiple series (<code>levels</code>) are predicted, the aggregated metrics (average, weighted average and pooled) are also returned.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If <code>None</code>, no intervals are estimated.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals. If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the backtesting  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>. If there is more than one level, this structure will be repeated for each of them.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def backtesting_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int],\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    levels: Optional[Union[str, list]] = None,\n    add_aggregated_metric: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    refit: Union[bool, int] = False,\n    interval: Optional[list] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting for multi-series and multivariate forecasters.\n\n    - If `refit` is `False`, the model will be trained only once using the \n    `initial_train_size` first observations. \n    - If `refit` is `True`, the model is trained on each iteration, increasing\n    the training set. \n    - If `refit` is an `integer`, the model will be trained every that number \n    of iterations.\n    - If `forecaster` is already trained and `initial_train_size` is `None`,\n    no initial train will be done and all data will be used to evaluate the model.\n    However, the first `len(forecaster.last_window)` observations are needed\n    to create the initial predictors, so no predictions are calculated for them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    skip_folds : int, list, default `None`\n        If `skip_folds` is an integer, every 'skip_folds'-th is returned. If `skip_folds`\n        is a list, the folds in the list are skipped. For example, if `skip_folds = 3`,\n        and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If `skip_folds`\n        is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    multi_series_forecasters = [\n        'ForecasterAutoregMultiSeries', \n        'ForecasterAutoregMultiVariate',\n        'ForecasterRnn'\n    ]\n\n    forecaster_name = type(forecaster).__name__\n\n    if forecaster_name not in multi_series_forecasters:\n        raise TypeError(\n            (f\"`forecaster` must be of type {multi_series_forecasters}, \"\n             f\"for all other types of forecasters use the functions available in \"\n             f\"the `model_selection` module. Got {forecaster_name}\")\n        )\n\n    check_backtesting_input(\n        forecaster              = forecaster,\n        steps                   = steps,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        series                  = series,\n        exog                    = exog,\n        initial_train_size      = initial_train_size,\n        fixed_train_size        = fixed_train_size,\n        gap                     = gap,\n        skip_folds              = skip_folds,\n        allow_incomplete_fold   = allow_incomplete_fold,\n        refit                   = refit,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(\n        forecaster              = forecaster,\n        series                  = series,\n        steps                   = steps,\n        levels                  = levels,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        initial_train_size      = initial_train_size,\n        fixed_train_size        = fixed_train_size,\n        gap                     = gap,\n        skip_folds              = skip_folds,\n        allow_incomplete_fold   = allow_incomplete_fold,\n        exog                    = exog,\n        refit                   = refit,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.grid_search_forecaster_multiseries","title":"<code>grid_search_forecaster_multiseries(forecaster, series, param_grid, metric, initial_train_size, steps=None, method='backtesting', aggregate_metric=['weighted_average', 'average', 'pooling'], fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, return_best=True, n_jobs='auto', verbose=True, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> <code>`None`</code> <code>method</code> <code>str</code> <p>Method used to evaluate the model.</p> <ul> <li>'backtesting': the model is evaluated using backtesting process in which the model predicts <code>steps</code> ahead in each iteration.</li> <li>'one_step_ahead': the model is evaluated using only one step ahead predictions. This is faster than backtesting but the results may not reflect the actual performance of the model when predicting multiple steps ahead. Arguments <code>steps</code>, <code>fixed_train_size</code>, <code>gap</code>, <code>skip_folds</code>, <code>allow_incomplete_fold</code> and <code>refit</code> are  ignored when <code>method</code> is 'one_step_ahead'. New in version 0.14.0</li> </ul> <code>`'backtesting'`</code> <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter  search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def grid_search_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    param_grid: dict,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    steps: Optional[int] = None,\n    method: str = 'backtesting',\n    aggregate_metric: Union[str, list] = ['weighted_average', 'average', 'pooling'],\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    levels: Optional[Union[str, list]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    lags_grid: Optional[Union[list, dict]] = None,\n    refit: Union[bool, int] = False,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    steps : int, default `None`\n        Number of steps to predict.\n    method : str, default `'backtesting'`\n        Method used to evaluate the model.\n\n        - 'backtesting': the model is evaluated using backtesting process in which\n        the model predicts `steps` ahead in each iteration.\n        - 'one_step_ahead': the model is evaluated using only one step ahead predictions.\n        This is faster than backtesting but the results may not reflect the actual\n        performance of the model when predicting multiple steps ahead. Arguments `steps`,\n        `fixed_train_size`, `gap`, `skip_folds`, `allow_incomplete_fold` and `refit` are \n        ignored when `method` is 'one_step_ahead'.\n        **New in version 0.14.0**\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    skip_folds : int, list, default `None`\n        If `skip_folds` is an integer, every 'skip_folds'-th is returned. If `skip_folds`\n        is a list, the folds in the list are skipped. For example, if `skip_folds = 3`,\n        and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If `skip_folds`\n        is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the hyperparameter \n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    if method not in ['backtesting', 'one_step_ahead']:\n        raise ValueError(\n            f\"`method` must be 'backtesting' or 'one_step_ahead'. Got {method}.\"\n        )\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                forecaster            = forecaster,\n                series                = series,\n                param_grid            = param_grid,\n                steps                 = steps,\n                method                = method,\n                metric                = metric,\n                aggregate_metric      = aggregate_metric,\n                initial_train_size    = initial_train_size,\n                fixed_train_size      = fixed_train_size,\n                gap                   = gap,\n                skip_folds            = skip_folds,\n                allow_incomplete_fold = allow_incomplete_fold,\n                levels                = levels,\n                exog                  = exog,\n                lags_grid             = lags_grid,\n                refit                 = refit,\n                n_jobs                = n_jobs,\n                return_best           = return_best,\n                verbose               = verbose,\n                show_progress         = show_progress,\n                suppress_warnings     = suppress_warnings,\n                output_file           = output_file\n            )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.random_search_forecaster_multiseries","title":"<code>random_search_forecaster_multiseries(forecaster, series, param_distributions, metric, initial_train_size, steps=None, method='backtesting', aggregate_metric=['weighted_average', 'average', 'pooling'], fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True, suppress_warnings=False, output_file=None)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> <code>`None`</code> <code>method</code> <code>str</code> <p>Method used to evaluate the model.</p> <ul> <li>'backtesting': the model is evaluated using backtesting process in which the model predicts <code>steps</code> ahead in each iteration.</li> <li>'one_step_ahead': the model is evaluated using only one step ahead predictions. This is faster than backtesting but the results may not reflect the actual performance of the model when predicting multiple steps ahead. Arguments <code>steps</code>, <code>fixed_train_size</code>, <code>gap</code>, <code>skip_folds</code>, <code>allow_incomplete_fold</code> and <code>refit</code> are  ignored when <code>method</code> is 'one_step_ahead'. New in version 0.14.0</li> </ul> <code>`'backtesting'`</code> <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter  search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def random_search_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    param_distributions: dict,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    steps: Optional[int] = None,\n    method: str = 'backtesting',\n    aggregate_metric: Union[str, list] = ['weighted_average', 'average', 'pooling'],\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    levels: Optional[Union[str, list]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    lags_grid: Optional[Union[list, dict]] = None,\n    refit: Union[bool, int] = False,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    steps : int, default `None`\n        Number of steps to predict.\n    method : str, default `'backtesting'`\n        Method used to evaluate the model.\n\n        - 'backtesting': the model is evaluated using backtesting process in which\n        the model predicts `steps` ahead in each iteration.\n        - 'one_step_ahead': the model is evaluated using only one step ahead predictions.\n        This is faster than backtesting but the results may not reflect the actual\n        performance of the model when predicting multiple steps ahead. Arguments `steps`,\n        `fixed_train_size`, `gap`, `skip_folds`, `allow_incomplete_fold` and `refit` are \n        ignored when `method` is 'one_step_ahead'.\n        **New in version 0.14.0**\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    skip_folds : int, list, default `None`\n        If `skip_folds` is an integer, every 'skip_folds'-th is returned. If `skip_folds`\n        is a list, the folds in the list are skipped. For example, if `skip_folds = 3`,\n        and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If `skip_folds`\n        is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the hyperparameter \n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    if method not in ['backtesting', 'one_step_ahead']:\n        raise ValueError(\n            f\"`method` must be 'backtesting' or 'one_step_ahead'. Got {method}.\"\n        )\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, \n                                       random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                forecaster            = forecaster,\n                series                = series,\n                param_grid            = param_grid,\n                steps                 = steps,\n                method                = method,\n                metric                = metric,\n                aggregate_metric      = aggregate_metric,\n                initial_train_size    = initial_train_size,\n                fixed_train_size      = fixed_train_size,\n                gap                   = gap,\n                skip_folds            = skip_folds,\n                allow_incomplete_fold = allow_incomplete_fold,\n                levels                = levels,\n                exog                  = exog,\n                lags_grid             = lags_grid,\n                refit                 = refit,\n                return_best           = return_best,\n                n_jobs                = n_jobs,\n                verbose               = verbose,\n                show_progress         = show_progress,\n                suppress_warnings     = suppress_warnings,\n                output_file            = output_file\n            )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.bayesian_search_forecaster_multiseries","title":"<code>bayesian_search_forecaster_multiseries(forecaster, series, search_space, metric, initial_train_size, steps=None, method='backtesting', aggregate_metric=['weighted_average', 'average', 'pooling'], fixed_train_size=True, gap=0, skip_folds=None, allow_incomplete_fold=True, levels=None, exog=None, refit=False, n_trials=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True, suppress_warnings=False, output_file=None, kwargs_create_study={}, kwargs_study_optimize={})</code>","text":"<p>Bayesian optimization for a Forecaster object using multi-series backtesting  and optuna library. New in version 0.12.0</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>search_space</code> <code>Callable</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> <code>`None`</code> <code>method</code> <code>str</code> <p>Method used to evaluate the model.</p> <ul> <li>'backtesting': the model is evaluated using backtesting process in which the model predicts <code>steps</code> ahead in each iteration.</li> <li>'one_step_ahead': the model is evaluated using only one step ahead predictions. This is faster than backtesting but the results may not reflect the actual performance of the model when predicting multiple steps ahead. Arguments <code>steps</code>, <code>fixed_train_size</code>, <code>gap</code>, <code>skip_folds</code>, <code>allow_incomplete_fold</code> and <code>refit</code> are  ignored when <code>method</code> is 'one_step_ahead'. New in version 0.14.0</li> </ul> <code>`'backtesting'`</code> <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>skip_folds</code> <code>(int, list)</code> <p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].</p> <code>`None`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <code>kwargs_create_study</code> <code>dict</code> <p>Keyword arguments (key, value mappings) to pass to optuna.create_study(). If default, the direction is set to 'minimize' and a TPESampler(seed=123)  sampler is used during optimization.</p> <code>`{}`</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Other keyword arguments (key, value mappings) to pass to study.optimize().</p> <code>`{}`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> <code>best_trial</code> <code>optuna object</code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def bayesian_search_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    search_space: Callable,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    steps: Optional[int] = None,\n    method: str = 'backtesting',\n    aggregate_metric: Union[str, list] = ['weighted_average', 'average', 'pooling'],\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: Optional[Union[int, list]] = None,\n    allow_incomplete_fold: bool = True,\n    levels: Optional[Union[str, list]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    refit: Union[bool, int] = False,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: Optional[str] = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {}\n) -&gt; Tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian optimization for a Forecaster object using multi-series backtesting \n    and optuna library.\n    **New in version 0.12.0**\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    search_space : Callable\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    steps : int, default `None`\n        Number of steps to predict.\n    method : str, default `'backtesting'`\n        Method used to evaluate the model.\n\n        - 'backtesting': the model is evaluated using backtesting process in which\n        the model predicts `steps` ahead in each iteration.\n        - 'one_step_ahead': the model is evaluated using only one step ahead predictions.\n        This is faster than backtesting but the results may not reflect the actual\n        performance of the model when predicting multiple steps ahead. Arguments `steps`,\n        `fixed_train_size`, `gap`, `skip_folds`, `allow_incomplete_fold` and `refit` are \n        ignored when `method` is 'one_step_ahead'.\n        **New in version 0.14.0**\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    skip_folds : int, list, default `None`\n        If `skip_folds` is an integer, every 'skip_folds'-th is returned. If `skip_folds`\n        is a list, the folds in the list are skipped. For example, if `skip_folds = 3`,\n        and there are 10 folds, the folds returned will be [0, 3, 6, 9]. If `skip_folds`\n        is a list [1, 2, 3], the folds returned will be [0, 4, 5, 6, 7, 8, 9].\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    n_trials : int, default `10`\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default `123`\n        Sets a seed to the sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the hyperparameter\n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n    kwargs_create_study : dict, default `{}`\n        Keyword arguments (key, value mappings) to pass to optuna.create_study().\n        If default, the direction is set to 'minimize' and a TPESampler(seed=123) \n        sampler is used during optimization.\n    kwargs_study_optimize : dict, default `{}`\n        Other keyword arguments (key, value mappings) to pass to study.optimize().\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n    best_trial : optuna object\n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(series)):\n        raise ValueError(\n            (f\"`exog` must have same number of samples as `series`. \"\n             f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n        )\n\n    if method not in ['backtesting', 'one_step_ahead']:\n        raise ValueError(\n            f\"`method` must be 'backtesting' or 'one_step_ahead'. Got {method}.\"\n        )\n\n    results, best_trial = _bayesian_search_optuna_multiseries(\n                            forecaster            = forecaster,\n                            series                = series,\n                            exog                  = exog,\n                            levels                = levels, \n                            search_space          = search_space,\n                            steps                 = steps,\n                            metric                = metric,\n                            method                = method,\n                            aggregate_metric      = aggregate_metric,\n                            refit                 = refit,\n                            initial_train_size    = initial_train_size,\n                            fixed_train_size      = fixed_train_size,\n                            gap                   = gap,\n                            skip_folds            = skip_folds,\n                            allow_incomplete_fold = allow_incomplete_fold,\n                            n_trials              = n_trials,\n                            random_state          = random_state,\n                            return_best           = return_best,\n                            n_jobs                = n_jobs,\n                            verbose               = verbose,\n                            show_progress         = show_progress,\n                            suppress_warnings     = suppress_warnings,\n                            output_file           = output_file,\n                            kwargs_create_study   = kwargs_create_study,\n                            kwargs_study_optimize = kwargs_study_optimize\n                        )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection_sarimax.html","title":"<code>model_selection_sarimax</code>","text":""},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.backtesting_sarimax","title":"<code>backtesting_sarimax(forecaster, y, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, alpha=None, interval=None, n_jobs='auto', verbose=False, suppress_warnings_fit=False, show_progress=True)</code>","text":"<p>Backtesting of ForecasterSarimax.</p> <ul> <li>If <code>refit</code> is <code>False</code>, the model will be trained only once using the  <code>initial_train_size</code> first observations. </li> <li>If <code>refit</code> is <code>True</code>, the model is trained on each iteration, increasing the training set. </li> <li>If <code>refit</code> is an <code>integer</code>, the model will be trained every that number  of iterations.</li> </ul> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>`0.05`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>`None`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>metric_values</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def backtesting_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    allow_incomplete_fold: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    refit: Union[bool, int] = False,\n    alpha: Optional[float] = None,\n    interval: Optional[list] = None,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n\n    - If `refit` is `False`, the model will be trained only once using the \n    `initial_train_size` first observations. \n    - If `refit` is `True`, the model is trained on each iteration, increasing\n    the training set. \n    - If `refit` is an `integer`, the model will be trained every that number \n    of iterations.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int\n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**     \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterSarimax`, for all other \"\n             \"types of forecasters use the functions available in the other \"\n             \"`model_selection` modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        y                     = y,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        alpha                 = alpha,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    metric_values, backtest_predictions = _backtesting_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        alpha                 = alpha,\n        interval              = interval,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.grid_search_sarimax","title":"<code>grid_search_sarimax(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, return_best=True, n_jobs='auto', verbose=True, suppress_warnings_fit=False, show_progress=True, output_file=None)</code>","text":"<p>Exhaustive search over specified parameter values for a ForecasterSarimax object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def grid_search_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    allow_incomplete_fold: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    refit: Union[bool, int] = False,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a ForecasterSarimax object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.random_search_sarimax","title":"<code>random_search_sarimax(forecaster, y, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, suppress_warnings_fit=False, show_progress=True, output_file=None)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an  integer, the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def random_search_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    allow_incomplete_fold: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    refit: Union[bool, int] = False,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = True,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True,\n    output_file: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an \n        integer, the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    output_file : str, default `None`\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/plot.html","title":"<code>plot</code>","text":""},{"location":"api/plot.html#skforecast.plot.plot.set_dark_theme","title":"<code>set_dark_theme(custom_style=None)</code>","text":"<p>Set aspects of the visual theme for all matplotlib plots. This function changes the global defaults for all plots using the matplotlib rcParams system. The theme includes specific colors for figure and axes backgrounds, gridlines, text, labels, and ticks. It also sets the font size and line width.</p> <p>Parameters:</p> Name Type Description Default <code>custom_style</code> <code>dict</code> <p>Optional dictionary containing custom styles to be added or override the default dark theme. It is applied after the default theme is set by using the <code>plt.rcParams.update()</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def set_dark_theme(\n    custom_style: Optional[dict] = None\n) -&gt; None:\n    \"\"\"\n    Set aspects of the visual theme for all matplotlib plots.\n    This function changes the global defaults for all plots using the matplotlib\n    rcParams system. The theme includes specific colors for figure and axes\n    backgrounds, gridlines, text, labels, and ticks. It also sets the font size\n    and line width.\n\n    Parameters\n    ----------\n    custom_style : dict, default None\n        Optional dictionary containing custom styles to be added or override the\n        default dark theme. It is applied after the default theme is set by\n        using the `plt.rcParams.update()` method.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    plt.style.use('fivethirtyeight')\n    dark_style = {\n        'figure.facecolor': '#212946',\n        'axes.facecolor': '#212946',\n        'savefig.facecolor': '#212946',\n        'axes.grid': True,\n        'axes.grid.which': 'both',\n        'axes.spines.left': False,\n        'axes.spines.right': False,\n        'axes.spines.top': False,\n        'axes.spines.bottom': False,\n        'grid.color': '#2A3459',\n        'grid.linewidth': '1',\n        'text.color': '0.9',\n        'axes.labelcolor': '0.9',\n        'xtick.color': '0.9',\n        'ytick.color': '0.9',\n        'font.size': 10,\n        'lines.linewidth': 1.5\n    }\n\n    if custom_style is not None:\n        dark_style.update(custom_style)\n\n    plt.rcParams.update(dark_style)\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_residuals","title":"<code>plot_residuals(residuals=None, y_true=None, y_pred=None, fig=None, **fig_kw)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>pandas Series, numpy ndarray</code> <p>Values of residuals. If <code>None</code>, residuals are calculated internally using <code>y_true</code> and <code>y_true</code>.</p> <code>`None`.</code> <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>Ground truth (correct) values. Ignored if residuals is not <code>None</code>.</p> <code>`None`.</code> <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Values of predictions. Ignored if residuals is not <code>None</code>.</p> <code>`None`. </code> <code>fig</code> <code>Figure</code> <p>Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure() internally.</p> <code>`None`. </code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.figure()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_residuals(\n    residuals: Union[np.ndarray, pd.Series] = None,\n    y_true: Union[np.ndarray, pd.Series] = None,\n    y_pred: Union[np.ndarray, pd.Series] = None,\n    fig: matplotlib.figure.Figure = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Parameters\n    ----------\n    residuals : pandas Series, numpy ndarray, default `None`.\n        Values of residuals. If `None`, residuals are calculated internally using\n        `y_true` and `y_true`.\n    y_true : pandas Series, numpy ndarray, default `None`.\n        Ground truth (correct) values. Ignored if residuals is not `None`.\n    y_pred : pandas Series, numpy ndarray, default `None`. \n        Values of predictions. Ignored if residuals is not `None`.\n    fig : matplotlib.figure.Figure, default `None`. \n        Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure()\n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.figure()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if residuals is None and (y_true is None or y_pred is None):\n        raise ValueError(\n            \"If `residuals` argument is None then, `y_true` and `y_pred` must be provided.\"\n        )\n\n    if residuals is None:\n        residuals = y_true - y_pred\n\n    if fig is None:\n        fig = plt.figure(constrained_layout=True, **fig_kw)\n\n    gs  = matplotlib.gridspec.GridSpec(2, 2, figure=fig)\n    ax1 = plt.subplot(gs[0, :])\n    ax2 = plt.subplot(gs[1, 0])\n    ax3 = plt.subplot(gs[1, 1])\n\n    ax1.plot(residuals)\n    sns.histplot(residuals, kde=True, bins=30, ax=ax2)\n    plot_acf(residuals, ax=ax3, lags=60)\n\n    ax1.set_title(\"Residuals\")\n    ax2.set_title(\"Distribution\")\n    ax3.set_title(\"Autocorrelation\")\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_multivariate_time_series_corr","title":"<code>plot_multivariate_time_series_corr(corr, ax=None, **fig_kw)</code>","text":"<p>Heatmap plot of a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>pandas DataFrame</code> <p>correlation matrix</p> required <code>ax</code> <code>Axes</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()  internally.</p> <code>`None`. </code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_multivariate_time_series_corr(\n    corr: pd.DataFrame,\n    ax: matplotlib.axes.Axes = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Heatmap plot of a correlation matrix.\n\n    Parameters\n    ----------\n    corr : pandas DataFrame\n        correlation matrix\n    ax : matplotlib.axes.Axes, default `None`. \n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() \n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n\n    sns.heatmap(\n        corr,\n        annot=True,\n        linewidths=.5,\n        ax=ax,\n        cmap=sns.color_palette(\"viridis\", as_cmap=True)\n    )\n\n    ax.set_xlabel('Time series')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_distribution","title":"<code>plot_prediction_distribution(bootstrapping_predictions, bw_method=None, **fig_kw)</code>","text":"<p>Ridge plot of bootstrapping predictions. This plot is very useful to understand  the uncertainty of forecasting predictions.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrapping_predictions</code> <code>pandas DataFrame</code> <p>Bootstrapping predictions created with <code>Forecaster.predict_bootstrapping</code>.</p> required <code>bw_method</code> <code>(str, scalar, Callable)</code> <p>The method used to calculate the estimator bandwidth. This can be 'scott',  'silverman', a scalar constant or a Callable. If None (default), 'scott'  is used. See scipy.stats.gaussian_kde for more information.</p> <code>`None`</code> <code>fig_kw</code> <code>dict</code> <p>All additional keyword arguments are passed to the <code>pyplot.figure</code> call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_prediction_distribution(\n    bootstrapping_predictions: pd.DataFrame,\n    bw_method: Optional[Any] = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Ridge plot of bootstrapping predictions. This plot is very useful to understand \n    the uncertainty of forecasting predictions.\n\n    Parameters\n    ----------\n    bootstrapping_predictions : pandas DataFrame\n        Bootstrapping predictions created with `Forecaster.predict_bootstrapping`.\n    bw_method : str, scalar, Callable, default `None`\n        The method used to calculate the estimator bandwidth. This can be 'scott', \n        'silverman', a scalar constant or a Callable. If None (default), 'scott' \n        is used. See scipy.stats.gaussian_kde for more information.\n    fig_kw : dict\n        All additional keyword arguments are passed to the `pyplot.figure` call.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    index = bootstrapping_predictions.index.astype(str).to_list()[::-1]\n    palette = sns.cubehelix_palette(len(index), rot=-.25, light=.7, reverse=False)\n    fig, axs = plt.subplots(len(index), 1, sharex=True, **fig_kw)\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    for i, step in enumerate(index):\n        plot = (\n            bootstrapping_predictions.loc[step, :]\n            .plot.kde(ax=axs[i], bw_method=bw_method, lw=0.5)\n        )\n\n        # Fill density area\n        x = plot.get_children()[0]._x\n        y = plot.get_children()[0]._y\n        axs[i].fill_between(x, y, color=palette[i])\n        prediction_mean = bootstrapping_predictions.loc[step, :].mean()\n\n        # Closest point on x to the prediction mean\n        idx = np.abs(x - prediction_mean).argmin()\n        axs[i].vlines(x[idx], ymin=0, ymax=y[idx], linestyle=\"dashed\", color='w')\n\n        axs[i].spines['top'].set_visible(False)\n        axs[i].spines['right'].set_visible(False)\n        axs[i].spines['bottom'].set_visible(False)\n        axs[i].spines['left'].set_visible(False)\n        axs[i].set_yticklabels([])\n        axs[i].set_yticks([])\n        axs[i].set_ylabel(step, rotation='horizontal')\n        axs[i].set_xlabel('prediction')\n\n    fig.subplots_adjust(hspace=-0)\n    fig.suptitle('Forecasting distribution per step')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_intervals","title":"<code>plot_prediction_intervals(predictions, y_true, target_variable, initial_x_zoom=None, title=None, xaxis_title=None, yaxis_title=None, ax=None)</code>","text":"<p>Plot predicted intervals vs real values using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values and intervals. Expected columns are 'pred', 'lower_bound' and 'upper_bound'.</p> required <code>y_true</code> <code>pandas DataFrame</code> <p>Real values of target variable.</p> required <code>target_variable</code> <code>str</code> <p>Name of target variable.</p> required <code>initial_x_zoom</code> <code>list</code> <p>Initial zoom of x-axis, by default None.</p> <code>`None`</code> <code>title</code> <code>str</code> <p>Title of the plot, by default None.</p> <code>`None`</code> <code>xaxis_title</code> <code>str</code> <p>Title of x-axis, by default None.</p> <code>`None`</code> <code>yaxis_title</code> <code>str</code> <p>Title of y-axis, by default None.</p> <code>`None`</code> <code>ax</code> <code>matplotlib axes</code> <p>Axes where to plot, by default None.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_prediction_intervals(\n    predictions: pd.DataFrame,\n    y_true: pd.DataFrame,\n    target_variable: str,\n    initial_x_zoom: list = None,\n    title: str = None,\n    xaxis_title: str = None,\n    yaxis_title: str = None,\n    ax: plt.Axes = None\n):\n    \"\"\"\n    Plot predicted intervals vs real values using matplotlib.\n\n    Parameters\n    ----------\n    predictions : pandas DataFrame\n        Predicted values and intervals. Expected columns are 'pred', 'lower_bound'\n        and 'upper_bound'.\n    y_true : pandas DataFrame\n        Real values of target variable.\n    target_variable : str\n        Name of target variable.\n    initial_x_zoom : list, default `None`\n        Initial zoom of x-axis, by default None.\n    title : str, default `None`\n        Title of the plot, by default None.\n    xaxis_title : str, default `None`\n        Title of x-axis, by default None.\n    yaxis_title : str, default `None`\n        Title of y-axis, by default None.\n    ax : matplotlib axes, default `None`\n        Axes where to plot, by default None.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(7, 3))\n\n    y_true.loc[predictions.index, target_variable].plot(ax=ax, label='Real value')\n    predictions['pred'].plot(ax=ax, label='prediction')\n    ax.fill_between(\n        predictions.index,\n        predictions['lower_bound'],\n        predictions['upper_bound'],\n        color = '#444444',\n        alpha = 0.3,\n    )\n    ax.set_ylabel(yaxis_title)\n    ax.set_xlabel(xaxis_title)\n    ax.set_title(title)\n    ax.legend()\n\n    if initial_x_zoom is not None:\n        ax.set_xlim(initial_x_zoom)\n</code></pre>"},{"location":"api/preprocessing.html","title":"<code>preprocessing</code>","text":""},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator(order=1)</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differentiated time series of order n. It also reverts the differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>int</code> <p>Order of differentiation.</p> <code>initial_values</code> <code>list</code> <p>List with the initial value of the time series after each differentiation. This is used to revert the differentiation.</p> <code>last_values</code> <code>list</code> <p>List with the last value of the time series after each differentiation. This is used to revert the differentiation of a new window of data. A new window of data is a time series that starts right after the time series used to fit the transformer.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def __init__(\n    self, \n    order: int = 1\n) -&gt; None:\n\n    if not isinstance(order, int):\n        raise TypeError(f\"Parameter 'order' must be an integer greater than 0. Found {type(order)}.\")\n    if order &lt; 1:\n        raise ValueError(f\"Parameter 'order' must be an integer greater than 0. Found {order}.\")\n\n    self.order = order\n    self.initial_values = []\n    self.last_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the transformer. This method only removes the values stored in <code>self.initial_values</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>TimeSeriesDifferentiator</code> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef fit(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; Self:\n    \"\"\"\n    Fits the transformer. This method only removes the values stored in\n    `self.initial_values`.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    self : TimeSeriesDifferentiator\n\n    \"\"\"\n\n    self.initial_values = []\n    self.last_values = []\n\n    for i in range(self.order):\n        if i == 0:\n            self.initial_values.append(X[0])\n            self.last_values.append(X[-1])\n            X_diff = np.diff(X, n=1)\n        else:\n            self.initial_values.append(X_diff[0])\n            self.last_values.append(X_diff[-1])\n            X_diff = np.diff(X_diff, n=1)\n\n    return self\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transforms a time series into a differentiated time series of order n and stores the values needed to revert the differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Differentiated time series. The length of the array is the same as the original time series but the first n=<code>order</code> values are nan.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef transform(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Transforms a time series into a differentiated time series of order n and\n    stores the values needed to revert the differentiation.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Differentiated time series. The length of the array is the same as\n        the original time series but the first n=`order` values are nan.\n\n    \"\"\"\n\n    X_diff = np.diff(X, n=self.order)\n    X_diff = np.append((np.full(shape=self.order, fill_value=np.nan)), X_diff)\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Reverts the differentiation. To do so, the input array is assumed to be the same time series used to fit the transformer but differentiated.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef inverse_transform(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reverts the differentiation. To do so, the input array is assumed to be\n    the same time series used to fit the transformer but differentiated.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    # Remove initial nan values if present\n    X = X[np.argmax(~np.isnan(X)):]\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.insert(X, 0, self.initial_values[-1])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n        else:\n            X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X, y=None)</code>","text":"<p>Reverts the differentiation. The input array <code>X</code> is assumed to be a  differentiated time series of order n that starts right after the the time series used to fit the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series. It is assumed o start right after the time series used to fit the transformer.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_undiff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=False)\ndef inverse_transform_next_window(\n    self,\n    X: np.ndarray,\n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reverts the differentiation. The input array `X` is assumed to be a \n    differentiated time series of order n that starts right after the\n    the time series used to fit the transformer.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series. It is assumed o start right after\n        the time series used to fit the transformer.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_undiff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    array_ndim = X.ndim\n    if array_ndim == 1:\n        X = X.reshape(-1, 1)\n\n    # Remove initial rows with nan values if present\n    X = X[~np.isnan(X).any(axis=1)]\n\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n        else:\n            X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n\n    if array_ndim == 1:\n        X_undiff = X_undiff.ravel()\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.series_long_to_dict","title":"<code>series_long_to_dict(data, series_id, index, values, freq)</code>","text":"<p>Convert long format series to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Long format series.</p> required <code>series_id</code> <code>str</code> <p>Column name with the series identifier.</p> required <code>index</code> <code>str</code> <p>Column name with the time index.</p> required <code>values</code> <code>str</code> <p>Column name with the values.</p> required <code>freq</code> <code>str</code> <p>Frequency of the series.</p> required <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def series_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    values: str,\n    freq: str,\n) -&gt; dict:\n    \"\"\"\n    Convert long format series to dictionary.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format series.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    values: str\n        Column name with the values.\n    freq: str\n        Frequency of the series.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index, values]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n    series_dict = {}\n    for k, v in data.groupby(series_id):\n        series_dict[k] = v.set_index(index)[values].asfreq(freq).rename(k)\n        series_dict[k].index.name = None\n\n    return series_dict\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.exog_long_to_dict","title":"<code>exog_long_to_dict(data, series_id, index, freq, dropna=False)</code>","text":"<p>Convert long format exogenous variables to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Long format exogenous variables.</p> required <code>series_id</code> <code>str</code> <p>Column name with the series identifier.</p> required <code>index</code> <code>str</code> <p>Column name with the time index.</p> required <code>freq</code> <code>str</code> <p>Frequency of the series.</p> required <code>dropna</code> <code>bool</code> <p>If True, drop columns with all values as NaN. This is useful when there are series without some exogenous variables.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variables.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def exog_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    freq: str,\n    dropna: bool = False,\n) -&gt; dict:\n    \"\"\"\n    Convert long format exogenous variables to dictionary.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    freq: str\n        Frequency of the series.\n    dropna: bool, default `False`\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n\n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n    exog_dict = dict(tuple(data.groupby(series_id)))\n    exog_dict = {\n        k: v.set_index(index).asfreq(freq).drop(columns=series_id)\n        for k, v in exog_dict.items()\n    }\n\n    for k in exog_dict.keys():\n        exog_dict[k].index.name = None\n\n    if dropna:\n        exog_dict = {k: v.dropna(how=\"all\", axis=1) for k, v in exog_dict.items()}\n\n    return exog_dict\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.DateTimeFeatureTransformer","title":"<code>DateTimeFeatureTransformer(features=None, encoding='cyclical', max_values=None)</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>A transformer for extracting datetime features from the DateTime index of a pandas DataFrame or Series. It can also apply encoding to the extracted features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list</code> <p>List of calendar features (strings) to extract from the index. When <code>None</code>, the following features are extracted: 'year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.</p> <code>`None`</code> <code>encoding</code> <code>str</code> <p>Encoding method for the extracted features. Options are None, 'cyclical' or 'onehot'.</p> <code>`'cyclical'`</code> <code>max_values</code> <code>dict</code> <p>Dictionary of maximum values for the cyclical encoding of calendar features. When <code>None</code>, the following values are used: {'month': 12, 'week': 52,  'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24,  'minute': 60, 'second': 60}.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>List of calendar features to extract from the index.</p> <code>encoding</code> <code>str</code> <p>Encoding method for the extracted features.</p> <code>max_values</code> <code>dict</code> <p>Dictionary of maximum values for the cyclical encoding of calendar features.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def __init__(\n    self,\n    features: Optional[list] = None,\n    encoding: str = \"cyclical\",\n    max_values: Optional[dict] = None\n) -&gt; None:\n\n    if encoding not in [\"cyclical\", \"onehot\", None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n    self.features = (\n        features\n        if features is not None\n        else [\n            \"year\",\n            \"month\",\n            \"week\",\n            \"day_of_week\",\n            \"day_of_month\",\n            \"day_of_year\",\n            \"weekend\",\n            \"hour\",\n            \"minute\",\n            \"second\",\n        ]\n    )\n    self.encoding = encoding\n    self.max_values = (\n        max_values\n        if max_values is not None\n        else {\n            \"month\": 12,\n            \"week\": 52,\n            \"day_of_week\": 7,\n            \"day_of_month\": 31,\n            \"day_of_year\": 365,\n            \"hour\": 24,\n            \"minute\": 60,\n            \"second\": 60,\n        }\n    )\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.DateTimeFeatureTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>A no-op method to satisfy the scikit-learn API.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    A no-op method to satisfy the scikit-learn API.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.DateTimeFeatureTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Create datetime features from the DateTime index of a pandas DataFrame or Series.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame</code> <p>Input DataFrame or Series with a datetime index.</p> required <p>Returns:</p> Name Type Description <code>X_new</code> <code>pandas DataFrame</code> <p>DataFrame with the extracted (and optionally encoded) datetime features.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def transform(\n    self,\n    X: Union[pd.Series, pd.DataFrame]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n\n    \"\"\"\n\n    X_new = create_datetime_features(\n                X          = X,\n                encoding   = self.encoding,\n                features   = self.features,\n                max_values = self.max_values,\n            )\n\n    return X_new\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.create_datetime_features","title":"<code>create_datetime_features(X, features=None, encoding='cyclical', max_values=None)</code>","text":"<p>Extract datetime features from the DateTime index of a pandas DataFrame or Series.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame</code> <p>Input DataFrame or Series with a datetime index.</p> required <code>features</code> <code>list</code> <p>List of calendar features (strings) to extract from the index. When <code>None</code>, the following features are extracted: 'year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.</p> <code>`None`</code> <code>encoding</code> <code>str</code> <p>Encoding method for the extracted features. Options are None, 'cyclical' or 'onehot'.</p> <code>`'cyclical'`</code> <code>max_values</code> <code>dict</code> <p>Dictionary of maximum values for the cyclical encoding of calendar features. When <code>None</code>, the following values are used: {'month': 12, 'week': 52,  'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24,  'minute': 60, 'second': 60}.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_new</code> <code>pandas DataFrame</code> <p>DataFrame with the extracted (and optionally encoded) datetime features.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def create_datetime_features(\n    X: Union[pd.Series, pd.DataFrame],\n    features: Optional[list] = None,\n    encoding: str = \"cyclical\",\n    max_values: Optional[dict] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n\n    \"\"\"\n\n    if not isinstance(X, (pd.DataFrame, pd.Series)):\n        raise TypeError(\"Input `X` must be a pandas Series or DataFrame\")\n    if not isinstance(X.index, pd.DatetimeIndex):\n        raise TypeError(\"Input `X` must have a pandas DatetimeIndex\")\n    if encoding not in [\"cyclical\", \"onehot\", None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n    default_features = [\n        \"year\",\n        \"month\",\n        \"week\",\n        \"day_of_week\",\n        \"day_of_month\",\n        \"day_of_year\",\n        \"weekend\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n    ]\n    features = features or default_features\n\n    default_max_values = {\n        \"month\": 12,\n        \"week\": 52,\n        \"day_of_week\": 7,\n        \"day_of_month\": 31,\n        \"day_of_year\": 365,\n        \"hour\": 24,\n        \"minute\": 60,\n        \"second\": 60,\n    }\n    max_values = max_values or default_max_values\n\n    X_new = pd.DataFrame(index=X.index)\n\n    datetime_attrs = {\n        \"year\": \"year\",\n        \"month\": \"month\",\n        \"week\": lambda idx: idx.isocalendar().week,\n        \"day_of_week\": \"dayofweek\",\n        \"day_of_year\": \"dayofyear\",\n        \"day_of_month\": \"day\",\n        \"weekend\": lambda idx: (idx.weekday &gt;= 5).astype(int),\n        \"hour\": \"hour\",\n        \"minute\": \"minute\",\n        \"second\": \"second\",\n    }\n\n    not_supported_features = set(features) - set(datetime_attrs.keys())\n    if not_supported_features:\n        raise ValueError(\n            f\"Features {not_supported_features} are not supported. \"\n            f\"Supported features are {list(datetime_attrs.keys())}.\"\n        )\n\n    for feature in features:\n        attr = datetime_attrs[feature]\n        X_new[feature] = (\n            attr(X.index) if callable(attr) else getattr(X.index, attr).astype(int)\n        )\n\n    if encoding == \"cyclical\":\n        cols_to_drop = []\n        for feature, max_val in max_values.items():\n            if feature in X_new.columns:\n                X_new[f\"{feature}_sin\"] = np.sin(2 * np.pi * X_new[feature] / max_val)\n                X_new[f\"{feature}_cos\"] = np.cos(2 * np.pi * X_new[feature] / max_val)\n                cols_to_drop.append(feature)\n        X_new = X_new.drop(columns=cols_to_drop)\n    elif encoding == \"onehot\":\n        X_new = pd.get_dummies(\n            X_new, columns=features, drop_first=False, sparse=False, dtype=int\n        )\n\n    return X_new\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures","title":"<code>RollingFeatures(stats, window_sizes, min_periods=None, features_names=None, fillna=None)</code>","text":"<p>This class computes rolling features. To avoid data leakage, the last point  in the window is excluded from calculations, ('closed': 'left' and  'center': False).</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>(str, list)</code> <p>Statistics to compute over the rolling window. Can be a <code>string</code> or a <code>list</code>, and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation'.</p> required <code>window_sizes</code> <code>(int, list)</code> <p>Size of the rolling window for each statistic. If an <code>int</code>, all stats share  the same window size. If a <code>list</code>, it should have the same length as stats.</p> required <code>min_periods</code> <code>(int, list)</code> <p>Minimum number of observations in window required to have a value.  Similar to pandas rolling <code>min_periods</code> argument. If <code>None</code>, defaults  to <code>window_sizes</code>.</p> <code>`None`</code> <code>features_names</code> <code>list</code> <p>Names of the output features. If <code>None</code>, default names will be used in the  format 'roll_stat_window_size', for example 'roll_mean_7'.</p> <code>`None`</code> <code>fillna</code> <code>(str, float)</code> <p>Fill missing values in <code>transform_batch</code> method. Available  methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>stats</code> <code>list</code> <p>Statistics to compute over the rolling window.</p> <code>n_stats</code> <code>int</code> <p>Number of statistics to compute.</p> <code>window_sizes</code> <code>list</code> <p>Size of the rolling window for each statistic.</p> <code>max_window_size</code> <code>int</code> <p>Maximum window size.</p> <code>min_periods</code> <code>list</code> <p>Minimum number of observations in window required to have a value.</p> <code>features_names</code> <code>list</code> <p>Names of the output features.</p> <code>fillna</code> <code>(str, float)</code> <p>Method to fill missing values in <code>transform_batch</code> method.</p> <code>unique_rolling_windows</code> <code>dict</code> <p>Dictionary containing unique rolling window parameters and the corresponding statistics.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def __init__(\n    self, \n    stats: Union[str, list],\n    window_sizes: Union[int, list],\n    min_periods: Optional[Union[int, list]] = None,\n    features_names: Optional[list] = None, \n    fillna: Optional[Union[str, float]] = None\n) -&gt; None:\n\n    self._validate_params(\n        stats,\n        window_sizes,\n        min_periods,\n        features_names,\n        fillna\n    )\n\n    if isinstance(stats, str):\n        stats = [stats]\n    self.stats = stats\n    self.n_stats = len(stats)\n\n    if isinstance(window_sizes, int):\n        window_sizes = [window_sizes] * self.n_stats\n    self.window_sizes = window_sizes\n    self.max_window_size = max(window_sizes)\n\n    if min_periods is None:\n        min_periods = self.window_sizes\n    elif isinstance(min_periods, int):\n        min_periods = [min_periods] * self.n_stats\n    self.min_periods = min_periods\n\n    if features_names is None:\n        features_names = [\n            f\"roll_{stat}_{window_size}\" \n            for stat, window_size in zip(self.stats, self.window_sizes)\n        ]\n    self.features_names = features_names\n\n    self.fillna = fillna\n\n    window_params_list = []\n    for i in range(len(self.stats)):\n        window_params = (self.window_sizes[i], self.min_periods[i])\n        window_params_list.append(window_params)\n\n    # Find unique window parameter combinations\n    unique_rolling_windows = {}\n    for i, params in enumerate(window_params_list):\n        key = f\"{params[0]}_{params[1]}\"\n        if key not in unique_rolling_windows:\n            unique_rolling_windows[key] = {\n                'params': {\n                    'window': params[0], \n                    'min_periods': params[1], \n                    'center': False,\n                    'closed': 'left'\n                },\n                'stats_idx': [], \n                'stats_names': [], \n                'rolling_obj': None\n            }\n        unique_rolling_windows[key]['stats_idx'].append(i)\n        unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n\n    self.unique_rolling_windows = unique_rolling_windows\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._validate_params","title":"<code>_validate_params(stats, window_sizes, min_periods=None, features_names=None, fillna=None)</code>","text":"<p>Validate the parameters of the RollingFeatures class.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>(str, list)</code> <p>Statistics to compute over the rolling window. Can be a <code>string</code> or a <code>list</code>, and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation'.</p> required <code>window_sizes</code> <code>(int, list)</code> <p>Size of the rolling window for each statistic. If an <code>int</code>, all stats share  the same window size. If a <code>list</code>, it should have the same length as stats.</p> required <code>min_periods</code> <code>(int, list)</code> <p>Minimum number of observations in window required to have a value.  Similar to pandas rolling <code>min_periods</code> argument. If <code>None</code>, defaults  to <code>window_sizes</code>.</p> <code>`None`</code> <code>features_names</code> <code>list</code> <p>Names of the output features. If <code>None</code>, default names will be used in the  format 'roll_stat_window_size', for example 'roll_mean_7'.</p> <code>`None`</code> <code>fillna</code> <code>(str, float)</code> <p>Fill missing values in <code>transform_batch</code> method. Available  methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def _validate_params(\n    self, \n    stats, \n    window_sizes, \n    min_periods: Optional[Union[int, list]] = None,\n    features_names: Optional[Union[str, list]] = None, \n    fillna: Optional[Union[str, float]] = None\n) -&gt; None:\n    \"\"\"\n    Validate the parameters of the RollingFeatures class.\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default `None`\n        Minimum number of observations in window required to have a value. \n        Similar to pandas rolling `min_periods` argument. If `None`, defaults \n        to `window_sizes`.\n    features_names : list, default `None`\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default `None`\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # stats\n    if not isinstance(stats, (str, list)):\n        raise TypeError(\n            f\"`stats` must be a string or a list of strings. Got {type(stats)}.\"\n        )        \n\n    if isinstance(stats, str):\n        stats = [stats]\n    allowed_stats = ['mean', 'std', 'min', 'max', 'sum', 'median', \n                     'ratio_min_max', 'coef_variation']\n    for stat in set(stats):\n        if stat not in allowed_stats:\n            raise ValueError(\n                f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\"\n            )\n\n    n_stats = len(stats)\n\n    # window_sizes\n    if not isinstance(window_sizes, (int, list)):\n        raise TypeError(\n            f\"`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.\"\n        )\n\n    if isinstance(window_sizes, list):\n        n_window_sizes = len(window_sizes)\n        if n_window_sizes != n_stats:\n            raise ValueError(\n                (f\"Length of `window_sizes` list ({n_window_sizes}) \"\n                 f\"must match length of `stats` list ({n_stats}).\")\n            )\n\n    # Check duplicates (stats, window_sizes)\n    if isinstance(window_sizes, int):\n        window_sizes = [window_sizes] * n_stats\n    if len(set(zip(stats, window_sizes))) != n_stats:\n        raise ValueError(\"Duplicate (stat, window_size) pairs are not allowed.\")\n\n    # min_periods\n    if not isinstance(min_periods, (int, list, type(None))):\n        raise TypeError(\n            f\"`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.\"\n        )\n\n    if min_periods is not None:\n        if isinstance(min_periods, int):\n            min_periods = [min_periods] * n_stats\n        elif isinstance(min_periods, list):\n            n_min_periods = len(min_periods)\n            if n_min_periods != n_stats:\n                raise ValueError(\n                    (f\"Length of `min_periods` list ({n_min_periods}) \"\n                     f\"must match length of `stats` list ({n_stats}).\")\n                )\n\n        for i, min_period in enumerate(min_periods):\n            if min_period &gt; window_sizes[i]:\n                raise ValueError(\n                    (\"Each min_period must be less than or equal to its \"\n                     \"corresponding window_size.\")\n                )\n\n    # features_names\n    if not isinstance(features_names, (list, type(None))):\n        raise TypeError(\n            f\"`features_names` must be a list of strings or None. Got {type(features_names)}.\"\n        )\n\n    if isinstance(features_names, list):\n        n_features_names = len(features_names)\n        if n_features_names != n_stats:\n            raise ValueError(\n                (f\"Length of `features_names` list ({n_features_names}) \"\n                 f\"must match length of `stats` list ({n_stats}).\")\n            )\n\n    # fillna\n    if fillna is not None:\n        if not isinstance(fillna, (int, float, str)):\n            raise TypeError(\n                f\"`fillna` must be a float, string, or None. Got {type(fillna)}.\"\n            )\n\n        if isinstance(fillna, str):\n            allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n            if fillna not in allowed_fill_strategy:\n                raise ValueError(\n                    (f\"'{fillna}' is not allowed. Allowed `fillna` \"\n                     f\"values are: {allowed_fill_strategy} or a float value.\")\n                )\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._apply_stat_pandas","title":"<code>_apply_stat_pandas(rolling_obj, stat)</code>","text":"<p>Apply the specified statistic to a pandas rolling object.</p> <p>Parameters:</p> Name Type Description Default <code>rolling_obj</code> <code>pandas Rolling</code> <p>Rolling object to apply the statistic.</p> required <code>stat</code> <code>str</code> <p>Statistic to compute.</p> required <p>Returns:</p> Name Type Description <code>stat_series</code> <code>pandas Series</code> <p>Series with the computed statistic.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def _apply_stat_pandas(\n    self, \n    rolling_obj: pd.core.window.rolling.Rolling, \n    stat: str\n) -&gt; pd.Series:\n    \"\"\"\n    Apply the specified statistic to a pandas rolling object.\n\n    Parameters\n    ----------\n    rolling_obj : pandas Rolling\n        Rolling object to apply the statistic.\n    stat : str\n        Statistic to compute.\n\n    Returns\n    -------\n    stat_series : pandas Series\n        Series with the computed statistic.\n\n    \"\"\"\n\n    if stat == 'mean':\n        return rolling_obj.mean()\n    elif stat == 'std':\n        return rolling_obj.std()\n    elif stat == 'min':\n        return rolling_obj.min()\n    elif stat == 'max':\n        return rolling_obj.max()\n    elif stat == 'sum':\n        return rolling_obj.sum()\n    elif stat == 'median':\n        return rolling_obj.median()\n    elif stat == 'ratio_min_max':\n        return rolling_obj.min() / rolling_obj.max()\n    elif stat == 'coef_variation':\n        return rolling_obj.std() / rolling_obj.mean()\n    else:\n        raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.transform_batch","title":"<code>transform_batch(X)</code>","text":"<p>Transform an entire pandas Series using rolling windows and compute the  specified statistics.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series</code> <p>The input data series to transform.</p> required <p>Returns:</p> Name Type Description <code>rolling_features</code> <code>pandas DataFrame</code> <p>A DataFrame containing the rolling features.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def transform_batch(\n    self, \n    X: pd.Series\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform an entire pandas Series using rolling windows and compute the \n    specified statistics.\n\n    Parameters\n    ----------\n    X : pandas Series\n        The input data series to transform.\n\n    Returns\n    -------\n    rolling_features : pandas DataFrame\n        A DataFrame containing the rolling features.\n\n    \"\"\"\n\n    for k in self.unique_rolling_windows.keys():\n        rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n        self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n\n    rolling_features = []\n    for i, stat in enumerate(self.stats):\n        window_size = self.window_sizes[i]\n        min_periods = self.min_periods[i]\n\n        key = f\"{window_size}_{min_periods}\"\n        rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n\n        stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)            \n        rolling_features.append(stat_series)\n\n    rolling_features = pd.concat(rolling_features, axis=1)\n    rolling_features.columns = self.features_names\n    rolling_features = rolling_features.iloc[self.max_window_size:]\n\n    if self.fillna is not None:\n        if self.fillna == 'mean':\n            rolling_features = rolling_features.fillna(rolling_features.mean())\n        elif self.fillna == 'median':\n            rolling_features = rolling_features.fillna(rolling_features.median())\n        elif self.fillna == 'ffill':\n            rolling_features = rolling_features.ffill()\n        elif self.fillna == 'bfill':\n            rolling_features = rolling_features.bfill()\n        else:\n            rolling_features = rolling_features.fillna(self.fillna)\n\n    return rolling_features\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._apply_stat_numpy_jit","title":"<code>_apply_stat_numpy_jit(X_window, stat)</code>","text":"<p>Apply the specified statistic to a numpy array using Numba JIT.</p> <p>Parameters:</p> Name Type Description Default <code>X_window</code> <code>numpy array</code> <p>Array with the rolling window.</p> required <code>stat</code> <code>str</code> <p>Statistic to compute.</p> required <p>Returns:</p> Name Type Description <code>stat_value</code> <code>float</code> <p>Value of the computed statistic.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def _apply_stat_numpy_jit(\n    self, \n    X_window: np.ndarray, \n    stat: str\n) -&gt; float:\n    \"\"\"\n    Apply the specified statistic to a numpy array using Numba JIT.\n\n    Parameters\n    ----------\n    X_window : numpy array\n        Array with the rolling window.\n    stat : str\n        Statistic to compute.\n\n    Returns\n    -------\n    stat_value : float\n        Value of the computed statistic.\n\n    \"\"\"\n\n    if stat == 'mean':\n        return _np_mean_jit(X_window)\n    elif stat == 'std':\n        return _np_std_jit(X_window)\n    elif stat == 'min':\n        return _np_min_jit(X_window)\n    elif stat == 'max':\n        return _np_max_jit(X_window)\n    elif stat == 'sum':\n        return _np_sum_jit(X_window)\n    elif stat == 'median':\n        return _np_median_jit(X_window)\n    elif stat == 'ratio_min_max':\n        return _np_min_max_ratio_jit(X_window)\n    elif stat == 'coef_variation':\n        return _np_cv_jit(X_window)\n    else:\n        raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Transform a numpy array using rolling windows and compute the  specified statistics. The input array must be a 1D array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>The input data array to transform.</p> required <p>Returns:</p> Name Type Description <code>rolling_features</code> <code>numpy ndarray</code> <p>An array containing the computed statistics.</p> Source code in <code>skforecast/preprocessing/preprocessing.py</code> <pre><code>def transform(\n    self, \n    X: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform a numpy array using rolling windows and compute the \n    specified statistics. The input array must be a 1D array.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        The input data array to transform.\n\n    Returns\n    -------\n    rolling_features : numpy ndarray\n        An array containing the computed statistics.\n\n    \"\"\"\n\n    rolling_features = np.full(shape=self.n_stats, fill_value=np.nan, dtype=float)\n    for i, stat in enumerate(self.stats):\n\n        X_window = X[-self.window_sizes[i]:]\n        X_window = X_window[~np.isnan(X_window)]\n\n        rolling_features[i] = self._apply_stat_numpy_jit(X_window, stat)\n\n    return rolling_features\n</code></pre>"},{"location":"api/utils.html","title":"<code>utils</code>","text":""},{"location":"api/utils.html#skforecast.utils.utils.save_forecaster","title":"<code>save_forecaster(forecaster, file_name, save_custom_functions=True, verbose=True)</code>","text":"<p>Save forecaster model using joblib. If custom functions are used to create predictors or weights, they are saved as .py files.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster created with skforecast library.</p> required <code>file_name</code> <code>str</code> <p>File name given to the object.</p> required <code>save_custom_functions</code> <code>bool</code> <p>If True, save custom functions used in the forecaster (fun_predictors and weight_func) as .py files. Custom functions need to be available in the environment where the forecaster is going to be loaded.</p> <code>`True`</code> <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster saved.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def save_forecaster(\n    forecaster: object, \n    file_name: str,\n    save_custom_functions: bool = True, \n    verbose: bool = True\n) -&gt; None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    predictors or weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object.\n    save_custom_functions : bool, default `True`\n        If True, save custom functions used in the forecaster (fun_predictors and\n        weight_func) as .py files. Custom functions need to be available in the\n        environment where the forecaster is going to be loaded.\n    verbose : bool, default `True`\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Save forecaster\n    joblib.dump(forecaster, filename=file_name)\n\n    if save_custom_functions:\n        # Save custom functions to create predictors\n        if hasattr(forecaster, 'fun_predictors') and forecaster.fun_predictors is not None:\n            file_name = forecaster.fun_predictors.__name__ + '.py'\n            with open(file_name, 'w') as file:\n                file.write(inspect.getsource(forecaster.fun_predictors))\n\n        # Save custom functions to create weights\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    else:\n        if ((hasattr(forecaster, 'fun_predictors') and forecaster.fun_predictors is not None)\n          or (hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None)):\n            warnings.warn(\n                (\"Custom functions used to create predictors or weights are not saved. \"\n                 \"To save them, set `save_custom_functions` to `True`.\")\n            )\n\n    if verbose:\n        forecaster.summary()\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.load_forecaster","title":"<code>load_forecaster(file_name, verbose=True)</code>","text":"<p>Load forecaster model using joblib. If the forecaster was saved with custom functions to create predictors or weights, these functions must be available in the environment where the forecaster is going to be loaded.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Object file name.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster loaded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>forecaster</code> <code>Forecaster</code> <p>Forecaster created with skforecast library.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def load_forecaster(\n    file_name: str,\n    verbose: bool = True\n) -&gt; object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with custom\n    functions to create predictors or weights, these functions must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n\n    \"\"\"\n\n    forecaster = joblib.load(filename=file_name)\n\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n\n    if forecaster_v != skforecast_v:\n        warnings.warn(\n            (f\"The skforecast version installed in the environment differs \"\n             f\"from the version used to create the forecaster.\\n\"\n             f\"    Installed Version  : {skforecast_v}\\n\"\n             f\"    Forecaster Version : {forecaster_v}\\n\"\n             f\"This may create incompatibilities when using the library.\"),\n             SkforecastVersionWarning\n        )\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Check lags argument input and generate the corresponding numpy ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>lags</code> <code>Any</code> <p>Lags used as predictors.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>numpy ndarray, None</code> <p>Lags used as predictors.</p> <code>max_lag</code> <code>(int, None)</code> <p>Maximum value of the lags.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -&gt; Union[Optional[np.ndarray], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n\n    \"\"\"\n\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if lags.size == 0:\n                raise ValueError(\"Argument `lags` must contain at least one value.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterAutoregMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n\n        max_lag = max(lags)\n\n    return lags, max_lag\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, regressor, weight_func, series_weights)</code>","text":"<p>Check weights arguments, <code>weight_func</code> and <code>series_weights</code> for the different  forecasters. Create <code>source_code_weight_func</code>, source code of the custom  function(s) used to create weights.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>Regressor of the forecaster.</p> required <code>weight_func</code> <code>(Callable, dict)</code> <p>Argument <code>weight_func</code> of the forecaster.</p> required <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> required <p>Returns:</p> Name Type Description <code>weight_func</code> <code>(Callable, dict)</code> <p>Argument <code>weight_func</code> of the forecaster.</p> <code>source_code_weight_func</code> <code>(str, dict)</code> <p>Argument <code>source_code_weight_func</code> of the forecaster.</p> <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -&gt; Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_transformer_series","title":"<code>initialize_transformer_series(forecaster_name, series_names_in_, encoding=None, transformer_series=None)</code>","text":"<p>Initialize <code>transformer_series_</code> attribute for the Forecasters Multiseries.</p> <ul> <li>If <code>transformer_series</code> is <code>None</code>, no transformation is applied.</li> <li>If <code>transformer_series</code> is a scikit-learn transformer (object), the same  transformer is applied to all series (<code>series_names_in_</code>).</li> <li>If <code>transformer_series</code> is a <code>dict</code>, a different transformer can be applied to each series. The keys of the dictionary must be the same as the names of the series in <code>series_names_in_</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (<code>ForecasterAutoregMultiSeries</code>).</p> <code>`None`</code> <code>transformer_series</code> <code>(object, dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list,\n    encoding: Optional[str] = None,\n    transformer_series: Optional[Union[object, dict]] = None\n) -&gt; dict:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterAutoregMultiSeries`).\n    transformer_series : object, dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n\n    \"\"\"\n\n    multiseries_forecasters = [\n        'ForecasterAutoregMultiSeries'\n    ]\n\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {serie: clone(transformer_series) \n                               for serie in series_names_in_}\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            (k, v) for k, v in deepcopy(transformer_series).items() \n            if k in transformer_series_\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\"),\n                IgnoredArgumentWarning\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_lags_grid","title":"<code>initialize_lags_grid(forecaster, lags_grid=None)</code>","text":"<p>Initialize lags grid and lags label for model selection. </p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model. ForecasterAutoreg, ForecasterAutoregDirect,  ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate.</p> required <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>lags_grid</code> <code>dict</code> <p>Dictionary with lags configuration for each iteration.</p> <code>lags_label</code> <code>str</code> <p>Label for lags representation in the results object.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -&gt; Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterAutoreg, ForecasterAutoregDirect, \n        ForecasterAutoregMultiSeries, ForecasterAutoregMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(regressor, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only the keys that are used by the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>object</code> <p>Regressor object.</p> required <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to pass to the `fit' method of the forecaster.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to be passed to the <code>fit</code> method of the  regressor after removing the unused keys.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -&gt; dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n\n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_y","title":"<code>check_y(y, series_id='`y`')</code>","text":"<p>Raise Exception if <code>y</code> is not pandas Series or if it has missing values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in the warning message.</p> <code>'`y`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_y(\n    y: Any,\n    series_id: str = \"`y`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n\n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(f\"{series_id} must be a pandas Series.\")\n\n    if y.isnull().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True, series_id='`exog`')</code>","text":"<p>Raise Exception if <code>exog</code> is not pandas Series or pandas DataFrame. If <code>allow_nan = True</code>, issue a warning if <code>exog</code> contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows the presence of NaN values in <code>exog</code>. If False (default), issue a warning if <code>exog</code> contains NaN values.</p> <code>`True`</code> <code>series_id</code> <code>str</code> <p>Identifier of the series for which the exogenous variable/s are used in the warning message.</p> <code>'`exog`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(\n                (f\"{series_id} has missing values. Most machine learning models \"\n                 f\"do not allow missing values. Fitting the forecaster may fail.\"), \n                 MissingValuesWarning\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Store dtypes of <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Name Type Description <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with the dtypes in <code>exog</code>.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def get_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -&gt; dict:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n\n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog, call_check_exog=True, series_id='`exog`')</code>","text":"<p>Raise Exception if <code>exog</code> has categorical columns with non integer values. This is needed when using machine learning regressors that allow categorical features. Issue a Warning if <code>exog</code> has columns that are not <code>init</code>, <code>float</code>, or <code>category</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>call_check_exog</code> <code>bool</code> <p>If <code>True</code>, call <code>check_exog</code> function.</p> <code>`True`</code> <code>series_id</code> <code>str</code> <p>Identifier of the series for which the exogenous variable/s are used in the warning message.</p> <code>'`exog`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default `True`\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                 f\"Most machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError(\n                    (\"Categorical dtypes in exog must contain only integer values. \"\n                     \"See skforecast docs for more info about how to include \"\n                     \"categorical features https://skforecast.org/\"\n                     \"latest/user_guides/categorical-features.html\")\n                )\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', \n        'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                 f\"machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int,\n        np.int32, np.int64]:\n            raise TypeError(\n                (\"Categorical dtypes in exog must contain only integer values. \"\n                 \"See skforecast docs for more info about how to include \"\n                 \"categorical features https://skforecast.org/\"\n                 \"latest/user_guides/categorical-features.html\")\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_interval","title":"<code>check_interval(interval=None, quantiles=None, alpha=None)</code>","text":"<p>Check provided confidence interval sequence is valid.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`None`</code> <code>quantiles</code> <code>list</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_interval(\n    interval: list = None,\n    quantiles: float = None,\n    alpha: float = None\n) -&gt; None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    quantiles : list, default `None`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError(\n                (\"`interval` must be a `list`. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                (\"`interval` must contain exactly 2 values, respectively the \"\n                 \"lower and upper interval bounds. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if (interval[0] &lt; 0.) or (interval[0] &gt;= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.) or (interval[1] &gt; 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                (f\"Lower interval bound ({interval[0]}) must be less than the \"\n                 f\"upper interval bound ({interval[1]}).\")\n            )\n\n    if quantiles is not None:\n        if not isinstance(quantiles, list):\n            raise TypeError(\n                (\"`quantiles` must be a `list`. For example, quantiles 0.05, \"\n                 \"0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.\")\n            )\n\n        for q in quantiles:\n            if (q &lt; 0.) or (q &gt; 1.):\n                raise ValueError(\n                    (\"All elements in `quantiles` must be &gt;= 0 and &lt;= 1.\")\n                )\n\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                (\"`alpha` must be a `float`. For example, interval of 95% \"\n                 \"should be as `alpha = 0.05`.\")\n            )\n\n        if (alpha &lt;= 0.) or (alpha &gt;= 1):\n            raise ValueError(\n                f\"`alpha` must have a value between 0 and 1. Got {alpha}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, is_fitted, exog_in_, index_type_, index_freq_, window_size, last_window, last_window_exog=None, exog=None, exog_type_in_=None, exog_names_in_=None, interval=None, alpha=None, max_steps=None, levels=None, levels_forecaster=None, series_names_in_=None, encoding=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>steps</code> <code>(int, list)</code> <p>Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to  <code>max_lag</code>.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame, None</code> <p>Values of the series used to create the predictors (lags) need in the  first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code> in  ForecasterSarimax predictions.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>`None`</code> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <code>max_steps</code> <code>Optional[int]</code> <p>Maximum number of steps allowed (<code>ForecasterAutoregDirect</code> and  <code>ForecasterAutoregMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted (<code>ForecasterAutoregMultiSeries</code> and `ForecasterRnn).</p> <code>`None`</code> <code>levels_forecaster</code> <code>(str, list)</code> <p>Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>`None`</code> <code>series_names_in_</code> <code>list</code> <p>Names of the columns used during fit (<code>ForecasterAutoregMultiSeries</code>,  <code>ForecasterAutoregMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>`None`</code> <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (<code>ForecasterAutoregMultiSeries</code>).</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, list],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Union[pd.Series, pd.DataFrame, None],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog_type_in_: Optional[type] = None,\n    exog_names_in_: Optional[list] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    max_steps: Optional[int] = None,\n    levels: Optional[Union[str, list]] = None,\n    levels_forecaster: Optional[Union[str, list]] = None,\n    series_names_in_: Optional[list] = None,\n    encoding: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    steps : int, list\n        Number of future steps predicted.\n    is_fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, None\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type_in_ : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_names_in_ : list, default `None`\n        Names of the exogenous variables used during training.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterAutoregDirect` and \n        `ForecasterAutoregMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterAutoregMultiSeries`\n        and `ForecasterRnn).\n    levels_forecaster : str, list, default `None`\n        Time series used as output data of a multiseries problem in a RNN problem\n        (`ForecasterRnn`).\n    series_names_in_ : list, default `None`\n        Names of the columns used during fit (`ForecasterAutoregMultiSeries`, \n        `ForecasterAutoregMultiVariate` and `ForecasterRnn`).\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterAutoregMultiSeries`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not is_fitted:\n        raise NotFittedError(\n            (\"This Forecaster instance is not fitted yet. Call `fit` with \"\n             \"appropriate arguments before using predict.\")\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n           (f\"The minimum value of `steps` must be equal to or greater than 1. \"\n            f\"Got {min(steps)}.\")\n        )\n\n    if max_steps is not None:\n        if max(steps) &gt; max_steps:\n            raise ValueError(\n                (f\"The maximum value of `steps` must be less than or equal to \"\n                 f\"the value of steps defined when initializing the forecaster. \"\n                 f\"Got {max(steps)}, but the maximum is {max_steps}.\")\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError(\n                (\"`levels` must be a `list` of column names, a `str` of a \"\n                 \"column name or `None`.\")\n            )\n\n        levels_to_check = (\n            levels_forecaster if forecaster_name == 'ForecasterRnn'\n            else series_names_in_\n        )\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(\n                    (f\"`levels` names must be included in the series used during fit \"\n                     f\"({levels_to_check}). Got {levels}.\")\n                )\n        else:\n            if len(unknown_levels) != 0 and last_window is not None and encoding is not None:\n                if encoding == 'onehot':\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. The resulting \"\n                         f\"one-hot encoded columns for this feature will be all zeros.\"),\n                         UnknownLevelWarning\n                    )\n                else:\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. \"\n                         f\"Unknown levels are encoded as NaN, which may cause the \"\n                         f\"prediction to fail if the regressor does not accept NaN values.\"),\n                         UnknownLevelWarning\n                    )\n\n    if exog is None and exog_in_:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. \"\n             \"Same variable/s must be provided when predicting.\")\n        )\n\n    if exog is not None and not exog_in_:\n        raise ValueError(\n            (\"Forecaster trained without exogenous variable/s. \"\n             \"`exog` must be `None` when predicting.\")\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if isinstance(last_window, type(None)) and forecaster_name not in [\n        'ForecasterAutoregMultiSeries', \n        'ForecasterRnn'\n    ]:\n        raise ValueError(\n            (\"`last_window` was not stored during training. If you don't want \"\n             \"to retrain the Forecaster, provide `last_window` as argument.\")\n        )\n\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterAutoregMultiVariate',\n                           'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        last_window_cols = last_window.columns.to_list()\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries', \n                               'ForecasterRnn'] and \\\n            len(set(levels) - set(last_window_cols)) != 0:\n            raise ValueError(\n                (f\"`last_window` must contain a column(s) named as the level(s) \"\n                 f\"to be predicted.\\n\"\n                 f\"    `levels` : {levels}\\n\"\n                 f\"    `last_window` columns : {last_window_cols}\")\n            )\n\n        if forecaster_name == 'ForecasterAutoregMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) &gt; 0:\n                raise ValueError(\n                    (f\"`last_window` columns must be the same as the `series` \"\n                     f\"column names used to create the X_train matrix.\\n\"\n                     f\"    `last_window` columns    : {last_window_cols}\\n\"\n                     f\"    `series` columns X train : {series_names_in_}\")\n                )\n    else:\n        if not isinstance(last_window, (pd.Series, pd.DataFrame)):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series or DataFrame. \"\n                f\"Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) &lt; window_size:\n        raise ValueError(\n            (f\"`last_window` must have as many values as needed to \"\n             f\"generate the predictors. For this forecaster it is {window_size}.\")\n        )\n    if last_window.isnull().any().all():\n        warnings.warn(\n            (\"`last_window` has missing values. Most of machine learning models do \"\n             \"not allow missing values. Prediction method may fail.\"), \n             MissingValuesWarning\n        )\n    _, last_window_index = preprocess_last_window(\n                               last_window   = last_window.iloc[:0],\n                               return_values = False\n                           ) \n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(\n            (f\"Expected index of type {index_type_} for `last_window`. \"\n             f\"Got {type(last_window_index)}.\")\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(\n                (f\"Expected frequency of type {index_freq_} for `last_window`. \"\n                 f\"Got {last_window_index.freqstr}.\")\n            )\n\n    # Checks exog\n    if exog is not None:\n\n        # Check type, nulls and expected type\n        if forecaster_name in ['ForecasterAutoregMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\"\n                )\n            if exog_type_in_ == dict and not isinstance(exog, dict):\n                raise TypeError(\n                    f\"Expected type for `exog`: {exog_type_in_}. Got {type(exog)}.\"\n                )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n                )\n\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(\n                    (f\"`exog` does not contain keys for levels {no_exog_levels}. \"\n                     f\"Missing levels are filled with NaN. Most of machine learning \"\n                     f\"models do not allow missing values. Prediction method may fail.\"),\n                     MissingExogWarning\n                )\n            exogs_to_check = [\n                (f\"`exog` for series '{k}'\", v) \n                for k, v in exog.items() \n                if v is not None and k in levels\n            ]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n\n        for exog_name, exog_to_check in exogs_to_check:\n\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}\"\n                )\n\n            if exog_to_check.isnull().any().any():\n                warnings.warn(\n                    (f\"{exog_name} has missing values. Most of machine learning models \"\n                     f\"do not allow missing values. Prediction method may fail.\"), \n                     MissingValuesWarning\n                )\n\n            # Check exog has many values as distance to max step predicted\n            last_step = max(steps) if isinstance(steps, list) else steps\n            if len(exog_to_check) &lt; last_step:\n                if forecaster_name in ['ForecasterAutoregMultiSeries']:\n                    warnings.warn(\n                        (f\"{exog_name} doesn't have as many values as steps \"\n                         f\"predicted, {last_step}. Missing values are filled \"\n                         f\"with NaN. Most of machine learning models do not \"\n                         f\"allow missing values. Prediction method may fail.\"),\n                         MissingValuesWarning\n                    )\n                else: \n                    raise ValueError(\n                        (f\"{exog_name} must have at least as many values as \"\n                         f\"steps predicted, {last_step}.\")\n                    )\n\n            # Check name/columns are in exog_names_in_\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterAutoregMultiSeries']:\n                        warnings.warn(\n                            (f\"{col_missing} not present in {exog_name}. All \"\n                             f\"values will be NaN.\"),\n                             MissingExogWarning\n                        ) \n                    else:\n                        raise ValueError(\n                            (f\"Missing columns in {exog_name}. Expected {exog_names_in_}. \"\n                             f\"Got {exog_to_check.columns.to_list()}.\")\n                        )\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(\n                        (f\"When {exog_name} is a pandas Series, it must have a name. Got None.\")\n                    )\n\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterAutoregMultiSeries']:\n                        warnings.warn(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"{exog_name} is ignored. Exogenous variables must be one \"\n                             f\"of: {exog_names_in_}.\"),\n                             IgnoredArgumentWarning\n                        )\n                    else:\n                        raise ValueError(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"Exogenous variables must be: {exog_names_in_}.\")\n                        )\n\n            # Check index dtype and freq\n            _, exog_index = preprocess_exog(\n                                exog          = exog_to_check.iloc[:0, ],\n                                return_values = False\n                            )\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for {exog_name}. \"\n                     f\"Got {type(exog_index)}.\")\n                )\n            if forecaster_name not in ['ForecasterAutoregMultiSeries']:\n                if isinstance(exog_index, pd.DatetimeIndex):\n                    if not exog_index.freqstr == index_freq_:\n                        raise TypeError(\n                            (f\"Expected frequency of type {index_freq_} for {exog_name}. \"\n                             f\"Got {exog_index.freqstr}.\")\n                        )\n\n            # Check exog starts one step ahead of last_window end.\n            expected_index = expand_index(last_window.index, 1)[0]\n            if expected_index != exog_to_check.index[0]:\n                if forecaster_name in ['ForecasterAutoregMultiSeries']:\n                    warnings.warn(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`. Missing values are filled \"\n                         f\"with NaN.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index       : {expected_index}.\"),\n                         MissingValuesWarning\n                    )  \n                else:\n                    raise ValueError(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index : {expected_index}.\")\n                    )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError(\n                    (\"Forecaster trained without exogenous variable/s. \"\n                     \"`last_window_exog` must be `None` when predicting.\")\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`last_window_exog` must be a pandas Series or a \"\n                     f\"pandas DataFrame. Got {type(last_window_exog)}.\")\n                )\n            if len(last_window_exog) &lt; window_size:\n                raise ValueError(\n                    (f\"`last_window_exog` must have as many values as needed to \"\n                     f\"generate the predictors. For this forecaster it is {window_size}.\")\n                )\n            if last_window_exog.isnull().any().all():\n                warnings.warn(\n                    (\"`last_window_exog` has missing values. Most of machine learning \"\n                     \"models do not allow missing values. Prediction method may fail.\"),\n                     MissingValuesWarning\n            )\n            _, last_window_exog_index = preprocess_last_window(\n                                            last_window   = last_window_exog.iloc[:0],\n                                            return_values = False\n                                        ) \n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for `last_window_exog`. \"\n                     f\"Got {type(last_window_exog_index)}.\")\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(\n                        (f\"Expected frequency of type {index_freq_} for \"\n                         f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\")\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        (f\"Missing columns in `last_window_exog`. Expected {exog_names_in_}. \"\n                         f\"Got {last_window_exog.columns.to_list()}.\") \n                    )\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError(\n                        (\"When `last_window_exog` is a pandas Series, it must have a \"\n                         \"name. Got None.\")\n                    )\n\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(\n                        (f\"'{last_window_exog.name}' was not observed during training. \"\n                         f\"Exogenous variables must be: {exog_names_in_}.\")\n                    )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_y","title":"<code>preprocess_y(y, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series, pandas DataFrame</code> <p>Time series.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>y</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>y_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>y</code>.</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>y</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"Series has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"Series has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n\n    return y_values, y_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_last_window","title":"<code>preprocess_last_window(last_window, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Time series values.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>last_window</code> as numpy ndarray. This option  is intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>last_window_values</code> <code>numpy ndarray</code> <p>Numpy array with values of <code>last_window</code>.</p> <code>last_window_index</code> <code>pandas Index</code> <p>Index of <code>last_window</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_last_window(\n    last_window: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n ) -&gt; Tuple[np.ndarray, pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn(\n            (\"`last_window` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n    else:\n        warnings.warn(\n            (\"`last_window` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n\n    last_window_values = last_window.to_numpy(copy=True).ravel() if return_values else None\n\n    return last_window_values, last_window_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_exog","title":"<code>preprocess_exog(exog, return_values=True)</code>","text":"<p>Return values and index of series or data frame separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>exog</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>exog_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>exog</code>.</p> <code>exog_index</code> <code>pandas Index</code> <p>Index of <code>exog</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn(\n            (\"`exog` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    else:\n        warnings.warn(\n            (\"`exog` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    exog_values = exog.to_numpy(copy=True) if return_values else None\n\n    return exog_values, exog_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.cast_exog_dtypes","title":"<code>cast_exog_dtypes(exog, exog_dtypes)</code>","text":"<p>Cast <code>exog</code> to a specified types. This is done because, for a forecaster to  accept a categorical exog, it must contain only integer values. Due to the  internal modifications of numpy, the values may be casted to <code>float</code>, so  they have to be re-converted to <code>int</code>.</p> <ul> <li>If <code>exog</code> is a pandas Series, <code>exog_dtypes</code> must be a dict with a  single value.</li> <li>If <code>exog_dtypes</code> is <code>category</code> but the current type of <code>exog</code> is <code>float</code>,  then the type is cast to <code>int</code> and then to <code>category</code>. </li> </ul> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with name and type of the series or data frame columns.</p> required <p>Returns:</p> Name Type Description <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables casted to the indicated dtypes.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def cast_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    exog_dtypes: dict,\n) -&gt; Union[pd.Series, pd.DataFrame]:  # pragma: no cover\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>pandas DataFrame</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def exog_to_direct(\n    exog: Union[pd.Series, pd.DataFrame],\n    steps: int\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : pandas DataFrame\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_column_transformed.index = pd.RangeIndex(len(exog_column_transformed))\n        exog_column_transformed.columns = [f\"{col}_step_{i + 1}\" \n                                           for col in exog_column_transformed.columns]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = pd.concat(exog_transformed, axis=1, copy=False)\n    else:\n        exog_transformed = exog_column_transformed\n\n    exog_transformed.index = exog_idx[-len(exog_transformed):]\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>numpy ndarray, shape(samples,)</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>numpy ndarray</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray,\n    steps: int\n) -&gt; np.ndarray:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : numpy ndarray, shape(samples,)\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : numpy ndarray\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, np.ndarray):\n        raise TypeError(f\"`exog` must be a numpy ndarray. Got {type(exog)}.\")\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog[i : n_rows - (steps - 1 - i)]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = np.concatenate(exog_transformed, axis=1)\n    else:\n        exog_transformed = exog_column_transformed.copy()\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index of length <code>steps</code> starting at the end of the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index, None</code> <p>Original index.</p> required <code>steps</code> <code>int</code> <p>Number of steps to expand.</p> required <p>Returns:</p> Name Type Description <code>new_index</code> <code>pandas Index</code> <p>New index.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def expand_index(\n    index: Union[pd.Index, None], \n    steps: int\n) -&gt; pd.Index:\n    \"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n\n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if isinstance(index, pd.Index):\n\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n\n    return new_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_numpy","title":"<code>transform_numpy(array, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of a numpy ndarray with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>numpy ndarray</code> <p>Array to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>`False`</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>array_transformed</code> <code>numpy ndarray</code> <p>Transformed array.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\n            f\"`array` argument must be a numpy ndarray. Got {type(array)}\"\n        )\n\n    if transformer is None:\n        return array\n\n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            array_transformed = transformer.fit_transform(array)\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \n                    message=\"X does not have valid feature names\", \n                    category=UserWarning\n                )\n                array_transformed = transformer.transform(array)\n    else:\n        array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_series","title":"<code>transform_series(series, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas Series with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series</code> <p>Series to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>`False`</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>series_transformed</code> <code>pandas Series, pandas DataFrame</code> <p>Transformed Series. Depending on the transformer used, the output may  be a Series or a DataFrame.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def transform_series(\n    series: pd.Series,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n\n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            (f\"`series` argument must be a pandas Series. Got {type(series)}.\")\n        )\n\n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n\n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.ravel(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>`False`</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>df_transformed</code> <code>pandas DataFrame</code> <p>Transformed DataFrame.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_optional_dependency(\n    package_name: str\n) -&gt; None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.multivariate_time_series_corr","title":"<code>multivariate_time_series_corr(time_series, other, lags, method='pearson')</code>","text":"<p>Compute correlation between a time_series and the lagged values of other  time series. </p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>pandas Series</code> <p>Target time series.</p> required <code>other</code> <code>pandas DataFrame</code> <p>Time series whose lagged values are correlated to <code>time_series</code>.</p> required <code>lags</code> <code>int, list, numpy ndarray</code> <p>Lags to be included in the correlation analysis.</p> required <code>method</code> <code>str</code> <ul> <li>'pearson': standard correlation coefficient.</li> <li>'kendall': Kendall Tau correlation coefficient.</li> <li>'spearman': Spearman rank correlation.</li> </ul> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>corr</code> <code>pandas DataFrame</code> <p>Correlation values.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: Union[int, list, np.array],\n    method: str = 'pearson'\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n\n    return corr\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_backtesting_input","title":"<code>check_backtesting_input(forecaster, cv, metric, add_aggregated_metric=True, y=None, series=None, exog=None, interval=None, alpha=None, n_boot=500, random_state=123, use_in_sample_residuals=True, use_binned_residuals=False, n_jobs='auto', show_progress=True, suppress_warnings=False)</code>","text":"<p>This is a helper function to check most inputs of backtesting functions in  modules <code>model_selection</code>, <code>model_selection_multiseries</code> and  <code>model_selection_sarimax</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model.</p> required <code>steps</code> <code>(int, list)</code> <p>Number of future steps predicted.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>add_aggregated_metric</code> <code>bool</code> <p>If <code>True</code>, the aggregated metrics (average, weighted average and pooling) over all levels are also returned (only multiseries).</p> <code>`True`</code> <code>y</code> <code>pandas Series</code> <p>Training time series for uni-series forecasters.</p> <code>`None`</code> <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series for multi-series forecasters.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals used in each bootstrapping iteration are selected conditioning on the predicted values. If <code>False</code>, residuals are selected randomly without conditioning on the predicted values.</p> <code>`False`</code> <code>n_jobs</code> <code>(int, auto)</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the fuction skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>`True`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the backtesting  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 500,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`, `model_selection_multiseries` and \n    `model_selection_sarimax`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    steps : int, list\n        Number of future steps predicted.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterAutoreg\",\n        \"ForecasterAutoregDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterAutoregMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterAutoregMultiSeries\"\n    ]\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(\n            (f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n        )\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                (f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                 f\"Got {type(series)}.\")\n            )\n\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    (f\"If `series` is a dictionary, all series must be a named \"\n                     f\"pandas Series or a pandas DataFrame with a single column. \"\n                     f\"Review series: {not_valid_series}\")\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    (f\"If `series` is a dictionary, all series must have a Pandas \"\n                     f\"DatetimeIndex as index with the same frequency. \"\n                     f\"Review series: {not_valid_index}\")\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    (f\"If `series` is a dictionary, all series must have a Pandas \"\n                     f\"DatetimeIndex as index with the same frequency. \"\n                     f\"Found frequencies: {indexes_freq}\")\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    (f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                     f\"Series/DataFrames or None. Got {type(exog)}.\")\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        (f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                         f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\")\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\")\n                )\n\n    if hasattr(forecaster, 'differentiaion'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                (f\"The differentiation included in the forecaster \"\n                 f\"({forecaster.differentiation}) differs from the differentiation \"\n                 f\" included in the cv ({cv.differentiation}). Set the same value \"\n                 f\"for both.\")\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            (f\"`metric` must be a string, a callable function, or a list containing \"\n             f\"multiple strings and/or callables. Got {type(metric)}.\")\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        pass\n    elif initial_train_size is not None:\n        if not isinstance(initial_train_size, (int, np.integer)):\n            raise TypeError(\n                (f\"If used, `initial_train_size` must be an integer greater than the \"\n                 f\"window_size of the forecaster. Got type {type(initial_train_size)}.\")\n            )\n        if initial_train_size &gt;= data_length:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer smaller \"\n                 f\"than the length of `{data_name}` ({data_length}).\")\n            )    \n        if initial_train_size &lt; forecaster.window_size:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer greater than \"\n                 f\"the window_size of the forecaster ({forecaster.window_size}).\")\n            )\n        if initial_train_size + gap &gt;= data_length:\n            raise ValueError(\n                (f\"The combination of initial_train_size {initial_train_size} and \"\n                 f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                 f\"({data_length}).\")\n            )\n    else:\n        if forecaster_name == 'ForecasterSarimax':\n            raise ValueError(\n                (f\"`initial_train_size` must be an integer smaller than the \"\n                 f\"length of `{data_name}` ({data_length}).\")\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    (\"`forecaster` must be already trained if no `initial_train_size` \"\n                     \"is provided.\")\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot &lt; 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state &lt; 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) &lt; steps:\n        raise ValueError(\n            (f\"There is not enough data to evaluate {steps} steps in a single \"\n             f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n             f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n             f\"    Steps                   : {steps}\")\n        )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.select_n_jobs_backtesting","title":"<code>select_n_jobs_backtesting(forecaster, refit)</code>","text":"<p>Select the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.</p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If <code>refit</code> is an integer, then <code>n_jobs = 1</code>. This is because parallelization doesn't  work with intermittent refit.</li> <li>If forecaster is 'ForecasterAutoreg' and regressor is a linear regressor,  then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterAutoreg' and regressor is not a linear  regressor and <code>refit = True</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterAutoreg' and regressor is not a linear  regressor and <code>refit = False</code>, then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and <code>refit = True</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and <code>refit = False</code>, then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterAutoregMultiSeries', then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate',  then <code>n_jobs = 1</code>.</li> <li>If regressor is a <code>LGBMRegressor</code>, then <code>n_jobs = 1</code>. This is because <code>lightgbm</code>  is highly optimized for gradient boosting and parallelizes operations at a very  fine-grained level, making additional parallelization unnecessary and  potentially harmful due to resource contention.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model.</p> required <code>refit</code> <code>(bool, int)</code> <p>If the forecaster is refitted during the backtesting process.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -&gt; int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterAutoreg' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterAutoreg' and regressor is not a linear \n    regressor and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterAutoreg' and regressor is not a linear \n    regressor and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterAutoregMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor`, then `n_jobs = 1`. This is because `lightgbm` \n    is highly optimized for gradient boosting and parallelizes operations at a very \n    fine-grained level, making additional parallelization unnecessary and \n    potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor_name = type(forecaster.regressor[-1]).__name__\n    else:\n        regressor_name = type(forecaster.regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterAutoreg']:\n            if regressor_name in linear_regressors or regressor_name == 'LGBMRegressor':\n                n_jobs = 1\n            else:\n                n_jobs = joblib.cpu_count() - 1 if refit else 1\n        elif forecaster_name in ['ForecasterAutoregDirect', 'ForecasterAutoregMultiVariate']:\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterAutoregMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = 1\n            else:\n                n_jobs = joblib.cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, regressor_name)</code>","text":"<p>Select the optimal number of jobs to use in the fitting process. This selection is based on heuristics and is not guaranteed to be optimal. </p> <p>The number of jobs is chosen as follows:</p> <ul> <li> <p>If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and regressor_name is a linear regressor then <code>n_jobs = 1</code>, otherwise <code>n_jobs = cpu_count() - 1</code>.</p> </li> <li> <p>If <code>LGBMRegressor</code> then <code>n_jobs = 1</code>. This is because <code>lightgbm</code>  is highly optimized for gradient boosting and parallelizes operations at a very  fine-grained level, making additional parallelization unnecessary and  potentially harmful due to resource contention.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>regressor_name</code> <code>str</code> <p>The type of regressor.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor_name: str,\n) -&gt; int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n\n    The number of jobs is chosen as follows:\n\n    - If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, otherwise `n_jobs = cpu_count() - 1`.\n\n    - If `LGBMRegressor` then `n_jobs = 1`. This is because `lightgbm` \n    is highly optimized for gradient boosting and parallelizes operations at a very \n    fine-grained level, making additional parallelization unnecessary and \n    potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor_name : str\n        The type of regressor.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterAutoregDirect', \n                           'ForecasterAutoregMultiVariate']:\n        if regressor_name in linear_regressors or regressor_name == 'LGBMRegressor':\n            n_jobs = 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_preprocess_series","title":"<code>check_preprocess_series(series)</code>","text":"<p>Check and preprocess <code>series</code> argument in <code>ForecasterAutoregMultiSeries</code> class.</p> <ul> <li>If <code>series</code> is a pandas DataFrame, it is converted to a dict of pandas  Series and index is overwritten according to the rules of preprocess_y.</li> <li>If <code>series</code> is a dict, all values are converted to pandas Series. Checks if all index are pandas DatetimeIndex and, at least, one Series has a non-null frequency. No multiple frequency is allowed.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> <code>series_indexes</code> <code>dict</code> <p>Dictionary with the index of each series.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_preprocess_series(\n    series: Union[pd.DataFrame, dict],\n) -&gt; Tuple[dict, pd.Index]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterAutoregMultiSeries` class.\n\n    - If `series` is a pandas DataFrame, it is converted to a dict of pandas \n    Series and index is overwritten according to the rules of preprocess_y.\n    - If `series` is a dict, all values are converted to pandas Series. Checks\n    if all index are pandas DatetimeIndex and, at least, one Series has a non-null\n    frequency. No multiple frequency is allowed.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n\n    \"\"\"\n\n    if isinstance(series, pd.DataFrame):\n\n        _, series_index = preprocess_y(y=series, return_values=False)\n        series = series.copy()\n        series.index = series_index\n        series_dict = series.to_dict(\"series\")\n\n    elif isinstance(series, dict):\n\n        not_valid_series = [\n            k \n            for k, v in series.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must be a named \"\n                 f\"pandas Series or a pandas DataFrame with a single column. \"\n                 f\"Review series: {not_valid_series}\")\n            )\n\n        series_dict = {\n            k: v.copy()\n            for k, v in series.items()\n        }\n\n        for k, v in series_dict.items():\n            if isinstance(v, pd.DataFrame):\n                if v.shape[1] != 1:\n                    raise ValueError(\n                        (f\"If `series` is a dictionary, all series must be a named \"\n                         f\"pandas Series or a pandas DataFrame with a single column. \"\n                         f\"Review series: '{k}'\")\n                    )\n                series_dict[k] = v.iloc[:, 0]\n\n            series_dict[k].name = k\n\n        not_valid_index = [\n            k \n            for k, v in series_dict.items()\n            if not isinstance(v.index, pd.DatetimeIndex)\n        ]\n        if not_valid_index:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Review series: {not_valid_index}\")\n            )\n\n        indexes_freq = [f\"{v.index.freq}\" for v in series_dict.values()]\n        indexes_freq = sorted(set(indexes_freq))\n        if not len(indexes_freq) == 1:\n            raise ValueError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Found frequencies: {indexes_freq}\")\n            )\n    else:\n        raise TypeError(\n            (f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n             f\"Got {type(series)}.\")\n        )\n\n    for k, v in series_dict.items():\n        if np.isnan(v).all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n    series_indexes = {\n        k: v.index\n        for k, v in series_dict.items()\n    }\n\n    return series_dict, series_indexes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_preprocess_exog_multiseries","title":"<code>check_preprocess_exog_multiseries(input_series_is_dict, series_indexes, series_names_in_, exog, exog_dict)</code>","text":"<p>Check and preprocess <code>exog</code> argument in <code>ForecasterAutoregMultiSeries</code> class.</p> <ul> <li>If input series is a pandas DataFrame (input_series_is_dict = False), checks that input exog (pandas Series, DataFrame or dict) has the same index  (type, length and frequency). Index is overwritten according to the rules  of preprocess_exog. Create a dict of exog with the same keys as series.</li> <li>If input series is a dict (input_series_is_dict = True), then input  exog must be a dict. Check exog has a pandas DatetimeIndex and convert all values to pandas DataFrames.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>input_series_is_dict</code> <code>bool</code> <p>Indicates if input series argument is a dict.</p> required <code>series_indexes</code> <code>dict</code> <p>Dictionary with the index of each series.</p> required <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s used during training.</p> required <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> required <p>Returns:</p> Name Type Description <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_preprocess_exog_multiseries(\n    input_series_is_dict: bool,\n    series_indexes: dict,\n    series_names_in_: list,\n    exog: Union[pd.Series, pd.DataFrame, dict],\n    exog_dict: dict,\n) -&gt; Tuple[dict, list]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterAutoregMultiSeries` class.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    checks that input exog (pandas Series, DataFrame or dict) has the same index \n    (type, length and frequency). Index is overwritten according to the rules \n    of preprocess_exog. Create a dict of exog with the same keys as series.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Check exog has a pandas DatetimeIndex and convert all\n    values to pandas DataFrames.\n\n    Parameters\n    ----------\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(\n            (f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n             f\"Series/DataFrames or None. Got {type(exog)}.\")\n        )\n\n    if not input_series_is_dict:\n        # If input series is a pandas DataFrame, all index are the same.\n        # Select the first index to check exog\n        series_index = series_indexes[series_names_in_[0]]\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)): \n\n        if input_series_is_dict:\n            raise TypeError(\n                (f\"`exog` must be a dict of DataFrames or Series if \"\n                 f\"`series` is a dict. Got {type(exog)}.\")\n            )\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog.index = exog_index\n\n        if len(exog) != len(series_index):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series_index)})\")\n            )\n\n        if not (exog_index == series_index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n        exog_dict = {serie: exog for serie in series_names_in_}\n\n    else:\n\n        not_valid_exog = [\n            k \n            for k, v in exog.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n        ]\n        if not_valid_exog:\n            raise TypeError(\n                (f\"If `exog` is a dictionary, all exog must be a named pandas \"\n                 f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\")\n            )\n\n        # Only elements already present in exog_dict are updated\n        exog_dict.update(\n            (k, v.copy())\n            for k, v in exog.items() \n            if k in exog_dict and v is not None\n        )\n\n        series_not_in_exog = set(series_names_in_) - set(exog.keys())\n        if series_not_in_exog:\n            warnings.warn(\n                (f\"{series_not_in_exog} not present in `exog`. All values \"\n                 f\"of the exogenous variables for these series will be NaN.\"),\n                 MissingExogWarning\n            )\n\n        for k, v in exog_dict.items():\n            if v is not None:\n                check_exog(exog=v, allow_nan=True)\n                if isinstance(v, pd.Series):\n                    v = v.to_frame()\n                exog_dict[k] = v\n\n        if not input_series_is_dict:\n            for k, v in exog_dict.items():\n                if v is not None:\n                    if len(v) != len(series_index):\n                        raise ValueError(\n                            (f\"`exog` for series '{k}' must have same number of \"\n                             f\"samples as `series`. length `exog`: ({len(v)}), \"\n                             f\"length `series`: ({len(series_index)})\")\n                        )\n\n                    _, v_index = preprocess_exog(exog=v, return_values=False)\n                    exog_dict[k].index = v_index\n                    if not (exog_dict[k].index == series_index).all():\n                        raise ValueError(\n                            (f\"Different index for series '{k}' and its exog. \"\n                             f\"When `series` is a pandas DataFrame, they must be \"\n                             f\"equal to ensure the correct alignment of values.\")\n                        )\n        else:\n            not_valid_index = [\n                k\n                for k, v in exog_dict.items()\n                if v is not None and not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise TypeError(\n                    (f\"All exog must have a Pandas DatetimeIndex as index with the \"\n                     f\"same frequency. Check exog for series: {not_valid_index}\")\n                )\n\n        # Check that all exog have the same dtypes for common columns\n        exog_dtypes_buffer = [df.dtypes for df in exog_dict.values() if df is not None]\n        exog_dtypes_buffer = pd.concat(exog_dtypes_buffer, axis=1)\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1).eq(1)\n        if not exog_dtypes_nunique.all():\n            non_unique_dtyeps_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(f\"Exog/s: {non_unique_dtyeps_exogs} have different dtypes in different series.\")\n\n    exog_names_in_ = list(\n        set(\n            column\n            for df in exog_dict.values()\n            if df is not None\n            for column in df.columns.to_list()\n        )\n    )\n\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(\n            (f\"`exog` cannot contain a column named the same as one of the series.\\n\"\n             f\"    `series` columns : {series_names_in_}.\\n\"\n             f\"    `exog`   columns : {exog_names_in_}.\")\n        )\n\n    return exog_dict, exog_names_in_\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.align_series_and_exog_multiseries","title":"<code>align_series_and_exog_multiseries(series_dict, input_series_is_dict, exog_dict=None)</code>","text":"<p>Align series and exog according to their index. If needed, reindexing is applied. Heading and trailing NaNs are removed from all series in  <code>series_dict</code>.</p> <ul> <li>If input series is a pandas DataFrame (input_series_is_dict = False), input exog (pandas Series, DataFrame or dict) must have the same index  (type, length and frequency). Reindexing is not applied.</li> <li>If input series is a dict (input_series_is_dict = True), then input  exog must be a dict. Both must have a pandas DatetimeIndex, but can have  different lengths. Reindexing is applied.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> required <code>input_series_is_dict</code> <code>bool</code> <p>Indicates if input series argument is a dict.</p> required <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def align_series_and_exog_multiseries(\n    series_dict: dict,\n    input_series_is_dict: bool,\n    exog_dict: dict = None\n) -&gt; Tuple[Union[pd.Series, pd.DataFrame], Union[pd.Series, pd.DataFrame]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    input exog (pandas Series, DataFrame or dict) must have the same index \n    (type, length and frequency). Reindexing is not applied.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Both must have a pandas DatetimeIndex, but can have \n    different lengths. Reindexing is applied.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    exog_dict : dict, default `None`\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    \"\"\"\n\n    for k in series_dict.keys():\n\n        first_valid_index = series_dict[k].first_valid_index()\n        last_valid_index = series_dict[k].last_valid_index()\n\n        series_dict[k] = series_dict[k].loc[first_valid_index : last_valid_index]\n\n        if exog_dict[k] is not None:\n            if input_series_is_dict:\n                index_intersection = (\n                    series_dict[k].index.intersection(exog_dict[k].index)\n                )\n                if len(index_intersection) == 0:\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same index. \"\n                         f\"All exog values will be NaN for the period of the series.\"),\n                         MissingValuesWarning\n                    )\n                elif len(index_intersection) != len(series_dict[k]):\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same length. \"\n                         f\"Exog values will be NaN for the not matched period of the series.\"),\n                         MissingValuesWarning\n                    )  \n                exog_dict[k] = exog_dict[k].loc[index_intersection]\n                if len(index_intersection) != len(series_dict[k]):\n                    exog_dict[k] = exog_dict[k].reindex(\n                                       series_dict[k].index, \n                                       fill_value = np.nan\n                                   )\n            else:\n                exog_dict[k] = exog_dict[k].loc[first_valid_index : last_valid_index]\n\n    return series_dict, exog_dict\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.prepare_levels_multiseries","title":"<code>prepare_levels_multiseries(X_train_series_names_in_, levels=None)</code>","text":"<p>Prepare list of levels to be predicted in multiseries Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code>.</p> required <code>levels</code> <code>(str, list)</code> <p>Names of the series (levels) to be predicted.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def prepare_levels_multiseries(\n    X_train_series_names_in_: list,\n    levels: Optional[Union[str, list]] = None\n) -&gt; Tuple[list, bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default `None`\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n\n    \"\"\"\n\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n\n    return levels, input_levels_is_list\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_levels_self_last_window_multiseries","title":"<code>preprocess_levels_self_last_window_multiseries(levels, input_levels_is_list, last_window_)</code>","text":"<p>Preprocess <code>levels</code> and <code>last_window</code> (when using self.last_window_) arguments  in multiseries Forecasters when predicting. Only levels whose last window  ends at the same datetime index will be predicted together.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> required <code>input_levels_is_list</code> <code>bool</code> <p>Indicates if input levels argument is a list.</p> required <code>last_window_</code> <code>dict</code> <p>Dictionary with the last window of each series (self.last_window_).</p> required <p>Returns:</p> Name Type Description <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_levels_self_last_window_multiseries(\n    levels: list,\n    input_levels_is_list: bool,\n    last_window_: dict\n) -&gt; Tuple[list, pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [level for level in levels \n                  if level not in not_available_last_window]\n        if not levels:\n            raise ValueError(\n                (f\"No series to predict. None of the series {not_available_last_window} \"\n                 f\"are present in `last_window_` attribute. Provide `last_window` \"\n                 f\"as argument in predict method.\")\n            )\n        else:\n            warnings.warn(\n                (f\"Levels {not_available_last_window} are excluded from \"\n                 f\"prediction since they were not stored in `last_window_` \"\n                 f\"attribute during training. If you don't want to retrain \"\n                 f\"the Forecaster, provide `last_window` as argument.\"),\n                 IgnoredArgumentWarning\n            )\n\n    last_index_levels = [\n        v.index[-1] \n        for k, v in last_window_.items()\n        if k in levels\n    ]\n    if len(set(last_index_levels)) &gt; 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [\n            k\n            for k, v in last_window_.items()\n            if k in levels and v.index[-1] == max_index_levels\n        ]\n\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(\n                (f\"Only series whose last window ends at the same index \"\n                 f\"can be predicted together. Series that do not reach \"\n                 f\"the maximum index, '{max_index_levels}', are excluded \"\n                 f\"from prediction: {series_excluded_from_last_window}.\"),\n                IgnoredArgumentWarning\n            )\n\n    last_window = pd.DataFrame(\n        {k: v \n         for k, v in last_window_.items() \n         if k in levels}\n    )\n\n    return levels, last_window\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.prepare_residuals_multiseries","title":"<code>prepare_residuals_multiseries(levels, use_in_sample_residuals, encoding=None, in_sample_residuals_=None, out_sample_residuals_=None)</code>","text":"<p>Prepare residuals for bootstrapping prediction in multiseries Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>Indicates if <code>forecaster.in_sample_residuals_</code> are used.</p> required <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (<code>ForecasterAutoregMultiSeries</code>).</p> <code>`None`</code> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>`None`</code> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>residuals</code> <code>dict</code> <p>Residuals of the model for each level to use in bootstrapping prediction.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def prepare_residuals_multiseries(\n    levels: list,\n    use_in_sample_residuals: bool,\n    encoding: Optional[str] = None,\n    in_sample_residuals_: Optional[dict] = None,\n    out_sample_residuals_: Optional[dict] = None\n) -&gt; Tuple[list, bool]:\n    \"\"\"\n    Prepare residuals for bootstrapping prediction in multiseries Forecasters.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    use_in_sample_residuals : bool\n        Indicates if `forecaster.in_sample_residuals_` are used.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterAutoregMultiSeries`).\n    in_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting training data. Only stored up to\n        1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting non-training data. Only stored\n        up to 1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    residuals : dict\n        Residuals of the model for each level to use in bootstrapping prediction.\n\n    \"\"\"\n\n    if use_in_sample_residuals:\n        unknown_levels = set(levels) - set(in_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(\n                (f\"`levels` {unknown_levels} are not present in `forecaster.in_sample_residuals_`, \"\n                 f\"most likely because they were not present in the training data. \"\n                 f\"A random sample of the residuals from other levels will be used. \"\n                 f\"This can lead to inaccurate intervals for the unknown levels.\"),\n                 UnknownLevelWarning\n            )\n        residuals = in_sample_residuals_.copy()\n    else:\n        if out_sample_residuals_ is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                 \"`use_in_sample_residuals=True` or the \"\n                 \"`set_out_sample_residuals()` method before predicting.\")\n            )\n        else:\n            unknown_levels = set(levels) - set(out_sample_residuals_.keys())\n            if unknown_levels and encoding is not None:\n                warnings.warn(\n                    (f\"`levels` {unknown_levels} are not present in `forecaster.out_sample_residuals_`. \"\n                     f\"A random sample of the residuals from other levels will be used. \"\n                     f\"This can lead to inaccurate intervals for the unknown levels. \"\n                     f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                     f\"predicting to set the residuals for these levels.\"),\n                     UnknownLevelWarning\n                )\n            residuals = out_sample_residuals_.copy()\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals_\" if use_in_sample_residuals\n        else \"forecaster.out_sample_residuals_\"\n    )\n    for level in levels:\n        if level in unknown_levels:\n            residuals[level] = residuals['_unknown_level']\n        if residuals[level] is None or len(residuals[level]) == 0:\n            raise ValueError(\n                (f\"Not available residuals for level '{level}'. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (any(element is None for element in residuals[level]) or\n              np.any(np.isnan(residuals[level]))):\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"or `NaNs` values. Check `{check_residuals}`.\")\n            )\n\n    return residuals\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.set_skforecast_warnings","title":"<code>set_skforecast_warnings(suppress_warnings, action='default')</code>","text":"<p>Set skforecast warnings action.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed. If <code>False</code>, skforecast warnings will be shown as default. See  skforecast.exceptions.warn_skforecast_categories for more information.</p> required <code>action</code> <code>str</code> <p>Action to be taken when a warning is raised. See the warnings module for more information.</p> <code>`'default'`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -&gt; None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)\n</code></pre>"},{"location":"authors/authors.html","title":"Authors","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.</p> <p>A detailed overview on how to contribute can be found in the contributing guide.</p>"},{"location":"authors/authors.html#core-development-team","title":"Core Development Team","text":"<ul> <li> <p>Joaqu\u00edn Amat Rodrigo (j.amatrodrigo@gmail.com)</p> </li> <li> <p>Javier Escobar Ortiz (javier.escobar.ortiz@gmail.com)</p> </li> </ul>"},{"location":"authors/authors.html#main-contributors","title":"Main Contributors","text":"<ul> <li>Fernando Carazo Melo, FernandoCarazoMelo</li> </ul>"},{"location":"authors/authors.html#contributions","title":"Contributions","text":"<ul> <li> <p>Josh Wong, josh-wong</p> </li> <li> <p>Edgar Bahilo Rodr\u00edguez, edgBR</p> </li> <li> <p>Mwainwright, syndct</p> </li> <li> <p>Kishan Manani, KishManani</p> </li> <li> <p>Sergio Quijano, Sergio-Quijano-Stratesys</p> </li> <li> <p>Fernando da Silva, schoulten</p> </li> <li> <p>Lillian Jensen, MPH, tyg3rr</p> </li> <li> <p>Ivan Liu , IvanLiuTW</p> </li> </ul>"},{"location":"examples/examples_english.html","title":"English","text":""},{"location":"examples/examples_english.html#examples-and-tutorials","title":"Examples and Tutorials","text":"<p>Practical examples and tutorials to help you understand and apply skforecast.</p>"},{"location":"examples/examples_english.html#getting-started-fundamental-forecasting","title":"Getting Started: Fundamental Forecasting","text":"<p>This section provides essential tutorials for users who are just getting started with time series forecasting. These examples cover the most fundamental models and techniques to help you build a strong foundation in forecasting.</p> <p> Skforecast: time series forecasting with machine learning</p> <p> ARIMA and SARIMAX models</p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost</p> <p> Forecasting with XGBoost</p>"},{"location":"examples/examples_english.html#global-models-multi-series-forecasting","title":"Global Models: Multi-Series Forecasting","text":"<p>These tutorials focus on global models and multi-series forecasting, where you can explore the use of techniques that handle multiple time series simultaneously and compare performance across different forecasting approaches.</p> <p> Global Forecasting Models: Multi-series forecasting</p> <p> Global Forecasting Models: Comparative Analysis of Single and Multi-Series Forecasting Modeling</p> <p> Forecasting with Deep Learning</p>"},{"location":"examples/examples_english.html#advanced-techniques-beyond-basic-models","title":"Advanced Techniques: Beyond Basic Models","text":"<p>For experienced users looking to deepen their forecasting skills, this section provides advanced techniques, including probabilistic forecasting, handling missing values, and more sophisticated ensemble methods.</p> <p> Probabilistic forecasting</p> <p> Modelling time series trend with tree based models</p> <p> Forecasting time series with missing values</p> <p> Interpretable forecasting models</p> <p> Stacking ensemble of machine learning models to improve forecasting</p>"},{"location":"examples/examples_english.html#real-world-challenges-and-case-studies","title":"Real-World Challenges and Case Studies","text":"<p>This section includes real-world applications of time series forecasting to tackle specific challenges, such as forecasting energy demand, web traffic, and even cryptocurrency prices. Learn how to apply forecasting techniques to practical use cases.</p> <p> Forecasting energy demand with machine learning</p> <p> Forecasting web traffic with machine learning and Python</p> <p> Intermittent demand forecasting</p> <p> Mitigating the impact of covid on forecasting models</p> <p> Bitcoin price prediction with Python</p>"},{"location":"examples/examples_spanish.html","title":"Spanish","text":""},{"location":"examples/examples_spanish.html#ejemplos-y-tutoriales","title":"Ejemplos y Tutoriales","text":"<p>Ejemplos pr\u00e1cticos y tutoriales para ayudarte a entender y aplicar skforecast.</p>"},{"location":"examples/examples_spanish.html#primeros-pasos-forecasting-fundamental","title":"Primeros Pasos: Forecasting Fundamental","text":"<p>Esta secci\u00f3n proporciona tutoriales esenciales para los usuarios que est\u00e1n comenzando con el pron\u00f3stico de series temporales. Estos ejemplos cubren los modelos y t\u00e9cnicas m\u00e1s fundamentales para ayudarte a construir una base s\u00f3lida en forecasting.</p> <p> Skforecast: forecasting series temporales con machine learning</p> <p> Modelos ARIMA y SARIMAX</p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost</p> <p> Forecasting con XGBoost</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p>"},{"location":"examples/examples_spanish.html#modelos-globales-forecasting-multi-serie","title":"Modelos Globales: Forecasting Multi-Serie","text":"<p>Estos tutoriales se centran en modelos globales y en el pron\u00f3stico de series m\u00faltiples, donde puedes explorar el uso de t\u00e9cnicas que manejan varias series temporales simult\u00e1neamente y comparar el rendimiento entre diferentes enfoques de forecasting.</p> <p> Global Forecasting Models: Multi-series forecasting</p> <p> Modelos de forecasting globales: An\u00e1lisis comparativo de modelos de una y m\u00faltiples series</p> <p> Forecasting con Deep Learning</p>"},{"location":"examples/examples_spanish.html#tecnicas-avanzadas-mas-alla-de-los-modelos-basicos","title":"T\u00e9cnicas Avanzadas: M\u00e1s All\u00e1 de los Modelos B\u00e1sicos","text":"<p>Para usuarios experimentados que buscan profundizar en sus habilidades de forecasting, esta secci\u00f3n ofrece t\u00e9cnicas avanzadas, como forecasting probabil\u00edstico y manejo de valores ausentes.</p> <p> Forecasting probabil\u00edstico</p> <p> Modelar series temporales con tendencia utilizando modelos de \u00e1rboles</p> <p> Forecasting de series incompletas con valores faltantes</p> <p> Interpretabilidad en modelos de forecasting</p>"},{"location":"examples/examples_spanish.html#desafios-del-mundo-real-y-estudios-de-caso","title":"Desaf\u00edos del Mundo Real y Estudios de Caso","text":"<p>Esta secci\u00f3n incluye aplicaciones del mundo real del pron\u00f3stico de series temporales para abordar desaf\u00edos espec\u00edficos, como el pron\u00f3stico de la demanda de energ\u00eda, el tr\u00e1fico web e incluso los precios de criptomonedas. Aprende c\u00f3mo aplicar t\u00e9cnicas de forecasting a casos pr\u00e1cticos.</p> <p> Forecasting de la demanda el\u00e9ctrica</p> <p> Forecasting de las visitas a una p\u00e1gina web</p> <p> Predicci\u00f3n demanda intermitente</p> <p> Reducir el impacto del Covid en modelos de forecasting</p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p>"},{"location":"faq/cyclical-features-time-series.html","title":"Cyclical features in time series","text":"<p>Cyclical features play an important role in time series prediction because they capture recurring patterns or oscillations within a data set. These patterns repeat at fixed intervals, and the effective incorporation of cyclical features into a machine learning model requires careful preprocessing and feature engineering.</p> <p>Due to the circular nature of cyclical features, it is not recommended to use them directly as numerical inputs in a machine learning model. Instead, they should be encoded in a format that captures their cyclical behavior. There are several common encoding techniques:</p> <ul> <li><p>One-hot encoding: If the cyclical feature consists of distinct categories, such as seasons or months, one-hot encoding can be used. This approach creates binary variables for each category, allowing the model to understand the presence or absence of specific categories.</p> </li> <li><p>Trigonometric coding: For periodic features such as time of day or day of the week, trigonometric functions such as sine and cosine can be used for coding. By mapping the cyclic feature onto a unit circle, these functions preserve the cyclic relationships. In addition, this method introduces only two additional features, making it an efficient coding technique.</p> </li> <li><p>Basis functions: Basis functions are mathematical functions that span a vector space and can be used to represent other functions within that space. When using basis functions, the cyclic feature is transformed into a new set of features based on the selected basis functions. Some commonly used basis functions for encoding cyclic features include Fourier basis functions, B-spline basis functions, and Gaussian basis functions. B-splines are a way to approximate nonlinear functions using a piecewise combination of polynomials.</p> </li> </ul> <p>By applying these encoding techniques, cyclic features can be effectively incorporated into a machine learning model, allowing it to capture and exploit the valuable recurring patterns present in time series data.</p> <p> \u270e Note </p> <p>The following examples are inspired by Time-related feature engineering, scikit-lego\u2019s documentation and Three Approaches to Encoding Time Information as Features for ML Models By Eryk Lewinson.</p> In\u00a0[6]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklego.preprocessing import RepeatingBasisFunction\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n\n# Warnings configuration\n# ==============================================================================\nimport warnings\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import SplineTransformer from sklearn.compose import make_column_transformer from sklego.preprocessing import RepeatingBasisFunction from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster  # Warnings configuration # ============================================================================== import warnings In\u00a0[7]: Copied! <pre># Data simulation\n# ==============================================================================\nnp.random.seed(123)\ndates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\")\ndata = pd.DataFrame(index=dates)\ndata.index.name = \"date\"\ndata[\"day_idx\"] = range(len(data))\ndata['month'] = data.index.month\n\n# Create the components that will be combined to get the target series\nsignal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi)\nsignal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2)\nnoise = np.random.normal(0, 0.85, len(data))\ny = signal_1 + signal_2 + noise\n\ndata[\"y\"] = y\ndata = data[[\"y\", \"month\"]]\ndata.head(3)\n</pre> # Data simulation # ============================================================================== np.random.seed(123) dates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\") data = pd.DataFrame(index=dates) data.index.name = \"date\" data[\"day_idx\"] = range(len(data)) data['month'] = data.index.month  # Create the components that will be combined to get the target series signal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi) signal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2) noise = np.random.normal(0, 0.85, len(data)) y = signal_1 + signal_2 + noise  data[\"y\"] = y data = data[[\"y\", \"month\"]] data.head(3) Out[7]: y month date 2020-01-01 2.928244 1 2020-01-02 4.866145 1 2020-01-03 4.425159 1 In\u00a0[8]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2022-06-30 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2022-06-30 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-01-01 00:00:00 --- 2022-06-30 00:00:00  (n=912)\nDates test  : 2022-07-01 00:00:00 --- 2023-12-31 00:00:00  (n=549)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(9, 3))\ndata_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax)\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax=plt.subplots(figsize=(9, 3)) data_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax) ax.legend(); In\u00a0[10]: Copied! <pre># One hot encoding of week_day and hour_day\n# ==============================================================================\none_hot_encoder = make_column_transformer(\n                      (\n                          OneHotEncoder(sparse_output=False, drop='if_binary'),\n                          ['month'],\n                      ),\n                      remainder=\"passthrough\",\n                      verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n\ndata_encoded_oh = one_hot_encoder.fit_transform(data)\ndata_encoded_oh.head(3)\n</pre> # One hot encoding of week_day and hour_day # ============================================================================== one_hot_encoder = make_column_transformer(                       (                           OneHotEncoder(sparse_output=False, drop='if_binary'),                           ['month'],                       ),                       remainder=\"passthrough\",                       verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\")  data_encoded_oh = one_hot_encoder.fit_transform(data) data_encoded_oh.head(3) Out[10]: month_1 month_2 month_3 month_4 month_5 month_6 month_7 month_8 month_9 month_10 month_11 month_12 y date 2020-01-01 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.928244 2020-01-02 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.866145 2020-01-03 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.425159 In\u00a0[11]: Copied! <pre># Cyclical encoding with sine/cosine transformation\n# ==============================================================================\ndef sin_transformer(period):\n\t\"\"\"\n\tReturns a transformer that applies sine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\ndef cos_transformer(period):\n\t\"\"\"\n\tReturns a transformer that applies cosine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n\ndata_encoded_sin_cos = data.copy()\ndata_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos.head()\n</pre> # Cyclical encoding with sine/cosine transformation # ============================================================================== def sin_transformer(period): \t\"\"\" \tReturns a transformer that applies sine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))  def cos_transformer(period): \t\"\"\" \tReturns a transformer that applies cosine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))  data_encoded_sin_cos = data.copy() data_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos.head() Out[11]: y month month_sin month_cos date 2020-01-01 2.928244 1 0.5 0.866025 2020-01-02 4.866145 1 0.5 0.866025 2020-01-03 4.425159 1 0.5 0.866025 2020-01-04 3.069222 1 0.5 0.866025 2020-01-05 4.021290 1 0.5 0.866025 In\u00a0[12]: Copied! <pre># Plot of the transformation\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(4., 3.5))\nsp = ax.scatter(\n        data_encoded_sin_cos[\"month_sin\"],\n        data_encoded_sin_cos[\"month_cos\"],\n        c=data_encoded_sin_cos[\"month\"],\n        cmap='viridis'\n     )\nax.set(\n    xlabel=\"sin(month)\",\n    ylabel=\"cos(month)\",\n)\n_ = fig.colorbar(sp)\ndata_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month')\n</pre> # Plot of the transformation # ============================================================================== fig, ax = plt.subplots(figsize=(4., 3.5)) sp = ax.scatter(         data_encoded_sin_cos[\"month_sin\"],         data_encoded_sin_cos[\"month_cos\"],         c=data_encoded_sin_cos[\"month\"],         cmap='viridis'      ) ax.set(     xlabel=\"sin(month)\",     ylabel=\"cos(month)\", ) _ = fig.colorbar(sp) data_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month') In\u00a0[13]: Copied! <pre># Create feature day of year\n# ==============================================================================\ndata['day_of_year'] = data.index.day_of_year\ndata.head(3)\n</pre> # Create feature day of year # ============================================================================== data['day_of_year'] = data.index.day_of_year data.head(3) Out[13]: y month day_of_year date 2020-01-01 2.928244 1 1 2020-01-02 4.866145 1 2 2020-01-03 4.425159 1 3 In\u00a0[14]: Copied! <pre># B-spline functions\n# ==============================================================================\ndef spline_transformer(period, degree=3, extrapolation=\"periodic\"):\n    \"\"\"\n    Returns a transformer that applies B-spline transformation.\n    \"\"\"\n    return SplineTransformer(\n               degree        = degree,\n               n_knots       = period+1,\n               knots         = 'uniform',\n               extrapolation = extrapolation,\n               include_bias  = True\n           ).set_output(transform=\"pandas\")\n\nsplines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']])\nsplines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))]\n</pre> # B-spline functions # ============================================================================== def spline_transformer(period, degree=3, extrapolation=\"periodic\"):     \"\"\"     Returns a transformer that applies B-spline transformation.     \"\"\"     return SplineTransformer(                degree        = degree,                n_knots       = period+1,                knots         = 'uniform',                extrapolation = extrapolation,                include_bias  = True            ).set_output(transform=\"pandas\")  splines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']]) splines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))] <p>The graph below shows the 12 spline functions generated using the day of the year as input. Since 12 splines are created with knots evenly distributed along the range 1 to 365 (day_of_year), each curve represents the proximity to the beginning of a particular month.</p> In\u00a0[15]: Copied! <pre># Location of the maximum value of each spline\n# ==============================================================================\nsplines_month.idxmax()\n</pre> # Location of the maximum value of each spline # ============================================================================== splines_month.idxmax() Out[15]: <pre>spline0    2020-12-01\nspline1    2020-01-01\nspline2    2020-01-31\nspline3    2020-03-02\nspline4    2020-04-01\nspline5    2020-05-02\nspline6    2020-06-01\nspline7    2020-07-02\nspline8    2020-08-01\nspline9    2020-08-31\nspline10   2020-10-01\nspline11   2020-10-31\ndtype: datetime64[ns]</pre> In\u00a0[16]: Copied! <pre># Plot of the B-splines functions for the first 365 days\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 5))\nsplines_month.head(365).plot(\n    ax       = ax,\n    subplots = True,\n    sharex   = True,\n    legend   = False,\n    yticks   = [],\n    title    = 'Splines functions for the first 365 days'\n);\n</pre> # Plot of the B-splines functions for the first 365 days # ============================================================================== fig, ax = plt.subplots(figsize=(8, 5)) splines_month.head(365).plot(     ax       = ax,     subplots = True,     sharex   = True,     legend   = False,     yticks   = [],     title    = 'Splines functions for the first 365 days' ); <pre>/tmp/ipykernel_5424/2664573984.py:4: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n  splines_month.head(365).plot(\n</pre> In\u00a0[17]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_splines = pd.concat([data, splines_month], axis=1)\ndata_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month'])\ndata_encoded_splines.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_splines = pd.concat([data, splines_month], axis=1) data_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month']) data_encoded_splines.head(3) Out[17]: y spline0 spline1 spline2 spline3 spline4 spline5 spline6 spline7 spline8 spline9 spline10 spline11 date 2020-01-01 2.928244 0.166667 0.666667 0.166667 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 4.866145 0.150763 0.665604 0.183628 0.000006 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 4.425159 0.135904 0.662485 0.201563 0.000047 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[18]: Copied! <pre># Radial basis functions\n# ==============================================================================\nrbf = RepeatingBasisFunction(\n          n_periods   = 12,\n          remainder   = 'drop',\n          column      = 'day_of_year',\n          input_range = (1, 365)\n      )\nrbf.fit(data)\nrbf_month = rbf.transform(data)\nrbf_month = pd.DataFrame(\n                data    = rbf_month,\n                index   = data.index,\n                columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]\n            )\nrbf_month.head(3)\n</pre> # Radial basis functions # ============================================================================== rbf = RepeatingBasisFunction(           n_periods   = 12,           remainder   = 'drop',           column      = 'day_of_year',           input_range = (1, 365)       ) rbf.fit(data) rbf_month = rbf.transform(data) rbf_month = pd.DataFrame(                 data    = rbf_month,                 index   = data.index,                 columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]             ) rbf_month.head(3) Out[18]: rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[19]: Copied! <pre># Location of the maximum value of each rbf\n# ==============================================================================\nrbf_month.idxmax()\n</pre> # Location of the maximum value of each rbf # ============================================================================== rbf_month.idxmax() Out[19]: <pre>rbf_0    2020-01-01\nrbf_1    2020-01-31\nrbf_2    2020-03-02\nrbf_3    2020-04-01\nrbf_4    2020-05-01\nrbf_5    2020-06-01\nrbf_6    2020-07-01\nrbf_7    2020-07-31\nrbf_8    2020-08-31\nrbf_9    2020-09-30\nrbf_10   2020-10-30\nrbf_11   2020-11-30\ndtype: datetime64[ns]</pre> In\u00a0[20]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_rbf = pd.concat([data, rbf_month], axis=1)\ndata_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month'])\ndata_encoded_rbf.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_rbf = pd.concat([data, rbf_month], axis=1) data_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month']) data_encoded_rbf.head(3) Out[20]: y rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 2.928244 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 4.866145 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 4.425159 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[21]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [70]\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [70]              ) In\u00a0[22]: Copied! <pre># Train and validate a forecaster using each encoding method\n# ==============================================================================\ndatasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,\n            data_encoded_rbf]\nencoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',\n                    'rbf encoding']\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\n\nfor i, data_encoded in enumerate(datasets):\n\n    metric, predictions = backtesting_forecaster(\n                              forecaster         = forecaster,\n                              y                  = data_encoded['y'],\n                              exog               = data_encoded.drop(columns='y'),\n                              initial_train_size = len(data_encoded.loc[:end_train]),\n                              fixed_train_size   = False,\n                              steps              = 365,\n                              refit              = False,\n                              metric             = 'mean_squared_error',\n                              verbose            = False, # Change to True to see detailed information\n                              show_progress      = False\n                          )\n\n    print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")\n    predictions.plot(label=encoding_methods[i], ax=ax)\n    ax.legend(labels=['test'] + encoding_methods);\n</pre> # Train and validate a forecaster using each encoding method # ============================================================================== datasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,             data_encoded_rbf] encoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',                     'rbf encoding']  fig, ax = plt.subplots(figsize=(8, 4)) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)  for i, data_encoded in enumerate(datasets):      metric, predictions = backtesting_forecaster(                               forecaster         = forecaster,                               y                  = data_encoded['y'],                               exog               = data_encoded.drop(columns='y'),                               initial_train_size = len(data_encoded.loc[:end_train]),                               fixed_train_size   = False,                               steps              = 365,                               refit              = False,                               metric             = 'mean_squared_error',                               verbose            = False, # Change to True to see detailed information                               show_progress      = False                           )      print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")     predictions.plot(label=encoding_methods[i], ax=ax)     ax.legend(labels=['test'] + encoding_methods); <pre>Backtest error using one hot encoding: 1.10\nBacktest error using sine/cosine encoding: 1.12\nBacktest error using spline encoding: 0.75\nBacktest error using rbf encoding: 0.74\n</pre>"},{"location":"faq/cyclical-features-time-series.html#cyclical-features-in-time-series-forecasting","title":"Cyclical features in time series forecasting\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#data","title":"Data\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#one-hot-encoding","title":"One hot encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#cyclical-encoding-with-sinecosine-transformation","title":"Cyclical encoding with sine/cosine transformation\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#b-splines-functions","title":"B-splines functions\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#radial-basis-functions-rbf","title":"Radial basis functions (RBF)\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#compare-forecasting-results","title":"Compare forecasting results\u00b6","text":"<p>A non-informative lag is included so that the impact of cyclical features can be assessed without being obscured by the autoregressive component.</p>"},{"location":"faq/faq.html","title":"Frequently Asked Questions and forecasting tips","text":"<p>Thank you for choosing skforecast and visiting our Frequently Asked Questions (FAQ) page. Here, we aim to provide solutions to commonly encountered issues on our Github repository. If your question is not answered on this page, we encourage you to create a new issue on our Github so that it can be addressed, and other users can also benefit from it. Additionally, we have included some forecasting tips to help you get the most out of skforecast.</p>"},{"location":"faq/faq.html#general-forecasting-tips","title":"General Forecasting Tips","text":"<ul> <li>Avoid negative predictions when forecasting</li> <li>Forecasting time series with missing values</li> <li>Forecasting with delayed historical data</li> </ul>"},{"location":"faq/faq.html#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Time series differentiation (modelling trend)</li> <li>Cyclical features in time series</li> <li>Time series aggregation</li> </ul>"},{"location":"faq/faq.html#advanced-techniques","title":"Advanced Techniques","text":"<ul> <li>Stacking (ensemble) machine learning models to improve forecasting</li> </ul>"},{"location":"faq/faq.html#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Parallelization in skforecast</li> <li>Profiling skforecast</li> </ul>"},{"location":"faq/forecasting-time-series-with-missing-values.html","title":"Time series with missing values","text":"<p>In many real use cases of forecasting, although historical data are available, it is common for the time series to be incomplete. The presence of missing values in the data is a major problem since most forecasting algorithms require the time series to be complete in order to train a model.</p> <p>A commonly employed strategy to overcome this problem is to impute missing values before training the model, for example, using a moving average. However, the quality of the imputations may not be good, impairing the training of the model. One way to improve the imputation strategy is to combine it with weighted time series forecasting. The latter consists of reducing the weight of the imputed observations and thus their influence during model training.</p> <p>This document shows two examples of how skforecast makes it easy to apply this strategy.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.datasets import fetch_dataset <p> \u270e Note </p> <p>In this document, a forecaster of type <code>ForecasterAutoreg</code> is used. The same strategy can be applied with any forecaster from skforecast.</p> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n    'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d')\ndata = data[['fecha', 'Usos bicis total d\u00eda']]\ndata.columns = ['date', 'users']\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Data download # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'     'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d') data = data[['fecha', 'Usos bicis total d\u00eda']] data.columns = ['date', 'users'] data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head(3) Out[2]: users date 2014-06-23 99 2014-06-24 72 2014-06-25 119 In\u00a0[3]: Copied! <pre># Generating gaps with missing values\n# ==============================================================================\ngaps = [\n    ['2020-09-01', '2020-10-10'],\n    ['2020-11-08', '2020-12-15'],\n]\n\nfor gap in gaps:\n    data.loc[gap[0]:gap[1]] = np.nan\n</pre> # Generating gaps with missing values # ============================================================================== gaps = [     ['2020-09-01', '2020-10-10'],     ['2020-11-08', '2020-12-15'], ]  for gap in gaps:     data.loc[gap[0]:gap[1]] = np.nan In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2020-06-01': '2021-06-01']\nend_train = '2021-03-01'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2020-06-01': '2021-06-01'] end_train = '2021-03-01' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-06-01 00:00:00 --- 2021-03-01 00:00:00  (n=274)\nDates test  : 2021-03-01 00:00:00 --- 2021-06-01 00:00:00  (n=93)\n</pre> In\u00a0[5]: Copied! <pre># Time series plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(12, 4))\ndata_train.users.plot(ax=ax, label='train', linewidth=1)\ndata_test.users.plot(ax=ax, label='test', linewidth=1)\n\nfor gap in gaps:\n    ax.plot(\n        [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],\n        [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],\n         data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],\n        color = 'red',\n        linestyle = '--',\n        label = 'gap'\n    )\n\nax.set_title('Number of users BiciMAD')\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax.legend(by_label.values(), by_label.keys(), loc='lower right');\n</pre> # Time series plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(12, 4)) data_train.users.plot(ax=ax, label='train', linewidth=1) data_test.users.plot(ax=ax, label='test', linewidth=1)  for gap in gaps:     ax.plot(         [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],         [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],          data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],         color = 'red',         linestyle = '--',         label = 'gap'     )  ax.set_title('Number of users BiciMAD') handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) ax.legend(by_label.values(), by_label.keys(), loc='lower right'); In\u00a0[6]: Copied! <pre># Value imputation using linear interpolation\n# ======================================================================================\ndata['users_imputed'] = data['users'].interpolate(method='linear')\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n</pre> # Value imputation using linear interpolation # ====================================================================================== data['users_imputed'] = data['users'].interpolate(method='linear') data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :] <p>To minimize the influence on the model of imputed values, a custom function is defined to create weights following the rules:</p> <ul> <li><p>Weight of 0 if the index date has been imputed or is within 14 days ahead of an imputed day.</p> </li> <li><p>Weight of 1 otherwise.</p> </li> </ul> <p>If an observation has a weight of 0, it has no influence at all during model training.</p> <p> \u270e Note </p> <p>Imputed values should neither participate in the training process as a target nor as a predictor (lag). Therefore, values within a window size as large as the lags used should also be excluded.</p> In\u00a0[7]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is in any gap.\n    \"\"\"\n    gaps = [\n        ['2020-09-01', '2020-10-10'],\n        ['2020-11-08', '2020-12-15'],\n    ]\n    \n    missing_dates = [pd.date_range(\n                        start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),\n                        end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),\n                        freq  = 'D'\n                    ) for gap in gaps]\n    missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))   \n    weights = np.where(index.isin(missing_dates), 0, 1)\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is in any gap.     \"\"\"     gaps = [         ['2020-09-01', '2020-10-10'],         ['2020-11-08', '2020-12-15'],     ]          missing_dates = [pd.date_range(                         start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),                         end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),                         freq  = 'D'                     ) for gap in gaps]     missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))        weights = np.where(index.isin(missing_dates), 0, 1)      return weights <p><code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[8]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = LGBMRegressor(random_state=123, verbose=-1),\n                 lags        = 14,\n                 weight_func = custom_weights\n             )\n\n# Backtesting: predict the next 7 days at a time.\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                            forecaster         = forecaster,\n                            y                  = data.users_imputed,\n                            initial_train_size = len(data.loc[:end_train]),\n                            fixed_train_size   = False,\n                            steps              = 7,\n                            metric             = 'mean_absolute_error',\n                            refit              = True,\n                            verbose            = False,\n                            show_progress      = True\n                        )\n\nmetric\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = LGBMRegressor(random_state=123, verbose=-1),                  lags        = 14,                  weight_func = custom_weights              )  # Backtesting: predict the next 7 days at a time. # ============================================================================== metric, predictions = backtesting_forecaster(                             forecaster         = forecaster,                             y                  = data.users_imputed,                             initial_train_size = len(data.loc[:end_train]),                             fixed_train_size   = False,                             steps              = 7,                             metric             = 'mean_absolute_error',                             refit              = True,                             verbose            = False,                             show_progress      = True                         )  metric <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> Out[8]: mean_absolute_error 0 1904.830714 In\u00a0[9]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head(4)\n</pre> # Backtest predictions # ============================================================================== predictions.head(4) Out[9]: pred 2021-03-02 10524.159747 2021-03-03 10087.283682 2021-03-04 8882.926166 2021-03-05 9474.810215"},{"location":"faq/forecasting-time-series-with-missing-values.html#forecasting-time-series-with-missing-values","title":"Forecasting time series with missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#data","title":"Data\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#impute-missing-values","title":"Impute missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#give-weight-of-zero-to-imputed-values","title":"Give weight of zero to imputed values\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html","title":"Forecasting with delayed data","text":"<p>In the world of forecasting, accurate predictions depend on historical data. In many real-world scenarios, however, the available data is often subject to delays.  Consider the retail industry, where sales data often arrive with delays ranging from a few days to several weeks. Such delays pose significant challenges for autoregressive models, which use past values of the target variable as predictors.</p> <p>One of the primary obstacles when working with delayed data is accurately evaluating model performance. Incorporating the delay into the evaluation becomes critical, as models must be evaluated based on the data available at the time of prediction. Failure to do so can lead to overly optimistic results, as the model may be accessing data that wasn't available during the prediction period.</p> <p>One way to mitigate this challenge is to include lags that are greater than the maximum delay that the historical data can have. For example, if the data is delayed by 7 days, the minimum lag should be 7 days. This ensures that the model always has access to the data it needs to make predictions. However, this approach will not always achieve great results because the model may be using data that is too far in the past to be useful for prediction.</p> <p> Predictions with lags (last window available) greater than the maximum delay. </p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import HistGradientBoostingRegressor from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Download data and preprocessing\n# ==============================================================================\ndata = fetch_dataset(name='h2o', raw=True, verbose=False,\n                     kwargs_read_csv={'header': 0, 'names': ['y', 'datetime']})\n\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\nprint(f\"Length of time series: {len(data)}\")\nprint(f\"Frequency: {data.index.freqstr}\")\ndata.head(3)\n</pre> # Download data and preprocessing # ============================================================================== data = fetch_dataset(name='h2o', raw=True, verbose=False,                      kwargs_read_csv={'header': 0, 'names': ['y', 'datetime']})  data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  print(f\"Length of time series: {len(data)}\") print(f\"Frequency: {data.index.freqstr}\") data.head(3) <pre>Length of time series: 204\nFrequency: MS\n</pre> Out[2]: y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 In\u00a0[3]: Copied! <pre># Train-validation dates\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\ndata_train = data[:end_train].copy()\ndata_test  = data[end_train:].copy()\n\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\nax.legend()\nplt.show()\n</pre> # Train-validation dates # ============================================================================== end_train = '2005-06-01 23:59:59' data_train = data[:end_train].copy() data_test  = data[end_train:].copy()  print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:, 'y'].plot(ax=ax, label='test') ax.legend() plt.show() <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> <p>The data used in this example is a time series of monthly values. Let's assume that the data is delayed by 3 months. This means that the data for January will not be available until April, the data for February will not be available until May, and so on.</p> <p>Ideally, we would like to forecast the entire next year using the last 12 months of data, starting with the month immediately preceding the forecast (lags 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12). However, because the data are delayed by 3 months, it is not possible to use lags 1, 2, or 3 to predict the target variable because these data are not available at the time of the forecast. Therefore, the minimum lag must be 4.</p> In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [4, 5, 6, 7, 8, 9, 10, 11, 12] \n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [4, 5, 6, 7, 8, 9, 10, 11, 12]               )  forecaster.fit(y=data_train['y']) <p>Using a backtesting process, three years are forecast in batches of 12 months.</p> <p> \ud83d\udca1 Tip </p> <p>To a better understanding of backtesting process visit the Backtesting user guide.</p> In\u00a0[5]: Copied! <pre># Backtesting forecaster on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 12,\n                          metric                = 'mean_absolute_error',\n                          initial_train_size    = len(data_train),\n                          refit                 = False,\n                          n_jobs                = 'auto',\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n\nmetric\n</pre> # Backtesting forecaster on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 12,                           metric                = 'mean_absolute_error',                           initial_train_size    = len(data_train),                           refit                 = False,                           n_jobs                = 'auto',                           verbose               = True,                           show_progress         = True                         )  metric <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number skipped folds: 0 \n    Number of steps per fold: 12\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[5]: mean_absolute_error 0 0.065997 In\u00a0[6]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head(5)\n</pre> # Backtest predictions # ============================================================================== predictions.head(5) Out[6]: pred 2005-07-01 1.077316 2005-08-01 1.070779 2005-09-01 1.088995 2005-10-01 1.101434 2005-11-01 1.122724 In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:, 'y'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Once the model has been validated, taking into account the delay, it can be used in production. In this case, the model will use the data available at the time of the forecast, which will be data starting 3 months ago.</p> <p>The way the model identifies the data to use is by position index. For example, lag 4 is the value at position 4 from the end of the last available window. The forecaster assumes that the last window provided ends just before the first step to be predicted, but because of the delay, the most recent data available will not be the most recent data in the time series. To ensure that the lags are taken from the correct position, the last window must be extended with dummy values. The number of dummy values must be equal to the number of steps between the last available data and the date just before the first forecast step. In this case, the lag is 3 months, so the number of dummy values must be 3.</p> <p>Let's take a real example, first we train the model with all available data.</p> In\u00a0[8]: Copied! <pre># Create and fit forecaster (whole data)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [4, 5, 6, 7, 8, 9, 10, 11] \n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster (whole data) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [4, 5, 6, 7, 8, 9, 10, 11]               )  forecaster.fit(y=data['y']) In\u00a0[9]: Copied! <pre># Last window \n# ==============================================================================\nlast_window = forecaster.last_window\nlast_window\n</pre> # Last window  # ============================================================================== last_window = forecaster.last_window last_window Out[9]: y datetime 2007-08-01 1.078219 2007-09-01 1.110982 2007-10-01 1.109979 2007-11-01 1.163534 2007-12-01 1.176589 2008-01-01 1.219941 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>Our latest available data date is <code>2008-06-01</code> and, as we know we have a 3 months delay, this means that we are actually sometime in September (the time at which we want to make predictions) and our first predicted point will be <code>2008-10-01</code>.</p> <p>Since the forecaster expects the last window to end in <code>2008-09-01</code> and the last available data is the <code>2008-06-01</code> value, the last window must be extended by 3 dummy values.</p> In\u00a0[10]: Copied! <pre># Dummy values to complete the last_window until the moment of prediction\n# ==============================================================================\n# These dummy values are never used by the model because they are always posterior to the\n# smallest lag.\ndate_start_prediction = pd.to_datetime(\"2008-09-30\")\ndummy_value = np.inf\n\nlast_window_extended = last_window.reindex(\n    pd.date_range(start=last_window.index[0], end=date_start_prediction, freq='MS'),\n    fill_value = dummy_value\n)\n\nlast_window_extended\n</pre> # Dummy values to complete the last_window until the moment of prediction # ============================================================================== # These dummy values are never used by the model because they are always posterior to the # smallest lag. date_start_prediction = pd.to_datetime(\"2008-09-30\") dummy_value = np.inf  last_window_extended = last_window.reindex(     pd.date_range(start=last_window.index[0], end=date_start_prediction, freq='MS'),     fill_value = dummy_value )  last_window_extended Out[10]: y 2007-08-01 1.078219 2007-09-01 1.110982 2007-10-01 1.109979 2007-11-01 1.163534 2007-12-01 1.176589 2008-01-01 1.219941 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 2008-07-01 inf 2008-08-01 inf 2008-09-01 inf <p> \u26a0 Warning </p> <p>Dummy values are never used by the model because they are always posterior to the smallest lag.</p> In\u00a0[11]: Copied! <pre># Predictions\n# ==============================================================================\npredictions = forecaster.predict(steps=12, last_window=last_window_extended)\npredictions.head(3)\n</pre> # Predictions # ============================================================================== predictions = forecaster.predict(steps=12, last_window=last_window_extended) predictions.head(3) Out[11]: <pre>2008-10-01    1.107654\n2008-11-01    1.182545\n2008-12-01    1.153173\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Plot predictions\n# ==============================================================================\nlast_window_used = (last_window_extended.index[0], last_window_extended.index[-4]) \ndummy_values = (last_window_extended.index[-3], last_window_extended.index[-1]) \n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(\n    [dummy_values[0], dummy_values[1]],\n    [last_window.iloc[-1, 0], predictions.iloc[0]],\n    color = 'red',\n    linestyle = '--',\n    label = 'Gap (Dummy values)'\n)\nax.fill_between(last_window_used, data['y'].min(), data['y'].max(), \n                facecolor='#f7931a', alpha=0.4, zorder=0, label='Last window used')\ndata['y'].plot(ax=ax, label='train')\npredictions.plot(ax=ax, label='predictions')\nax.legend()\nplt.show();\n</pre> # Plot predictions # ============================================================================== last_window_used = (last_window_extended.index[0], last_window_extended.index[-4])  dummy_values = (last_window_extended.index[-3], last_window_extended.index[-1])   fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(     [dummy_values[0], dummy_values[1]],     [last_window.iloc[-1, 0], predictions.iloc[0]],     color = 'red',     linestyle = '--',     label = 'Gap (Dummy values)' ) ax.fill_between(last_window_used, data['y'].min(), data['y'].max(),                  facecolor='#f7931a', alpha=0.4, zorder=0, label='Last window used') data['y'].plot(ax=ax, label='train') predictions.plot(ax=ax, label='predictions') ax.legend() plt.show(); <p> \u270e Note </p> <p>Some forecasting models, such as ARIMA and SARIMAX, do not have as much flexibility in terms of changing the last window values. In these cases, forecasts must be made from the last available data to the desired forecast horizon. The forecast values for the delayed data may be discarded as they are already past values.</p> <p> \ud83d\udca1 Tip </p> <p>For a better understanding of how to deploy Forecaster models,  visit forecaster models in production.</p>"},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-with-delayed-historical-data","title":"Forecasting with delayed historical data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#data","title":"Data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-with-delayed-data","title":"Forecasting with delayed data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-in-production","title":"Forecasting in production\u00b6","text":""},{"location":"faq/non-negative-predictions.html","title":"Non negative predictions","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import GammaRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.linear_model import Ridge from sklearn.linear_model import GammaRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.metrics import mean_squared_error from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'\n       'learning-python/master/data/bike_sharing_dataset_clean.csv')\ndata = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000)\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('h')\ndata = data.sort_index()\n</pre> # Downloading data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'        'learning-python/master/data/bike_sharing_dataset_clean.csv') data = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000) data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('h') data = data.sort_index() In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2011-01-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2011-01-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train: 2011-01-01 00:00:00 --- 2011-01-31 23:00:00  (n=744)\nDates test : 2011-02-01 00:00:00 --- 2011-02-11 15:00:00  (n=256)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['users'].plot(ax=ax, label='train')\ndata_test['users'].plot(ax=ax, label='test')\nax.set_title('Number of users')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['users'].plot(ax=ax, label='train') data_test['users'].plot(ax=ax, label='test') ax.set_title('Number of users') ax.set_xlabel('') ax.legend(); In\u00a0[5]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24\n             )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 24              ) In\u00a0[6]: Copied! <pre># Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.set_xlabel('') ax.legend(); <p>The graph above shows that some predictions are negative.</p> In\u00a0[8]: Copied! <pre># Negative predictions\n# ==============================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ============================================================================== predictions[predictions.pred &lt; 0] Out[8]: pred 2011-02-01 00:00:00 -23.337236 2011-02-01 01:00:00 -17.691405 2011-02-02 00:00:00 -5.243456 2011-02-02 01:00:00 -19.363139 2011-02-03 01:00:00 -6.441943 2011-02-04 01:00:00 -10.579940 2011-02-05 00:00:00 -6.026119 2011-02-05 01:00:00 -21.396841 2011-02-07 04:00:00 -3.412043 2011-02-07 05:00:00 -3.701964 2011-02-08 00:00:00 -17.045913 2011-02-08 01:00:00 -13.233004 2011-02-09 00:00:00 -25.315228 2011-02-09 01:00:00 -24.743686 2011-02-10 01:00:00 -5.704407 2011-02-11 01:00:00 -11.758940 In\u00a0[9]: Copied! <pre># Transform data into a logarithmic scale\n# =============================================================================\ndata_log = np.log1p(data)\ndata_train_log = np.log1p(data_train)\ndata_test_log  = np.log1p(data_test)\n</pre> # Transform data into a logarithmic scale # ============================================================================= data_log = np.log1p(data) data_train_log = np.log1p(data_train) data_test_log  = np.log1p(data_test) In\u00a0[10]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\n# Since the data has been transformed outside the forecaster, the predictions\n# and metric are not in the original scale. They need to be transformed back.\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data_log['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train_log),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== # Since the data has been transformed outside the forecaster, the predictions # and metric are not in the original scale. They need to be transformed back. metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data_log['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train_log),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Revert the transformation\n# ==============================================================================\npredictions = np.expm1(predictions)\npredictions.head(4)\n</pre> # Revert the transformation # ============================================================================== predictions = np.expm1(predictions) predictions.head(4) Out[11]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 In\u00a0[12]: Copied! <pre># Backtesting metric (test data)\n# ==============================================================================\n# The error metric is calculated once the transformation is reversed.\nmetric = mean_squared_error(y_true=data_test['users'], y_pred=predictions)\nprint(f\"Backtesting metric (test data): {metric}\")\n</pre> # Backtesting metric (test data) # ============================================================================== # The error metric is calculated once the transformation is reversed. metric = mean_squared_error(y_true=data_test['users'], y_pred=predictions) print(f\"Backtesting metric (test data): {metric}\") <pre>Backtesting metric (test data): 1991.9332571759883\n</pre> In\u00a0[13]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation. If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[14]: Copied! <pre># Create a custom transformer\n# =============================================================================\ntransformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n\n# Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 24,\n                 transformer_y    = transformer_y,\n                 transformer_exog = None\n             )\nforecaster.fit(data['users'])\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\n# Since the transformation is included in the forecaster, predictions are\n# automatically transformed back into the original scale. And the metric is\n# calculated in the original scale.\nmetric\n</pre> # Create a custom transformer # ============================================================================= transformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)  # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 24,                  transformer_y    = transformer_y,                  transformer_exog = None              ) forecaster.fit(data['users'])  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  # Since the transformation is included in the forecaster, predictions are # automatically transformed back into the original scale. And the metric is # calculated in the original scale. metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[14]: mean_squared_error 0 1991.933257 In\u00a0[15]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head()\n</pre> # Backtest predictions # ============================================================================== predictions.head() Out[15]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 2011-02-01 04:00:00 3.865169 <p>The same results are obtained as if the data were log-transformed outside the model. However, by including the log transformation in the model, all inverse transformations are handled automatically.</p> <p>A forecaster with a linear model (scikit learn GammaRegressor) that uses a different link function is evaluated.</p> In\u00a0[16]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = GammaRegressor(alpha=1, max_iter=100000),\n                 lags      = 20,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\nmetric\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = GammaRegressor(alpha=1, max_iter=100000),                  lags      = 20,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[16]: mean_squared_error 0 3509.672762 In\u00a0[17]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head()\n</pre> # Backtest predictions # ============================================================================== predictions.head() Out[17]: pred 2011-02-01 00:00:00 6.685814 2011-02-01 01:00:00 10.238382 2011-02-01 02:00:00 11.950783 2011-02-01 03:00:00 9.194086 2011-02-01 04:00:00 6.372470 <p>In this case the gamma regressor model performed poorly, but if we look to the negative results:</p> In\u00a0[18]: Copied! <pre># Negative predictions\n# ======================================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ====================================================================================== predictions[predictions.pred &lt; 0] Out[18]: pred In\u00a0[19]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Now a XGBoost model is trained, but with the objective function set to <code>reg:gamma</code>, so that the output is a mean of the gamma distribution, which is defined for positive values only.</p> In\u00a0[20]: Copied! <pre># Create forecaster and train\n# ==============================================================================\nparams = {\n    'tree_method': 'hist',\n    'objective': 'reg:gamma'\n}\n\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(**params),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\nmetric\n</pre> # Create forecaster and train # ============================================================================== params = {     'tree_method': 'hist',     'objective': 'reg:gamma' }  forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(**params),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[20]: mean_squared_error 0 1915.458228 In\u00a0[21]: Copied! <pre># Negative predictions\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions predictions[predictions.pred &lt; 0] Out[21]: pred In\u00a0[22]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>The forecaster's performance is better than using a Ridge regressor with a log-transformer, and no negative predictions are made.</p>"},{"location":"faq/non-negative-predictions.html#avoid-negative-predictions-in-forecasting","title":"Avoid Negative Predictions in Forecasting\u00b6","text":"<p>When training a forecasting model, even if none of the training observations are negative, some predictions may turn out to be negative. This is quite common when trying to predict attendance, sales, or rainfall, among other things. To avoid this, it is possible to use the following approaches:</p> <ul> <li><p>Use a log+K transformation to always have positive values in the transformed time series. K is a positive integer to avoid the undefinability of the $log(x)$ function at 0. Despite the simplicity of the approach, one must be aware that some caveats may arise depending on the metric we use to optimize our models.</p> </li> <li><p>Use a different link function in the models. The link function provides the relationship between the linear predictor and the expected value of the response variable. When using machine learning models, several algorithms support different objective functions to account for this situation. For example, when predicting the number of visitors, one can use <code>count:poisson</code> in XGBoost or LightGBM as well as gamma regression if the target is always strictly positive.</p> </li> </ul> <p>The following tutorial will explore both possibilities.</p>"},{"location":"faq/non-negative-predictions.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/non-negative-predictions.html#data","title":"Data\u00b6","text":""},{"location":"faq/non-negative-predictions.html#forecaster","title":"Forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#modeling-time-series-in-logarithmic-scale","title":"Modeling time series in logarithmic scale\u00b6","text":""},{"location":"faq/non-negative-predictions.html#include-a-logarithmic-transformer-as-part-of-the-forecaster","title":"Include a logarithmic transformer as part of the forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-link-functions","title":"Usage of link functions\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-xgboost-objective-functions","title":"Usage of XGBoost objective functions\u00b6","text":""},{"location":"faq/parallelization-skforecast.html","title":"Parallelization","text":"<p> \u26a0 Warning </p> <p>The automatic selection of the parallelization level relies on heuristics and is therefore not guaranteed to be optimal. In addition, it is important to keep in mind that many regressors already parallelize their fitting procedures inherently. As a result, introducing additional parallelization may not necessarily improve overall performance. For a more detailed look at parallelization, visit select_n_jobs_backtesting and select_n_jobs_fit_forecaster.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport platform\nimport psutil\nimport skforecast\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport sklearn\nimport time\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multivariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multivariate\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n</pre> # Libraries # ============================================================================== import platform import psutil import skforecast import pandas as pd import numpy as np import scipy import sklearn import time import warnings import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from lightgbm import LGBMRegressor from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multivariate from skforecast.model_selection_multiseries import backtesting_forecaster_multivariate from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate In\u00a0[7]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"scipy version       : {scipy.__version__}\")\nprint(f\"psutil version      : {psutil.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Machine type: {platform.machine()}\")\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"scipy version       : {scipy.__version__}\") print(f\"psutil version      : {psutil.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Machine type: {platform.machine()}\") print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.11.4\nscikit-learn version: 1.3.0\nskforecast version  : 0.9.1\npandas version      : 2.0.3\nnumpy version       : 1.25.2\nscipy version       : 1.11.1\npsutil version      : 5.9.5\n\nComputer network name: EU-HYYV0J3\nMachine type: AMD64\nProcessor type: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\nPlatform type: Windows-10-10.0.19045-SP0\nOperating system: Windows\nOperating system release: 10\nOperating system version: 10.0.19045\nNumber of physical cores: 4\nNumber of logical cores: 8\n</pre> In\u00a0[8]: Copied! <pre># Data\n# ==============================================================================\nn = 5_000\nrgn = np.random.default_rng(seed=123)\ny = pd.Series(rgn.random(size=(n)), name=\"y\")\nexog = pd.DataFrame(rgn.random(size=(n, 10)))\nexog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])]\nmulti_series = pd.DataFrame(rgn.random(size=(n, 10)))\nmulti_series.columns = [f\"series_{i+1}\" for i in range(multi_series.shape[1])]\ny_train = y[:-int(n/2)]\ndisplay(y.head())\ndisplay(exog.head())   \ndisplay(multi_series.head())\n</pre> # Data # ============================================================================== n = 5_000 rgn = np.random.default_rng(seed=123) y = pd.Series(rgn.random(size=(n)), name=\"y\") exog = pd.DataFrame(rgn.random(size=(n, 10))) exog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])] multi_series = pd.DataFrame(rgn.random(size=(n, 10))) multi_series.columns = [f\"series_{i+1}\" for i in range(multi_series.shape[1])] y_train = y[:-int(n/2)] display(y.head()) display(exog.head())    display(multi_series.head()) <pre>0    0.682352\n1    0.053821\n2    0.220360\n3    0.184372\n4    0.175906\nName: y, dtype: float64</pre> exog_0 exog_1 exog_2 exog_3 exog_4 exog_5 exog_6 exog_7 exog_8 exog_9 0 0.593121 0.353471 0.336277 0.399734 0.915459 0.822278 0.480418 0.929802 0.950948 0.863556 1 0.764104 0.638191 0.956624 0.178105 0.434077 0.137480 0.837667 0.768947 0.244235 0.815336 2 0.475312 0.312415 0.353596 0.272162 0.772064 0.110216 0.596551 0.688549 0.651380 0.191837 3 0.039253 0.962713 0.189194 0.910629 0.169796 0.697751 0.830913 0.484824 0.634634 0.862865 4 0.872447 0.861421 0.394829 0.877763 0.286779 0.131008 0.450185 0.898167 0.590147 0.045838 series_1 series_2 series_3 series_4 series_5 series_6 series_7 series_8 series_9 series_10 0 0.967448 0.580646 0.643348 0.461737 0.450859 0.894496 0.037967 0.097698 0.094356 0.893528 1 0.207450 0.194904 0.377063 0.975065 0.351034 0.812253 0.265956 0.262733 0.784995 0.674256 2 0.520431 0.985069 0.039559 0.541797 0.612761 0.640336 0.823467 0.768387 0.561777 0.600835 3 0.866694 0.165510 0.819767 0.691179 0.717778 0.392694 0.094067 0.271990 0.467866 0.041054 4 0.406310 0.657688 0.630730 0.694424 0.943934 0.888538 0.470363 0.518283 0.719674 0.010789 In\u00a0[9]: Copied! <pre>warnings.filterwarnings(\"ignore\")\n\nprint(\"-----------------\")\nprint(\"ForecasterAutoreg\")\nprint(\"-----------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoreg(\n                        regressor=regressor,\n                        lags=lags,\n                        transformer_exog=StandardScaler()\n                )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoreg, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\")\n</pre> warnings.filterwarnings(\"ignore\")  print(\"-----------------\") print(\"ForecasterAutoreg\") print(\"-----------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoreg(                         regressor=regressor,                         lags=lags,                         transformer_exog=StandardScaler()                 )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                 forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoreg, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\") <pre>-----------------\nForecasterAutoreg\n-----------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 9.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 0.571370 0.416455 27.112945 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 6.569071 2.720030 58.593380 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.007874 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.277259 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 6.699500 6.123097 8.603664 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 0.751951 0.546975 27.259220 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 7.001708 2.424567 65.371771 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.008184 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 0.275215 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 6.601839 7.825191 -18.530468 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.744464 0.378917 49.102068 11 Ridge(alpha=0.1, random_state=77) backtest_refit 1.571386 12.706505 -708.617632 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.011935 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.024534 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 4.548398 2.756721 39.391399 Out[9]: <pre>Text(0.5, 0, 'Method')</pre> In\u00a0[10]: Copied! <pre>print(\"-----------------------\")\nprint(\"ForecasterAutoregDirect\")\nprint(\"-----------------------\")\nsteps = 25\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregDirect(\n                    regressor=regressor,\n                    lags=lags,\n                    steps=steps,\n                    transformer_exog=StandardScaler()\n                )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n\n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregDirect, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"-----------------------\") print(\"ForecasterAutoregDirect\") print(\"-----------------------\") steps = 25 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregDirect(                     regressor=regressor,                     lags=lags,                     steps=steps,                     transformer_exog=StandardScaler()                 )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                 forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)           print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregDirect, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\"); <pre>-----------------------\nForecasterAutoregDirect\n-----------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 9.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 5.992304 10.283535 -71.612377 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 141.952965 75.919333 46.517966 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.072996 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 4.144056 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 43.838513 76.157306 -73.722375 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 5.886309 10.738465 -82.431196 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 143.764385 64.597212 55.067305 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.060571 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 4.122978 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 42.607671 75.015008 -76.059866 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.323168 0.319153 1.242526 11 Ridge(alpha=0.1, random_state=77) backtest_refit 5.785780 2.094785 63.794244 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.033940 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.430777 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 4.431143 3.941903 11.040943 In\u00a0[11]: Copied! <pre>print(\"----------------------------\")\nprint(\"ForecasterAutoregMultiseries\")\nprint(\"----------------------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregMultiSeries(\n        regressor=regressor,\n        lags=lags,\n        transformer_exog=StandardScaler()\n    )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit and parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultiseries, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"----------------------------\") print(\"ForecasterAutoregMultiseries\") print(\"----------------------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregMultiSeries(         regressor=regressor,         lags=lags,         transformer_exog=StandardScaler()     )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit and parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultiseries, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\"); <pre>----------------------------\nForecasterAutoregMultiseries\n----------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n9 models compared for 10 level(s). Number of iterations: 9.\nProfiling GridSearch no refit no parallel\n9 models compared for 10 level(s). Number of iterations: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 10 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 10 level(s). Number of iterations: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 10 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 10 level(s). Number of iterations: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 3.856087 1.785516 53.696183 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 26.799302 14.820522 44.698105 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.082853 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 1.439756 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 42.083872 29.374772 30.199456 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 5.492170 2.099951 61.764638 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 30.017390 14.576546 51.439661 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.079670 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 1.465376 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 72.783301 36.314506 50.105992 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 3.122103 1.041682 66.635227 11 Ridge(alpha=0.1, random_state=77) backtest_refit 6.525677 2.931308 55.080398 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.119956 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.277841 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 26.023329 13.008654 50.011569 In\u00a0[12]: Copied! <pre>print(\"-----------------------------\")\nprint(\"ForecasterAutoregMultivariate\")\nprint(\"-----------------------------\")\nsteps = 25\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregMultiVariate(\n        regressor=regressor,\n        lags=lags,\n        steps=steps,\n        level=\"series_1\",\n        transformer_exog=StandardScaler()\n    )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultivariate, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"-----------------------------\") print(\"ForecasterAutoregMultivariate\") print(\"-----------------------------\") steps = 25 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregMultiVariate(         regressor=regressor,         lags=lags,         steps=steps,         level=\"series_1\",         transformer_exog=StandardScaler()     )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)  methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultivariate, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\");  <pre>-----------------------------\nForecasterAutoregMultivariate\n-----------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n9 models compared for 1 level(s). Number of iterations: 9.\nProfiling GridSearch no refit no parallel\n9 models compared for 1 level(s). Number of iterations: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 1 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 1 level(s). Number of iterations: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 1 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 1 level(s). Number of iterations: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 34.915484 41.451426 -18.719322 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 773.195707 615.900236 20.343552 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.163573 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 34.095271 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 315.079898 407.191276 -29.234292 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 30.237485 29.182785 3.488057 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 536.445092 462.887896 13.711971 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.110270 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 24.590466 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 313.850108 366.675623 -16.831447 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 2.171713 2.316724 -6.677227 11 Ridge(alpha=0.1, random_state=77) backtest_refit 80.197614 44.966058 43.930928 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.228073 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 5.533158 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 19.418475 21.956889 -13.072160"},{"location":"faq/parallelization-skforecast.html#parallelization-in-skforecast","title":"Parallelization in skforecast\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#parallelization-rules","title":"Parallelization rules\u00b6","text":"<p>The <code>n_jobs</code> argument facilitates the parallelization of specific functionalities to enhance speed within the skforecast library. Parallelization has been strategically integrated at two key levels: during the process of forecaster fitting and during the backtesting phase, which also encompasses hyperparameter search. When the <code>n_jobs</code> argument is set to its default value of <code>auto</code>, the library dynamically determines the number of jobs to employ, guided by the ensuing guidelines:</p> <p>Forecaster Fitting</p> <ul> <li><p>If the forecaster is either <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code>, and the underlying regressor happens to be a linear regressor, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>Otherwise, if none of the above conditions hold, the <code>n_jobs</code> value is determined as <code>cpu_count()</code>, aligning with the number of available CPU cores.</p> </li> </ul> <p>Backtesting</p> <ul> <li><p>If <code>refit</code> is an integer, then <code>n_jobs=1</code>. This is because parallelization doesn`t work with intermittent refit.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code> and the underlying regressor is linear, <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code>,  the underlying regressor regressor is not a linear regressor and <code>refit=True</code>, then <code>n_jobs</code> is set to  <code>cpu_count()</code>.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code>, the underlying regressor is not linear and <code>refit=False</code>, n_jobs is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code> and refit=<code>True</code>, then <code>n_jobs</code> is set to <code>cpu_count()</code>.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code> and refit=<code>False</code>, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregMultiseries</code>, then <code>n_jobs</code> is set to <code>cpu_count()</code>.</p> </li> </ul>"},{"location":"faq/parallelization-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoreg","title":"Benchmark ForecasterAutoreg\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoreg","title":"Benchmark ForecasterAutoreg\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoregmultiseries","title":"Benchmark ForecasterAutoregMultiSeries\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoregmultivariate","title":"Benchmark ForecasterAutoregMultivariate\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html","title":"Backtesting vs One-step-ahead","text":"<p>Hyperparameter and lag tuning involves systematically testing different values or combinations of hyperparameters (and/or lags) to find the optimal configuration that gives the best performance. The skforecast library provides two different methods to evaluate each candidate configuration:</p> <ul> <li><p>Backtesting: In this method, the model predicts several steps ahead in each iteration, using the same forecast horizon and retraining frequency strategy that would be used if the model were deployed. This simulates a real forecasting scenario where the model is retrained and updated over time.</p> </li> <li><p>One-Step Ahead: Evaluates the model using only one-step-ahead predictions. This method is faster because it requires fewer iterations, but it only tests the model's performance in the immediate next time step.</p> </li> </ul> <p>Each method uses a different evaluation strategy, so they may produce different results. However, in the long run, both methods are expected to converge to similar selections of optimal hyperparameters. The one-step-ahead method is much faster than backtesting because it requires fewer iterations, but it only tests the model's performance in the immediate next time step. It is recommended to backtest the final model for a more accurate multi-step performance estimate.</p> <p>The document compares the performance of these two methods when applied to various datasets and forecaster types. The process is outlined as follows:</p> <ul> <li><p>Optimal hyperparameters and lags are identified through a search using both backtesting and one-step-ahead evaluation methods. This search is performed on the validation partition, and the best configuration is stored along with the time taken to complete the search.</p> </li> <li><p>Finally, the selected best configuration is evaluated on the test partition using a backtesting procedure.</p> </li> </ul> <p>It is important to note that the final evaluation is consistently performed using backtesting to simulate a real-world multi-step forecasting scenario.</p> <p>The results show a significant reduction in the time required to find the optimal configuration using the one-step-ahead method (top panel). However, the performance of the selected configuration on the test partition is similar for both methods (lower panel), with no clear winner. These results are consistent for both grid search and Bayesian search approaches.</p> <p> \u270e Note </p> <p>The purpose of this analysis is to compare the time and forecasting performance of the two available evaluation methods, not to compare different forecasters.</p> In\u00a0[2]: Copied! <pre># Libraries\n# ==============================================================================\nimport platform\nimport psutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom copy import copy\nimport sklearn\nimport skforecast\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import bayesian_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\nfrom skforecast.preprocessing import series_long_to_dict\nfrom skforecast.preprocessing import exog_long_to_dict\n\n# Warnings\n# ==============================================================================\nimport warnings\nfrom skforecast.exceptions import IgnoredArgumentWarning\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n</pre> # Libraries # ============================================================================== import platform import psutil import numpy as np import pandas as pd import matplotlib.pyplot as plt from time import time from copy import copy import sklearn import skforecast import lightgbm from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from skforecast.datasets import fetch_dataset from skforecast.plot import set_dark_theme from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import bayesian_search_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries from skforecast.preprocessing import series_long_to_dict from skforecast.preprocessing import exog_long_to_dict  # Warnings # ============================================================================== import warnings from skforecast.exceptions import IgnoredArgumentWarning warnings.filterwarnings('ignore') warnings.simplefilter('ignore', category=IgnoredArgumentWarning) In\u00a0[3]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"lightgbm version    : {lightgbm.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"psutil version      : {psutil.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Machine type: {platform.machine()}\")\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"lightgbm version    : {lightgbm.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"psutil version      : {psutil.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Machine type: {platform.machine()}\") print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.12.4\nscikit-learn version: 1.5.1\nskforecast version  : 0.14.0\nlightgbm version    : 4.4.0\npandas version      : 2.2.2\nnumpy version       : 1.26.4\npsutil version      : 5.9.0\n\nMachine type: x86_64\nProcessor type: x86_64\nPlatform type: Linux-5.15.0-1070-aws-x86_64-with-glibc2.31\nOperating system: Linux\nOperating system release: 5.15.0-1070-aws\nOperating system version: #76~20.04.1-Ubuntu SMP Mon Sep 2 12:20:36 UTC 2024\nNumber of physical cores: 4\nNumber of logical cores: 8\n</pre> In\u00a0[4]: Copied! <pre># Import data\n# ==============================================================================\ndata_bike = fetch_dataset('bike_sharing_extended_features', verbose=False)\ndata_sales = fetch_dataset(name=\"items_sales\", verbose=False)\ndata_sales = data_sales * 100\ndata_sales['day_of_week'] = data_sales.index.dayofweek\n\ndata_website = fetch_dataset(name=\"website_visits\", raw=True, verbose=False)\ndata_website['date'] = pd.to_datetime(data_website['date'], format='%d/%m/%y')\ndata_website = data_website.set_index('date')\ndata_website = data_website.asfreq('1D')\ndata_website = data_website.sort_index()\ndata_website['month'] = data_website.index.month\ndata_website['month_day'] = data_website.index.day\ndata_website['week_day'] = data_website.index.day_of_week\ndata_website = pd.get_dummies(data_website, columns=['month', 'week_day', 'month_day'], dtype='int64')\n\ndata_electricity = fetch_dataset(name='vic_electricity', raw=False, verbose=False)\ndata_electricity = data_electricity.drop(columns=\"Date\")\ndata_electricity = (\n    data_electricity\n    .resample(rule=\"h\", closed=\"left\", label=\"right\")\n    .agg({\n        \"Demand\": \"mean\",\n        \"Temperature\": \"mean\",\n        \"Holiday\": \"mean\",\n    })\n)\ndata_electricity = data_electricity.loc['2012-01-01 00:00:00': '2013-12-30 23:00:00'].copy()\n\n\nseries_dict = pd.read_csv(\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series.csv'\n)\nexog_dict = pd.read_csv(\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series_exog.csv'\n)\nseries_dict ['timestamp'] = pd.to_datetime(series_dict ['timestamp'])\nexog_dict['timestamp'] = pd.to_datetime(exog_dict['timestamp'])\nseries_dict = series_long_to_dict(\n    data      = series_dict,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    values    = 'value',\n    freq      = 'D'\n)\nexog_dict = exog_long_to_dict(\n    data      = exog_dict,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    freq      = 'D'\n)\n</pre> # Import data # ============================================================================== data_bike = fetch_dataset('bike_sharing_extended_features', verbose=False) data_sales = fetch_dataset(name=\"items_sales\", verbose=False) data_sales = data_sales * 100 data_sales['day_of_week'] = data_sales.index.dayofweek  data_website = fetch_dataset(name=\"website_visits\", raw=True, verbose=False) data_website['date'] = pd.to_datetime(data_website['date'], format='%d/%m/%y') data_website = data_website.set_index('date') data_website = data_website.asfreq('1D') data_website = data_website.sort_index() data_website['month'] = data_website.index.month data_website['month_day'] = data_website.index.day data_website['week_day'] = data_website.index.day_of_week data_website = pd.get_dummies(data_website, columns=['month', 'week_day', 'month_day'], dtype='int64')  data_electricity = fetch_dataset(name='vic_electricity', raw=False, verbose=False) data_electricity = data_electricity.drop(columns=\"Date\") data_electricity = (     data_electricity     .resample(rule=\"h\", closed=\"left\", label=\"right\")     .agg({         \"Demand\": \"mean\",         \"Temperature\": \"mean\",         \"Holiday\": \"mean\",     }) ) data_electricity = data_electricity.loc['2012-01-01 00:00:00': '2013-12-30 23:00:00'].copy()   series_dict = pd.read_csv(     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series.csv' ) exog_dict = pd.read_csv(     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series_exog.csv' ) series_dict ['timestamp'] = pd.to_datetime(series_dict ['timestamp']) exog_dict['timestamp'] = pd.to_datetime(exog_dict['timestamp']) series_dict = series_long_to_dict(     data      = series_dict,     series_id = 'series_id',     index     = 'timestamp',     values    = 'value',     freq      = 'D' ) exog_dict = exog_long_to_dict(     data      = exog_dict,     series_id = 'series_id',     index     = 'timestamp',     freq      = 'D' ) In\u00a0[5]: Copied! <pre># Functions to compare results using backtesting and one step ahead\n# ==============================================================================\ndef run_benchmark(\n    data,\n    forecaster_to_benchmark,\n    search_method = None,\n    lags_grid = None,\n    param_grid = None,\n    search_space = None,\n    end_train = None,\n    end_validation = None,\n    target = None,\n    exog_features = None,\n    steps = None,\n    metric = None\n):\n    \"\"\"\n    Compare results of grid search and bayesian search using backtesting and one-step-ahead.\n    \"\"\"\n    \n    # backtesting\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    if search_method == 'grid_search':\n        results_1 = grid_search_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.loc[:end_validation, target],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        steps              = steps,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False\n                    )\n    else:\n        results_1, _ = bayesian_search_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.loc[:end_validation, target],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        search_space       = search_space,\n                        steps              = steps,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        n_trials           = 15,\n                        random_state       = 123,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False\n                    )\n\n    end = time()\n    time_1 = end - start\n    best_params = results_1.loc[0, 'params']\n    best_lags = results_1.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    metric_1, pred_1 = backtesting_forecaster(\n                        forecaster          = forecaster,\n                        y                   = data.loc[:, target],\n                        exog                = data.loc[:, exog_features] if exog_features else None,\n                        initial_train_size  = len(data.loc[:end_validation]),\n                        steps               = steps,\n                        metric              = metric,\n                        verbose             = False,\n                        show_progress       = False\n                        )\n\n    # One step ahead\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    if search_method == 'grid_search':\n        results_2 = grid_search_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.loc[:end_validation, target],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'one_step_ahead',\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False\n                    )\n    else:\n        results_2, _ = bayesian_search_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.loc[:end_validation, target],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        steps              = steps,\n                        search_space       = search_space,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'one_step_ahead',\n                        n_trials           = 15,\n                        random_state       = 123,\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False\n                    )\n\n    end = time()\n    time_2 = end - start\n    best_params = results_2.loc[0, 'params']\n    best_lags = results_2.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    metric_2, pred_2 = backtesting_forecaster(\n                        forecaster          = forecaster,\n                        y                   = data.loc[:, target],\n                        exog                = data.loc[:, exog_features] if exog_features else None,\n                        initial_train_size  = len(data.loc[:end_validation]),\n                        steps               = steps,\n                        metric              = metric,\n                        verbose             = False,\n                        show_progress       = False\n                        )\n\n    print(\"-----------------\")\n    print(\"Benchmark results\")\n    print(\"-----------------\")\n    print('Execution time backtesting   :', time_1)\n    print('Execution time one step ahead:', time_2)\n    print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\n    print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\n    print(\"\")\n    print(\"Method: backtesting\")\n    print(f\"    lags   : {results_1.loc[0, 'lags']}\")\n    print(f\"    params : {results_1.loc[0, 'params']}\")\n    print(f\"    {metric}: {metric_1.loc[0, metric]}\")\n    print(\"\")\n    print(\"Method: one step ahead\")\n    print(f\"    lags   : {results_2.loc[0, 'lags']}\")\n    print(f\"    params : {results_2.loc[0, 'params']}\")\n    print(f\"    {metric}: {metric_2.loc[0, metric]}\")\n    \n    return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]\n\n\n# Functions to compare results using backtesting and one step ahead\n# ==============================================================================\ndef run_benchmark_multiseries(\n    data = None,\n    forecaster_to_benchmark = None,\n    search_method = None,\n    lags_grid = None,\n    param_grid = None,\n    search_space = None,\n    end_train = None,\n    end_validation = None,\n    levels = None,\n    exog_features = None,\n    steps = None,\n    metric = None\n):\n    \"\"\"\n    Compare results of grid search using backtesting and one-step-ahead.\n    \"\"\"\n    \n    # Backtesting\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    if search_method == 'grid_search':\n        results_1 = grid_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = data.loc[:end_validation, levels],\n                        levels             = levels,\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        steps              = steps,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False\n                    )\n    else:\n        results_1, _ = bayesian_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = data.loc[:end_validation, levels],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        levels             = levels,\n                        search_space       = search_space,\n                        steps              = steps,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        n_trials           = 15,\n                        random_state       = 123,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False\n                    )\n    end = time()\n    time_1 = end - start\n    best_params = results_1.loc[0, 'params']\n    best_lags = results_1.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    metric_1, pred_1 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = data.loc[:, levels],\n                            exog                = data.loc[:, exog_features] if exog_features else None,\n                            initial_train_size  = len(data.loc[:end_validation]),\n                            levels              = levels,\n                            steps               = steps,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False\n                        )\n\n    # One step ahead\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    if search_method == 'grid_search':\n        results_2 = grid_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = data.loc[:end_validation, levels],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        levels             = levels,\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'one_step_ahead',\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False\n                    )\n    else:\n        results_2, _ = bayesian_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = data.loc[:end_validation, levels],\n                        exog               = data.loc[:end_validation, exog_features] if exog_features else None,\n                        levels             = levels,\n                        steps              = steps,\n                        search_space       = search_space,\n                        metric             = metric,\n                        initial_train_size = len(data.loc[:end_train]),\n                        method             = 'one_step_ahead',\n                        n_trials           = 15,\n                        random_state       = 123,\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False\n                    )\n\n    end = time()\n    time_2 = end - start\n    best_params = results_2.loc[0, 'params']\n    best_lags = results_2.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    metric_2, pred_2 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = data.loc[:, levels],\n                            exog                = data.loc[:, exog_features] if exog_features else None,\n                            levels              = levels,\n                            initial_train_size  = len(data.loc[:end_validation]),\n                            steps               = steps,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False\n                        )\n\n    print(\"Benchmark results\")\n    print(\"-----------------\")\n    print('Execution time backtesting   :', time_1)\n    print('Execution time one step ahead:', time_2)\n    print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\n    print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\n    print(\"\")\n    print(\"Method: backtesting\")\n    print(f\"    lags   : {results_1.loc[0, 'lags']}\")\n    print(f\"    params : {results_1.loc[0, 'params']}\")\n    print(f\"    {metric_1.loc[0, metric]}\")\n    print(\"\")\n    print(\"Method: one step ahead\")\n    print(f\"    lags   : {results_2.loc[0, 'lags']}\")\n    print(f\"    params : {results_2.loc[0, 'params']}\")\n    print(f\"    {metric_2.loc[0, metric]}\")\n    \n    return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]\n\n\ndef summarize_results(results, metric, title, plot=True, save_plot=None, fig_size=(8, 4)):\n    \"\"\"\n    Summarize results of benchmark.\n    \"\"\"\n\n    results = pd.DataFrame(\n        results,\n        columns=[\n            \"dataset\",\n            \"forecaster\",\n            \"time_search_backtesting\",\n            \"time_search_one_step\",\n            \"metric_backtesting\",\n            \"metric_one_step\",\n        ]\n    )\n    results['ratio_speed'] = (\n        results['time_search_backtesting'] / results['time_search_one_step']\n    ).round(2)\n    results['ratio_metric'] = (\n        results['metric_backtesting'] / results['metric_one_step']\n    ).round(2)\n    results[\"dataset_forecaster\"] = (\n        results[\"dataset\"]\n        + \" \\n \"\n        + results[\"forecaster\"].str.replace(\"Forecaster\", \"\")\n    )\n    display(results)\n\n    if plot:\n        set_dark_theme()\n        fig, axs = plt.subplots(2, 1, figsize=fig_size, sharex=True)\n        results.plot.bar(\n            x='dataset_forecaster',\n            y=['time_search_backtesting', 'time_search_one_step'],\n            ax=axs[0],\n        )\n        axs[0].set_ylabel('time (s)')\n        axs[0].legend([\"backtesting\", \"one-step-ahead\"])\n        results.plot.bar(\n            x='dataset_forecaster',\n            y=['metric_backtesting', 'metric_one_step'],\n            ax=axs[1],\n            legend=False\n        )\n        axs[1].set_ylabel(f'{metric}')\n        axs[1].set_xlabel('')\n        plt.xticks(rotation=90)\n        plt.suptitle(title)\n        plt.tight_layout()\n\n        if save_plot:\n            plt.savefig(save_plot, dpi=300, bbox_inches='tight')\n</pre> # Functions to compare results using backtesting and one step ahead # ============================================================================== def run_benchmark(     data,     forecaster_to_benchmark,     search_method = None,     lags_grid = None,     param_grid = None,     search_space = None,     end_train = None,     end_validation = None,     target = None,     exog_features = None,     steps = None,     metric = None ):     \"\"\"     Compare results of grid search and bayesian search using backtesting and one-step-ahead.     \"\"\"          # backtesting     forecaster = copy(forecaster_to_benchmark)     start  = time()     if search_method == 'grid_search':         results_1 = grid_search_forecaster(                         forecaster         = forecaster,                         y                  = data.loc[:end_validation, target],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         steps              = steps,                         refit              = False,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'backtesting',                         fixed_train_size   = False,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False                     )     else:         results_1, _ = bayesian_search_forecaster(                         forecaster         = forecaster,                         y                  = data.loc[:end_validation, target],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         search_space       = search_space,                         steps              = steps,                         refit              = False,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'backtesting',                         fixed_train_size   = False,                         n_trials           = 15,                         random_state       = 123,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False                     )      end = time()     time_1 = end - start     best_params = results_1.loc[0, 'params']     best_lags = results_1.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     metric_1, pred_1 = backtesting_forecaster(                         forecaster          = forecaster,                         y                   = data.loc[:, target],                         exog                = data.loc[:, exog_features] if exog_features else None,                         initial_train_size  = len(data.loc[:end_validation]),                         steps               = steps,                         metric              = metric,                         verbose             = False,                         show_progress       = False                         )      # One step ahead     forecaster = copy(forecaster_to_benchmark)     start  = time()     if search_method == 'grid_search':         results_2 = grid_search_forecaster(                         forecaster         = forecaster,                         y                  = data.loc[:end_validation, target],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'one_step_ahead',                         return_best        = False,                         verbose            = False,                         show_progress      = False                     )     else:         results_2, _ = bayesian_search_forecaster(                         forecaster         = forecaster,                         y                  = data.loc[:end_validation, target],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         steps              = steps,                         search_space       = search_space,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'one_step_ahead',                         n_trials           = 15,                         random_state       = 123,                         return_best        = False,                         verbose            = False,                         show_progress      = False                     )      end = time()     time_2 = end - start     best_params = results_2.loc[0, 'params']     best_lags = results_2.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     metric_2, pred_2 = backtesting_forecaster(                         forecaster          = forecaster,                         y                   = data.loc[:, target],                         exog                = data.loc[:, exog_features] if exog_features else None,                         initial_train_size  = len(data.loc[:end_validation]),                         steps               = steps,                         metric              = metric,                         verbose             = False,                         show_progress       = False                         )      print(\"-----------------\")     print(\"Benchmark results\")     print(\"-----------------\")     print('Execution time backtesting   :', time_1)     print('Execution time one step ahead:', time_2)     print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")     print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")     print(\"\")     print(\"Method: backtesting\")     print(f\"    lags   : {results_1.loc[0, 'lags']}\")     print(f\"    params : {results_1.loc[0, 'params']}\")     print(f\"    {metric}: {metric_1.loc[0, metric]}\")     print(\"\")     print(\"Method: one step ahead\")     print(f\"    lags   : {results_2.loc[0, 'lags']}\")     print(f\"    params : {results_2.loc[0, 'params']}\")     print(f\"    {metric}: {metric_2.loc[0, metric]}\")          return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]   # Functions to compare results using backtesting and one step ahead # ============================================================================== def run_benchmark_multiseries(     data = None,     forecaster_to_benchmark = None,     search_method = None,     lags_grid = None,     param_grid = None,     search_space = None,     end_train = None,     end_validation = None,     levels = None,     exog_features = None,     steps = None,     metric = None ):     \"\"\"     Compare results of grid search using backtesting and one-step-ahead.     \"\"\"          # Backtesting     forecaster = copy(forecaster_to_benchmark)     start  = time()     if search_method == 'grid_search':         results_1 = grid_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = data.loc[:end_validation, levels],                         levels             = levels,                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         steps              = steps,                         refit              = False,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'backtesting',                         fixed_train_size   = False,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False                     )     else:         results_1, _ = bayesian_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = data.loc[:end_validation, levels],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         levels             = levels,                         search_space       = search_space,                         steps              = steps,                         refit              = False,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'backtesting',                         fixed_train_size   = False,                         n_trials           = 15,                         random_state       = 123,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False                     )     end = time()     time_1 = end - start     best_params = results_1.loc[0, 'params']     best_lags = results_1.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     metric_1, pred_1 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = data.loc[:, levels],                             exog                = data.loc[:, exog_features] if exog_features else None,                             initial_train_size  = len(data.loc[:end_validation]),                             levels              = levels,                             steps               = steps,                             metric              = metric,                             verbose             = False,                             show_progress       = False                         )      # One step ahead     forecaster = copy(forecaster_to_benchmark)     start  = time()     if search_method == 'grid_search':         results_2 = grid_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = data.loc[:end_validation, levels],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         levels             = levels,                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'one_step_ahead',                         return_best        = False,                         verbose            = False,                         show_progress      = False                     )     else:         results_2, _ = bayesian_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = data.loc[:end_validation, levels],                         exog               = data.loc[:end_validation, exog_features] if exog_features else None,                         levels             = levels,                         steps              = steps,                         search_space       = search_space,                         metric             = metric,                         initial_train_size = len(data.loc[:end_train]),                         method             = 'one_step_ahead',                         n_trials           = 15,                         random_state       = 123,                         return_best        = False,                         verbose            = False,                         show_progress      = False                     )      end = time()     time_2 = end - start     best_params = results_2.loc[0, 'params']     best_lags = results_2.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     metric_2, pred_2 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = data.loc[:, levels],                             exog                = data.loc[:, exog_features] if exog_features else None,                             levels              = levels,                             initial_train_size  = len(data.loc[:end_validation]),                             steps               = steps,                             metric              = metric,                             verbose             = False,                             show_progress       = False                         )      print(\"Benchmark results\")     print(\"-----------------\")     print('Execution time backtesting   :', time_1)     print('Execution time one step ahead:', time_2)     print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")     print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")     print(\"\")     print(\"Method: backtesting\")     print(f\"    lags   : {results_1.loc[0, 'lags']}\")     print(f\"    params : {results_1.loc[0, 'params']}\")     print(f\"    {metric_1.loc[0, metric]}\")     print(\"\")     print(\"Method: one step ahead\")     print(f\"    lags   : {results_2.loc[0, 'lags']}\")     print(f\"    params : {results_2.loc[0, 'params']}\")     print(f\"    {metric_2.loc[0, metric]}\")          return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]   def summarize_results(results, metric, title, plot=True, save_plot=None, fig_size=(8, 4)):     \"\"\"     Summarize results of benchmark.     \"\"\"      results = pd.DataFrame(         results,         columns=[             \"dataset\",             \"forecaster\",             \"time_search_backtesting\",             \"time_search_one_step\",             \"metric_backtesting\",             \"metric_one_step\",         ]     )     results['ratio_speed'] = (         results['time_search_backtesting'] / results['time_search_one_step']     ).round(2)     results['ratio_metric'] = (         results['metric_backtesting'] / results['metric_one_step']     ).round(2)     results[\"dataset_forecaster\"] = (         results[\"dataset\"]         + \" \\n \"         + results[\"forecaster\"].str.replace(\"Forecaster\", \"\")     )     display(results)      if plot:         set_dark_theme()         fig, axs = plt.subplots(2, 1, figsize=fig_size, sharex=True)         results.plot.bar(             x='dataset_forecaster',             y=['time_search_backtesting', 'time_search_one_step'],             ax=axs[0],         )         axs[0].set_ylabel('time (s)')         axs[0].legend([\"backtesting\", \"one-step-ahead\"])         results.plot.bar(             x='dataset_forecaster',             y=['metric_backtesting', 'metric_one_step'],             ax=axs[1],             legend=False         )         axs[1].set_ylabel(f'{metric}')         axs[1].set_xlabel('')         plt.xticks(rotation=90)         plt.suptitle(title)         plt.tight_layout()          if save_plot:             plt.savefig(save_plot, dpi=300, bbox_inches='tight') In\u00a0[6]: Copied! <pre># Results\n# ==============================================================================\nresults_grid_search = []\nmetric = 'mean_absolute_error'\n</pre> # Results # ============================================================================== results_grid_search = [] metric = 'mean_absolute_error' In\u00a0[7]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterAutoreg\n# ==============================================================================\nend_train = '2012-03-31 23:59:00'\nend_validation = '2012-08-31 23:59:00'\nexog_features = [\n    'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',\n    'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',\n    'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',\n    'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',\n    'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',\n    'temp', 'holiday'\n]\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_grid_search.append([\n    'bike',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterAutoreg # ============================================================================== end_train = '2012-03-31 23:59:00' end_validation = '2012-08-31 23:59:00' exog_features = [     'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',     'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',     'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',     'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',     'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',     'temp', 'holiday' ]  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              ) lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {     'n_estimators': [100, 200],     'max_depth': [3, 5],     'learning_rate': [0.01, 0.1] }  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_grid_search.append([     'bike',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 78.62683033943176\nExecution time one step ahead: 7.300451040267944\nSame lags  : False\nSame params: True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n    mean_absolute_error: 58.276762590192014\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n    mean_absolute_error: 64.04254202108999\n</pre> In\u00a0[8]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'bike',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'bike',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 71.92292070388794\nExecution time one step ahead: 1.4280896186828613\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 112.88378916846884}\n    mean_absolute_error: 79.14111581771634\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'alpha': 12.742749857031322}\n    mean_absolute_error: 111.95615163625295\n</pre> In\u00a0[9]: Copied! <pre># Dataset website_visits - ForecasterAutoreg\n# ==============================================================================\nend_train = '2021-03-30 23:59:00'\nend_validation = '2021-06-30 23:59:00'\nexog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]\n\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterAutoreg # ============================================================================== end_train = '2021-03-30 23:59:00' end_validation = '2021-06-30 23:59:00' exog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]  forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric ) results_grid_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 5.51067590713501\nExecution time one step ahead: 0.4799065589904785\nSame lags  : True\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 162.11396980738706\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 2.976351441631316}\n    mean_absolute_error: 162.3516346601722\n</pre> In\u00a0[10]: Copied! <pre># Dataset website_visits - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor = Ridge(random_state=123),\n                 steps     = 24,\n                 lags      = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor = Ridge(random_state=123),                  steps     = 24,                  lags      = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 12.132128715515137\nExecution time one step ahead: 1.5563743114471436\nSame lags  : True\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 277.8362513175169\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 1.438449888287663}\n    mean_absolute_error: 236.28560218972729\n</pre> In\u00a0[11]: Copied! <pre># Dataset vic_electricity - ForecasterAutoreg\n# ==============================================================================\nend_train = '2013-06-30 23:59:00'\nend_validation = '2013-11-30 23:59:00'\nexog_features = ['Temperature', 'Holiday']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterAutoreg # ============================================================================== end_train = '2013-06-30 23:59:00' end_validation = '2013-11-30 23:59:00' exog_features = ['Temperature', 'Holiday']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {     'n_estimators': [100, 200],     'max_depth': [3, 5],     'learning_rate': [0.01, 0.1] }  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 76.80228281021118\nExecution time one step ahead: 6.293450355529785\nSame lags  : False\nSame params: True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n    mean_absolute_error: 194.83553235066182\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n    mean_absolute_error: 188.8782299908785\n</pre> In\u00a0[12]: Copied! <pre># Dataset vic_electricity - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 70.91320610046387\nExecution time one step ahead: 1.4723453521728516\nSame lags  : True\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 304.22332781257694\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 1.438449888287663}\n    mean_absolute_error: 301.7070971763055\n</pre> In\u00a0[13]: Copied! <pre># Dataset sales - ForecasterAutoregMultiSeries\n# ==============================================================================\nend_train = '2014-05-15 23:59:00'\nend_validation = '2014-07-15 23:59:00'\nlevels = ['item_1', 'item_2', 'item_3']\nexog_features = ['day_of_week']\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = {\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 36,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterAutoregMultiSeries # ============================================================================== end_train = '2014-05-15 23:59:00' end_validation = '2014-07-15 23:59:00' levels = ['item_1', 'item_2', 'item_3'] exog_features = ['day_of_week']  forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = {     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 36,     metric                  = metric ) results_grid_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 1.6487984657287598\nExecution time one step ahead: 1.0693504810333252\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n    params : {'max_depth': 7, 'n_estimators': 200}\n    137.16940500432474\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'max_depth': 3, 'n_estimators': 50}\n    134.76669158338447\n</pre> In\u00a0[14]: Copied! <pre># Dataset sales - ForecasterAutoregMultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 steps              = 5,\n                 level              = 'item_1',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = {\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 5,\n    metric                  = metric\n)\n\nresults_grid_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterAutoregMultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  steps              = 5,                  level              = 'item_1',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = {     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 5,     metric                  = metric )  results_grid_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 6.838553428649902\nExecution time one step ahead: 1.404649019241333\nSame lags  : False\nSame params: True\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    100.16441146410313\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    95.20010578089475\n</pre> In\u00a0[15]: Copied! <pre># Dataset series_dict - ForecasterAutoregMultiSeries\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\nend_validation = '2016-07-31 23:59:00'\nlevels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004']\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n\nforecaster_to_benchmark = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\nlags_grid = [7, 14]\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\n# Backtesting\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\nresults_1 = grid_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                        exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        steps              = 24,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = 100,\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False,\n                        suppress_warnings  = True\n                    )\nend = time()\ntime_1 = end - start\nbest_params = results_1.loc[0, 'params']\nbest_lags = results_1.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\nmetric_1, pred_1 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = series_dict,\n                            exog                = exog_dict,\n                            initial_train_size  = 213,\n                            levels              = levels,\n                            steps               = 24,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False,\n                            suppress_warnings  = True\n                        )\n\n# One step ahead\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\nresults_2 = grid_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                        exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                        levels             = levels,\n                        param_grid         = param_grid,\n                        lags_grid          = lags_grid,\n                        metric             = metric,\n                        initial_train_size = 100,\n                        method             = 'one_step_ahead',\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False,\n                        suppress_warnings  = True\n                    )\nend = time()\ntime_2 = end - start\nbest_params = results_2.loc[0, 'params']\nbest_lags = results_2.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\nmetric_2, pred_2 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = series_dict,\n                            exog                = exog_dict,\n                            initial_train_size  = 213,\n                            levels              = levels,\n                            steps               = 24,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False,\n                            suppress_warnings  = True\n                        )\nprint(\"Benchmark results\")\nprint(\"-----------------\")\nprint('Execution time backtesting   :', time_1)\nprint('Execution time one step ahead:', time_2)\nprint(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\nprint(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\nprint(\"\")\nprint(\"Method: backtesting\")\nprint(f\"    lags   : {results_1.loc[0, 'lags']}\")\nprint(f\"    params : {results_1.loc[0, 'params']}\")\nprint(f\"    {metric_1.loc[0, metric]}\")\nprint(\"\")\nprint(\"Method: one step ahead\")\nprint(f\"    lags   : {results_2.loc[0, 'lags']}\")\nprint(f\"    params : {results_2.loc[0, 'params']}\")\nprint(f\"    {metric_2.loc[0, metric]}\")\n\nresults_grid_search.append([\n    'series_dict',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1.loc[0, metric],\n    metric_2.loc[0, metric],\n])\n</pre> # Dataset series_dict - ForecasterAutoregMultiSeries # ============================================================================== end_train = '2016-05-31 23:59:00' end_validation = '2016-07-31 23:59:00' levels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004'] series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}  forecaster_to_benchmark = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              ) lags_grid = [7, 14] param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  # Backtesting forecaster = copy(forecaster_to_benchmark) start  = time() results_1 = grid_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                         exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         steps              = 24,                         refit              = False,                         metric             = metric,                         initial_train_size = 100,                         method             = 'backtesting',                         fixed_train_size   = False,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False,                         suppress_warnings  = True                     ) end = time() time_1 = end - start best_params = results_1.loc[0, 'params'] best_lags = results_1.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) metric_1, pred_1 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = series_dict,                             exog                = exog_dict,                             initial_train_size  = 213,                             levels              = levels,                             steps               = 24,                             metric              = metric,                             verbose             = False,                             show_progress       = False,                             suppress_warnings  = True                         )  # One step ahead forecaster = copy(forecaster_to_benchmark) start  = time() results_2 = grid_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                         exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                         levels             = levels,                         param_grid         = param_grid,                         lags_grid          = lags_grid,                         metric             = metric,                         initial_train_size = 100,                         method             = 'one_step_ahead',                         return_best        = False,                         verbose            = False,                         show_progress      = False,                         suppress_warnings  = True                     ) end = time() time_2 = end - start best_params = results_2.loc[0, 'params'] best_lags = results_2.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) metric_2, pred_2 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = series_dict,                             exog                = exog_dict,                             initial_train_size  = 213,                             levels              = levels,                             steps               = 24,                             metric              = metric,                             verbose             = False,                             show_progress       = False,                             suppress_warnings  = True                         ) print(\"Benchmark results\") print(\"-----------------\") print('Execution time backtesting   :', time_1) print('Execution time one step ahead:', time_2) print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\") print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\") print(\"\") print(\"Method: backtesting\") print(f\"    lags   : {results_1.loc[0, 'lags']}\") print(f\"    params : {results_1.loc[0, 'params']}\") print(f\"    {metric_1.loc[0, metric]}\") print(\"\") print(\"Method: one step ahead\") print(f\"    lags   : {results_2.loc[0, 'lags']}\") print(f\"    params : {results_2.loc[0, 'params']}\") print(f\"    {metric_2.loc[0, metric]}\")  results_grid_search.append([     'series_dict',     type(forecaster).__name__,     time_1,     time_2,     metric_1.loc[0, metric],     metric_2.loc[0, metric], ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 1.6000556945800781\nExecution time one step ahead: 0.3853023052215576\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [1 2 3 4 5 6 7]\n    params : {'max_depth': 3, 'n_estimators': 50}\n    180.46141171905165\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    164.23659500870002\n</pre> In\u00a0[16]: Copied! <pre># Results\n# ==============================================================================\nsummarize_results(\n    results   = results_grid_search,\n    metric    = metric,\n    plot      = True,\n    fig_size  = (8, 6),\n    title     = 'Grid search using backtesting vs one-step-ahead',\n    save_plot = \"../img/grid_search_benchmarck.png\"\n)\n</pre> # Results # ============================================================================== summarize_results(     results   = results_grid_search,     metric    = metric,     plot      = True,     fig_size  = (8, 6),     title     = 'Grid search using backtesting vs one-step-ahead',     save_plot = \"../img/grid_search_benchmarck.png\" ) dataset forecaster time_search_backtesting time_search_one_step metric_backtesting metric_one_step ratio_speed ratio_metric dataset_forecaster 0 bike ForecasterAutoreg 78.626830 7.300451 58.276763 64.042542 10.77 0.91 bike \\n Autoreg 1 bike ForecasterAutoregDirect 71.922921 1.428090 79.141116 111.956152 50.36 0.71 bike \\n AutoregDirect 2 website ForecasterAutoreg 5.510676 0.479907 162.113970 162.351635 11.48 1.00 website \\n Autoreg 3 website ForecasterAutoregDirect 12.132129 1.556374 277.836251 236.285602 7.80 1.18 website \\n AutoregDirect 4 electricity ForecasterAutoreg 76.802283 6.293450 194.835532 188.878230 12.20 1.03 electricity \\n Autoreg 5 electricity ForecasterAutoregDirect 70.913206 1.472345 304.223328 301.707097 48.16 1.01 electricity \\n AutoregDirect 6 sales ForecasterAutoregMultiSeries 1.648798 1.069350 137.169405 134.766692 1.54 1.02 sales \\n AutoregMultiSeries 7 sales ForecasterAutoregMultiVariate 6.838553 1.404649 100.164411 95.200106 4.87 1.05 sales \\n AutoregMultiVariate 8 series_dict ForecasterAutoregMultiSeries 1.600056 0.385302 180.461412 164.236595 4.15 1.10 series_dict \\n AutoregMultiSeries In\u00a0[17]: Copied! <pre># Table to store results\n# ==============================================================================\nresults_bayesian_search = []\nmetric = 'mean_absolute_error'\n</pre> # Table to store results # ============================================================================== results_bayesian_search = [] metric = 'mean_absolute_error' In\u00a0[18]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterAutoreg\n# ==============================================================================\nend_train = '2012-03-31 23:59:00'\nend_validation = '2012-08-31 23:59:00'\nexog_features = [\n    'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',\n    'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',\n    'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',\n    'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',\n    'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',\n    'temp', 'holiday'\n]\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 400, 1200, step=100),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),\n        'gamma'           : trial.suggest_float('gamma', 0, 1),\n        'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 1),\n        'lags'            : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'bike_sharing',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterAutoreg # ============================================================================== end_train = '2012-03-31 23:59:00' end_validation = '2012-08-31 23:59:00' exog_features = [     'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',     'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',     'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',     'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',     'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',     'temp', 'holiday' ]  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  def search_space(trial):     search_space  = {         'n_estimators'    : trial.suggest_int('n_estimators', 400, 1200, step=100),         'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),         'gamma'           : trial.suggest_float('gamma', 0, 1),         'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 1),         'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 1),         'lags'            : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_bayesian_search.append([     'bike_sharing',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 81.104501247406\nExecution time one step ahead: 28.4353928565979\nSame lags  : True\nSame params: True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.017833474222028703, 'gamma': 0.2285821738161964, 'reg_alpha': 0.2379772800670556, 'reg_lambda': 0.9887301767538853}\n    mean_absolute_error: 55.80577702511616\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.017833474222028703, 'gamma': 0.2285821738161964, 'reg_alpha': 0.2379772800670556, 'reg_lambda': 0.9887301767538853}\n    mean_absolute_error: 55.80577702511616\n</pre> In\u00a0[19]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\ndef search_space(trial):\n    search_space  = {\n        'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags'  : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'bike_sharing',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  def search_space(trial):     search_space  = {         'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags'  : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_bayesian_search.append([     'bike_sharing',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 19.26004123687744\nExecution time one step ahead: 1.4251313209533691\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 121.0898788312409}\n    mean_absolute_error: 79.1498337214025\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'alpha': 15.094374246471325}\n    mean_absolute_error: 111.96208734026862\n</pre> In\u00a0[20]: Copied! <pre># Dataset website_visits - ForecasterAutoreg\n# ==============================================================================\nend_train = '2021-03-30 23:59:00'\nend_validation = '2021-06-30 23:59:00'\nexog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]\n\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\ndef search_space(trial):\n    search_space  = {\n        'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags'  : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterAutoreg # ============================================================================== end_train = '2021-03-30 23:59:00' end_validation = '2021-06-30 23:59:00' exog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]  forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]  def search_space(trial):     search_space  = {         'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags'  : trial.suggest_categorical('lags', lags_grid)     }      return search_space  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric )  results_bayesian_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 1.2653906345367432\nExecution time one step ahead: 0.2625236511230469\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n    params : {'alpha': 0.07474245141964296}\n    mean_absolute_error: 136.76802274106467\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 0.03182234592129467}\n    mean_absolute_error: 173.5282998809151\n</pre> In\u00a0[21]: Copied! <pre># Dataset website_visits - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10,\n                 steps         = 7\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\ndef search_space(trial):\n    search_space  = {\n        'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags'  : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10,                  steps         = 7              )  lags_grid = [7, 14, 21, [7, 14, 21]]  def search_space(trial):     search_space  = {         'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags'  : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric )  results_bayesian_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 1.5849392414093018\nExecution time one step ahead: 0.3827948570251465\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n    params : {'alpha': 0.07474245141964296}\n    mean_absolute_error: 139.40123604696382\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 0.03182234592129467}\n    mean_absolute_error: 153.6723680506666\n</pre> In\u00a0[22]: Copied! <pre># Dataset vic_electricity - ForecasterAutoreg\n# ==============================================================================\nend_train = '2013-06-30 23:59:00'\nend_validation = '2013-11-30 23:59:00'\nexog_features = ['Temperature', 'Holiday']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 400, 1200, step=100),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),\n        'gamma'           : trial.suggest_float('gamma', 0, 1),\n        'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 1),\n        'lags'            : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterAutoreg # ============================================================================== end_train = '2013-06-30 23:59:00' end_validation = '2013-11-30 23:59:00' exog_features = ['Temperature', 'Holiday']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  def search_space(trial):     search_space  = {         'n_estimators'    : trial.suggest_int('n_estimators', 400, 1200, step=100),         'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),         'gamma'           : trial.suggest_float('gamma', 0, 1),         'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 1),         'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 1),         'lags'            : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_bayesian_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 92.89738893508911\nExecution time one step ahead: 26.816620111465454\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'n_estimators': 1200, 'max_depth': 8, 'learning_rate': 0.020288327487155415, 'gamma': 0.9893221948178936, 'reg_alpha': 0.0026751307734329544, 'reg_lambda': 0.0033431281459104997}\n    mean_absolute_error: 196.74829952595292\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.056896300053531614, 'gamma': 0.2725691628660212, 'reg_alpha': 0.24605588251006993, 'reg_lambda': 0.9687485406819449}\n    mean_absolute_error: 191.37491441780287\n</pre> In\u00a0[23]: Copied! <pre># Dataset vic_electricity - ForecasterAutoregDirect\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10,\n                 steps         = 24\n             )\n\nlags_grid = (48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169))\n\ndef search_space(trial):\n    search_space  = {\n        'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags'  : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterAutoregDirect # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10,                  steps         = 24              )  lags_grid = (48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169))  def search_space(trial):     search_space  = {         'alpha' : trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags'  : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_bayesian_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection/model_selection.py:1739: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 17.570114374160767\nExecution time one step ahead: 0.8226943016052246\nSame lags  : True\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 16.432489069228232}\n    mean_absolute_error: 307.13365278620506\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 0.36149961722510493}\n    mean_absolute_error: 300.87028133154746\n</pre> In\u00a0[24]: Copied! <pre># Dataset sales - ForecasterAutoregMultiSeries\n# ==============================================================================\nend_train = '2014-05-15 23:59:00'\nend_validation = '2014-07-15 23:59:00'\nlevels = ['item_1', 'item_2', 'item_3']\nexog_features = ['day_of_week']\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = [48, 72]\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),\n        'lags'            : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 36,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterAutoregMultiSeries # ============================================================================== end_train = '2014-05-15 23:59:00' end_validation = '2014-07-15 23:59:00' levels = ['item_1', 'item_2', 'item_3'] exog_features = ['day_of_week']  forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = [48, 72]  def search_space(trial):     search_space  = {         'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),         'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),         'lags'            : trial.suggest_categorical('lags', lags_grid)     }      return search_space  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 36,     metric                  = metric ) results_bayesian_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:2349: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>Benchmark results\n-----------------\nExecution time backtesting   : 3.5031938552856445\nExecution time one step ahead: 2.78940486907959\nSame lags  : True\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 199, 'max_depth': 3, 'learning_rate': 0.01901626315047264}\n    135.45451272241843\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.06045266837878549}\n    123.59899056676193\n</pre> In\u00a0[25]: Copied! <pre># Dataset sales - ForecasterAutoregMultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 steps              = 5,\n                 level              = 'item_1',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = [48, 72]\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),\n        'lags'            : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 5,\n    metric                  = metric\n)\nresults_bayesian_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterAutoregMultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  steps              = 5,                  level              = 'item_1',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = [48, 72]  def search_space(trial):     search_space  = {         'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),         'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 1),         'lags'            : trial.suggest_categorical('lags', lags_grid)     }      return search_space  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 5,     metric                  = metric ) results_bayesian_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:83: IgnoredArgumentWarning: `levels` argument have no use when the forecaster is of type `ForecasterAutoregMultiVariate`. The level of this forecaster is 'item_1', to predict another level, change the `level` argument when initializing the forecaster. \n \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:83: IgnoredArgumentWarning: `levels` argument have no use when the forecaster is of type `ForecasterAutoregMultiVariate`. The level of this forecaster is 'item_1', to predict another level, change the `level` argument when initializing the forecaster. \n \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:2349: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:83: IgnoredArgumentWarning: `levels` argument have no use when the forecaster is of type `ForecasterAutoregMultiVariate`. The level of this forecaster is 'item_1', to predict another level, change the `level` argument when initializing the forecaster. \n \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:83: IgnoredArgumentWarning: `levels` argument have no use when the forecaster is of type `ForecasterAutoregMultiVariate`. The level of this forecaster is 'item_1', to predict another level, change the `level` argument when initializing the forecaster. \n \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> <pre>Benchmark results\n-----------------\nExecution time backtesting   : 21.014704942703247\nExecution time one step ahead: 5.041488170623779\nSame lags  : False\nSame params: False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 178, 'max_depth': 4, 'learning_rate': 0.029392307095288957}\n    98.66981600939468\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'n_estimators': 98, 'max_depth': 5, 'learning_rate': 0.23598059857016607}\n    101.07276932380157\n</pre> In\u00a0[26]: Copied! <pre># Dataset series_dict - ForecasterAutoregMultiSeries\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\nend_validation = '2016-07-31 23:59:00'\nlevels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004']\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n\nforecaster_to_benchmark = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 7, step=1),\n        'lags'            : trial.suggest_categorical('lags', [7, 14])\n    } \n    return search_space\n\n# Backtesting\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\nresults_1, _ = bayesian_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                        exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                        search_space       = search_space,\n                        n_trials           = 10,\n                        steps              = 24,\n                        refit              = False,\n                        metric             = metric,\n                        initial_train_size = 100,\n                        method             = 'backtesting',\n                        fixed_train_size   = False,\n                        return_best        = False,\n                        n_jobs             = 'auto',\n                        verbose            = False,\n                        show_progress      = False,\n                        suppress_warnings  = True\n                    )\nend = time()\ntime_1 = end - start\nbest_params = results_1.loc[0, 'params']\nbest_lags = results_1.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\nmetric_1, pred_1 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = series_dict,\n                            exog                = exog_dict,\n                            initial_train_size  = 213,\n                            levels              = levels,\n                            steps               = 24,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False,\n                            suppress_warnings  = True\n                        )\n\n# One step ahead\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\nresults_2, _ = bayesian_search_forecaster_multiseries(\n                        forecaster         = forecaster,\n                        series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                        exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                        levels             = levels,\n                        search_space       = search_space,\n                        n_trials           = 10,\n                        metric             = metric,\n                        initial_train_size = 100,\n                        method             = 'one_step_ahead',\n                        return_best        = False,\n                        verbose            = False,\n                        show_progress      = False,\n                        suppress_warnings  = True\n                    )\nend = time()\ntime_2 = end - start\nbest_params = results_2.loc[0, 'params']\nbest_lags = results_2.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\nmetric_2, pred_2 = backtesting_forecaster_multiseries(\n                            forecaster          = forecaster,\n                            series              = series_dict,\n                            exog                = exog_dict,\n                            initial_train_size  = 213,\n                            levels              = levels,\n                            steps               = 24,\n                            metric              = metric,\n                            verbose             = False,\n                            show_progress       = False,\n                            suppress_warnings  = True\n                        )\nprint(\"Benchmark results\")\nprint(\"-----------------\")\nprint('Execution time backtesting   :', time_1)\nprint('Execution time one step ahead:', time_2)\nprint(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\nprint(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\nprint(\"\")\nprint(\"Method: backtesting\")\nprint(f\"    lags   : {results_1.loc[0, 'lags']}\")\nprint(f\"    params : {results_1.loc[0, 'params']}\")\nprint(f\"    {metric_1.loc[0, metric]}\")\nprint(\"\")\nprint(\"Method: one step ahead\")\nprint(f\"    lags   : {results_2.loc[0, 'lags']}\")\nprint(f\"    params : {results_2.loc[0, 'params']}\")\nprint(f\"    {metric_2.loc[0, metric]}\")\n\nresults_bayesian_search.append([\n    'series_dict',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1.loc[0, metric],\n    metric_2.loc[0, metric],\n])\n</pre> # Dataset series_dict - ForecasterAutoregMultiSeries # ============================================================================== end_train = '2016-05-31 23:59:00' end_validation = '2016-07-31 23:59:00' levels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004'] series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}  forecaster_to_benchmark = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  def search_space(trial):     search_space  = {         'n_estimators'    : trial.suggest_int('n_estimators', 50, 200),         'max_depth'       : trial.suggest_int('max_depth', 3, 7, step=1),         'lags'            : trial.suggest_categorical('lags', [7, 14])     }      return search_space  # Backtesting forecaster = copy(forecaster_to_benchmark) start  = time() results_1, _ = bayesian_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                         exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                         search_space       = search_space,                         n_trials           = 10,                         steps              = 24,                         refit              = False,                         metric             = metric,                         initial_train_size = 100,                         method             = 'backtesting',                         fixed_train_size   = False,                         return_best        = False,                         n_jobs             = 'auto',                         verbose            = False,                         show_progress      = False,                         suppress_warnings  = True                     ) end = time() time_1 = end - start best_params = results_1.loc[0, 'params'] best_lags = results_1.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) metric_1, pred_1 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = series_dict,                             exog                = exog_dict,                             initial_train_size  = 213,                             levels              = levels,                             steps               = 24,                             metric              = metric,                             verbose             = False,                             show_progress       = False,                             suppress_warnings  = True                         )  # One step ahead forecaster = copy(forecaster_to_benchmark) start  = time() results_2, _ = bayesian_search_forecaster_multiseries(                         forecaster         = forecaster,                         series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                         exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                         levels             = levels,                         search_space       = search_space,                         n_trials           = 10,                         metric             = metric,                         initial_train_size = 100,                         method             = 'one_step_ahead',                         return_best        = False,                         verbose            = False,                         show_progress      = False,                         suppress_warnings  = True                     ) end = time() time_2 = end - start best_params = results_2.loc[0, 'params'] best_lags = results_2.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) metric_2, pred_2 = backtesting_forecaster_multiseries(                             forecaster          = forecaster,                             series              = series_dict,                             exog                = exog_dict,                             initial_train_size  = 213,                             levels              = levels,                             steps               = 24,                             metric              = metric,                             verbose             = False,                             show_progress       = False,                             suppress_warnings  = True                         ) print(\"Benchmark results\") print(\"-----------------\") print('Execution time backtesting   :', time_1) print('Execution time one step ahead:', time_2) print(f\"Same lags  : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\") print(f\"Same params: {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\") print(\"\") print(\"Method: backtesting\") print(f\"    lags   : {results_1.loc[0, 'lags']}\") print(f\"    params : {results_1.loc[0, 'params']}\") print(f\"    {metric_1.loc[0, metric]}\") print(\"\") print(\"Method: one step ahead\") print(f\"    lags   : {results_2.loc[0, 'lags']}\") print(f\"    params : {results_2.loc[0, 'params']}\") print(f\"    {metric_2.loc[0, metric]}\")  results_bayesian_search.append([     'series_dict',     type(forecaster).__name__,     time_1,     time_2,     metric_1.loc[0, metric],     metric_2.loc[0, metric], ]) <pre>/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:2349: UserWarning: One-step-ahead predictions are used for faster model comparison, but they may not fully represent multi-step prediction performance. It is recommended to backtest the final model for a more accurate multi-step performance estimate.\n  warnings.warn(\n</pre> <pre>Benchmark results\n-----------------\nExecution time backtesting   : 1.9651103019714355\nExecution time one step ahead: 0.7873244285583496\nSame lags  : True\nSame params: True\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'n_estimators': 77, 'max_depth': 3}\n    208.60243551060555\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'n_estimators': 77, 'max_depth': 3}\n    208.60243551060555\n</pre> In\u00a0[27]: Copied! <pre># Results\n# ==============================================================================\nsummarize_results(\n    results   = results_bayesian_search,\n    metric    = metric,\n    plot      = True,\n    fig_size  = (8, 6),\n    title     = 'Bayesian search using backtesting vs one-step-ahead',\n    save_plot = \"../img/bayesian_search_benchmarck.png\"\n)\n</pre> # Results # ============================================================================== summarize_results(     results   = results_bayesian_search,     metric    = metric,     plot      = True,     fig_size  = (8, 6),     title     = 'Bayesian search using backtesting vs one-step-ahead',     save_plot = \"../img/bayesian_search_benchmarck.png\" ) dataset forecaster time_search_backtesting time_search_one_step metric_backtesting metric_one_step ratio_speed ratio_metric dataset_forecaster 0 bike_sharing ForecasterAutoreg 81.104501 28.435393 55.805777 55.805777 2.85 1.00 bike_sharing \\n Autoreg 1 bike_sharing ForecasterAutoregDirect 19.260041 1.425131 79.149834 111.962087 13.51 0.71 bike_sharing \\n AutoregDirect 2 website ForecasterAutoreg 1.265391 0.262524 136.768023 173.528300 4.82 0.79 website \\n Autoreg 3 website ForecasterAutoregDirect 1.584939 0.382795 139.401236 153.672368 4.14 0.91 website \\n AutoregDirect 4 electricity ForecasterAutoreg 92.897389 26.816620 196.748300 191.374914 3.46 1.03 electricity \\n Autoreg 5 electricity ForecasterAutoregDirect 17.570114 0.822694 307.133653 300.870281 21.36 1.02 electricity \\n AutoregDirect 6 sales ForecasterAutoregMultiSeries 3.503194 2.789405 135.454513 123.598991 1.26 1.10 sales \\n AutoregMultiSeries 7 sales ForecasterAutoregMultiVariate 21.014705 5.041488 98.669816 101.072769 4.17 0.98 sales \\n AutoregMultiVariate 8 series_dict ForecasterAutoregMultiSeries 1.965110 0.787324 208.602436 208.602436 2.50 1.00 series_dict \\n AutoregMultiSeries"},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#hyperparameters-and-lags-search-backtesting-vs-one-step-ahead","title":"Hyperparameters and lags search: backtesting vs one-step-ahead\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#results","title":"Results\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#data-sets","title":"Data sets\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#benchmark","title":"Benchmark\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#grid-search","title":"Grid search\u00b6","text":""},{"location":"faq/parameters-search-backetesting-vs-one-step-ahead.html#bayesian-search","title":"Bayesian search\u00b6","text":""},{"location":"faq/profiling-skforecast.html","title":"Profiling skforecast","text":"<p>This document shows the profiling of the main classes, methods and functions available in skforecast. Understanding the bottlenecks will help to:</p> <ul> <li>Use it more efficiently</li> <li>Improve the code for future releases</li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport platform\nimport psutil\n\nimport skforecast\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\n%load_ext pyinstrument\n</pre> # Libraries # ============================================================================== import time import numpy as np import pandas as pd import matplotlib.pyplot as plt import platform import psutil  import skforecast from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import backtesting_forecaster import sklearn from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import HistGradientBoostingRegressor from lightgbm import LGBMRegressor  %load_ext pyinstrument In\u00a0[\u00a0]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"psutil version      : {psutil.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Machine type: {platform.machine()}\")\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"psutil version      : {psutil.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Machine type: {platform.machine()}\") print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <p>A time series of length 1000 with random values is created.</p> In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\nnp.random.seed(123)\nn = 1_000\ndata = pd.Series(data = np.random.normal(size=n))\n</pre> # Data # ============================================================================== np.random.seed(123) n = 1_000 data = pd.Series(data = np.random.normal(size=n)) <p>To isolate the training process of the regressor from the other parts of the code, a dummy regressor class is created. This dummy regressor has a fit method that does nothing, and a predict method that returns a constant value.</p> In\u00a0[3]: Copied! <pre>class DummyRegressor(LinearRegression):\n    \"\"\"\n    Dummy regressor with dummy fit and predict methods.\n    \"\"\"\n    \n    def fit(self, X, y):\n        pass\n\n    def predict(self, y):\n        predictions = np.ones(shape = len(y))\n        return predictions\n</pre> class DummyRegressor(LinearRegression):     \"\"\"     Dummy regressor with dummy fit and predict methods.     \"\"\"          def fit(self, X, y):         pass      def predict(self, y):         predictions = np.ones(shape = len(y))         return predictions          In\u00a0[4]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 0 is constant and will be replaced with 0.\n  warnings.warn(\n</pre> <p>Almost all of the time spent by <code>fit</code> is required by the <code>create_train_X_y</code> method.</p> In\u00a0[5]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) <p>When training a forecaster with a real machine learning regressor, the time spent by <code>create_train_X_y</code> is negligible compared to the time needed by the <code>fit</code> method of the regressor. Therefore, improving the speed of <code>create_train_X_y</code> will not have much impact.</p> <p>Understand how the <code>create_train_X_y</code> method is influenced by the length of the series and the number of lags.</p> In\u00a0[6]: Copied! <pre># Profiling `create_train_X_y` for different length of series and number of lags\n# ======================================================================================\nseries_length = np.linspace(1000, 1000000, num=5, dtype=int)\nn_lags = [5, 10, 50, 100, 200]\nresults = {}\n\nfor lags in n_lags:\n    execution_time = []\n    forecaster = ForecasterAutoreg(\n                     regressor = DummyRegressor(),\n                     lags      = lags\n                 )\n\n    for n in series_length:\n        y = pd.Series(data = np.random.normal(size=n))\n        tic = time.perf_counter()\n        _ = forecaster.create_train_X_y(y=y)\n        toc = time.perf_counter()\n        execution_time.append(toc-tic)\n\n    results[lags] = execution_time\n\nresults = pd.DataFrame(\n              data =  results,\n              index = series_length\n          )\n\nresults\n</pre> # Profiling `create_train_X_y` for different length of series and number of lags # ====================================================================================== series_length = np.linspace(1000, 1000000, num=5, dtype=int) n_lags = [5, 10, 50, 100, 200] results = {}  for lags in n_lags:     execution_time = []     forecaster = ForecasterAutoreg(                      regressor = DummyRegressor(),                      lags      = lags                  )      for n in series_length:         y = pd.Series(data = np.random.normal(size=n))         tic = time.perf_counter()         _ = forecaster.create_train_X_y(y=y)         toc = time.perf_counter()         execution_time.append(toc-tic)      results[lags] = execution_time  results = pd.DataFrame(               data =  results,               index = series_length           )  results Out[6]: 5 10 50 100 200 1000 0.001045 0.004704 0.007673 0.038916 0.069427 250750 0.007231 0.014726 0.213554 0.364044 0.561126 500500 0.023345 0.052342 0.427991 0.775216 1.522044 750250 0.037870 0.068540 0.666305 1.062016 2.148227 1000000 0.042683 0.092421 0.832870 1.425935 2.702640 In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 4))\nresults.plot(ax=ax, marker='.')\nax.set_xlabel('length of series')\nax.set_ylabel('time (seconds)')\nax.set_title('Profiling create_train_X_y()')\nax.legend(title='number of lags');\n</pre> fig, ax = plt.subplots(figsize=(7, 4)) results.plot(ax=ax, marker='.') ax.set_xlabel('length of series') ax.set_ylabel('time (seconds)') ax.set_title('Profiling create_train_X_y()') ax.legend(title='number of lags'); In\u00a0[8]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 0 is constant and will be replaced with 0.\n  warnings.warn(\n</pre> In\u00a0[9]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) In\u00a0[10]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[11]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) <p>Inside the <code>predict</code> method, the <code>append</code> action is the most expensive but, similar to what happen with <code>fit</code>, it is negligible compared to the time need by the <code>predict</code> method of the regressor.</p>"},{"location":"faq/profiling-skforecast.html#profiling-skforecast","title":"Profiling skforecast\u00b6","text":""},{"location":"faq/profiling-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/profiling-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"faq/profiling-skforecast.html#dummy-regressor","title":"Dummy regressor\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-fit","title":"Profiling fit\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-create_train_x_y","title":"Profiling create_train_X_y\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-predict","title":"Profiling predict\u00b6","text":""},{"location":"faq/time-series-aggregation.html","title":"Time series aggregation","text":"<p>Time series aggregation involves summarizing or transforming data over specific time intervals. Two of the most common use cases are</p> <ul> <li><p>Aggregating data from one frequency to another. For example, converting hourly data to daily data.</p> </li> <li><p>Aggregating data across a sliding window. For example, calculating a rolling average over the last 7 days.</p> </li> </ul> <p>These aggregations not only greatly reduce the total volume of data, but also help you find interesting features for your model faster.</p> <p>Pandas provides an easy and efficient way to aggregate data from time series. This document shows how to use the <code>resampling</code> and <code>rolling</code> methods to aggregate data, with special emphasis on how to avoid data leakage, which is a common mistake when aggregating time series.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='vic_electricity')\ndata = data[['Demand', 'Temperature', 'Holiday']]\ndata.head(5)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='vic_electricity') data = data[['Demand', 'Temperature', 'Holiday']] data.head(5) <pre>vic_electricity\n---------------\nHalf-hourly electricity demand for Victoria, Australia\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse\nDatasets for 'tsibble'. https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/.\nhttps://tsibbledata.tidyverts.org/reference/vic_elec.html\nShape of the dataset: (52608, 4)\n</pre> Out[2]: Demand Temperature Holiday Time 2011-12-31 13:00:00 4382.825174 21.40 True 2011-12-31 13:30:00 4263.365526 21.05 True 2011-12-31 14:00:00 4048.966046 20.70 True 2011-12-31 14:30:00 3877.563330 20.55 True 2011-12-31 15:00:00 4036.229746 20.40 True <p>This dataset contains the electricity demand in Victoria (Australia) at half-hourly frequency.</p> In\u00a0[3]: Copied! <pre># Index Frequency\n# ==============================================================================\nprint(f\"Frequency: {data.index.freq}\")\n</pre> # Index Frequency # ============================================================================== print(f\"Frequency: {data.index.freq}\") <pre>Frequency: &lt;30 * Minutes&gt;\n</pre> <p>To change the frequency of a time series, use the <code>resample</code> method. This method allows you to specify a frequency and an aggregation function. It works similarly to the <code>groupby</code> method, but it works with time series indices.</p> <p>When aggregating data, it is very important to use the <code>closed</code> and <code>label</code> arguments correctly. This avoids introducing future information into the training (data leakage).</p> <ul> <li><p>The <code>closed</code> argument specifies whether the interval is closed on the left-side, right-side, both or neither.</p> </li> <li><p>The <code>label</code> argument specifies whether the result is labeled with the beginning or the end of the interval.</p> </li> </ul> <p>Suppose that values are available for 10:10, 10:30, 10:45, 11:00, 11:12, and 11:30. To obtain the hourly average, the value assigned to 11:00 must be calculated using the values for 10:10, 10:30, and 10:45; and the value assigned to 12:00 must be calculated using the value for 11:00, 11:12 and 11:30. The 11:00 average does not include the 11:00 point value because in reality the value is not available at that exact time.</p> <p>In this case, the correct arguments are <code>closed='left'</code> and <code>label='right'</code>.</p> <p> Diagram of data aggregation using the resample method without including future information. </p> <p>For example, the code in the next cell converts the data from half-hourly to hourly frequency. Since there are multiple columns, an aggregation function must be specified for each column. In this case, the <code>sum</code> is calculated for the <code>Demand</code> column and the <code>average</code> is calculated for the rest.</p> In\u00a0[4]: Copied! <pre># Aggregate data from 30 minutes to 1 hour\n# ==============================================================================\ndata = data.resample(rule='1H', closed='left', label ='right').agg({\n           'Demand': 'sum',\n           'Temperature': 'mean',\n           'Holiday': 'mean'\n       })\n\ndata\n</pre> # Aggregate data from 30 minutes to 1 hour # ============================================================================== data = data.resample(rule='1H', closed='left', label ='right').agg({            'Demand': 'sum',            'Temperature': 'mean',            'Holiday': 'mean'        })  data <pre>C:\\Users\\jaesc2\\AppData\\Local\\Temp\\ipykernel_19428\\1816291895.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  data = data.resample(rule='1H', closed='left', label ='right').agg({\n</pre> Out[4]: Demand Temperature Holiday Time 2011-12-31 14:00:00 8646.190700 21.225 1.0 2011-12-31 15:00:00 7926.529376 20.625 1.0 2011-12-31 16:00:00 7901.826990 20.325 1.0 2011-12-31 17:00:00 7255.721350 19.850 1.0 2011-12-31 18:00:00 6792.503352 19.025 1.0 ... ... ... ... 2014-12-31 09:00:00 8139.251100 21.600 0.0 2014-12-31 10:00:00 7818.461408 20.300 0.0 2014-12-31 11:00:00 7801.201802 19.650 0.0 2014-12-31 12:00:00 7516.472988 18.100 0.0 2014-12-31 13:00:00 7571.301440 17.200 0.0 <p>26304 rows \u00d7 3 columns</p> <p>Rolling window aggregation is used to calculate statistics over a sliding window of time. For example, the 24h rolling average is the average of the last 24 hours of data. As with the <code>resample</code> method, it is very important to use the <code>closed='left'</code> and <code>center=False</code> arguments correctly to avoid introducing future information into the training (data leakage).</p> In\u00a0[5]: Copied! <pre># Rolling mean for 4 hours\n# ==============================================================================\ndata.rolling(window=4, min_periods=4, closed='left', center=False).mean()\n</pre> # Rolling mean for 4 hours # ============================================================================== data.rolling(window=4, min_periods=4, closed='left', center=False).mean() Out[5]: Demand Temperature Holiday Time 2011-12-31 14:00:00 NaN NaN NaN 2011-12-31 15:00:00 NaN NaN NaN 2011-12-31 16:00:00 NaN NaN NaN 2011-12-31 17:00:00 NaN NaN NaN 2011-12-31 18:00:00 7932.567104 20.50625 1.0 ... ... ... ... 2014-12-31 09:00:00 8490.517461 23.71250 0.0 2014-12-31 10:00:00 8482.825404 23.41250 0.0 2014-12-31 11:00:00 8314.896216 22.67500 0.0 2014-12-31 12:00:00 8076.417548 21.30000 0.0 2014-12-31 13:00:00 7818.846825 19.91250 0.0 <p>26304 rows \u00d7 3 columns</p> <p>The average values for <code>2011-12-31 18:00:00</code> are calculated from the values of the previous 4 hours (from 14:00:00 to 17:00:00).</p> In\u00a0[6]: Copied! <pre># '2011-12-31 18:00:00' mean\n# ==============================================================================\ndata.iloc[0:4, :].mean()\n</pre> # '2011-12-31 18:00:00' mean # ============================================================================== data.iloc[0:4, :].mean() Out[6]: <pre>Demand         7932.567104\nTemperature      20.506250\nHoliday           1.000000\ndtype: float64</pre> <p> \u26a0 Warning </p> <p>When transforming time series data, such as aggregating, it is very important to avoid data leakage, which means using information from the future to calculate the current value.</p>"},{"location":"faq/time-series-aggregation.html#time-series-aggregation","title":"Time series aggregation\u00b6","text":""},{"location":"faq/time-series-aggregation.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/time-series-aggregation.html#data","title":"Data\u00b6","text":""},{"location":"faq/time-series-aggregation.html#change-frequency-resample","title":"Change frequency (resample)\u00b6","text":""},{"location":"faq/time-series-aggregation.html#rolling-window-aggregation","title":"Rolling window aggregation\u00b6","text":""},{"location":"introduction-forecasting/introduction-forecasting.html","title":"Introduction to forecasting","text":""},{"location":"introduction-forecasting/introduction-forecasting.html#time-series-and-forecasting","title":"Time series and forecasting","text":"<p>A time series is a sequence of data arranged chronologically and spaced at equal or irregular intervals. The forecasting process consists of predicting the future value of a time series, either by modeling the series solely based on its past behavior (autoregressive) or by incorporating other external variables.</p> <p> </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#machine-learning-for-forecasting","title":"Machine learning for forecasting","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model.</p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <p>This type of transformation also allows to include additional variables.</p> <p> Time series transformation including an exogenous variable. </p> <p>Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ... p are considered predictors for the target quantity of the time series at time step p+1. </p> <p> Diagram of training a machine learning model with time series data. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#single-step-forecasting","title":"Single-step forecasting","text":"<p>Single-step prediction is used when the goal is to predict only the next value of the series.</p> <p> Diagram of single-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-step-forecasting","title":"Multi-step forecasting","text":"<p>When working with time series, it is seldom needed to predict only the next element in the series (t+1). Instead, the most common goal is to predict a whole future interval (t+1, ..., t+n)  or a far point in time (t+n). Several strategies allow generating this type of prediction.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting","text":"<p>Since the value t(n-1) is required to predict t(n), and t(n-1) is unknown, a recursive process is applied in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting and can be easily generated with the <code>ForecasterAutoreg</code> class.</p> <p> Diagram of recursive multi-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#direct-multi-step-forecasting","title":"Direct multi-step forecasting","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. This entire process is automated in the <code>ForecasterAutoregDirect</code> class. </p> <p> Diagram of direct multi-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multiple-output-forecasting","title":"Multiple output forecasting","text":"<p>Some machine learning models, such as long short-term memory (LSTM) neural network, can predict simultaneously several values of a sequence (one-shot). This strategy is not currently implemented in skforecast library.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#global-forecasting-models","title":"Global forecasting models","text":"<p>Univariate time series forecasting models a single time series as a linear or nonlinear combination of its lags, using past values of the series to predict its future. Global forecasting, involves building a single predictive model that considers all time series simultaneously. It attempts to capture the core patterns that govern the series, thereby mitigating the potential noise that each series might introduce. This approach is computationally efficient, easy to maintain, and can yield more robust generalizations across time series. There are two different strategies for global forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#independent-multi-series-forecasting","title":"Independent Multi-Series Forecasting","text":"<p>A single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied</p> <p> Diagram of recursive forecasting with multiple independent time series. </p> <p>The <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes cover this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#dependent-multi-series-forecasting-multivariate-time-series","title":"Dependent Multi-Series Forecasting (multivariate time series)","text":"<p>All series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-variate-series context. </p> <p>The <code>ForecasterAutoregMultiVariate</code> class covers this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRNN \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#exogenous-variables-features","title":"Exogenous variables (features)","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p> <p> Time series transformation including an exogenous variable. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-forecasting-models","title":"Backtesting forecasting models","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-without-refit","title":"Backtesting without refit","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)","text":"<p>In this approach, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin). </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin). </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit","text":"<p>The model is retrained every n iterations, a method often used when the model retraining is limited to certain time intervals, such as weekly, but a different time window, such as a day, needs to be used for prediction.</p> <p>This refit strategy can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p> Backtesting with intermittent refit. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-including-gap","title":"Backtesting including gap","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#which-strategy-should-i-use","title":"Which strategy should I use?","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy.</p> <p>To determine an appropriate strategy, factors such as use case characteristics, available computing resources, and time intervals between predictions must be considered. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li> <p>Prediction horizon: Suppose you need to predict the users of an App every Monday for the entire week. In this case, each iteration would be a seven-step prediction representing the seven days of the week.</p> </li> <li> <p>Refit strategy: Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to retrain the model when the error metric shows a consistent upward trend. This behavior can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with different <code>refit</code> strategies: <code>False</code> (no refit between predictions), refit every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (refit after every predictions). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>For a code example illustrating the backtesting process, refer to the Backtesting user guide.</p>"},{"location":"quick-start/forecaster-attributes.html","title":"Forecaster Attributes","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 5\n             )\n\nforecaster.fit(y=data['y'])\nforecaster\n</pre> # Download data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv') data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 5              )  forecaster.fit(y=data['y']) forecaster Out[2]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:35:06 \nLast fit date: 2024-07-29 16:35:06 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[3]: Copied! <pre># List of attributes\n# ==============================================================================\nfor attribute, value in forecaster.__dict__.items():\n    print(attribute)\n</pre> # List of attributes # ============================================================================== for attribute, value in forecaster.__dict__.items():     print(attribute) <pre>regressor\ntransformer_y\ntransformer_exog\nweight_func\nsource_code_weight_func\ndifferentiation\ndifferentiator\nlast_window\nindex_type\nindex_freq\ntraining_range\nincluded_exog\nexog_type\nexog_dtypes\nexog_col_names\nX_train_col_names\nin_sample_residuals\nout_sample_residuals\nin_sample_residuals_by_bin\nout_sample_residuals_by_bin\nfitted\ncreation_date\nfit_date\nskforecast_version\npython_version\nforecaster_id\nlags\nmax_lag\nwindow_size\nwindow_size_diff\nbinner_kwargs\nbinner\nbinner_intervals\nfit_kwargs\n</pre> In\u00a0[4]: Copied! <pre># Forecaster regressor\n# ==============================================================================\nforecaster.regressor\n</pre> # Forecaster regressor # ============================================================================== forecaster.regressor Out[4]: <pre>RandomForestRegressor(random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestRegressor?Documentation for RandomForestRegressoriFitted<pre>RandomForestRegressor(random_state=123)</pre> In\u00a0[5]: Copied! <pre># Show regressor parameters\n# ==============================================================================\nforecaster.regressor.get_params(deep=True)\n</pre> # Show regressor parameters # ============================================================================== forecaster.regressor.get_params(deep=True) Out[5]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'criterion': 'squared_error',\n 'max_depth': None,\n 'max_features': 1.0,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 123,\n 'verbose': 0,\n 'warm_start': False}</pre> <p> \u270e Note </p> <p>In the forecasters that follows a Direct Strategy, one instance of the regressor is trained for each step. All of them are stored in <code>self.regressors_</code></p> In\u00a0[6]: Copied! <pre># Forecaster lags\n# ==============================================================================\nforecaster.lags\n</pre> # Forecaster lags # ============================================================================== forecaster.lags Out[6]: <pre>array([1, 2, 3, 4, 5])</pre> In\u00a0[7]: Copied! <pre># Forecaster last window\n# ==============================================================================\nforecaster.last_window\n</pre> # Forecaster last window # ============================================================================== forecaster.last_window Out[7]: y datetime 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p> \ud83d\udca1 Tip </p> <p>Learn how to get your forecasters into production and get the most out of them with <code>last_window</code>. Using forecasting models in production.</p> In\u00a0[8]: Copied! <pre># Forecaster window size\n# ==============================================================================\nforecaster.window_size\n</pre> # Forecaster window size # ============================================================================== forecaster.window_size Out[8]: <pre>5</pre> <p> \u270e Note </p> <p>In the forecasters that follows a Direct Strategy and in the  Global Forecasting Models: Independent multi-series forecasting this parameter is a <code>dict</code> containing the residuals for each regressor/serie.</p> In\u00a0[9]: Copied! <pre># Forecaster in-sample residuals\n# ==============================================================================\nprint(\"Length:\", len(forecaster.in_sample_residuals))\nforecaster.in_sample_residuals[:5]\n</pre> # Forecaster in-sample residuals # ============================================================================== print(\"Length:\", len(forecaster.in_sample_residuals)) forecaster.in_sample_residuals[:5] <pre>Length: 199\n</pre> Out[9]: <pre>array([-0.10463712, -0.03318622, -0.00291908, -0.02020715,  0.01054163])</pre> In\u00a0[10]: Copied! <pre># Forecaster out-of-sample residuals\n# ==============================================================================\nforecaster.out_sample_residuals\n</pre> # Forecaster out-of-sample residuals # ============================================================================== forecaster.out_sample_residuals"},{"location":"quick-start/forecaster-attributes.html#understanding-the-forecaster-attributes","title":"Understanding the forecaster attributes\u00b6","text":"<p>During the process of creating and training a forecaster, the object stores a lot of information in its attributes that can be useful to the user. We will explore the main attributes included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p>"},{"location":"quick-start/forecaster-attributes.html#create-and-train-a-forecaster","title":"Create and train a forecaster\u00b6","text":"<p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"quick-start/forecaster-attributes.html#regressor","title":"Regressor\u00b6","text":"<p>Skforecast is a Python library that facilitates using <code>scikit-learn</code> regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API.</p>"},{"location":"quick-start/forecaster-attributes.html#lags","title":"Lags\u00b6","text":"<p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p>"},{"location":"quick-start/forecaster-attributes.html#last-window","title":"Last window\u00b6","text":"<p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p>"},{"location":"quick-start/forecaster-attributes.html#window-size","title":"Window size\u00b6","text":"<p>The size of the data window needed to create the predictors. It is equal to <code>forecaster.max_lag</code>.</p>"},{"location":"quick-start/forecaster-attributes.html#in-sample-residuals","title":"In-sample residuals\u00b6","text":"<p>Residuals from models predicting training data. Only stored up to 1000 values. If <code>transformer_series</code> is not <code>None</code>, the residuals are stored in the transformed scale.</p>"},{"location":"quick-start/forecaster-attributes.html#out-of-sample-residuals","title":"Out-of-sample residuals\u00b6","text":"<p>Residuals from models predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <p>As no values have been added, the parameter is <code>None</code>.</p>"},{"location":"quick-start/forecaster-parameters.html","title":"Understanding the forecaster parameters","text":"<p>Understanding what can be done when initializing a forecaster with skforecast can have a significant impact on the accuracy and effectiveness of the model. This guide highlights key considerations to keep in mind when initializing a forecaster and how these functionalities can be used to create more powerful and accurate forecasting models in Python.</p> <p>We will explore the arguments that can be included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = None,\n                 lags             = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 binner_kwargs    = None,\n                 forecaster_id    = None\n             )\n</code></pre> <p>Tip</p> <p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"quick-start/forecaster-parameters.html#general-parameters","title":"General parameters","text":""},{"location":"quick-start/forecaster-parameters.html#regressor","title":"Regressor","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API. Therefore, any of these regressors can be used to create a forecaster:</p> <ul> <li> <p>HistGradientBoostingRegressor</p> </li> <li> <p>LGBMRegressor</p> </li> <li> <p>XGBoost</p> </li> <li> <p>CatBoost</p> </li> </ul> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = None\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#lags","title":"Lags","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>Learn more about machine learning for forecasting. </p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <pre><code># Create a forecaster using 5 lags\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = 5\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#transformers","title":"Transformers","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <p>Both arguments expect an instance of a transformer (preprocessor) compatible with the <code>scikit-learn</code> preprocessing API with the methods: <code>fit</code>, <code>transform</code>, <code>fit_transform</code> and, <code>inverse_transform</code>.</p> <p>More information: Scikit-learn transformers and pipelines.</p> <p>Example</p> <p>In this example, a scikit-learn <code>StandardScaler</code> preprocessor is used for both the time series and the exogenous variables.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = StandardScaler()\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#custom-weights","title":"Custom weights","text":"<p>The <code>weight_func</code> parameter allows the user to define custom weights for each observation in the time series. These custom weights can be used to assign different levels of importance to different time periods. For example, assign higher weights to recent data points and lower weights to older data points to emphasize the importance of recent observations in the forecast model.</p> <p>More information: Weighted time series forecasting.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = custom_weights\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#differentiation","title":"Differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Skforecast, version 0.10.0 or higher, introduces a novel differentiation parameter within its Forecasters. </p> <p>More information: Time series differentiation.</p> <p>Warning</p> <p>The <code>differentiation</code> parameter is only available for the <code>ForecasterAutoreg</code> and <code>ForecasterAutoregMultiSeries</code> in the following versions it will be incorporated to the rest of the Forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = 1\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#inclusion-of-kwargs-in-the-regressor-fit-method","title":"Inclusion of kwargs in the regressor fit method","text":"<p>Some regressors include the possibility to add some additional configuration during the fitting method. The predictor parameter <code>fit_kwargs</code> allows these arguments to be set when the forecaster is declared.</p> <p>Danger</p> <p>To add weights to the forecaster, it must be done through the <code>weight_func</code> argument and not through a <code>fit_kwargs</code>.</p> <p>Example</p> <p>The following example demonstrates the inclusion of categorical features in an LGBM regressor. This must be done during the <code>LGBMRegressor</code> fit method. Fit parameters lightgbm</p> <p>More information: Categorical features.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = {'categorical_feature': ['exog_1', 'exog_2']}\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#intervals-conditioned-on-predicted-values-binned-residuals","title":"Intervals conditioned on predicted values (binned residuals)","text":"<p>When creating prediction intervals, skforecast uses a <code>sklearn.preprocessing.KBinsDiscretizer</code> to bin the residuals. The <code>binner_kwargs</code> parameter allows the user to pass additional arguments to the <code>KBinsDiscretizer</code> object. </p> <p>More information: Intervals conditioned on predicted values (binned residuals).</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 binner_kwargs    = {'n_bins': 10}\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#forecaster-id","title":"Forecaster ID","text":"<p>Name used as an identifier of the forecaster. It may be used, for example to identify the time series being modeled.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#direct-multi-step-parameters","title":"Direct multi-step parameters","text":"<p>For the Forecasters that follow a direct multi-step strategy (<code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>), there are two additional parameters in addition to those mentioned above.</p>"},{"location":"quick-start/forecaster-parameters.html#steps","title":"Steps","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. </p> <p>The number of models to be trained is specified by the <code>steps</code> parameter.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoregDirect(\n                 regressor        = RandomForestRegressor(),\n                 steps            = 5,\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#number-of-jobs","title":"Number of jobs","text":"<p>The <code>n_jobs</code> parameter allows multi-process parallelization to train regressors for all <code>steps</code> simultaneously. </p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoregDirect(\n                 regressor        = RandomForestRegressor(),\n                 steps            = 5,\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 n_jobs           = 'auto',\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/how-to-install.html","title":"Installation Guide","text":"<p>This guide will help you install <code>skforecast</code>, a powerful library for time series forecasting in Python. The default installation of <code>skforecast</code> includes only the essential dependencies required for basic functionality. Additional optional dependencies can be installed for extended features.</p> <p> </p>"},{"location":"quick-start/how-to-install.html#basic-installation","title":"Basic installation","text":"<p>To install the basic version of <code>skforecast</code> with its core dependencies, run:</p> <pre><code>pip install skforecast\n</code></pre> <p>Specific version:</p> <pre><code>pip install skforecast==0.14.0\n</code></pre> <p>Latest (unstable):</p> <pre><code>pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master\n</code></pre> <p>The following dependencies are installed with the default installation:</p> <ul> <li>numpy&gt;=1.22</li> <li>pandas&gt;=1.5</li> <li>tqdm&gt;=4.57</li> <li>scikit-learn&gt;=1.2</li> <li>optuna&gt;=2.10</li> <li>joblib&gt;=1.1</li> <li>numba&gt;=0.59</li> </ul>"},{"location":"quick-start/how-to-install.html#optional-dependencies","title":"Optional dependencies","text":"<p>To install the full version with all optional dependencies:</p> <pre><code>pip install skforecast[full]\n</code></pre> <p>For specific use cases, you can install these dependencies as needed:</p>"},{"location":"quick-start/how-to-install.html#sarimax","title":"Sarimax","text":"<pre><code>pip install skforecast[sarimax]\n</code></pre> <ul> <li>statsmodels&gt;=0.12, &lt;0.15</li> </ul>"},{"location":"quick-start/how-to-install.html#plotting","title":"Plotting","text":"<pre><code>pip install skforecast[plotting]\n</code></pre> <ul> <li>matplotlib&gt;=3.3, &lt;3.10</li> <li>seaborn&gt;=0.11, &lt;0.14</li> <li>statsmodels&gt;=0.12, &lt;0.15</li> </ul>"},{"location":"quick-start/how-to-install.html#deeplearning","title":"Deeplearning","text":"<pre><code>pip install skforecast[deeplearning]\n</code></pre> <ul> <li>matplotlib&gt;=3.3, &lt;3.10</li> <li>keras&gt;=2.6, &lt;4.0</li> </ul>"},{"location":"quick-start/quick-start-skforecast.html","title":"Quick start","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom skforecast.datasets import load_demo_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error  from skforecast.datasets import load_demo_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = load_demo_dataset()\ndata.head(5)\n</pre> # Download data # ============================================================================== data = load_demo_dataset() data.head(5) Out[2]: <pre>datetime\n1991-07-01    0.429795\n1991-08-01    0.400906\n1991-09-01    0.432159\n1991-10-01    0.492543\n1991-11-01    0.502369\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[3]: Copied! <pre># Data partition train-test\n# ==============================================================================\nend_train = '2005-06-01 23:59:00'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend()\nplt.show();\n</pre> # Data partition train-test # ============================================================================== end_train = '2005-06-01 23:59:00' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend() plt.show(); <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[4]: Copied! <pre># Create and fit a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data.loc[:end_train])\nforecaster\n</pre> # Create and fit a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              )  forecaster.fit(y=data.loc[:end_train]) forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:37:54 \nLast fit date: 2024-07-29 16:37:55 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> <p> \ud83d\udca1 Tip </p> <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data.loc[end_train:]))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data.loc[end_train:])) predictions.head(3) Out[5]: <pre>2005-07-01    1.020833\n2005-08-01    1.021721\n2005-09-01    1.093488\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error on test data\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data.loc[end_train:],\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error on test data # ============================================================================== error_mse = mean_squared_error(                 y_true = data.loc[end_train:],                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.006023571131075229\n</pre> In\u00a0[8]: Copied! <pre># Backtesting\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster            = forecaster,\n                                   y                     = data,\n                                   steps                 = 10,\n                                   metric                = 'mean_squared_error',\n                                   initial_train_size    = len(data.loc[:end_train]),\n                                   fixed_train_size      = False,\n                                   gap                   = 0,\n                                   allow_incomplete_fold = True,\n                                   refit                 = True,\n                                   n_jobs                = 'auto',\n                                   verbose               = True,\n                                   show_progress         = True\n                               )\n\nprint(f\"Backtest error: {metric}\")\n</pre> # Backtesting # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster            = forecaster,                                    y                     = data,                                    steps                 = 10,                                    metric                = 'mean_squared_error',                                    initial_train_size    = len(data.loc[:end_train]),                                    fixed_train_size      = False,                                    gap                   = 0,                                    allow_incomplete_fold = True,                                    refit                 = True,                                    n_jobs                = 'auto',                                    verbose               = True,                                    show_progress         = True                                )  print(f\"Backtest error: {metric}\") <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 4\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 6 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=178)\n    Validation: 2006-05-01 00:00:00 -- 2007-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2007-02-01 00:00:00  (n=188)\n    Validation: 2007-03-01 00:00:00 -- 2007-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2007-12-01 00:00:00  (n=198)\n    Validation: 2008-01-01 00:00:00 -- 2008-06-01 00:00:00  (n=6)\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error:    mean_squared_error\n0            0.006473\n</pre> In\u00a0[9]: Copied! <pre># Grid search hyperparameter and lags\n# ==============================================================================\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data,\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 10,\n                   refit              = False,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   n_jobs             = 'auto',\n                   verbose            = False,\n                   show_progress      = True\n               )\n</pre> # Grid search hyperparameter and lags # ============================================================================== # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data,                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 10,                    refit              = False,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    n_jobs             = 'auto',                    verbose            = False,                    show_progress      = True                ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 5, 'n_estimators': 50}\n  Backtesting metric: 0.016405876616325348\n\n</pre> In\u00a0[10]: Copied! <pre># Grid results\n# ==============================================================================\nresults_grid\n</pre> # Grid results # ============================================================================== results_grid Out[10]: lags lags_label params mean_squared_error max_depth n_estimators 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.016406 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.017525 10 50 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.017525 15 50 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.019588 5 100 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.019902 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.019902 15 100 3 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.040534 10 100 5 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.040534 15 100 2 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.044829 10 50 4 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.044829 15 50 14 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.044924 10 50 16 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.044924 15 50 0 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.045323 5 50 12 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.047795 5 50 1 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.050033 5 100 13 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.056595 5 100 15 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.059472 10 100 17 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.059472 15 100 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[11]: Copied! <pre># Print forecaster information\n# ==============================================================================\nforecaster\n</pre> # Print forecaster information # ============================================================================== forecaster Out[11]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(max_depth=5, n_estimators=50, random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 50, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:37:54 \nLast fit date: 2024-07-29 16:37:56 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre>"},{"location":"quick-start/quick-start-skforecast.html#quick-start-skforecast","title":"Quick start skforecast\u00b6","text":"<p>Welcome to a quick start guide to using skforecast! In this guide, we will provide you with a code example that demonstrates how to create, validate, and optimize a recursive multi-step forecaster, <code>ForecasterAutoreg</code>, using skforecast.</p> <p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>If you need more detailed documentation or guidance, you can visit the User Guides section.</p> <p>Without further ado, let's jump into the code example!</p>"},{"location":"quick-start/quick-start-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"quick-start/quick-start-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"quick-start/quick-start-skforecast.html#train-a-forecaster","title":"Train a forecaster\u00b6","text":"<p>Let's start by training a forecaster! For a more in-depth guide to using <code>ForecasterAutoreg</code>, visit the User guide ForecasterAutoreg.</p>"},{"location":"quick-start/quick-start-skforecast.html#prediction","title":"Prediction\u00b6","text":"<p>After training the forecaster, the <code>predict</code> method can be used to make predictions for the future $n$ steps.</p>"},{"location":"quick-start/quick-start-skforecast.html#backtesting-forecaster-validation","title":"Backtesting: forecaster validation\u00b6","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data. For more detailed documentation on backtesting, visit: User guide Backtesting forecaster.</p>"},{"location":"quick-start/quick-start-skforecast.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The Skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search. For more detailed documentation on Hyperparameter tuning, visit: Hyperparameter tuning and lags selection.</p>"},{"location":"releases/releases.html","title":"Changelog","text":"<p>All significant changes to this project are documented in this release file.</p> Legend Feature New feature Enhancement Improvement in existing functionality API Change Changes in the API Fix Bug fix"},{"location":"releases/releases.html#0.14.0","title":"0.14.0 In development","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature WINDOW FEATURES......... <code>RollingFeatures</code></p> </li> <li> <p>Enhancement Refactor the prediction process in <code>ForecasterAutoregMultiSeries</code> to improve performance when predicting multiple series.</p> </li> <li> <p>Enhancement The bootstrapping process in the <code>predict_bootstrapping</code> method of all forecasters has been optimized to improve performance. This may result in slightly different results when using the same seed as in previous versions.</p> </li> <li> <p>API Change <code>ForecasterAutoregCustom</code> has been deprecated. Window features can be added using the <code>window_features</code> argument in the <code>ForecasterAutoreg</code>.</p> </li> <li> <p>API Change The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the <code>skforecast.Sarimax.Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>numba&gt;=0.59</code> as hard dependency.</p> </li> <li> <p>Added <code>window_features</code> argument to all forecasters. This argument allows the user to add window features to the training matrix. See <code>RollingFeatures</code>.</p> </li> <li> <p>Differentiation has been extended to all forecasters. The <code>differentiation</code> argument has been added to all forecasters to model the n-order differentiated time series.</p> </li> <li> <p>Create <code>transform_numpy</code> function in the <code>utils</code> module to carry out the transformation of the modeled time series and exogenous variables as numpy arrays.</p> </li> <li> <p><code>random_state</code> argument in the <code>fit</code> method of <code>ForecasterAutoreg</code> to set a seed for the random generator so that the stored sample residuals are always deterministic.</p> </li> <li> <p>New argument <code>method</code> in <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, <code>bayesian_search_forecaster</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> that allows the user to select the method used to identify the best combination of hyperparameters and lags. The available methods are <code>'backtesting'</code> (default) and <code>'one_step_ahead'</code>.</p> </li> <li> <p>New private method <code>_train_test_split_one_step_ahead</code> in all forecasters.</p> </li> <li> <p>New private function <code>_calculate_metrics_one_step_ahead</code> to <code>model_selection</code> module to calculate the metrics when predicting one step ahead.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregCustom</code> has been deprecated. Window features can be added using the <code>window_features</code> argument in the <code>ForecasterAutoreg</code>.</p> </li> <li> <p>Refactor <code>recursive_predict</code> in <code>ForecasterAutoregMultiSeries</code> to predict all series at once and include option of adding residuals. This improves performance when predicting multiple series.</p> </li> <li> <p>Refactor <code>predict_bootstrapping</code> in all Forecasters. The bootstrapping process has been optimized to improve performance. This may result in slightly different results when using the same seed as in previous versions.</p> </li> <li> <p>Change the default value of <code>encoding</code> to <code>ordinal</code> in <code>ForecasterAutoregMultiSeries</code>. This will avoid conflicts if the regressor does not support categorical variables by default.</p> </li> <li> <p>Removed argument <code>engine</code> from <code>bayesian_search_forecaster</code> and <code>bayesian_search_forecaster_multiseries</code>.</p> </li> <li> <p>The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the <code>skforecast.Sarimax.Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.</p> </li> <li> <p><code>initialize_lags</code> now returns the maximum lag, <code>max_lag</code>.</p> </li> <li> <p>Removed attribute <code>window_size_diff</code> from all Forecasters. The window size extended by the order of differentiation is now calculated on <code>window_size</code>.</p> </li> <li> <p><code>lags</code> can be None when initializing any Forecaster that includes window features.</p> </li> <li> <p><code>model_selection</code> module has been divided internally into different modules to improve code organization (<code>_validation</code>, <code>_search</code>, <code>_split</code>).</p> </li> <li> <p>Added <code>feature_selection</code> module. The functions <code>select_features</code> and <code>select_features_multiseries</code> have been moved to this module.</p> </li> <li> <p>Renamed attributes in all Forecasters:</p> <ul> <li> <p><code>encoding_mapping</code> has been renamed to <code>encoding_mapping_</code>.</p> </li> <li> <p><code>last_window</code> has been renamed to <code>last_window_</code>.</p> </li> <li> <p><code>index_type</code> has been renamed to <code>index_type_</code>.</p> </li> <li> <p><code>index_freq</code> has been renamed to <code>index_freq_</code>.</p> </li> <li> <p><code>training_range</code> has been renamed to <code>training_range_</code>.</p> </li> <li> <p><code>series_col_names</code> has been renamed to <code>series_names_in_</code>.</p> </li> <li> <p><code>included_exog</code> has been renamed to <code>exog_in_</code>.</p> </li> <li> <p><code>exog_type</code> has been renamed to <code>exog_type_in_</code>.</p> </li> <li> <p><code>exog_dtypes</code> has been renamed to <code>exog_dtypes_in_</code>.</p> </li> <li> <p><code>exog_col_names</code> has been renamed to <code>exog_names_in_</code>.</p> </li> <li> <p><code>series_X_train</code> has been renamed to <code>X_train_series_names_in_</code>.</p> </li> <li> <p><code>X_train_col_names</code> has been renamed to <code>X_train_features_names_out_</code>.</p> </li> <li> <p><code>binner_intervals</code> has been renamed to <code>binner_intervals_</code>.</p> </li> <li> <p><code>in_sample_residuals</code> has been renamed to <code>in_sample_residuals_</code>.</p> </li> <li> <p><code>out_sample_residuals</code> has been renamed to <code>out_sample_residuals_</code>.</p> </li> <li> <p><code>fitted</code> has been renamed to <code>is_fitted</code>.</p> </li> </ul> </li> <li> <p>Renamed arguments in different functions and methods:</p> <ul> <li> <p><code>in_sample_residuals</code> has been renamed to <code>use_in_sample_residuals</code>.</p> </li> <li> <p><code>binned_residuals</code> has been renamed to <code>use_binned_residuals</code>.</p> </li> <li> <p><code>series_col_names</code> has been renamed to <code>series_names_in_</code> in the <code>check_predict_input</code>, <code>check_preprocess_exog_multiseries</code> and <code>initialize_transformer_series</code> functions in the <code>utils</code> module.</p> </li> <li> <p><code>series_X_train</code> has been renamed to <code>X_train_series_names_in_</code> in the <code>prepare_levels_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_col_names</code> has been renamed to <code>exog_names_in_</code> in the <code>check_predict_input</code> and <code>check_preprocess_exog_multiseries</code> functions in the <code>utils</code> module.</p> </li> <li> <p><code>index_type</code> has been renamed to <code>index_type_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>index_freq</code> has been renamed to <code>index_freq_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>included_exog</code> has been renamed to <code>exog_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_type</code> has been renamed to <code>exog_type_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_dtypes</code> has been renamed to <code>exog_dtypes_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>fitted</code> has been renamed to <code>is_fitted</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>use_in_sample</code> has been renamed to <code>use_in_sample_residuals</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>in_sample_residuals</code> has been renamed to <code>use_in_sample_residuals</code> in the <code>backtesting_forecaster</code>, <code>backtesting_forecaster_multiseries</code> and <code>check_backtesting_input</code> (<code>utils</code> module) functions.</p> </li> </ul> </li> <li> <p><code>binned_residuals</code> has been renamed to <code>use_binned_residuals</code> in the <code>backtesting_forecaster</code> function.</p> <ul> <li> <p><code>in_sample_residuals</code> has been renamed to <code>in_sample_residuals_</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>out_sample_residuals</code> has been renamed to <code>out_sample_residuals_</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>last_window</code> has been renamed to <code>last_window_</code> in the <code>preprocess_levels_self_last_window_multiseries</code> function in the <code>utils</code> module.</p> </li> </ul> </li> </ul> <p>Fixed</p> <ul> <li>...</li> </ul>"},{"location":"releases/releases.html#0.13.0","title":"0.13.0 Aug 01, 2024","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> are able to predict series not seen during training. This is useful when the user wants to predict a new series that was not included in the training data.</p> </li> <li> <p>Feature <code>encoding</code> can be set to <code>None</code> in Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. This option does not add the encoded series ids to the regressor training matrix.</p> </li> <li> <p>Feature New <code>create_predict_X</code> method in all recursive and direct Forecasters to allow the user to inspect the matrix passed to the predict method of the regressor.</p> </li> <li> <p>Feature New module <code>metrics</code> with functions to calculate metrics for time series forecasting such as <code>mean_absolute_scaled_error</code> and <code>root_mean_squared_scaled_error</code>. Visit Time Series Forecasting Metrics for more information.</p> </li> <li> <p>Feature New argument <code>add_aggregated_metric</code> in <code>backtesting_forecaster_multiseries</code> to include, in addition to the metrics for each level, the aggregated metric of all levels using the average (arithmetic mean), weighted average (weighted by the number of predicted values of each level) or pooling (the values of all levels are pooled and then the metric is calculated).</p> </li> <li> <p>Feature New argument <code>skip_folds</code> in <code>model_selection</code> and <code>model_selection_multiseries</code> functions. It allows the user to skip some folds during backtesting, which can be useful to speed up the backtesting process and thus the hyperparameter search.</p> </li> <li> <p>API Change backtesting procedures now pass the training series to the metric functions so it can be used to calculate metrics that depend on the training series.</p> </li> <li> <p>API Change Changed the default value of the <code>transformer_series</code> argument to <code>None</code> in the Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. In most cases, tree-based models are used as regressors in these forecasters, so no transformation is applied by default as it is not necessary.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>python 3.12</code>.</p> </li> <li> <p><code>keras</code> has been added as an optional dependency, tag deeplearning, to use the <code>ForecasterRnn</code>.</p> </li> <li> <p><code>PyTorch</code> backend for the <code>ForecasterRnn</code>.</p> </li> <li> <p>New <code>create_predict_X</code> method in all recursive and direct Forecasters to allow the user to inspect the matrix passed to the predict method of the regressor.</p> </li> <li> <p>New <code>_create_predict_inputs</code> method in all Forecasters to unify the inputs of the predict methods.</p> </li> <li> <p>New plot function <code>plot_prediction_intervals</code> in the <code>plot</code> module to plot predicted intervals.</p> </li> <li> <p>New module <code>metrics</code> with functions to calculate metrics for time series forecasting such as <code>mean_absolute_scaled_error</code> and <code>root_mean_squared_scaled_error</code>.</p> </li> <li> <p>New argument <code>skip_folds</code> in <code>model_selection</code> and <code>model_selection_multiseries</code> functions. It allows the user to skip some folds during backtesting, which can be useful to speed up the backtesting process and thus the hyperparameter search.</p> </li> <li> <p>New function <code>plot_prediction_intervals</code> in module <code>plot</code>.</p> </li> <li> <p>Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> are able to predict series not seen during training. This is useful when the user wants to predict a new series that was not included in the training data.</p> </li> <li> <p><code>encoding</code> can be set to <code>None</code> in Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. This option does not add the encoded series ids to the regressor training matrix.</p> </li> <li> <p>New argument <code>add_aggregated_metric</code> in <code>backtesting_forecaster_multiseries</code> to include, in addition to the metrics for each level, the aggregated metric of all levels using the average (arithmetic mean), weighted average (weighted by the number of predicted values of each level) or pooling (the values of all levels are pooled and then the metric is calculated).</p> </li> <li> <p>New argument <code>aggregate_metric</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> to select the aggregation method used to combine the metric(s) of all levels during the hyperparameter search. The available methods are: mean (arithmetic mean), weighted (weighted by the number of predicted values of each level) and pool (the values of all levels are pooled and then the metric is calculated). If more than one metric and/or aggregation method is used, all are reported in the results, but the first of each is used to select the best model.</p> </li> <li> <p>New class <code>DateTimeFeatureTransformer</code> and function <code>create_datetime_features</code> in the <code>preprocessing</code> module to create datetime and calendar features from a datetime index.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.8</code> compatibility.</p> </li> <li> <p>Update project dependencies.</p> </li> <li> <p>Change default value of <code>n_bins</code> when initializing <code>ForecasterAutoreg</code> from 15 to 10.</p> </li> <li> <p>Refactor <code>_recursive_predict</code> in all recursive forecasters.</p> </li> <li> <p>Change default value of <code>transformer_series</code> when initializing <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> from <code>StandardScaler()</code> to <code>None</code>.</p> </li> <li> <p>Function <code>_get_metric</code> moved from <code>model_selection</code> to <code>metrics</code>.</p> </li> <li> <p>Change information message when <code>verbose</code> is <code>True</code> in <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p><code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit</code> in <code>utils</code> return <code>n_jobs = 1</code> if regressor is <code>LGBMRegressor</code>. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</p> </li> <li> <p><code>metric_values</code> returned by <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code> is a <code>pandas DataFrame</code> with one column per metric instead of a <code>list</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>Bug fix in <code>backtesting_forecaster_multiseries</code> using a <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiSeriesCustom</code> that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.12.1","title":"0.12.1 May 20, 2024","text":"<p>Fix This is a minor release to fix a bug.</p> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Bug fix when storing <code>last_window</code> using a [<code>ForecasterAutoregMultiSeries</code>] that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.12.0","title":"0.12.0 May 05, 2024","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature Multiseries forecaster (Global Models) can be trained using series of different lengths and with different exogenous variables per series.</p> </li> <li> <p>Feature New functionality to select features using scikit-learn selectors (<code>select_features</code> and <code>select_features_multiseries</code>).</p> </li> <li> <p>Feature Added new forecaster <code>ForecasterRnn</code> to create forecasting models based on deep learning (RNN and LSTM).</p> </li> <li> <p>Feature New method to predict intervals conditioned on the range of the predicted values. This is can help to improve the interval coverage when the residuals are not homoscedastic (<code>ForecasterAutoreg</code>).</p> </li> <li> <p>Enhancement Bayesian hyperparameter search is now available for all multiseries forecasters using <code>optuna</code> as the search engine.</p> </li> <li> <p>Enhancement All Recursive Forecasters are now able to differentiate the time series before modeling it.</p> </li> <li> <p>API Change Changed the default value of the <code>transformer_series</code> argument to use a <code>StandardScaler()</code> in the Global Forecasters (<code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>bayesian_search_forecaster_multiseries</code> function to <code>model_selection_multiseries</code> module. This function performs a Bayesian hyperparameter search for the <code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code>, and <code>ForecasterAutoregMultiVariate</code> using <code>optuna</code> as the search engine.</p> </li> <li> <p><code>ForecasterAutoregMultiVariate</code> allows to include None when lags is a dict so that a series does not participate in the construction of X_train.</p> </li> <li> <p>The <code>output_file</code> argument has been added to the hyperparameter search functions in the <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> modules to save the results of the hyperparameter search in a tab-separated values (TSV) file.</p> </li> <li> <p>New argument <code>binned_residuals</code> in method <code>predict_interval</code> allows to condition the bootstraped residuals on range of the predicted values. </p> </li> <li> <p>Added <code>save_custom_functions</code> argument to the <code>save_forecaster</code> function in the <code>utils</code> module. If <code>True</code>, save custom functions used in the forecaster (<code>fun_predictors</code> and <code>weight_func</code>) as .py files. Custom functions must be available in the environment where the forecaster is loaded.</p> </li> <li> <p>Added <code>select_features</code> and <code>select_features_multiseries</code> functions to the <code>model_selection</code> and <code>model_selection_multiseries</code> modules to perform feature selection using scikit-learn selectors.</p> </li> <li> <p>Added <code>sort_importance</code> argument to <code>get_feature_importances</code> method in all Forecasters. If <code>True</code>, sort the feature importances in descending order.</p> </li> <li> <p>Added <code>initialize_lags_grid</code> function to <code>model_selection</code> module. This function initializes the lags to be used in the hyperparameter search functions in <code>model_selection</code> and <code>model_selection_multiseries</code>.</p> </li> <li> <p>Added <code>_initialize_levels_model_selection_multiseries</code> function to <code>model_selection_multiseries</code> module. This function initializes the levels of the series to be used in the model selection functions.</p> </li> <li> <p>Added <code>set_dark_theme</code> function to the <code>plot</code> module to set a dark theme for matplotlib plots.</p> </li> <li> <p>Allow tuple type for <code>lags</code> argument in all Forecasters.</p> </li> <li> <p>Argument <code>differentiation</code> in all Forecasters to model the n-order differentiated time series.</p> </li> <li> <p>Added <code>window_size_diff</code> attribute to all Forecasters. It stores the size of the window (<code>window_size</code>) extended by the order of differentiation. Added  to all Forecasters for API consistency.</p> </li> <li> <p>Added <code>store_last_window</code> parameter to <code>fit</code> method in Forecasters. If <code>True</code>, store the last window of the training data.</p> </li> <li> <p>Added <code>utils.set_skforecast_warnings</code> function to set the warnings of the skforecast package.</p> </li> <li> <p>Added new forecaster <code>ForecasterRnn</code> to create forecasting models based on deep learning (RNN and LSTM).</p> </li> <li> <p>Added new function <code>create_and_compile_model</code> to module <code>skforecast.ForecasterRnn.utils</code> to help to create and compile a RNN or LSTM models to be used in <code>ForecasterRnn</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated argument <code>lags_grid</code> in <code>bayesian_search_forecaster</code>. Use <code>search_space</code> to define the candidate values for the lags. This allows the lags to be optimized along with the other hyperparameters of the regressor in the bayesian search.</p> </li> <li> <p><code>n_boot</code> argument in <code>predict_interval</code>changed from 500 to 250.</p> </li> <li> <p>Changed the default value of the <code>transformer_series</code> argument to use a <code>StandardScaler()</code> in the Global Forecasters (<code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> </li> <li> <p>Refactor <code>utils.select_n_jobs_backtesting</code> to use the forecaster directly instead of <code>forecaster_name</code> and <code>regressor_name</code>.</p> </li> <li> <p>Remove <code>_backtesting_forecaster_verbose</code> in model_selection in favor of <code>_create_backtesting_folds</code>, (deprecated since 0.8.0).</p> </li> </ul> <p>Fixed</p> <ul> <li>Small bug in <code>utils.select_n_jobs_backtesting</code>, rename <code>ForecasterAutoregMultiseries</code> to <code>ForecasterAutoregMultiSeries</code>.</li> </ul>"},{"location":"releases/releases.html#0.11.0","title":"0.11.0 Nov 16, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p>New <code>predict_quantiles</code> method in all Autoreg Forecasters to calculate the specified quantiles for each step.</p> </li> <li> <p>Create <code>ForecasterBaseline.ForecasterEquivalentDate</code>, a Forecaster to create simple model that serves as a basic reference for evaluating the performance of more complex models.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>skforecast.datasets</code> module. It contains functions to load data for our examples and user guides.</p> </li> <li> <p>Added <code>predict_quantiles</code> method to all Autoreg Forecasters.</p> </li> <li> <p>Added <code>SkforecastVersionWarning</code> to the <code>exception</code> module. This warning notify that the skforecast version installed in the environment differs from the version used to initialize the forecaster when using <code>load_forecaster</code>.</p> </li> <li> <p>Create <code>ForecasterBaseline.ForecasterEquivalentDate</code>, a Forecaster to create simple model that serves as a basic reference for evaluating the performance of more complex models.</p> </li> </ul> <p>Changed</p> <ul> <li>Enhance the management of internal copying in skforecast to minimize the number of copies, thereby accelerating data processing.</li> </ul> <p>Fixed</p> <ul> <li> <p>Rename <code>self.skforcast_version</code> attribute to <code>self.skforecast_version</code> in all Forecasters.</p> </li> <li> <p>Fixed a bug where the <code>create_train_X_y</code> method did not correctly align lags and exogenous variables when the index was not a Pandas index in all Forecasters.</p> </li> </ul>"},{"location":"releases/releases.html#0.10.1","title":"0.10.1 Sep 26, 2023","text":"<p>This is a minor release to fix a bug when using <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</p> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Bug fix <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.10.0","title":"0.10.0 Sep 07, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API and can be used with the <code>ForecasterSarimax</code>.</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series using the new skforecast preprocessor <code>TimeSeriesDifferentiator</code>.</p> </li> </ul> <p>Added</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API.</p> </li> <li> <p>Added <code>skforecast.preprocessing.TimeSeriesDifferentiator</code> to preprocess time series by differentiating or integrating them (reverse differentiation).</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Refactor <code>ForecasterSarimax</code> to work with both skforecast Sarimax and pmdarima ARIMA models.</p> </li> <li> <p>Replace <code>setup.py</code> with <code>pyproject.toml</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.9.1","title":"0.9.1 Jul 14, 2023","text":"<p>The main changes in this release are:</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul>"},{"location":"releases/releases.html#0.9.0","title":"0.9.0 Jul 09, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> include the <code>n_jobs</code> argument in their <code>fit</code> method, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>All backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> can be trained using series of different lengths. This means that the model can handle datasets with different numbers of data points in each series.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>scikit-learn 1.3.x</code>.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to <code>fit</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> allow to use series of different lengths for training.</p> </li> <li> <p>Added <code>show_progress</code> to grid search functions.</p> </li> <li> <p>Added functions <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> to <code>utils</code> to select the number of jobs to use during multi-process parallelization.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Remove <code>get_feature_importance</code> in favor of <code>get_feature_importances</code> in all Forecasters, (deprecated since 0.8.0).</p> </li> <li> <p>The <code>model_selection._create_backtesting_folds</code> function now also returns the last window indices and whether or not to train the forecaster.</p> </li> <li> <p>The <code>model_selection</code> functions <code>_backtesting_forecaster_refit</code> and <code>_backtesting_forecaster_no_refit</code> have been unified in <code>_backtesting_forecaster</code>.</p> </li> <li> <p>The <code>model_selection_multiseries</code> functions <code>_backtesting_forecaster_multiseries_refit</code> and <code>_backtesting_forecaster_multiseries_no_refit</code> have been unified in <code>_backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>The <code>model_selection_sarimax</code> functions <code>_backtesting_refit_sarimax</code> and <code>_backtesting_no_refit_sarimax</code> have been unified in <code>_backtesting_sarimax</code>.</p> </li> <li> <p><code>utils.preprocess_y</code> allows a pandas DataFrame as input.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Ensure reproducibility of Direct Forecasters when using <code>predict_bootstrapping</code>, <code>predict_dist</code> and <code>predict_interval</code> with a <code>list</code> of steps.</p> </li> <li> <p>The <code>create_train_X_y</code> method returns a dict of pandas Series as <code>y_train</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. This ensures that each series has the appropriate index according to the step to be trained.</p> </li> <li> <p>The <code>filter_train_X_y_for_step</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> now updates the index of <code>X_train_step</code> to ensure correct alignment with <code>y_train_step</code>.</p> </li> </ul>"},{"location":"releases/releases.html#0.8.1","title":"0.8.1 May 27, 2023","text":"<p>Added</p> <ul> <li>Argument <code>store_in_sample_residuals=True</code> in <code>fit</code> method added to all forecasters to speed up functions such as backtesting.</li> </ul> <p>Changed</p> <ul> <li>Refactor <code>utils.exog_to_direct</code> and <code>utils.exog_to_direct_numpy</code> to increase performance.</li> </ul> <p>Fixed</p> <ul> <li><code>utils.check_exog_dtypes</code> now compares the <code>dtype.name</code> instead of the <code>dtype</code>. (suggested by Metaming https://github.com/Metaming)</li> </ul>"},{"location":"releases/releases.html#0.8.0","title":"0.8.0 May 16, 2023","text":"<p>Added</p> <ul> <li> <p>Added the <code>fit_kwargs</code> argument to all forecasters to allow the inclusion of additional keyword arguments passed to the regressor's <code>fit</code> method.</p> </li> <li> <p>Added the <code>set_fit_kwargs</code> method to set the <code>fit_kwargs</code> attribute.</p> </li> <li> <p>Support for <code>pandas 2.0.x</code>.</p> </li> <li> <p>Added <code>exceptions</code> module with custom warnings.</p> </li> <li> <p>Added function <code>utils.check_exog_dtypes</code> to issue a warning if exogenous variables are one of type <code>init</code>, <code>float</code>, or <code>category</code>. Raise Exception if <code>exog</code> has categorical columns with non integer values.</p> </li> <li> <p>Added function <code>utils.get_exog_dtypes</code> to get the data types of the exogenous variables included during the training of the forecaster model. </p> </li> <li> <p>Added function <code>utils.cast_exog_dtypes</code> to cast data types of the exogenous variables using a dictionary as a mapping.</p> </li> <li> <p>Added function <code>utils.check_select_fit_kwargs</code> to check if the argument <code>fit_kwargs</code> is a dictionary and select only the keys used by the <code>fit</code> method of the regressor.</p> </li> <li> <p>Added function <code>model_selection._create_backtesting_folds</code> to provide train/test indices (position) for backtesting functions.</p> </li> <li> <p>Added argument <code>gap</code> to functions in <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to omit observations between training and prediction.</p> </li> <li> <p>Added argument <code>show_progress</code> to functions <code>model_selection.backtesting_forecaster</code>, <code>model_selection_multiseries.backtesting_forecaster_multiseries</code> and <code>model_selection_sarimax.backtesting_forecaster_sarimax</code> to indicate weather to show a progress bar.</p> </li> <li> <p>Added argument <code>remove_suffix</code>, default <code>False</code>, to the method <code>filter_train_X_y_for_step()</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the column names of the training matrices.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename optional dependency package <code>statsmodels</code> to <code>sarimax</code>. Now only <code>pmdarima</code> will be installed, <code>statsmodels</code> is no longer needed.</p> </li> <li> <p>Rename <code>get_feature_importance()</code> to <code>get_feature_importances()</code> in all Forecasters. <code>get_feature_importance()</code> method will me removed in skforecast 0.9.0.</p> </li> <li> <p>Refactor <code>get_feature_importances()</code> in all Forecasters.</p> </li> <li> <p>Remove <code>model_selection_statsmodels</code> in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>, (deprecated since 0.7.0).</p> </li> <li> <p>Remove attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> in favor of <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>, (deprecated since 0.7.0).</p> </li> <li> <p>The <code>utils.check_exog</code> function now includes a new optional parameter, <code>allow_nan</code>, that controls whether a warning should be issued if the input <code>exog</code> contains NaN values. </p> </li> <li> <p><code>utils.check_exog</code> is applied before and after <code>exog</code> transformations.</p> </li> <li> <p>The <code>utils.preprocess_y</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>The <code>utils.preprocess_exog</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>Replaced <code>tqdm.tqdm</code> by <code>tqdm.auto.tqdm</code>.</p> </li> <li> <p>Refactor <code>utils.exog_to_direct</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>The dtypes of exogenous variables are maintained when generating the training matrices with the <code>create_train_X_y</code> method in all the Forecasters.</li> </ul>"},{"location":"releases/releases.html#0.7.0","title":"0.7.0 Mar 21, 2023","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultiSeriesCustom</code>.</p> </li> <li> <p>Class <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code> (wrapper of pmdarima).</p> </li> <li> <p>Method <code>predict_interval()</code> to <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li> <p>Method <code>predict_bootstrapping()</code> to all forecasters, generate multiple forecasting predictions using a bootstrapping process.</p> </li> <li> <p>Method <code>predict_dist()</code> to all forecasters, fit a given probability distribution for each step using a bootstrapping process.</p> </li> <li> <p>Function <code>plot_prediction_distribution</code> in module <code>plot</code>.</p> </li> <li> <p>Alias <code>backtesting_forecaster_multivariate</code> for <code>backtesting_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>grid_search_forecaster_multivariate</code> for <code>grid_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>random_search_forecaster_multivariate</code> for <code>random_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Attribute <code>forecaster_id</code> to all Forecasters.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.7</code> compatibility.</p> </li> <li> <p>Added <code>python 3.11</code> compatibility.</p> </li> <li> <p><code>model_selection_statsmodels</code> is deprecated in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>. It will be removed in version 0.8.0.</p> </li> <li> <p>Remove <code>levels_weights</code> argument in <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, deprecated since version 0.6.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> renamed to <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>. Old names will be removed in version 0.8.0.</p> </li> <li> <p>Remove engine <code>'skopt'</code> in <code>bayesian_search_forecaster</code> in favor of engine <code>'optuna'</code>. To continue using it, use skforecast 0.6.0.</p> </li> <li> <p><code>in_sample_residuals</code> and <code>out_sample_residuals</code> are stored as numpy ndarrays instead of pandas series.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>set_out_sample_residuals()</code> is now expecting a <code>dict</code> for the <code>residuals</code> argument instead of a <code>pandas DataFrame</code>.</p> </li> <li> <p>Remove the <code>scikit-optimize</code> dependency.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Remove operator <code>**</code> in <code>set_params()</code> method for all forecasters.</p> </li> <li> <p>Replace <code>getfullargspec</code> in favor of <code>inspect.signature</code> (contribution by @jordisilv).</p> </li> </ul>"},{"location":"releases/releases.html#0.6.0","title":"0.6.0 Nov 30, 2022","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultivariate</code>.</p> </li> <li> <p>Function <code>initialize_lags</code> in <code>utils</code> module  to create lags values in the initialization of forecasters (applies to all forecasters).</p> </li> <li> <p>Function <code>initialize_weights</code> in <code>utils</code> module to check and initialize arguments <code>series_weights</code>and <code>weight_func</code> (applies to all forecasters).</p> </li> <li> <p>Argument <code>weights_func</code> in all Forecasters to allow weighted time series forecasting. Individual time based weights can be assigned to each value of the series during the model training.</p> </li> <li> <p>Argument <code>series_weights</code> in <code>ForecasterAutoregMultiSeries</code> to define individual weights each series.</p> </li> <li> <p>Include argument <code>random_state</code> in all Forecasters <code>set_out_sample_residuals</code> methods for random sampling with reproducible output.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>predict</code> and <code>predict_interval</code> methods allow the simultaneous prediction of multiple levels.</p> </li> <li> <p><code>backtesting_forecaster_multiseries</code> allows backtesting multiple levels simultaneously.</p> </li> <li> <p><code>metric</code> argument can be a list in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Function <code>multivariate_time_series_corr</code> in module <code>utils</code>.</p> </li> <li> <p>Function <code>plot_multivariate_time_series_corr</code> in module <code>plot</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> allows to predict specific steps.</p> </li> <li> <p>Remove <code>ForecasterAutoregMultiOutput</code> in favor of <code>ForecasterAutoregDirect</code>, (deprecated since 0.5.0).</p> </li> <li> <p>Rename function <code>exog_to_multi_output</code> to <code>exog_to_direct</code> in <code>utils</code> module.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, rename parameter <code>series_levels</code> to <code>series_col_names</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code> change type of <code>out_sample_residuals</code> to a <code>dict</code> of numpy ndarrays.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, delete argument <code>level</code> from method <code>set_out_sample_residuals</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>check_predict_input</code> function, argument <code>level</code> renamed to <code>levels</code> and <code>series_levels</code> renamed to <code>series_col_names</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>metrics_levels</code> output is now a pandas DataFrame.</p> </li> <li> <p>In <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, argument <code>levels_weights</code> is deprecated since version 0.6.0, and will be removed in version 0.7.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Refactor <code>_create_lags_</code> in <code>ForecasterAutoreg</code>, <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiSeries</code>. (suggested by Bennett https://github.com/Bennett561)</p> </li> <li> <p>Refactor <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, <code>filter_train_X_y_for_step</code> now starts at 1 (before 0).</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, DataFrame <code>y_train</code> now start with 1, <code>y_step_1</code> (before <code>y_step_0</code>).</p> </li> <li> <p>Remove <code>cv_forecaster</code> from module <code>model_selection</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, argument <code>last_window</code> predict method now works when it is a pandas DataFrame.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, fix bug transformers initialization.</p> </li> </ul>"},{"location":"releases/releases.html#0.5.1","title":"0.5.1 Oct 05, 2022","text":"<p>Added</p> <ul> <li> <p>Check that <code>exog</code> and <code>y</code> have the same length in <code>_evaluate_grid_hyperparameters</code> and <code>bayesian_search_forecaster</code> to avoid fit exception when <code>return_best</code>.</p> </li> <li> <p>Check that <code>exog</code> and <code>series</code> have the same length in <code>_evaluate_grid_hyperparameters_multiseries</code> to avoid fit exception when <code>return_best</code>.</p> </li> </ul> <p>Changed</p> <ul> <li>Argument <code>levels_list</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code> renamed to <code>levels</code>.</li> </ul> <p>Fixed</p> <ul> <li> <p><code>ForecasterAutoregMultiOutput</code> updated to match <code>ForecasterAutoregDirect</code>.</p> </li> <li> <p>Fix Exception to raise when <code>level_weights</code> does not add up to a number close to 1.0 (before was exactly 1.0) in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p><code>Create_train_X_y</code> in <code>ForecasterAutoregMultiSeries</code> now works when the forecaster is not fitted.</p> </li> </ul>"},{"location":"releases/releases.html#0.5.0","title":"0.5.0 Sep 23, 2022","text":"<p>Added</p> <ul> <li> <p>New arguments <code>transformer_y</code> (<code>transformer_series</code> for multiseries) and <code>transformer_exog</code> in all forecaster classes. It is for transforming (scaling, max-min, ...) the modeled time series and exogenous variables inside the forecaster.</p> </li> <li> <p>Functions in utils <code>transform_series</code> and <code>transform_dataframe</code> to carry out the transformation of the modeled time series and exogenous variables.</p> </li> <li> <p>Functions <code>_backtesting_forecaster_verbose</code>, <code>random_search_forecaster</code>, <code>_evaluate_grid_hyperparameters</code>, <code>bayesian_search_forecaster</code>, <code>_bayesian_search_optuna</code> and <code>_bayesian_search_skopt</code> in model_selection.</p> </li> <li> <p>Created <code>ForecasterAutoregMultiSeries</code> class for modeling multiple time series simultaneously.</p> </li> <li> <p>Created module <code>model_selection_multiseries</code>. Functions: <code>_backtesting_forecaster_multiseries_refit</code>, <code>_backtesting_forecaster_multiseries_no_refit</code>, <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p>Function <code>_check_interval</code> in utils. (suggested by Thomas Karaouzene https://github.com/tkaraouzene)</p> </li> <li> <p><code>metric</code> can be a list in <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, <code>backtesting_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Skforecast works with python 3.10.</p> </li> <li> <p>Functions <code>save_forecaster</code> and <code>load_forecaster</code> to module utils.</p> </li> <li> <p><code>get_feature_importance()</code> method checks if the forecast is fitted.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster</code> change default value of argument <code>fixed_train_size: bool=True</code>.</p> </li> <li> <p>Remove argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster</code> (deprecated since 0.4.2).</p> </li> <li> <p><code>backtesting_forecaster</code> verbose now includes fold size.</p> </li> <li> <p><code>grid_search_forecaster</code> results include the name of the used metric as column name.</p> </li> <li> <p>Remove <code>get_coef</code> method from <code>ForecasterAutoreg</code>, <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiOutput</code> (deprecated since 0.4.3).</p> </li> <li> <p><code>_get_metric</code> now allows <code>mean_squared_log_error</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput</code> has been renamed to <code>ForecasterAutoregDirect</code>. <code>ForecasterAutoregMultiOutput</code> will be removed in version 0.6.0.</p> </li> <li> <p><code>check_predict_input</code> updated to check <code>ForecasterAutoregMultiSeries</code> inputs.</p> </li> <li> <p><code>set_out_sample_residuals</code> has a new argument <code>transform</code> to transform the residuals before being stored.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p><code>fit</code> now stores <code>last_window</code> values with len = forecaster.max_lag in ForecasterAutoreg and ForecasterAutoregCustom.</p> </li> <li> <p><code>in_sample_residuals</code> stored as a <code>pd.Series</code> when <code>len(residuals) &gt; 1000</code>.</p> </li> </ul>"},{"location":"releases/releases.html#0.4.3","title":"0.4.3 Mar 18, 2022","text":"<p>Added</p> <ul> <li> <p>Checks if all elements in lags are <code>int</code> when creating ForecasterAutoreg and ForecasterAutoregMultiOutput.</p> </li> <li> <p>Add <code>fixed_train_size: bool=False</code> argument to <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code></p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename <code>get_metric</code> to <code>_get_metric</code>.</p> </li> <li> <p>Functions in model_selection module allow custom metrics.</p> </li> <li> <p>Functions in model_selection_statsmodels module allow custom metrics.</p> </li> <li> <p>Change function <code>set_out_sample_residuals</code> (ForecasterAutoreg and ForecasterAutoregCustom), <code>residuals</code> argument must be a <code>pandas Series</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p>Returned value of backtesting functions (model_selection and model_selection_statsmodels) is now a <code>float</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p><code>get_coef</code> and <code>get_feature_importance</code> methods unified in <code>get_feature_importance</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Requirements versions.</p> </li> <li> <p>Method <code>fit</code> doesn't remove <code>out_sample_residuals</code> each time the forecaster is fitted.</p> </li> <li> <p>Added random seed to residuals downsampling (ForecasterAutoreg and ForecasterAutoregCustom)</p> </li> </ul>"},{"location":"releases/releases.html#0.4.2","title":"0.4.2 Jan 08, 2022","text":"<p>Added</p> <ul> <li> <p>Increased verbosity of function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Random state argument in <code>backtesting_forecaster()</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Function <code>backtesting_forecaster()</code> do not modify the original forecaster.</p> </li> <li> <p>Deprecated argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Function <code>model_selection.time_series_spliter</code> renamed to <code>model_selection.time_series_splitter</code></p> </li> </ul> <p>Fixed</p> <ul> <li>Methods <code>get_coef</code> and <code>get_feature_importance</code> of <code>ForecasterAutoregMultiOutput</code> class return proper feature names.</li> </ul>"},{"location":"releases/releases.html#0.4.1","title":"0.4.1 Dec 13, 2021","text":"<p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li><code>fit</code> and <code>predict</code> transform pandas series and dataframes to numpy arrays if regressor is XGBoost.</li> </ul>"},{"location":"releases/releases.html#0.4.0","title":"0.4.0 Dec 10, 2021","text":"<p>Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit).</p> <p>Added</p> <ul> <li><code>ForecasterBase</code> as parent class</li> </ul> <p>Changed</p> <ul> <li> <p>Argument <code>y</code> must be pandas Series. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Argument <code>exog</code> must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Output of <code>predict</code> is a pandas Series with index according to the steps predicted.</p> </li> <li> <p>Scikitlearn pipelines are allowed as regressors.</p> </li> <li> <p><code>backtesting_forecaster</code> and <code>backtesting_forecaster_intervals</code> have been combined in a single function.</p> <ul> <li>It is possible to backtest forecasters already trained.</li> <li><code>ForecasterAutoregMultiOutput</code> allows incomplete folds.</li> <li>It is possible to update <code>out_sample_residuals</code> with backtesting residuals.</li> </ul> </li> <li> <p><code>cv_forecaster</code> has the option to update <code>out_sample_residuals</code> with backtesting residuals.</p> </li> <li> <p><code>backtesting_sarimax_statsmodels</code> and <code>cv_sarimax_statsmodels</code> have been combined in a single function.</p> </li> <li> <p><code>gridsearch_forecaster</code> use backtesting as validation strategy with the option of refit.</p> </li> <li> <p>Extended information when printing <code>Forecaster</code> object.</p> </li> <li> <p>All static methods for checking and preprocessing inputs moved to module utils.</p> </li> <li> <p>Remove deprecated class <code>ForecasterCustom</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.3.0","title":"0.3.0 Sep 01, 2021","text":"<p>Added</p> <ul> <li> <p>New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library:</p> <ul> <li><code>backtesting_autoreg_statsmodels</code></li> <li><code>cv_autoreg_statsmodels</code></li> <li><code>backtesting_sarimax_statsmodels</code></li> <li><code>cv_sarimax_statsmodels</code></li> <li><code>grid_search_sarimax_statsmodels</code></li> </ul> </li> <li> <p>Added attribute window_size to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>. It is equal to <code>max_lag</code>.</p> </li> </ul> <p>Changed</p> <ul> <li><code>cv_forecaster</code> returns cross-validation metrics and cross-validation predictions.</li> <li>Added an extra column for each parameter in the dataframe returned by <code>grid_search_forecaster</code>.</li> <li>statsmodels 0.12.2 added to requirements</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.2.0","title":"0.2.0 Aug 26, 2021","text":"<p>Added</p> <ul> <li> <p>Multiple exogenous variables can be passed as pandas DataFrame.</p> </li> <li> <p>Documentation at https://joaquinamatrodrigo.github.io/skforecast/</p> </li> <li> <p>New unit test</p> </li> <li> <p>Increased typing</p> </li> </ul> <p>Changed</p> <ul> <li>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.9","title":"0.1.9 Jul 27, 2021","text":"<p>Added</p> <ul> <li> <p>Logging total number of models to fit in <code>grid_search_forecaster</code>.</p> </li> <li> <p>Class <code>ForecasterAutoregCustom</code>.</p> </li> <li> <p>Method <code>create_train_X_y</code> to facilitate access to the training data matrix created from <code>y</code> and <code>exog</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p>Class <code>ForecasterCustom</code> has been renamed to <code>ForecasterAutoregCustom</code>. However, <code>ForecasterCustom</code> will still remain to keep backward compatibility.</p> </li> <li> <p>Argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p>Check if argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p><code>time_series_spliter</code> doesn't include the remaining observations in the last complete fold but in a new one when <code>allow_incomplete_fold=True</code>. Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.</p> </li> </ul> <p>Fixed</p> <ul> <li>Update lags of  <code>ForecasterAutoregMultiOutput</code> after <code>grid_search_forecaster</code>.</li> </ul>"},{"location":"releases/releases.html#0.1.8.1","title":"0.1.8.1 May 17, 2021","text":"<p>Added</p> <ul> <li><code>set_out_sample_residuals</code> method to store or update out of sample residuals used by <code>predict_interval</code>.</li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster_intervals</code> and <code>backtesting_forecaster</code> print number of steps per fold.</p> </li> <li> <p>Only stored up to 1000 residuals.</p> </li> <li> <p>Improved verbose in <code>backtesting_forecaster_intervals</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Warning of inclompleted folds when using <code>backtesting_forecast</code> with a  <code>ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput.predict</code> allow exog data longer than needed (steps).</p> </li> <li> <p><code>backtesting_forecast</code> prints correctly the number of folds when remainder observations are cero.</p> </li> <li> <p>Removed named argument X in <code>self.regressor.predict(X)</code> to allow using XGBoost regressor.</p> </li> <li> <p>Values stored in <code>self.last_window</code> when training <code>ForecasterAutoregMultiOutput</code>. </p> </li> </ul>"},{"location":"releases/releases.html#0.1.8","title":"0.1.8 Apr 02, 2021","text":"<p>Added</p> <ul> <li>Class <code>ForecasterAutoregMultiOutput.py</code>: forecaster with direct multi-step predictions.</li> <li>Method <code>ForecasterCustom.predict_interval</code> and  <code>ForecasterAutoreg.predict_interval</code>: estimate prediction interval using bootstrapping.</li> <li><code>skforecast.model_selection.backtesting_forecaster_intervals</code> perform backtesting and return prediction intervals.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.7","title":"0.1.7 Mar 19, 2021","text":"<p>Added</p> <ul> <li>Class <code>ForecasterCustom</code>: same functionalities as <code>ForecasterAutoreg</code> but allows custom definition of predictors.</li> </ul> <p>Changed</p> <ul> <li><code>grid_search forecaster</code> adapted to work with objects <code>ForecasterCustom</code> in addition to <code>ForecasterAutoreg</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.6","title":"0.1.6 Mar 14, 2021","text":"<p>Added</p> <ul> <li>Method <code>get_feature_importances</code> to <code>skforecast.ForecasterAutoreg</code>.</li> <li>Added backtesting strategy in <code>grid_search_forecaster</code>.</li> <li>Added <code>backtesting_forecast</code> to <code>skforecast.model_selection</code>.</li> </ul> <p>Changed</p> <ul> <li>Method <code>create_lags</code> return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor.</li> <li>Renamed argument <code>X</code> to <code>last_window</code> in method <code>predict</code>.</li> <li>Renamed <code>ts_cv_forecaster</code> to <code>cv_forecaster</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.4","title":"0.1.4 Feb 15, 2021","text":"<p>Added</p> <ul> <li>Method <code>get_coef</code> to <code>skforecast.ForecasterAutoreg</code>.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"user_guides/autoregresive-forecaster.html","title":"Recursive multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.datasets import fetch_dataset\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.datasets import fetch_dataset from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:38:13 \nLast fit date: 2024-07-29 16:38:13 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[4]: <pre>2005-07-01    1.020833\n2005-08-01    1.021721\n2005-09-01    1.093488\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.006023571131075229\n</pre> In\u00a0[7]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100) predictions.head(3) Out[7]: pred lower_bound upper_bound 2005-07-01 1.020833 0.975794 1.082322 2005-08-01 1.021721 0.976413 1.064850 2005-09-01 1.093488 1.010195 1.154976 In\u00a0[8]: Copied! <pre># Plot prediction interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_test.plot(ax=ax, label=\"test\")\npredictions['pred'].plot(ax=ax, label=\"prediction\")\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'blue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.legend(loc=\"upper left\");\n</pre> # Plot prediction interval # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_test.plot(ax=ax, label=\"test\") predictions['pred'].plot(ax=ax, label=\"prediction\") ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'blue',     alpha = 0.3,     label = '80% interval' ) ax.legend(loc=\"upper left\"); In\u00a0[9]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[9]: feature importance 11 lag_12 100 10 lag_11 43 0 lag_1 41 1 lag_2 41 5 lag_6 36 13 lag_14 33 2 lag_3 31 6 lag_7 30 14 lag_15 30 8 lag_9 26 12 lag_13 26 9 lag_10 22 7 lag_8 17 3 lag_4 15 4 lag_5 15 In\u00a0[10]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) X_train.head() Out[10]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1992-12-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1993-01-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 1993-02-01 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 In\u00a0[11]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[11]: <pre>datetime\n1992-10-01    0.568606\n1992-11-01    0.595223\n1992-12-01    0.771258\n1993-01-01    0.751503\n1993-02-01    0.387554\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) In\u00a0[13]: Copied! <pre># Predict using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressor.predict(X_train)\npredictions_training[:4]\n</pre> # Predict using the internal regressor # ============================================================================== predictions_training = forecaster.regressor.predict(X_train) predictions_training[:4] Out[13]: <pre>array([0.55145568, 0.57320468, 0.73750462, 0.73500079])</pre> <p> \u26a0 Warning </p> <p>If the Forecaster contains a transformation, it is necessary to apply the inverse transformation to the predictions. This is because the predictions are made on the transformed data, and the inverse transformation is required to obtain the original scale.</p> <p>Check issue 715 for more information.</p> In\u00a0[14]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=5)\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=5) X_predict Out[14]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 2005-07-01 0.842263 0.695248 0.670505 0.652590 0.597639 1.170690 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986 2005-08-01 1.020833 0.842263 0.695248 0.670505 0.652590 0.597639 1.170690 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 2005-09-01 1.021721 1.020833 0.842263 0.695248 0.670505 0.652590 0.597639 1.170690 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 2005-10-01 1.093488 1.021721 1.020833 0.842263 0.695248 0.670505 0.652590 0.597639 1.170690 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 2005-11-01 1.145198 1.093488 1.021721 1.020833 0.842263 0.695248 0.670505 0.652590 0.597639 1.170690 1.257238 1.216037 1.181011 1.134432 0.994864 <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[15]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  differentiation = 1              )  forecaster.fit(y=data_train) forecaster Out[15]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: 1 \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:38:18 \nLast fit date: 2024-07-29 16:38:18 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[16]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[16]: <pre>2005-07-01    0.909529\n2005-08-01    0.941633\n2005-09-01    1.007650\nFreq: MS, Name: pred, dtype: float64</pre>"},{"location":"user_guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting\u00b6","text":"<p>Recursive multi-step forecasting involves using the predicted values from previous time steps as input to forecast the values for the subsequent time steps. The model initially predicts one time step ahead and then uses that forecast as an input for the next time step, continuing this recursive process until the desired forecast horizon is reached.</p> <p>To clarify, since the value of $t_{n-1}$ is required to predict $t_{n}$, and $t_{n-1}$ is unknown, a recursive process is applied in which, each new prediction is based on the previous one.</p> <p> Diagram of recursive multi-step forecasting. </p> <p>When using machine learning models for recursive multi-step forecasting, one of the major challenges is transforming the time series data into a matrix format that relates each value of the series to the preceding time window (lags). This process can be complex and time-consuming, requiring careful feature engineering to ensure the model can accurately capture the underlying patterns in the data.</p> <p> Time series transformation including an exogenous variable. </p> <p>By using the classes <code>ForecasterAutoreg</code>, it is possible to build machine learning models for recursive multi-step forecasting with ease. These classes can automatically transform the time series data into a matrix format suitable for input to a machine learning algorithm, and provide a range of options for tuning the model parameters to achieve the best possible performance.</p>"},{"location":"user_guides/autoregresive-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction-intervals","title":"Prediction intervals\u00b6","text":"<p>A prediction interval defines the interval within which the true value of <code>y</code> is expected to be found with a given probability. For example, the prediction interval (1, 99) can be expected to contain the true prediction value with 98% probability.</p> <p>By default, every forecaster can estimate prediction intervals using the method <code>predict_interval</code>. For a detailed explanation of the different prediction intervals available in skforecast visit Probabilistic forecasting.</p>"},{"location":"user_guides/autoregresive-forecaster.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p>"},{"location":"user_guides/autoregresive-forecaster.html#prediction-on-training-data","title":"Prediction on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data, the <code>predict()</code> method of the regressor stored inside the forecaster object can be accessed, and the training matrix can be passed as an input to this method. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p>"},{"location":"user_guides/autoregresive-forecaster.html#extract-prediction-matrices","title":"Extract prediction matrices\u00b6","text":"<p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p>"},{"location":"user_guides/autoregresive-forecaster.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/backtesting.html","title":"Backtesting forecaster","text":"<p> \u270e Note </p> <p>Since skforecast 0.9.0, all backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance. This applies to all functions of the different <code>model_selection</code> modules.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='validation')\nax.legend()\nplt.show()\n\ndisplay(data.head(4))\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='validation') ax.legend() plt.show()  display(data.head(4)) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\nTrain dates      : 1991-07-01 00:00:00 --- 2002-01-01 00:00:00  (n=127)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 In\u00a0[3]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          n_jobs                = 'auto',\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           n_jobs                = 'auto',                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[4]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.00829 Out[4]: pred 2002-02-01 0.579336 2002-03-01 0.732396 2002-04-01 0.708196 2002-05-01 0.752660 In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend()\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax) predictions.plot(ax=ax) ax.legend() plt.show(); <p> \u270e Note </p> <p>With an intermittent refit, the model is trained every <code>refit</code> * <code>steps</code> observations. In this case, 3 * 10 = 30 observations. This can be easily observed in the <code>verbose</code> logs.</p> In\u00a0[6]: Copied! <pre># Backtesting with intermittent refit\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          allow_incomplete_fold = True,\n                          refit                 = 3,\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting with intermittent refit # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           allow_incomplete_fold = True,                           refit                 = 3,                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   No training in this fold\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   No training in this fold\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   No training in this fold\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.008878 Out[7]: pred 2002-02-01 0.579336 2002-03-01 0.732396 2002-04-01 0.708196 2002-05-01 0.752660 <p> \u270e Note </p> <p>In this example, although only the last 10 predictions (steps) are stored for model evaluation, the total number of predicted steps in each fold is 15 (steps + gap).</p> In\u00a0[8]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 5,\n                          allow_incomplete_fold = False,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 5,                           allow_incomplete_fold = False,                           refit                 = True,                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 7\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 5\n    Last fold has been excluded because it was incomplete.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-07-01 00:00:00 -- 2003-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2003-05-01 00:00:00 -- 2004-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2004-03-01 00:00:00 -- 2004-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2005-01-01 00:00:00 -- 2005-10-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-11-01 00:00:00 -- 2006-08-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-09-01 00:00:00 -- 2007-06-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-07-01 00:00:00 -- 2008-04-01 00:00:00  (n=10)\n\n</pre> <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> In\u00a0[9]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.01006 Out[9]: pred 2002-07-01 0.855047 2002-08-01 0.914259 2002-09-01 0.933206 2002-10-01 0.997783 In\u00a0[10]: Copied! <pre># Backtesting forecaster with refit and skip folds\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          skip_folds            = 2,\n                          n_jobs                = 'auto',\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting forecaster with refit and skip folds # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           skip_folds            = 2,                           n_jobs                = 'auto',                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 4 [1, 3, 5, 7]\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Fold skipped\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Fold skipped\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Fold skipped\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Fold skipped\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.005116 Out[11]: pred 2002-02-01 0.579336 2002-03-01 0.732396 2002-04-01 0.708196 2002-05-01 0.752660 In\u00a0[12]: Copied! <pre># Backtesting forecaster with prediction intervals\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          interval              = [5, 95],\n                          n_boot                = 500,\n                          verbose               = True,\n                          show_progress         = True\n                      )\n</pre> # Backtesting forecaster with prediction intervals # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           interval              = [5, 95],                           n_boot                = 500,                           verbose               = True,                           show_progress         = True                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[13]: Copied! <pre># Interval predictions\n# ==============================================================================\npredictions.head()\n</pre> # Interval predictions # ============================================================================== predictions.head() Out[13]: pred lower_bound upper_bound 2002-02-01 0.703579 0.601346 0.834616 2002-03-01 0.673522 0.572124 0.779922 2002-04-01 0.698319 0.590535 0.814450 2002-05-01 0.703042 0.599340 0.816911 2002-06-01 0.733776 0.630187 0.853085 In\u00a0[14]: Copied! <pre># Plot prediction intervals\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'red',\n    alpha = 0.2,\n    label = 'prediction interval'\n)\nax.legend();\n</pre> # Plot prediction intervals # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'red',     alpha = 0.2,     label = 'prediction interval' ) ax.legend(); In\u00a0[15]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=False)\ndata.index.name = 'datetime'\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=False) data.index.name = 'datetime' <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[16]: Copied! <pre># Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(6, 3))\ndata.plot(ax=ax);\n</pre> # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(6, 3)) data.plot(ax=ax); <pre>Train dates      : 1992-04-01 00:00:00 --- 2002-01-01 00:00:00  (n=118)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> In\u00a0[17]: Copied! <pre># Backtest forecaster exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = True\n                      )\n</pre> # Backtest forecaster exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = True,                           show_progress         = True                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 118\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2002-01-01 00:00:00  (n=118)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2002-11-01 00:00:00  (n=128)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2003-09-01 00:00:00  (n=138)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1992-04-01 00:00:00 -- 2004-07-01 00:00:00  (n=148)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1992-04-01 00:00:00 -- 2005-05-01 00:00:00  (n=158)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1992-04-01 00:00:00 -- 2006-03-01 00:00:00  (n=168)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1992-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=178)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1992-04-01 00:00:00 -- 2007-11-01 00:00:00  (n=188)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[18]: Copied! <pre># Backtest error\n# ==============================================================================\nprint(\"Backtest error with exogenous variables:\")\nmetric\n</pre> # Backtest error # ============================================================================== print(\"Backtest error with exogenous variables:\") metric <pre>Backtest error with exogenous variables:\n</pre> Out[18]: mean_squared_error 0 0.00711 In\u00a0[19]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend(loc=\"upper left\")\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:].plot(ax=ax) predictions.plot(ax=ax) ax.legend(loc=\"upper left\") plt.show(); In\u00a0[20]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = custom_metric,\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = False,\n                          show_progress         = True\n                      )\n\nmetric\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric   metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = custom_metric,                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = False,                           show_progress         = True                       )  metric <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> Out[20]: custom_metric 0 0.006907 In\u00a0[21]: Copied! <pre># Backtesting with multiple metric\n# ==============================================================================\nmultiple_metrics = ['mean_squared_error', 'mean_absolute_error', custom_metric]\n\nmetrics, predictions = backtesting_forecaster(\n                           forecaster            = forecaster,\n                           y                     = data['y'],\n                           steps                 = 10,\n                           metric                = multiple_metrics,\n                           initial_train_size    = len(data.loc[:end_train]),\n                           fixed_train_size      = False,\n                           gap                   = 0,\n                           allow_incomplete_fold = True,\n                           refit                 = True,\n                           verbose               = False,\n                           show_progress         = True\n                       )\n\nmetrics\n</pre> # Backtesting with multiple metric # ============================================================================== multiple_metrics = ['mean_squared_error', 'mean_absolute_error', custom_metric]  metrics, predictions = backtesting_forecaster(                            forecaster            = forecaster,                            y                     = data['y'],                            steps                 = 10,                            metric                = multiple_metrics,                            initial_train_size    = len(data.loc[:end_train]),                            fixed_train_size      = False,                            gap                   = 0,                            allow_incomplete_fold = True,                            refit                 = True,                            verbose               = False,                            show_progress         = True                        )  metrics <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> Out[21]: mean_squared_error mean_absolute_error custom_metric 0 0.007114 0.068497 0.006907 In\u00a0[22]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15 \n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15               )  forecaster.fit(y=data['y']) In\u00a0[23]: Copied! <pre># Backtesting on training data\n# ==============================================================================\nmetric, predictions_training = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data['y'],\n                                   steps              = 1,\n                                   metric             = 'mean_squared_error',\n                                   initial_train_size = None,\n                                   refit              = False,\n                                   verbose            = False,\n                                   show_progress      = True\n                               )\n\nmetric\n</pre> # Backtesting on training data # ============================================================================== metric, predictions_training = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data['y'],                                    steps              = 1,                                    metric             = 'mean_squared_error',                                    initial_train_size = None,                                    refit              = False,                                    verbose            = False,                                    show_progress      = True                                )  metric <pre>  0%|          | 0/180 [00:00&lt;?, ?it/s]</pre> Out[23]: mean_squared_error 0 0.000732 In\u00a0[24]: Copied! <pre># Training predictions\n# ==============================================================================\npredictions_training.head(4)\n</pre> # Training predictions # ============================================================================== predictions_training.head(4) Out[24]: pred 1993-07-01 0.524240 1993-08-01 0.566664 1993-09-01 0.597642 1993-10-01 0.633346 <p>It is important to note that the first 15 observations are excluded from the predictions since they are required to generate the lags that serve as predictors in the model.</p> In\u00a0[25]: Copied! <pre># Plot training predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata['y'].plot(ax=ax)\npredictions_training.plot(ax=ax)\nax.set_title(\"Backtesting on training data\")\nax.legend()\nplt.show();\n</pre> # Plot training predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data['y'].plot(ax=ax) predictions_training.plot(ax=ax) ax.set_title(\"Backtesting on training data\") ax.legend() plt.show();"},{"location":"user_guides/backtesting.html#backtesting","title":"Backtesting\u00b6","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"user_guides/backtesting.html#backtesting-without-refit","title":"Backtesting without refit\u00b6","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit. </p> <p>Note: The argument needed to achieve this configuration is <code>refit=False</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-increasing-training-size","title":"Backtesting with refit and increasing training size\u00b6","text":"<p>In this strategy, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin). </p> <p>Note: The arguments needed to achieve this configuration are <code>refit=True</code> and <code>fixed_train_size=False</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-fixed-training-size","title":"Backtesting with refit and fixed training size\u00b6","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin). </p> <p>Note: The arguments needed to achieve this configuration are <code>refit=True</code> and <code>fixed_train_size=True</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit\u00b6","text":"<p>The model is retrained every $n$ iterations of prediction. This method is often used when the frequency of retraining and prediction is different. It can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p> \ud83d\udca1 Tip </p> <p>This strategy usually achieves a good balance between the computational cost of retraining and avoiding model degradation.</p> <p> Backtesting with intermittent refit. </p> <p>Note: The argument needed to achieve this configuration is <code>refit=n</code>, where $n$ is an integer. This configuration also allows the use of <code>fixed_train_size</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-including-gap","title":"Backtesting including gap\u00b6","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap. </p> <p>Note: After setting the desired values for <code>refit</code> and <code>fixed_train_size</code>. The argument needed to achieve this configuration is <code>gap=n</code>, where $n$ is an integer.</p>"},{"location":"user_guides/backtesting.html#skip-folds","title":"Skip folds\u00b6","text":"<p>All of the above backtesting strategies can be combined with the option to skip a certain number of folds by setting the <code>skip_folds</code> argument. Since the model predicts fewer points in time, the computational cost is reduced and the backtesting process is faster. This is particularly useful when one is interested in an approximate estimate of the model's performance, but does not require an exact evaluation, for example, when searching for hyperparameters. If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the returned folds will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the returned folds will be [0, 4, 5, 6, 7, 8, 9].</p>"},{"location":"user_guides/backtesting.html#which-strategy-should-i-use","title":"Which strategy should I use?\u00b6","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy. Factors such as use case characteristics, available computing resources and time intervals between predictions need to be considered to determine which strategy to use. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li><p>Prediction horizon (<code>steps</code>): suppose you need to predict the users of an application every Monday for the whole week. In this case, each iteration of backtesting would be a seven-step prediction, representing the seven days of the week.</p> </li> <li><p>Refit strategy (<code>refit</code>): Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to compare different retrain frequencies and select the one for which the error metric shows a consistent upward trend. This behaviour can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with <code>steps=24</code> (predict 24 hours) and different <code>refit</code> strategies: <code>False</code> (no re-fitting between predictions), re-fitting every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (re-fitting after every prediction). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>In general, the more closely the backtesting process resembles the actual scenario in which the model is used, the more reliable the estimated metric will be.</p>"},{"location":"user_guides/backtesting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/backtesting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/backtesting.html#backtest","title":"Backtest\u00b6","text":"<p>An example of a backtesting with refit consists of the following steps:</p> <ol> <li><p>Train the model using an initial training set, the length of which is specified by <code>initial_train_size</code>.</p> </li> <li><p>Once the model is trained, it is used to make predictions for the next 10 steps (<code>steps=10</code>) in the data. These predictions are saved for later evaluation.</p> </li> <li><p>As <code>refit</code> is set to <code>True</code>, the size of the training set is increased by adding the lats 10 data points (the previously predicted 10 steps), while the next 10 steps are used as test data.</p> </li> <li><p>After expanding the training set, the model is retrained using the updated training data and then used to predict the next 10 steps.</p> </li> <li><p>Repeat steps 3 and 4 until the entire series has been tested.</p> </li> </ol> <p>By following these steps, you can ensure that the model is evaluated on multiple sets of test data, thereby providing a more accurate assessment of its predictive power.</p>"},{"location":"user_guides/backtesting.html#backtesting-intermittent-refit","title":"Backtesting intermittent refit\u00b6","text":"<p>The same backtesting as above is repeated, but this time with <code>refit = 3</code>. The training of the model is done every 3 folds instead of every fold.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-gap","title":"Backtesting with gap\u00b6","text":"<p>The gap size can be adjusted with the <code>gap</code> argument. In addition, the <code>allow_incomplete_fold</code> parameter allows the last fold to be excluded from the analysis if it doesn't have the same size as the required number of <code>steps</code>.</p> <p>These arguments can be used in conjunction with either refit set to <code>True</code> or <code>False</code>, depending on the needs and objectives of the use case.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-skip-folds","title":"Backtesting with skip folds\u00b6","text":"<ul> <li><p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. For example, if <code>skip_folds = 3</code> and there are 10 folds, the returned folds will be [0, 3, 6, 9].</p> </li> <li><p>If <code>skip_folds</code> is a list, the folds in the list are skipped. If <code>skip_folds = [1, 2, 3]</code> and there are 10 folds, the returned folds will be [0, 4, 5, 6, 7, 8, 9].</p> </li> </ul>"},{"location":"user_guides/backtesting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>Backtesting can be used not only to obtain point estimate predictions but also to obtain prediction intervals. Prediction intervals provide a range of values within which the actual values are expected to fall with a certain level of confidence. By estimating prediction intervals during backtesting, one can get a better understanding of the uncertainty associated with your model's predictions. This information can be used to assess the reliability of the model's predictions and to make more informed decisions. To learn more about probabilistic forecasting, visit Probabilistic forecasting.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies discussed in this document can also be applied when incorporating exogenous variables in the forecasting model.</p> <p>Exogenous variables are additional independent variables that can impact the value of the target variable being forecasted. These variables can provide valuable information to the model and improve its forecasting accuracy.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-custom-metric","title":"Backtesting with custom metric\u00b6","text":"<p>In addition to the commonly used metrics such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-multiple-metrics","title":"Backtesting with multiple metrics\u00b6","text":"<p>The <code>backtesting_forecaster</code> function provides a convenient way to estimate multiple metrics simultaneously by accepting a list of metrics as an input argument. This list can include any combination of built-in metrics, such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, as well as user-defined custom metrics.</p> <p>By specifying multiple metrics, users can obtain a more comprehensive evaluation of the model's predictive performance, which can help in selecting the best model for a particular task. Additionally, the ability to include custom metrics allows users to tailor the evaluation to specific use cases and domain-specific requirements.</p>"},{"location":"user_guides/backtesting.html#backtesting-on-training-data","title":"Backtesting on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data,the forecaster must first be fitted using the training dataset. Then, backtesting can be performed using  <code>backtesting_forecaster</code> and specifying the arguments <code>initial_train_size=None</code> and <code>refit=False</code>. This configuration enables backtesting on the same data that was used to train the model.</p>"},{"location":"user_guides/calendar-features.html","title":"Calendars features","text":"<p>Calendar features serve as key elements in time series forecasting. These features decompose date and time into basic units such as year, month, day, weekday, etc., allowing models to identify recurring patterns, understand seasonal variations, and identify trends. Calendar features can be used as exogenous variables because they are known for the period for which predictions are being made (the forecast horizon).</p> <p>Dates and time in Pandas</p> <p>Pandas provides a comprehensive set of capabilities tailored for handling time series data in various domains. Using the NumPy <code>datetime64</code> and <code>timedelta64</code> data types, Pandas combines a wide range of functionality from various Python libraries while introducing a wealth of novel tools to effectively manipulate time series data. This includes:</p> <ul> <li><p>Easily parse date and time data from multiple sources and formats.</p> </li> <li><p>Generate sequences of fixed-frequency dates and time spans.</p> </li> <li><p>Streamline the manipulation and conversion of date-time information, including time zones.</p> </li> <li><p>Facilitate the resampling or conversion of time series data to specific frequencies.</p> </li> </ul> <p>For an in-depth exploration of Pandas' comprehensive time series and date capabilities, please refer to this resource.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\", raw=True)\ndata = data[['date_time', 'users']]\ndata.head()\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(name=\"bike_sharing\", raw=True) data = data[['date_time', 'users']] data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> Out[2]: date_time users 0 2011-01-01 00:00:00 16.0 1 2011-01-01 01:00:00 40.0 2 2011-01-01 02:00:00 32.0 3 2011-01-01 03:00:00 13.0 4 2011-01-01 04:00:00 1.0 <p>To take advantage of the date-time functionality offered by Pandas, the column of interest must be stored as <code>datetime</code>. Although not required, it is recommended to set it as an index for further integration with skforecast.</p> In\u00a0[3]: Copied! <pre># Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('h')\ndata = data.sort_index()\ndata.head()\n</pre> # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('h') data = data.sort_index() data.head() Out[3]: users date_time 2011-01-01 00:00:00 16.0 2011-01-01 01:00:00 40.0 2011-01-01 02:00:00 32.0 2011-01-01 03:00:00 13.0 2011-01-01 04:00:00 1.0 <p>Next, several features are created from the date and time information: year, month, day of the week, and hour.</p> In\u00a0[4]: Copied! <pre># Create calendar features\n# ==============================================================================\ndata['year'] = data.index.year\ndata['month'] = data.index.month\ndata['day_of_week'] = data.index.dayofweek\ndata['hour'] = data.index.hour\ndata.head()\n</pre> # Create calendar features # ============================================================================== data['year'] = data.index.year data['month'] = data.index.month data['day_of_week'] = data.index.dayofweek data['hour'] = data.index.hour data.head() Out[4]: users year month day_of_week hour date_time 2011-01-01 00:00:00 16.0 2011 1 5 0 2011-01-01 01:00:00 40.0 2011 1 5 1 2011-01-01 02:00:00 32.0 2011 1 5 2 2011-01-01 03:00:00 13.0 2011 1 5 3 2011-01-01 04:00:00 1.0 2011 1 5 4 <p> \ud83d\udca1 Tip </p> <p>Numerous calendar-related features can be generated, including day of the year, week of the year, hour of the day, and others. An easy approach to automate their extraction is to use the <code>DatetimeFeatures</code> transformer within the Feature-engine Python library. This class integrates seamlessly into the scikit-learn pipeline, making it compatible with skforecast as well. For a deeper understanding and detailed information, please refer to DatetimeFeatures.</p> In\u00a0[5]: Copied! <pre># Create calendar features with Feature-engine\n# ==============================================================================\nfrom feature_engine.datetime import DatetimeFeatures\n\ntransformer = DatetimeFeatures(\n                  variables           = \"index\",\n                  features_to_extract = \"all\" # It is also possible to select specific features\n              )\ncalendar_features = transformer.fit_transform(data)\ncalendar_features.head()\n</pre> # Create calendar features with Feature-engine # ============================================================================== from feature_engine.datetime import DatetimeFeatures  transformer = DatetimeFeatures(                   variables           = \"index\",                   features_to_extract = \"all\" # It is also possible to select specific features               ) calendar_features = transformer.fit_transform(data) calendar_features.head() Out[5]: users year month day_of_week hour quarter semester week day_of_month day_of_year ... month_start month_end quarter_start quarter_end year_start year_end leap_year days_in_month minute second date_time 2011-01-01 00:00:00 16.0 2011 1 5 0 1 1 52 1 1 ... 1 0 1 0 1 0 0 31 0 0 2011-01-01 01:00:00 40.0 2011 1 5 1 1 1 52 1 1 ... 1 0 1 0 1 0 0 31 0 0 2011-01-01 02:00:00 32.0 2011 1 5 2 1 1 52 1 1 ... 1 0 1 0 1 0 0 31 0 0 2011-01-01 03:00:00 13.0 2011 1 5 3 1 1 52 1 1 ... 1 0 1 0 1 0 0 31 0 0 2011-01-01 04:00:00 1.0 2011 1 5 4 1 1 52 1 1 ... 1 0 1 0 1 0 0 31 0 0 <p>5 rows \u00d7 21 columns</p> <p>Sunlight often plays a key role in time series patterns. For example, a household's hourly electricity consumption may correlate significantly with whether it's nighttime, as more electricity is typically used for lighting during those hours. Understanding and incorporating sunlight-related characteristics into analyses can provide valuable insights into consumption patterns and behavioral trends. In addition, factors such as sunrise/sunset times, seasonal changes affecting daylight, and their influence on different data sets can provide deeper context and help predict consumption fluctuations. There are several Python libraries available for extracting sunrise and sunset times. Two of the most commonly used are <code>ephem</code> and <code>astral</code>.</p> In\u00a0[6]: Copied! <pre># Features based on the sunligth\n# ==============================================================================\nfrom astral.sun import sun\nfrom astral import LocationInfo\n\nlocation = LocationInfo(\"Washington, D.C.\", \"USA\")\nsunrise_hour = [sun(location.observer, date=date)['sunrise'].hour for date in data.index]\nsunset_hour = [sun(location.observer, date=date)['sunset'].hour for date in data.index]\n\nsun_light_features = pd.DataFrame({\n                         'sunrise_hour': sunrise_hour,\n                         'sunset_hour': sunset_hour}, \n                         index = data.index\n                     )\nsun_light_features['daylight_hours'] = sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour']\nsun_light_features.head()\n</pre> # Features based on the sunligth # ============================================================================== from astral.sun import sun from astral import LocationInfo  location = LocationInfo(\"Washington, D.C.\", \"USA\") sunrise_hour = [sun(location.observer, date=date)['sunrise'].hour for date in data.index] sunset_hour = [sun(location.observer, date=date)['sunset'].hour for date in data.index]  sun_light_features = pd.DataFrame({                          'sunrise_hour': sunrise_hour,                          'sunset_hour': sunset_hour},                           index = data.index                      ) sun_light_features['daylight_hours'] = sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour'] sun_light_features.head() Out[6]: sunrise_hour sunset_hour daylight_hours date_time 2011-01-01 00:00:00 8 16 8 2011-01-01 01:00:00 8 16 8 2011-01-01 02:00:00 8 16 8 2011-01-01 03:00:00 8 16 8 2011-01-01 04:00:00 8 16 8 <p>Certain aspects of the calendar, such as hours of the day or days of the week, behave in cycles. For example, the hours of a day range from 0 to 23. If interpreted as a continuous variable, the hour of 23:00 would be 23 units away from the hour of 00:00. However, this is not true because 23:00 is only one hour away from 00:00. The same is true for the months of the year, since December is only one month away from January. Using techniques such as trigonometric functions - sine and cosine transformations - makes it possible to represent cyclic patterns and avoid inconsistencies in data representation. This technique is called cyclic coding and can significantly improve the predictive power of models.</p> In\u00a0[7]: Copied! <pre># Cyclical encoding\n# ==============================================================================\ndef cyclical_encoding(data: pd.Series, cycle_length: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Encode a cyclical feature with two new features sine and cosine.\n    The minimum value of the feature is assumed to be 0. The maximum value\n    of the feature is passed as an argument.\n      \n    Parameters\n    ----------\n    data : pandas Series\n        Series with the feature to encode.\n    cycle_length : int\n        The length of the cycle. For example, 12 for months, 24 for hours, etc.\n        This value is used to calculate the angle of the sin and cos.\n\n    Returns\n    -------\n    result : pandas DataFrame\n        Dataframe with the two new features, sin and cos.\n\n    \"\"\"\n\n    sin = np.sin(2 * np.pi * data/cycle_length)\n    cos = np.cos(2 * np.pi * data/cycle_length)\n    result =  pd.DataFrame({\n                  f\"{data.name}_sin\": sin,\n                  f\"{data.name}_cos\": cos\n              })\n\n    return result\n</pre> # Cyclical encoding # ============================================================================== def cyclical_encoding(data: pd.Series, cycle_length: int) -&gt; pd.DataFrame:     \"\"\"     Encode a cyclical feature with two new features sine and cosine.     The minimum value of the feature is assumed to be 0. The maximum value     of the feature is passed as an argument.            Parameters     ----------     data : pandas Series         Series with the feature to encode.     cycle_length : int         The length of the cycle. For example, 12 for months, 24 for hours, etc.         This value is used to calculate the angle of the sin and cos.      Returns     -------     result : pandas DataFrame         Dataframe with the two new features, sin and cos.      \"\"\"      sin = np.sin(2 * np.pi * data/cycle_length)     cos = np.cos(2 * np.pi * data/cycle_length)     result =  pd.DataFrame({                   f\"{data.name}_sin\": sin,                   f\"{data.name}_cos\": cos               })      return result In\u00a0[8]: Copied! <pre># Ciclical encoding of month, day of week and hour\n# ==============================================================================\nmonth_encoded = cyclical_encoding(calendar_features['month'], cycle_length=12)\nday_of_week_encoded = cyclical_encoding(calendar_features['day_of_week'], cycle_length=7)\nhour_encoded = cyclical_encoding(calendar_features['hour'], cycle_length=24)\n\ncyclical_features = pd.concat([month_encoded, day_of_week_encoded, hour_encoded], axis=1)\ncyclical_features.head()\n</pre> # Ciclical encoding of month, day of week and hour # ============================================================================== month_encoded = cyclical_encoding(calendar_features['month'], cycle_length=12) day_of_week_encoded = cyclical_encoding(calendar_features['day_of_week'], cycle_length=7) hour_encoded = cyclical_encoding(calendar_features['hour'], cycle_length=24)  cyclical_features = pd.concat([month_encoded, day_of_week_encoded, hour_encoded], axis=1) cyclical_features.head() Out[8]: month_sin month_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date_time 2011-01-01 00:00:00 0.5 0.866025 -0.974928 -0.222521 0.000000 1.000000 2011-01-01 01:00:00 0.5 0.866025 -0.974928 -0.222521 0.258819 0.965926 2011-01-01 02:00:00 0.5 0.866025 -0.974928 -0.222521 0.500000 0.866025 2011-01-01 03:00:00 0.5 0.866025 -0.974928 -0.222521 0.707107 0.707107 2011-01-01 04:00:00 0.5 0.866025 -0.974928 -0.222521 0.866025 0.500000 In\u00a0[9]: Copied! <pre># Plot value of sin and cos for each hour\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsp = ax.scatter(\n         hour_encoded[\"hour_sin\"],\n         hour_encoded[\"hour_cos\"],\n         c    = calendar_features['hour'],\n         cmap = 'viridis'\n     )\nax.set(\n    xlabel=\"sin(hour)\",\n    ylabel=\"cos(hour)\",\n)\n_ = fig.colorbar(sp)\nplt.show();\n</pre> # Plot value of sin and cos for each hour # ============================================================================== fig, ax = plt.subplots(figsize=(3.5, 3)) sp = ax.scatter(          hour_encoded[\"hour_sin\"],          hour_encoded[\"hour_cos\"],          c    = calendar_features['hour'],          cmap = 'viridis'      ) ax.set(     xlabel=\"sin(hour)\",     ylabel=\"cos(hour)\", ) _ = fig.colorbar(sp) plt.show(); <p> \u270e Note </p> <p>See Cyclical features in time series forecasting for a more detailed description of strategies for encoding cyclic features.</p>"},{"location":"user_guides/calendar-features.html#calendar-features","title":"Calendar features\u00b6","text":""},{"location":"user_guides/calendar-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/calendar-features.html#extract-calendar-features","title":"Extract calendar features\u00b6","text":""},{"location":"user_guides/calendar-features.html#sunlight-related-features","title":"Sunlight-Related Features\u00b6","text":""},{"location":"user_guides/calendar-features.html#cyclical-encoding","title":"Cyclical encoding\u00b6","text":""},{"location":"user_guides/categorical-features.html","title":"Categorical features","text":"<p>In the field of machine learning, categorical features play a crucial role in determining the predictive ability of a model. Categorical features are features that can take a limited number of values, such as color, gender or location. While these features can provide useful insights into patterns and relationships within data, they also present unique challenges for machine learning models.</p> <p>One of these challenges is the need to transform categorical features before they can be used by most models. This transformation involves converting categorical values into numerical values that can be processed by machine learning algorithms.</p> <p>Another challenge is dealing with infrequent categories, which can lead to biased models. If a categorical feature has a large number of categories, but some of them are rare or appear infrequently in the data, the model may not be able to learn accurately from these categories, resulting in biased predictions and inaccurate results.</p> <p>Despite these difficulties, categorical features are still an essential component in many use cases. When properly encoded and handled, machine learning models can effectively learn from patterns and relationships in categorical data, leading to better predictions.</p> <p>This document provides an overview of three of the most commonly used transformations: one-hot encoding, ordinal encoding, and target encoding. It explains how to apply them in the skforecast package using scikit-learn encoders, which provide a convenient and flexible way to pre-process data. It also shows how to use the native implementation of four popular gradient boosting frameworks \u2013 XGBoost, LightGBM, scikit-learn's HistogramGradientBoosting and CatBoost \u2013 to handle categorical features directly in the model.</p> <p>For a comprehensive demonstration of the use of categorical features in time series forecasting, check out the article Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost.</p> <p> \u270e Note </p> <p>All of the transformations described in this document can be applied to the entire dataset, regardless of the forecaster. However, it is important to ensure that the transformations are learned only from the training data to avoid information leakage. Furthermore, the same transformation should be applied to the input data during prediction. To reduce the likelihood of errors and to ensure consistent application of the transformations, it is advisable to include the transformation within the forecaster object, so that it is handled internally.</p> In\u00a0[28]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport lightgbm\nimport xgboost\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import TargetEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.datasets import fetch_dataset\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ncolor = '\\033[1m\\033[38;5;208m' \nprint(f\"{color}Version scikit-learn: {sklearn.__version__}\")\nprint(f\"{color}Version lightgbm: {lightgbm.__version__}\")\nprint(f\"{color}Version xgboost: {xgboost.__version__}\")\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt import sklearn import lightgbm import xgboost from lightgbm import LGBMRegressor from sklearn.ensemble import HistGradientBoostingRegressor from xgboost import XGBRegressor from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import TargetEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import make_column_transformer from sklearn.compose import make_column_selector from sklearn.pipeline import make_pipeline from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.datasets import fetch_dataset  plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 color = '\\033[1m\\033[38;5;208m'  print(f\"{color}Version scikit-learn: {sklearn.__version__}\") print(f\"{color}Version lightgbm: {lightgbm.__version__}\") print(f\"{color}Version xgboost: {xgboost.__version__}\") <pre>Version scikit-learn: 1.5.1\nVersion lightgbm: 4.4.0\nVersion xgboost: 2.1.0\n</pre> In\u00a0[29]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing', raw=True)\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(name='bike_sharing', raw=True) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> In\u00a0[30]: Copied! <pre># Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('H')\ndata = data.sort_index()\ndata['holiday'] = data['holiday'].astype(int)\ndata = data[['holiday', 'weather', 'temp', 'hum', 'users']]\ndata[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str)\nprint(data.dtypes)\ndata.head(3)\n</pre>  # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('H') data = data.sort_index() data['holiday'] = data['holiday'].astype(int) data = data[['holiday', 'weather', 'temp', 'hum', 'users']] data[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str) print(data.dtypes) data.head(3) <pre>holiday     object\nweather     object\ntemp       float64\nhum        float64\nusers      float64\ndtype: object\n</pre> <pre>/tmp/ipykernel_10939/511558958.py:5: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  data = data.asfreq('H')\n</pre> Out[30]: holiday weather temp hum users date_time 2011-01-01 00:00:00 0 clear 9.84 81.0 16.0 2011-01-01 01:00:00 0 clear 9.02 80.0 40.0 2011-01-01 02:00:00 0 clear 9.02 80.0 32.0 <p>Only part of the data is used to simplify the example.</p> In\u00a0[31]: Copied! <pre># Split train-test\n# ==============================================================================\nstart_train = '2012-06-01 00:00:00'\nend_train = '2012-07-31 23:59:00'\nend_test = '2012-08-15 23:59:00'\ndata_train = data.loc[start_train:end_train, :]\ndata_test  = data.loc[end_train:end_test, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split train-test # ============================================================================== start_train = '2012-06-01 00:00:00' end_train = '2012-07-31 23:59:00' end_test = '2012-08-15 23:59:00' data_train = data.loc[start_train:end_train, :] data_test  = data.loc[end_train:end_test, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Dates train : 2012-06-01 00:00:00 --- 2012-07-31 23:00:00  (n=1464)\nDates test  : 2012-08-01 00:00:00 --- 2012-08-15 23:00:00  (n=360)\n</pre> In\u00a0[32]: Copied! <pre># ColumnTransformer with one-hot encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical features (no numerical)\n# using one-hot encoding. Numeric features are left untouched. For binary\n# features, only one column is created.\none_hot_encoder = make_column_transformer(\n                    (\n                        OneHotEncoder(sparse_output=False, drop='if_binary'),\n                        make_column_selector(dtype_exclude=np.number)\n                    ),\n                    remainder=\"passthrough\",\n                    verbose_feature_names_out=False,\n                ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with one-hot encoding # ============================================================================== # A ColumnTransformer is used to transform categorical features (no numerical) # using one-hot encoding. Numeric features are left untouched. For binary # features, only one column is created. one_hot_encoder = make_column_transformer(                     (                         OneHotEncoder(sparse_output=False, drop='if_binary'),                         make_column_selector(dtype_exclude=np.number)                     ),                     remainder=\"passthrough\",                     verbose_feature_names_out=False,                 ).set_output(transform=\"pandas\") In\u00a0[33]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags = 5,\n                 transformer_exog = one_hot_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags = 5,                  transformer_exog = one_hot_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[33]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7ff5685a1b80&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: h \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-08-09 11:10:12 \nLast fit date: 2024-08-09 11:10:12 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>Once the forecaster has been trained, the transformer can be inspected (feature_names_in, feature_names_out, ...) by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[34]: Copied! <pre># Access to the transformer used for exogenous features\n# ==============================================================================\nprint(forecaster.transformer_exog.get_feature_names_out())\nforecaster.transformer_exog\n</pre> # Access to the transformer used for exogenous features # ============================================================================== print(forecaster.transformer_exog.get_feature_names_out()) forecaster.transformer_exog <pre>['holiday_1' 'weather_clear' 'weather_mist' 'weather_rain' 'temp' 'hum']\n</pre> Out[34]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7ff5685a1b80&gt;)],\n                  verbose_feature_names_out=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0ColumnTransformer?Documentation for ColumnTransformeriFitted<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7ff5685a1b80&gt;)],\n                  verbose_feature_names_out=False)</pre> onehotencoder<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7ff5685a1b80&gt;</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(drop='if_binary', sparse_output=False)</pre> remainder<pre>['temp', 'hum']</pre> passthrough<pre>passthrough</pre> In\u00a0[35]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[35]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> <p> \u270e Note </p> <p>It is possible to apply a transformation to the entire dataset independent of the forecaster. However, it is crucial to ensure that the transformations are only learned from the training data to avoid information leakage. In addition, the same transformation should be applied to the input data during prediction. It is therefore advisable to incorporate the transformation into the forecaster, so that it is handled internally. This approach ensures consistency in the application of transformations and reduces the likelihood of errors.</p> <p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y()</code> method to generate the matrices used by the forecaster to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p> In\u00a0[36]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\n\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    )  print(X_train.dtypes) X_train.head() <pre>lag_1            float64\nlag_2            float64\nlag_3            float64\nlag_4            float64\nlag_5            float64\nholiday_1        float64\nweather_clear    float64\nweather_mist     float64\nweather_rain     float64\ntemp             float64\nhum              float64\ndtype: object\n</pre> Out[36]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 0.0 1.0 0.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 1.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 1.0 0.0 0.0 13.12 76.0 In\u00a0[37]: Copied! <pre># Transform exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features])\nexog_transformed.head()\n</pre> # Transform exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features]) exog_transformed.head() Out[37]: holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 00:00:00 0.0 1.0 0.0 0.0 9.84 81.0 2011-01-01 01:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 02:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 03:00:00 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 04:00:00 0.0 1.0 0.0 0.0 9.84 75.0 In\u00a0[38]: Copied! <pre># ColumnTransformer with ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\nordinal_encoder = make_column_transformer(\n                      (\n                          OrdinalEncoder(\n                              handle_unknown='use_encoded_value',\n                              unknown_value=-1,\n                              encoded_missing_value=-1\n                          ),\n                          make_column_selector(dtype_exclude=np.number)\n                      ),\n                      remainder=\"passthrough\",\n                      verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. ordinal_encoder = make_column_transformer(                       (                           OrdinalEncoder(                               handle_unknown='use_encoded_value',                               unknown_value=-1,                               encoded_missing_value=-1                           ),                           make_column_selector(dtype_exclude=np.number)                       ),                       remainder=\"passthrough\",                       verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\") In\u00a0[39]: Copied! <pre># Create and fit a forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 5,\n                 transformer_exog = ordinal_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit a forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 5,                  transformer_exog = ordinal_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[39]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7ff5685a32f0&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: h \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-08-09 11:10:13 \nLast fit date: 2024-08-09 11:10:13 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> In\u00a0[40]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'],\n                       exog = data.loc[:end_train, exog_features]\n                   )\n\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'],                        exog = data.loc[:end_train, exog_features]                    )  print(X_train.dtypes) X_train.head() <pre>lag_1      float64\nlag_2      float64\nlag_3      float64\nlag_4      float64\nlag_5      float64\nholiday    float64\nweather    float64\ntemp       float64\nhum        float64\ndtype: object\n</pre> Out[40]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 1.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 0.0 13.12 76.0 <p>Once the forecaster has been trained, the transformer can be inspected by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[41]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[41]: <pre>2012-08-01 00:00:00    89.096098\n2012-08-01 01:00:00    57.749964\n2012-08-01 02:00:00    29.263922\nFreq: h, Name: pred, dtype: float64</pre> <p> \u26a0 Warning </p> <p>TargetEncoder differs from the other transformers in scikit-learn in that it requires not only the features to be transformed but also the response variable (target), in the context of prediction, this is the time series. Currently, the only transformers allowed in the prediction classes are those that do not require the target variable to be fitted. Therefore, to use target encoding, transformations must be applied outside the Forecaster object.</p> In\u00a0[42]: Copied! <pre># ColumnTransformer with target encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using target encoding. Numeric features are left untouched. TargetEncoder\n# considers missing values, such as np.nan or None, as another category and\n# encodes them like any other category. Categories that are not seen during fit\n# are encoded with the target mean\n\ntarget_encoder = make_column_transformer(\n                     (\n                         TargetEncoder(\n                             categories   = 'auto',\n                             target_type  = 'continuous',\n                             smooth       = 'auto',\n                             random_state = 9874\n                         ),\n                         make_column_selector(dtype_exclude=np.number)\n                     ),\n                     remainder=\"passthrough\",\n                     verbose_feature_names_out=False,\n                 ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with target encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using target encoding. Numeric features are left untouched. TargetEncoder # considers missing values, such as np.nan or None, as another category and # encodes them like any other category. Categories that are not seen during fit # are encoded with the target mean  target_encoder = make_column_transformer(                      (                          TargetEncoder(                              categories   = 'auto',                              target_type  = 'continuous',                              smooth       = 'auto',                              random_state = 9874                          ),                          make_column_selector(dtype_exclude=np.number)                      ),                      remainder=\"passthrough\",                      verbose_feature_names_out=False,                  ).set_output(transform=\"pandas\") In\u00a0[43]: Copied! <pre># Transform the exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = target_encoder.fit_transform(\n                       X = data.loc[:end_train, exog_features],\n                       y = data.loc[:end_train, 'users']\n                   )\nexog_transformed.head()\n</pre> # Transform the exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = target_encoder.fit_transform(                        X = data.loc[:end_train, exog_features],                        y = data.loc[:end_train, 'users']                    ) exog_transformed.head() Out[43]: holiday weather temp hum date_time 2011-01-01 00:00:00 172.823951 188.121327 9.84 81.0 2011-01-01 01:00:00 172.607889 187.330734 9.02 80.0 2011-01-01 02:00:00 173.476675 189.423278 9.02 80.0 2011-01-01 03:00:00 172.823951 188.121327 9.84 75.0 2011-01-01 04:00:00 172.823951 188.121327 9.84 75.0 <p> \u26a0 Warning </p> <p>When deploying models in production, it is strongly recommended to avoid using automatic detection based on pandas <code>category</code> type columns. Although pandas provides an internal coding for these columns, it is not consistent across different datasets and may vary depending on the categories present in each one. It is therefore crucial to be aware of this issue and to take appropriate measures to ensure consistency in the coding of categorical features when deploying models in production.</p> <p>At the time of writing, the authors have observed that <code>LightGBM</code> and <code>HistGradientBoostingRegressor</code> internally manage changes in the coding of categories to ensure consistency.</p> <p>If the user still wishes to rely on automatic detection of categorical features based on pandas data types, categorical variables must first be encoded as integers (ordinal encoding) and then stored as category type. This is necessary because skforecast uses a numeric numpy array internally to speed up the calculation.</p> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>When creating a forecaster with <code>LGBMRegressor</code>, it is necessary to specify the names of the categorical columns using the <code>fit_kwargs</code> argument. This is because the <code>categorical_feature</code> argument is only specified in the <code>fit</code> method of <code>LGBMRegressor</code>, and not during its initialization.</p> In\u00a0[44]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[45]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(random_state=963, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs       = {'categorical_feature': categorical_features}\n             )      \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterAutoreg(                  regressor        = LGBMRegressor(random_state=963, verbose=-1),                  lags             = 5,                  transformer_exog = transformer_exog,                  fit_kwargs       = {'categorical_feature': categorical_features}              )       forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[46]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[46]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[47]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nregressor = forecaster.regressor\ncat_index = regressor.booster_.params.get('categorical_column')\nif cat_index is not None:\n    features_in_model = regressor.booster_.feature_name()\n    cat_features_in_model = [features_in_model[i] for i in cat_index]\ncat_features_in_model\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== regressor = forecaster.regressor cat_index = regressor.booster_.params.get('categorical_column') if cat_index is not None:     features_in_model = regressor.booster_.feature_name()     cat_features_in_model = [features_in_model[i] for i in cat_index] cat_features_in_model Out[47]: <pre>['holiday', 'weather']</pre> In\u00a0[48]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[49]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[50]: Copied! <pre># Create and fit forecaster with automatic detection of categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(random_state=963, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs       = {'categorical_feature': 'auto'}\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster with automatic detection of categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor        = LGBMRegressor(random_state=963, verbose=-1),                  lags             = 5,                  transformer_exog = transformer_exog,                  fit_kwargs       = {'categorical_feature': 'auto'}              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[51]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[51]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> <p>As with any other forecaster, the matrices used during model training can be created with <code>create_train_X_y</code>.</p> In\u00a0[52]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'],\n                       exog = data.loc[:end_train, exog_features]\n                   )\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'],                        exog = data.loc[:end_train, exog_features]                    ) X_train.head() Out[52]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0 0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0 0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0 0 13.12 76.0 In\u00a0[53]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nregressor = forecaster.regressor\ncat_index = regressor.booster_.params.get('categorical_column')\nif cat_index is not None:\n    features_in_model = regressor.booster_.feature_name()\n    cat_features_in_model = [features_in_model[i] for i in cat_index]\ncat_features_in_model\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== regressor = forecaster.regressor cat_index = regressor.booster_.params.get('categorical_column') if cat_index is not None:     features_in_model = regressor.booster_.feature_name()     cat_features_in_model = [features_in_model[i] for i in cat_index] cat_features_in_model Out[53]: <pre>['holiday', 'weather']</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>When creating a forecaster using <code>HistogramGradientBoosting</code>, the names of the categorical columns should be specified during the instantiation by passing them as a list to the <code>categorical_feature</code> argument.</p> In\u00a0[54]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[55]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(\n                                 categorical_features = categorical_features,\n                                 random_state = 963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(                                  categorical_features = categorical_features,                                  random_state = 963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[56]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[56]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: h, Name: pred, dtype: float64</pre> <p><code>HistGradientBoostingRegressor</code> stores a boolean mask indicating which features were considered categorical. It will be <code>None</code> if there are no categorical features.</p> In\u00a0[57]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nforecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_]\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== forecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_] Out[57]: <pre>array(['holiday', 'weather'], dtype=object)</pre> In\u00a0[58]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[30]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[31]: Copied! <pre># Create and fit forecaster with automatic detection of categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor        = HistGradientBoostingRegressor(random_state=963, categorical_features='from_dtype'),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster with automatic detection of categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor        = HistGradientBoostingRegressor(random_state=963, categorical_features='from_dtype'),                  lags             = 5,                  transformer_exog = transformer_exog,              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[32]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nforecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_]\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== forecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_] Out[32]: <pre>array(['holiday', 'weather'], dtype=object)</pre> In\u00a0[33]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[33]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: h, Name: pred, dtype: float64</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>At the time of writing, the <code>XGBRegressor</code> module does not provide an option to specify the names of categorical features. Instead, the feature types are specified by passing a list of strings to the <code>feature_types</code> argument, where 'c' denotes categorical and 'q' numeric features. The <code>enable_categorical</code> argument must also be set to <code>True</code>.</p> <p>Determining the positions of each column to create a list of feature types can be a challenging task. The shape of the data matrix depends on two factors, the number of lags used and the transformations applied to the exogenous variables. However, there is a workaround to this problem. First, create a forecaster without specifying the <code>feature_types</code> argument. Next, the <code>create_train_X_y</code> method can be used with a small sample of data to determine the position of each feature. Once the position of each feature has been determined, the <code>set_params()</code> method can be used to specify the values of <code>feature_types</code>. By following this approach it is possible to ensure that the feature types are correctly specified, thus avoiding any errors that may occur due to incorrect specification.</p> In\u00a0[34]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") <p>A forecaster is created without specifying the <code>feature_types</code> argument.</p> In\u00a0[35]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 tree_method        = 'hist',\n                                 random_state       = 12345,\n                                 enable_categorical = True,\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  tree_method        = 'hist',                                  random_state       = 12345,                                  enable_categorical = True,                              ),                  lags = 5,                  transformer_exog = transformer_exog              ) <p>Once the forecaster is instantiated, its <code>create_train_X_y()</code> method is used to generate the training matrices that allow the user to identify the positions of the variables.</p> In\u00a0[36]: Copied! <pre># Create training matrices using a sample of the training data\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'][:10],\n                       exog = data.loc[:end_train, exog_features][:10]\n                   )\nX_train.head(2)\n</pre> # Create training matrices using a sample of the training data # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'][:10],                        exog = data.loc[:end_train, exog_features][:10]                    ) X_train.head(2) Out[36]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 <p>Create a list to identify which columns in the training matrix are numeric ('q') and categorical ('c').</p> In\u00a0[37]: Copied! <pre>feature_types = [\n    \"c\" if X_train[col].dtype.name in [\"object\", \"category\"] or col in categorical_features\n    else \"q\"\n    for col in X_train.columns\n]\nfeature_types\n</pre> feature_types = [     \"c\" if X_train[col].dtype.name in [\"object\", \"category\"] or col in categorical_features     else \"q\"     for col in X_train.columns ] feature_types Out[37]: <pre>['q', 'q', 'q', 'q', 'q', 'c', 'c', 'q', 'q']</pre> <p>Update the regressor parameters using the forecaster's <code>set_params</code> method and fit.</p> In\u00a0[38]: Copied! <pre># Update regressor parameters\n# ==============================================================================\nforecaster.set_params({'feature_types': feature_types})\n</pre> # Update regressor parameters # ============================================================================== forecaster.set_params({'feature_types': feature_types}) In\u00a0[39]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[40]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[40]: <pre>2012-08-01 00:00:00    82.127357\n2012-08-01 01:00:00    45.740013\n2012-08-01 02:00:00    23.580956\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[41]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nfeature_types = np.array(forecaster.regressor.get_booster().feature_types)\nfeatures_in_model = np.array(forecaster.regressor.get_booster().feature_names)\nfeatures_in_model[feature_types == 'c']\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== feature_types = np.array(forecaster.regressor.get_booster().feature_types) features_in_model = np.array(forecaster.regressor.get_booster().feature_names) features_in_model[feature_types == 'c'] Out[41]: <pre>array(['holiday', 'weather'], dtype='&lt;U7')</pre> In\u00a0[59]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[42]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After the encoding, the features are converted back to category type so \n# that they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After the encoding, the features are converted back to category type so  # that they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[43]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 enable_categorical=True,\n                                 tree_method='hist',\n                                 random_state=963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  enable_categorical=True,                                  tree_method='hist',                                  random_state=963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[44]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[44]: <pre>2012-08-01 00:00:00    82.127357\n2012-08-01 01:00:00    45.740013\n2012-08-01 02:00:00    23.580956\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[45]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nfeature_types = np.array(forecaster.regressor.get_booster().feature_types)\nfeatures_in_model = np.array(forecaster.regressor.get_booster().feature_names)\nfeatures_in_model[feature_types == 'c']\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== feature_types = np.array(forecaster.regressor.get_booster().feature_types) features_in_model = np.array(forecaster.regressor.get_booster().feature_names) features_in_model[feature_types == 'c'] Out[45]: <pre>array(['holiday', 'weather'], dtype='&lt;U7')</pre>"},{"location":"user_guides/categorical-features.html#categorical-features","title":"Categorical features\u00b6","text":""},{"location":"user_guides/categorical-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":"<p>The dataset used in this user guide consists of information on the number of users of a bicycle rental service, in addition to weather variables and holiday data. Two of the variables in the dataset, <code>holiday</code> and <code>weather</code>, are categorical.</p>"},{"location":"user_guides/categorical-features.html#one-hot-encoding","title":"One Hot Encoding\u00b6","text":"<p>One hot encoding, also known as dummy encoding or one-of-K encoding, consists of replacing the categorical variable with a set of binary variables that take the value 0 or 1 to indicate whether a particular category is present in an observation. For example, suppose a dataset contains a categorical variable called \"color\" with the possible values of \"red,\" \"blue,\" and \"green\". Using one hot encoding, this variable is converted into three binary variables such as <code>color_red</code>, <code>color_blue</code>, and <code>color_green</code>, where each variable takes a value of 0 or 1 depending on the category.</p> <p>The OneHotEncoder class in scikit-learn can be used to transform any categorical feature with n possible values into n new binary features, where one of them takes the value 1, and all the others take the value 0. The <code>OneHotEncoder</code> can be configured to handle certain corner cases, including unknown categories, missing values, and infrequent categories.</p> <ul> <li><p>When <code>handle_unknown='ignore'</code> and <code>drop</code> is not <code>None</code>, unknown categories are encoded as zeros. Additionally, if a feature contains both <code>np.nan</code> and <code>None</code>, they are considered separate categories.</p> </li> <li><p>It supports the aggregation of infrequent categories into a single output for each feature. The parameters to enable the aggregation of infrequent categories are <code>min_frequency</code> and <code>max_categories</code>. By setting <code>handle_unknown</code> to 'infrequent_if_exist', unknown categories are considered infrequent.</p> </li> <li><p>To avoid collinearity between features, it is possible to drop one of the categories per feature using the <code>drop</code> argument. This is especially important when using linear models.</p> </li> </ul> <p>ColumnTransformers in scikit-learn provide a powerful way to define transformations and apply them to specific features. By encapsulating the <code>OneHotEncoder</code> in a <code>ColumnTransformer</code> object, it can be passed to a forecaster using the <code>transformer_exog</code> argument.</p>"},{"location":"user_guides/categorical-features.html#ordinal-encoding","title":"Ordinal encoding\u00b6","text":"<p>Ordinal encoding is a technique used to convert categorical variables into numerical variables. Each category is assigned a unique numerical value based on its order or rank, as determined by a chosen criterion such as frequency or importance. This encoding method is particularly useful when categories have a natural order or ranking, such as educational qualifications. However, it is important to note that the numerical values assigned to each category do not represent any inherent numerical difference between them, but simply provide a numerical representation.</p> <p>The scikit-learn library provides the OrdinalEncoder class, which allows users to replace categorical variables with ordinal numbers ranging from 0 to n_categories-1. In addition, this class includes the <code>encoded_missing_value</code> parameter, which allows for the encoding of missing values. It is important to note that this implementation arbitrarily assigns numbers to categories on a first-seen-first-served basis. Users should therefore exercise caution when interpreting the numerical values assigned to the categories. Other implementations, such as the Feature-engine, numbers can be ordered based on the mean of the target.</p>"},{"location":"user_guides/categorical-features.html#target-encoding","title":"Target encoding\u00b6","text":"<p>Target encoding is a technic that encodes categorical variables based on the relationship between the categories and the target variable. Each category is encoded based on a shrinked estimate of the average target values for observations belonging to the category. The encoding scheme mixes the global target mean with the target mean conditioned on the value of the category.</p> <p>For example, suppose a categorical variable \"City\" with categories \"New York,\" \"Los Angeles,\" and \"Chicago,\" and a target variable \"Salary.\" One can calculate the mean salary for each city based on the training data, and use these mean values to encode the categories.</p> <p>This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories is location-based such as zip code or region.</p> <p>The TargetEncoder class is available in Scikit-learn (since version 1.3). <code>TargetEncoder</code> considers missing values, such as <code>np.nan</code> or <code>None</code>, as another category and encodes them like any other category. Categories that are not seen during fit are encoded with the target mean, i.e. <code>target_mean_</code>. A more detailed description of target encoding can be found in the scikit-learn user guide.</p>"},{"location":"user_guides/categorical-features.html#native-implementation-for-categorical-features","title":"Native implementation for categorical features\u00b6","text":"<p>Some machine learning models, including XGBoost, LightGBM, CatBoost, and HistGradientBoostingRegressor, provide built-in methods to handle categorical features, but they assume that the input categories are integers starting from 0 up to the number of categories [0, 1, ..., n_categories-1]. In practice, categorical variables are not coded with numbers but with strings, so an intermediate transformation step is necessary. Two options are:</p> <ul> <li><p>Set columns with categorical variables to the type <code>category</code>. For each column, the data structure consists of an array of categories and an array of integer values (codes) that point to the actual value of the array of categories. That is, internally it is a numeric array with a mapping that relates each value to a category. Models are able to automatically identify the columns of type <code>category</code> and access their internal codes.</p> </li> <li><p>Preprocess the categorical columns with an <code>OrdinalEncoder</code> to transform their values to integers and explicitly indicate that the columns should be treated as categorical.</p> </li> </ul>"},{"location":"user_guides/categorical-features.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/categorical-features.html#scikit-learn-histogramgradientboosting","title":"Scikit-learn HistogramGradientBoosting\u00b6","text":""},{"location":"user_guides/categorical-features.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/categorical-features.html#catboost","title":"CatBoost\u00b6","text":"<p>Unfortunately, the current version of skforecast is not compatible with CatBoost's built-in handling of categorical features. The issue arises because CatBoost only accepts categorical features as integers, while skforecast converts input data to floats for faster computation using numpy arrays in the internal prediction process. If a CatBoost model is required, an external encoder should be used for the categorical variables.</p>"},{"location":"user_guides/datasets.html","title":"Datasets","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== from skforecast.datasets import fetch_dataset <p>By default, the data is structured as a pandas dataframe with a datetime index and frequency. Additionally, a concise description is printed for quick reference.</p> In\u00a0[2]: Copied! <pre># Download data \n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\")\ndata.head()\n</pre> # Download data  # ============================================================================== data = fetch_dataset(name=\"bike_sharing\") data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[2]: holiday workingday weather temp atemp hum windspeed users month hour weekday date_time 2011-01-01 00:00:00 0.0 0.0 clear 9.84 14.395 81.0 0.0 16.0 1 0 5 2011-01-01 01:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 40.0 1 1 5 2011-01-01 02:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 32.0 1 2 5 2011-01-01 03:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 13.0 1 3 5 2011-01-01 04:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 1.0 1 4 5 <p>Downloading raw data, without any preprocessing, is possible by specifying the <code>raw=True</code> argument.</p> In\u00a0[3]: Copied! <pre># Download raw data \n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\", raw=True)\ndata.head()\n</pre> # Download raw data  # ============================================================================== data = fetch_dataset(name=\"bike_sharing\", raw=True) data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> Out[3]: date_time holiday workingday weather temp atemp hum windspeed users month hour weekday 0 2011-01-01 00:00:00 0.0 0.0 clear 9.84 14.395 81.0 0.0 16.0 1 0 5 1 2011-01-01 01:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 40.0 1 1 5 2 2011-01-01 02:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 32.0 1 2 5 3 2011-01-01 03:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 13.0 1 3 5 4 2011-01-01 04:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 1.0 1 4 5"},{"location":"user_guides/datasets.html#data-sets","title":"Data sets\u00b6","text":"<p>All datasets used in the skforecast library and related tutorials are accessible using the <code>skforecast.datasets.fetch_dataset()</code> function. Each dataset in this collection comes with a description of its time series and a reference to its original source.</p> <p>Available data sets are stored at skforecast-datasets.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html","title":"Dependent multivariate series forecasting\"","text":"<p> \u270e Note </p> <p>Skforecast offers additional approaches to create Global Forecasting Models:</p> <ul> <li> Global Forecasting Models: Independent multi-series forecasting </li> <li> Global Forecasting Models: Time series with different lengths and different exogenous variables </li> <li> Global Forecasting Models: Independent multi-series forecasting using window and custom features </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import random_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import random_search_forecaster_multiseries from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/guangyuan_air_pollution.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata = data[['CO', 'SO2', 'PM2.5']]\ndata.head()\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/guangyuan_air_pollution.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data = data[['CO', 'SO2', 'PM2.5']] data.head() Out[2]: CO SO2 PM2.5 date 2013-03-01 9600.0 204.0 181.0 2013-03-02 20198.0 674.0 633.0 2013-03-03 47195.0 1661.0 1956.0 2013-03-04 15000.0 485.0 438.0 2013-03-05 59594.0 2001.0 3388.0 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2016-05-31 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2013-03-01 00:00:00 --- 2016-05-31 00:00:00(n=1188)\nTest dates  : 2016-06-01 00:00:00 --- 2017-02-28 00:00:00(n=273)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['CO'].plot(label='train', ax=axes[0])\ndata_test['CO'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('Concentration (ug/m^3)')\naxes[0].set_title('CO')\naxes[0].legend()\n\ndata_train['SO2'].plot(label='train', ax=axes[1])\ndata_test['SO2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('Concentration (ug/m^3)')\naxes[1].set_title('SO2')\n\ndata_train['PM2.5'].plot(label='train', ax=axes[2])\ndata_test['PM2.5'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('Concentration (ug/m^3)')\naxes[2].set_title('PM2.5')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['CO'].plot(label='train', ax=axes[0]) data_test['CO'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('Concentration (ug/m^3)') axes[0].set_title('CO') axes[0].legend()  data_train['SO2'].plot(label='train', ax=axes[1]) data_test['SO2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('Concentration (ug/m^3)') axes[1].set_title('SO2')  data_train['PM2.5'].plot(label='train', ax=axes[2]) data_test['PM2.5'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('Concentration (ug/m^3)') axes[2].set_title('PM2.5')  fig.tight_layout() plt.show(); <p> \u270e Note </p> <p>Starting from version skforecast 0.9.0, the <code>ForecasterAutoregMultiVariate</code> now includes the <code>n_jobs</code> parameter, allowing multi-process parallelization. This allows to train regressors for all steps simultaneously.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[5]: Copied! <pre># Create and fit forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 n_jobs             = 'auto'\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None,                  n_jobs             = 'auto'              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: StandardScaler() \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:45:15 \nLast fit date: 2024-07-29 16:45:16 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> <p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul> In\u00a0[6]: Copied! <pre># Predict with forecaster MultiVariate\n# ==============================================================================\n# Predict as many steps as defined in the forecaster initialization\npredictions = forecaster.predict()\ndisplay(predictions)\n</pre> # Predict with forecaster MultiVariate # ============================================================================== # Predict as many steps as defined in the forecaster initialization predictions = forecaster.predict() display(predictions) CO 2016-06-01 15774.629629 2016-06-02 20083.444808 2016-06-03 20050.403708 2016-06-04 13376.404100 2016-06-05 19181.270877 2016-06-06 16992.101160 2016-06-07 12816.943637 In\u00a0[7]: Copied! <pre># Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) CO 2016-06-01 15774.629629 2016-06-05 19181.270877 In\u00a0[8]: Copied! <pre># Predict with prediction intervals\npredictions = forecaster.predict_interval(random_state=9871)\ndisplay(predictions)\n</pre> # Predict with prediction intervals predictions = forecaster.predict_interval(random_state=9871) display(predictions) CO CO_lower_bound CO_upper_bound 2016-06-01 15774.629629 9558.136276 23292.089686 2016-06-02 20083.444808 12871.968341 29431.747334 2016-06-03 20050.403708 11856.264198 31166.420347 2016-06-04 13376.404100 4630.535566 22596.844854 2016-06-05 19181.270877 12141.471031 28024.180402 2016-06-06 16992.101160 8467.479012 27957.219037 2016-06-07 12816.943637 4641.922139 21971.627676 In\u00a0[9]: Copied! <pre># Backtesting MultiVariate\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data,\n                                           steps                 = 7,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_train),\n                                           fixed_train_size      = False,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = False,\n                                           n_jobs                = 'auto',\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting MultiVariate # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data,                                            steps                 = 7,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_train),                                            fixed_train_size      = False,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = False,                                            n_jobs                = 'auto',                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/39 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 CO 16577.420604 <pre>\nBacktest predictions\n</pre> Out[9]: CO 2016-06-01 15774.629629 2016-06-02 20083.444808 2016-06-03 20050.403708 2016-06-04 13376.404100 In\u00a0[10]: Copied! <pre># Create and forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n</pre> # Create and forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None              ) In\u00a0[11]: Copied! <pre># Random search MultiVariate\n# ==============================================================================\nlags_grid = [7, 14]\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),\n    'max_depth': np.arange(start=3, stop=6, step=1, dtype=int)\n}\n\nresults = random_search_forecaster_multiseries(\n              forecaster            = forecaster,\n              series                = data,\n              exog                  = None,\n              lags_grid             = lags_grid,\n              param_distributions   = param_distributions,\n              steps                 = 7,\n              metric                = 'mean_absolute_error',\n              aggregate_metric      = 'weighted_average',\n              initial_train_size    = len(data_train),\n              fixed_train_size      = False,\n              gap                   = 0,\n              allow_incomplete_fold = True,\n              refit                 = False,\n              n_iter                = 5,\n              return_best           = False,\n              n_jobs                = 'auto',\n              verbose               = False,\n              show_progress         = True\n          )\n\nresults\n</pre> # Random search MultiVariate # ============================================================================== lags_grid = [7, 14] param_distributions = {     'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),     'max_depth': np.arange(start=3, stop=6, step=1, dtype=int) }  results = random_search_forecaster_multiseries(               forecaster            = forecaster,               series                = data,               exog                  = None,               lags_grid             = lags_grid,               param_distributions   = param_distributions,               steps                 = 7,               metric                = 'mean_absolute_error',               aggregate_metric      = 'weighted_average',               initial_train_size    = len(data_train),               fixed_train_size      = False,               gap                   = 0,               allow_incomplete_fold = True,               refit                 = False,               n_iter                = 5,               return_best           = False,               n_jobs                = 'auto',               verbose               = False,               show_progress         = True           )  results <pre>10 models compared for 1 level(s). Number of iterations: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[11]: levels lags lags_label params mean_absolute_error n_estimators max_depth 0 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 19, 'max_depth': 5} 15345.360485 19 5 1 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'max_depth': 5} 15380.547039 16 5 2 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 18, 'max_depth': 3} 15556.476445 18 3 3 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 17, 'max_depth': 3} 15581.563438 17 3 4 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 15, 'max_depth': 3} 15644.444887 15 3 5 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 18, 'max_depth': 3} 15707.586121 18 3 6 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 15, 'max_depth': 3} 15715.665149 15 3 7 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 17, 'max_depth': 3} 15718.957694 17 3 8 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 19, 'max_depth': 5} 15895.928301 19 5 9 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 16, 'max_depth': 5} 15909.808527 16 5 <p>It is also possible to perform a bayesian optimization with <code>optuna</code> using the <code>bayesian_search_forecaster_multiseries</code> function. For more information about this type of optimization, see the user guide here.</p> In\u00a0[12]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 level     = 'CO',\n                 lags      = 7,\n                 steps     = 7\n             )\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [7, 14]),\n        'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1., 10),\n        'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n\n    return search_space\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n                          forecaster            = forecaster,\n                          series                = data,\n                          exog                  = None, \n                          search_space          = search_space,\n                          steps                 = 7,\n                          metric                = 'mean_absolute_error',\n                          aggregate_metric      = 'weighted_average',\n                          refit                 = False,\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          n_trials              = 5,\n                          random_state          = 123,\n                          return_best           = False,\n                          n_jobs                = 'auto',\n                          verbose               = False,\n                          show_progress         = True,\n                          engine                = 'optuna',\n                          kwargs_create_study   = {},\n                          kwargs_study_optimize = {}\n                      )\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  level     = 'CO',                  lags      = 7,                  steps     = 7              )  # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [7, 14]),         'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1., 10),         'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  results, best_trial = bayesian_search_forecaster_multiseries(                           forecaster            = forecaster,                           series                = data,                           exog                  = None,                            search_space          = search_space,                           steps                 = 7,                           metric                = 'mean_absolute_error',                           aggregate_metric      = 'weighted_average',                           refit                 = False,                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           n_trials              = 5,                           random_state          = 123,                           return_best           = False,                           n_jobs                = 'auto',                           verbose               = False,                           show_progress         = True,                           engine                = 'optuna',                           kwargs_create_study   = {},                           kwargs_study_optimize = {}                       )  results.head(4) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[12]: levels lags params mean_absolute_error n_estimators min_samples_leaf max_features 0 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'min_samples_leaf': 9, 'm... 15375.511274 16 9 log2 1 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 14, 'min_samples_leaf': 8, 'm... 16217.723758 14 8 log2 2 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 13, 'min_samples_leaf': 3, 'm... 16290.935447 13 3 sqrt 3 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 12, 'min_samples_leaf': 6, 'm... 16321.561731 12 6 log2 <p><code>best_trial</code> contains information of the trial which achived the best results. See more in Study class.</p> In\u00a0[13]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[13]: <pre>FrozenTrial(number=3, state=1, values=[15375.511274206323], datetime_start=datetime.datetime(2024, 7, 29, 16, 45, 29, 687109), datetime_complete=datetime.datetime(2024, 7, 29, 16, 45, 30, 430502), params={'lags': 14, 'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(7, 14)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=3, value=None)</pre> In\u00a0[14]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\ndisplay(predictions)\n</pre> # Create and fit forecaster MultiVariate Custom lags # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},                  steps              = 7,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) display(predictions) CO 2016-06-01 19689.480156 2016-06-02 20868.146279 2016-06-03 19580.755477 2016-06-04 14345.425771 2016-06-05 18420.497473 2016-06-06 17743.247080 2016-06-07 8241.801326 <p>If a <code>None</code> is passed to any of the keys of the <code>lags</code> argument, that series will not be used to create the <code>X</code> training matrix.</p> <p>In this example, no lags are created for the <code>'CO'</code> series, but since it is the <code>level</code> of the forecaster, the <code>'CO'</code> column will be used to create the <code>y</code> training matrix.</p> In\u00a0[15]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags with None\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = {'CO': None, 'SO2': [1, 7], 'PM2.5': 2},\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\ndisplay(predictions)\n</pre> # Create and fit forecaster MultiVariate Custom lags with None # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = {'CO': None, 'SO2': [1, 7], 'PM2.5': 2},                  steps              = 7,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) display(predictions) CO 2016-06-01 23729.888429 2016-06-02 25293.547939 2016-06-03 25514.092134 2016-06-04 24602.438413 2016-06-05 22365.681790 2016-06-06 20922.543045 2016-06-07 9068.361606 <p>It is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific lags that have been created.</p> In\u00a0[16]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step    = 1,\n               X_train = X,\n               y_train = y,\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step    = 1,                X_train = X,                y_train = y,            )  X_1.head(4) Out[16]: SO2_lag_1 SO2_lag_7 PM2.5_lag_1 PM2.5_lag_2 date 2013-03-08 3.727899 -0.495413 2.427373 1.806709 2013-03-09 2.102055 0.421426 1.870726 2.427373 2013-03-10 0.825226 2.346788 -0.444267 1.870726 2013-03-11 0.185389 0.052740 -0.702775 -0.444267 In\u00a0[17]: Copied! <pre># Extract training matrix\n# ==============================================================================\ny_1.head(4)\n</pre> # Extract training matrix # ============================================================================== y_1.head(4) Out[17]: <pre>date\n2013-03-08    2.153622\n2013-03-09   -0.221359\n2013-03-10   -0.531358\n2013-03-11    0.496391\nFreq: D, Name: CO_step_1, dtype: float64</pre> In\u00a0[18]: Copied! <pre># Transformers in MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},\n                 transformer_exog   = None,\n                 weight_func        = None,\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Transformers in MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},                  transformer_exog   = None,                  weight_func        = None,              )  forecaster.fit(series=data_train) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\utils\\utils.py:255: IgnoredArgumentWarning: {'PM2.5'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> Out[18]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: {'CO': StandardScaler(), 'SO2': StandardScaler()} \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:45:32 \nLast fit date: 2024-07-29 16:45:32 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[19]: Copied! <pre># Weights in MultiVariate\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = custom_weights\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=7).head(3)\n</pre> # Weights in MultiVariate # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = custom_weights              )  forecaster.fit(series=data_train) forecaster.predict(steps=7).head(3) Out[19]: CO 2016-06-01 15774.629629 2016-06-02 20083.444808 2016-06-03 20050.403708 <p> \u26a0 Warning </p> <p>The <code>weight_func</code> argument will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>.</p> In\u00a0[20]: Copied! <pre># Source code weight function\n# ==============================================================================\nprint(forecaster.source_code_weight_func)\n</pre> # Source code weight function # ============================================================================== print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> In\u00a0[21]: Copied! <pre># Grid search MultiVariate with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7\n             )    \n\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [7, 14]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              exog                = None,\n              steps               = 7,\n              metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              aggregate_metric    = 'weighted_average',\n              lags_grid           = lags_grid,\n              param_grid          = param_grid,\n              initial_train_size  = len(data_train),\n              refit               = False,\n              fixed_train_size    = False,\n              return_best         = True,\n              n_jobs              = 'auto',\n              verbose             = False,\n              show_progress       = True\n          )\n\nresults\n</pre> # Grid search MultiVariate with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'CO',                  lags               = 7,                  steps              = 7              )      def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [7, 14] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               exog                = None,               steps               = 7,               metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],               aggregate_metric    = 'weighted_average',               lags_grid           = lags_grid,               param_grid          = param_grid,               initial_train_size  = len(data_train),               refit               = False,               fixed_train_size    = False,               return_best         = True,               n_jobs              = 'auto',               verbose             = False,               show_progress       = True           )  results <pre>6 models compared for 1 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n  Parameters: {'alpha': 0.01}\n  Backtesting metric: 16044.246752675152\n  Levels: ['CO']\n\n</pre> Out[21]: levels lags lags_label params mean_absolute_error custom_metric mean_squared_error alpha 0 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.01} 16044.246753 22748.759670 6.564519e+08 0.01 1 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.1} 16044.246753 22748.759670 6.564519e+08 0.10 2 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 1} 16044.246753 22748.759670 6.564519e+08 1.00 3 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.01} 16577.420604 23913.019815 6.614748e+08 0.01 4 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.1} 16577.420604 23913.019815 6.614748e+08 0.10 5 [CO] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 1} 16577.420604 23913.019815 6.614748e+08 1.00 In\u00a0[22]: Copied! <pre># Feature importances for step 1\n# ==============================================================================\nforecaster.get_feature_importances(step=1)\n</pre> # Feature importances for step 1 # ============================================================================== forecaster.get_feature_importances(step=1) Out[22]: feature importance 0 CO_lag_1 234 1 CO_lag_2 147 29 PM2.5_lag_2 119 30 PM2.5_lag_3 116 14 SO2_lag_1 111 3 CO_lag_4 105 28 PM2.5_lag_1 103 2 CO_lag_3 99 15 SO2_lag_2 88 9 CO_lag_10 87 7 CO_lag_8 78 8 CO_lag_9 76 6 CO_lag_7 74 5 CO_lag_6 73 31 PM2.5_lag_4 70 12 CO_lag_13 69 13 CO_lag_14 68 20 SO2_lag_7 68 4 CO_lag_5 65 36 PM2.5_lag_9 65 33 PM2.5_lag_6 65 37 PM2.5_lag_10 60 17 SO2_lag_4 60 10 CO_lag_11 60 22 SO2_lag_9 58 21 SO2_lag_8 57 34 PM2.5_lag_7 56 11 CO_lag_12 56 16 SO2_lag_3 55 26 SO2_lag_13 53 40 PM2.5_lag_13 51 32 PM2.5_lag_5 50 35 PM2.5_lag_8 50 41 PM2.5_lag_14 50 24 SO2_lag_11 46 38 PM2.5_lag_11 44 27 SO2_lag_14 40 39 PM2.5_lag_12 38 25 SO2_lag_12 35 23 SO2_lag_10 34 19 SO2_lag_6 34 18 SO2_lag_5 33 In\u00a0[23]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step          = 1,\n               X_train       = X,\n               y_train       = y,\n               remove_suffix = False\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step          = 1,                X_train       = X,                y_train       = y,                remove_suffix = False            )  X_1.head(4) Out[23]: CO_lag_1 CO_lag_2 CO_lag_3 CO_lag_4 CO_lag_5 CO_lag_6 CO_lag_7 CO_lag_8 CO_lag_9 CO_lag_10 ... PM2.5_lag_5 PM2.5_lag_6 PM2.5_lag_7 PM2.5_lag_8 PM2.5_lag_9 PM2.5_lag_10 PM2.5_lag_11 PM2.5_lag_12 PM2.5_lag_13 PM2.5_lag_14 date 2013-03-15 0.244207 -0.706409 0.620101 0.459426 -0.520511 -0.224933 2.039562 2.740821 2.170409 1.253719 ... -0.698503 -0.446475 1.810484 2.353176 1.748071 0.829120 -0.924377 -0.022069 -0.808468 -1.077140 2013-03-16 1.038288 0.244207 -0.706409 0.620101 0.459426 -0.520511 -0.224933 2.039562 2.740821 2.170409 ... 0.517651 -0.698503 -0.446475 1.810484 2.353176 1.748071 0.829120 -0.924377 -0.022069 -0.808468 2013-03-17 1.152151 1.038288 0.244207 -0.706409 0.620101 0.459426 -0.520511 -0.224933 2.039562 2.740821 ... 0.494469 0.517651 -0.698503 -0.446475 1.810484 2.353176 1.748071 0.829120 -0.924377 -0.022069 2013-03-18 1.405691 1.152151 1.038288 0.244207 -0.706409 0.620101 0.459426 -0.520511 -0.224933 2.039562 ... -0.975496 0.494469 0.517651 -0.698503 -0.446475 1.810484 2.353176 1.748071 0.829120 -0.924377 <p>4 rows \u00d7 42 columns</p> In\u00a0[24]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[24]: <pre>date\n2013-03-15    1.038288\n2013-03-16    1.152151\n2013-03-17    1.405691\n2013-03-18   -0.275548\nFreq: D, Name: CO_step_1, dtype: float64</pre>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#global-forecasting-models-dependent-multi-series-forecasting-multivariate-forecasting","title":"Global Forecasting Models: Dependent multi-series forecasting (Multivariate forecasting)\u00b6","text":"<p>Univariate time series forecasting models a single time series as a linear or nonlinear combination of its lags, using past values of the series to predict its future. Global forecasting, involves building a single predictive model that considers all time series simultaneously. It attempts to capture the core patterns that govern the series, thereby mitigating the potential noise that each series might introduce. This approach is computationally efficient, easy to maintain, and can yield more robust generalizations across time series.</p> <p>In dependent multi-series forecasting (multivariate time series), all series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p> Internal Forecaster time series transformation to train a forecaster with multiple dependent time series. </p> <p>Since as many training matrices are created as there are series in the dataset, it must be decided on which level the forecasting will be performed. To predict the next n steps a model is trained for each step to be predicted, the selected level in the figure is <code>Series 1</code>. This strategy is of the type direct multi-step forecasting.</p> <p> Diagram of direct forecasting with multiple dependent time series. </p> <p>Using the <code>ForecasterAutoregMultiVariate</code> class, it is possible to easily build machine learning models for dependent multi-series forecasting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#train-and-predict-forecasterautoregmultivariate","title":"Train and predict ForecasterAutoregMultiVariate\u00b6","text":"<p>When initializing the forecaster, the <code>level</code> to be predicted and the maximum number of <code>steps</code> must be indicated since a different model will be created for each step.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#backtesting-multivariate","title":"Backtesting MultiVariate\u00b6","text":"<p>See the backtesting user guide to learn more about backtesting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#hyperparameter-tuning-and-lags-selection-multivariate","title":"Hyperparameter tuning and lags selection MultiVariate\u00b6","text":"<p>The <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> functions in the <code>model_selection_multiseries</code> module allow for lags and hyperparameter optimization. It is performed using the backtesting strategy for validation as in other Forecasters, see the user guide here.</p> <p>The following example shows how to use <code>random_search_forecaster_multiseries</code> to find the best lags and model hyperparameters.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#different-lags-for-each-time-series","title":"Different lags for each time series\u00b6","text":"<p>If a <code>dict</code> is passed to the <code>lags</code> argument, it allows setting different lags for each of the series. The keys of the dictionary must be the names of the series to be used during training.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#exogenous-variables-in-multivariate","title":"Exogenous variables in MultiVariate\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process.</p> <p>In the <code>ForecasterAutoregMultiVariate</code>, as in the other forecasters, exogenous variables can be easily included as predictors using the <code>exog</code> argument.</p> <p>To learn more about exogenous variables in skforecast visit the exogenous variables user guide.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#scikit-learn-transformers-in-multivariate","title":"Scikit-learn transformers in MultiVariate\u00b6","text":"<p>By default, the <code>ForecasterAutoregMultiVariate</code> class uses the scikit-learn <code>StandardScaler</code> transformer to scale the data. This transformer is applied to all series. However, it is possible to use different transformers for each series or not to apply any transformation at all:</p> <ul> <li><p>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</p> </li> <li><p>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them (check warning message).</p> </li> </ul> <p>Learn more about using scikit-learn transformers with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#weights-in-multivariate","title":"Weights in MultiVariate\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model.</p> <p>Learn more about weighted time series forecasting with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All four functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>,  <code>random_search_forecaster_multiseries</code>, and <code>bayesian_search_forecaster_multiseries</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list can include custom metrics, and the best model is selected based on the first metric in the list and the first aggregation method (if more than one is provided).</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregMultiVariate</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#extract-training-matrix","title":"Extract training matrix\u00b6","text":"<p>Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/direct-multi-step-forecasting.html","title":"Direct multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.datasets import fetch_dataset\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.datasets import fetch_dataset from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> <p> \u270e Note </p> <p>Starting from version skforecast 0.9.0, the <code>ForecasterAutoregDirect</code> now includes the <code>n_jobs</code> parameter, allowing multi-process parallelization. This allows to train regressors for all steps simultaneously.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(),\n                 steps         = 36,\n                 lags          = 15,\n                 transformer_y = None,\n                 n_jobs        = 'auto'\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(),                  steps         = 36,                  lags          = 15,                  transformer_y = None,                  n_jobs        = 'auto'              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>======================= \nForecasterAutoregDirect \n======================= \nRegressor: Ridge() \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWeight function included: False \nWindow size: 15 \nMaximum steps predicted: 36 \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:46:20 \nLast fit date: 2024-07-29 16:46:20 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\n# Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict # ============================================================================== # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) <pre>2005-07-01    0.952051\n2005-11-01    1.179922\nName: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Predict all steps defined in the initialization.\npredictions = forecaster.predict()\ndisplay(predictions.head(3))\n</pre> # Predict all steps defined in the initialization. predictions = forecaster.predict() display(predictions.head(3)) <pre>2005-07-01    0.952051\n2005-08-01    1.004145\n2005-09-01    1.114590\nName: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\n\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n            \nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== predictions = forecaster.predict(steps=36)  error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )              print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.00841959727883196\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[8]: feature importance 11 lag_12 0.551652 10 lag_11 0.154030 0 lag_1 0.139299 12 lag_13 0.057513 1 lag_2 0.051089 2 lag_3 0.044192 9 lag_10 0.020511 8 lag_9 0.011918 7 lag_8 -0.012591 5 lag_6 -0.013233 4 lag_5 -0.017935 3 lag_4 -0.019868 6 lag_7 -0.021063 14 lag_15 -0.035237 13 lag_14 -0.071071 In\u00a0[9]: Copied! <pre># Create the whole train matrix\nX, y = forecaster.create_train_X_y(data_train)\n\n# Extract X and y to train the model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step          = 1,\n               X_train       = X,\n               y_train       = y,\n               remove_suffix = False\n           )\n\nX_1.head(4)\n</pre> # Create the whole train matrix X, y = forecaster.create_train_X_y(data_train)  # Extract X and y to train the model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step          = 1,                X_train       = X,                y_train       = y,                remove_suffix = False            )  X_1.head(4) Out[9]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1992-12-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1993-01-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 In\u00a0[10]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[10]: <pre>datetime\n1992-10-01    0.568606\n1992-11-01    0.595223\n1992-12-01    0.771258\n1993-01-01    0.751503\nFreq: MS, Name: y_step_1, dtype: float64</pre> In\u00a0[11]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=5)\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=5) X_predict Out[11]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 2005-07-01 0.842263 0.695248 0.670505 0.65259 0.597639 1.17069 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986 2005-08-01 0.842263 0.695248 0.670505 0.65259 0.597639 1.17069 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986 2005-09-01 0.842263 0.695248 0.670505 0.65259 0.597639 1.17069 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986 2005-10-01 0.842263 0.695248 0.670505 0.65259 0.597639 1.17069 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986 2005-11-01 0.842263 0.695248 0.670505 0.65259 0.597639 1.17069 1.257238 1.216037 1.181011 1.134432 0.994864 1.001593 0.856803 0.795129 0.739986"},{"location":"user_guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","title":"Direct multi-step forecaster\u00b6","text":"<p>This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the <code>ForecasterAutoregDirect</code> class and can also include one or multiple exogenous variables.</p> <p>Direct multi-step forecasting is a time series forecasting strategy in which a separate model is trained to predict each step in the forecast horizon. This is in contrast to recursive multi-step forecasting, where a single model is used to make predictions for all future time steps by recursively using its own output as input.</p> <p>Direct multi-step forecasting can be more computationally expensive than recursive forecasting since it requires training multiple models. However, it can often achieve better accuracy in certain scenarios, particularly when there are complex patterns and dependencies in the data that are difficult to capture with a single model.</p> <p>This approach can be performed using the <code>ForecasterAutoregDirect</code> class, which can also incorporate one or multiple exogenous variables to improve the accuracy of the forecasts.</p> <p> Diagram of direct multi-step forecasting. </p> <p>To train a <code>ForecasterAutoregDirect</code> a different training matrix is created for each model.</p> <p> Transformation of a time series into matrices to train a direct multi-step forecasting model. </p>"},{"location":"user_guides/direct-multi-step-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#prediction","title":"Prediction\u00b6","text":"<p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul>"},{"location":"user_guides/direct-multi-step-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregDirect</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/direct-multi-step-forecasting.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>Two steps are needed to extract the training matrices. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/direct-multi-step-forecasting.html#extract-prediction-matrices","title":"Extract prediction matrices\u00b6","text":"<p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p>"},{"location":"user_guides/exogenous-variables.html","title":"Exogenous variables","text":"<p> \u270e Note </p> <p>When exogenous variables are used, their values must be aligned so that y[i] regresses on exog[i].</p> <p> Time series transformation including an exogenous variable. </p> <p> \u26a0 Warning </p> <p>When exogenous variables are included in a forecasting model, it is assumed that all exogenous inputs are known in the future. Do not include exogenous variables as predictors if their future value will not be known when making predictions.</p> <p> \u270e Note </p> <p>For a detailed guide on how to include categorical exogenous variables, please visit Categorical Features.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.datasets import fetch_dataset\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.datasets import fetch_dataset from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=False)\ndata.index.name = 'datetime'\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=False) data.index.name = 'datetime'  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data.plot(ax=ax); <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Split data in train and test\n# ==============================================================================\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Split data in train and test # ============================================================================== steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\n\nforecaster.fit(\n    y    = data_train['y'],\n    exog = data_train[['exog_1', 'exog_2']]\n)\n\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              )  forecaster.fit(     y    = data_train['y'],     exog = data_train[['exog_1', 'exog_2']] )  forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 16:46:47 \nLast fit date: 2024-07-29 16:46:47 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(\n                  steps = steps,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\n\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(                   steps = steps,                   exog  = data_test[['exog_1', 'exog_2']]               )  predictions.head(3) Out[5]: <pre>2005-07-01    1.023969\n2005-08-01    1.044023\n2005-09-01    1.110078\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (MSE): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (MSE): {error_mse}\") <pre>Test error (MSE): 0.005576949968874203\n</pre> In\u00a0[8]: Copied! <pre># Feature importances with exogenous variables\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances with exogenous variables # ============================================================================== forecaster.get_feature_importances() Out[8]: feature importance 11 lag_12 66 15 exog_1 49 16 exog_2 37 10 lag_11 36 5 lag_6 31 13 lag_14 26 4 lag_5 26 2 lag_3 25 14 lag_15 24 3 lag_4 23 12 lag_13 23 1 lag_2 22 9 lag_10 18 0 lag_1 16 7 lag_8 16 6 lag_7 15 8 lag_9 12"},{"location":"user_guides/exogenous-variables.html#exogenous-variables-features","title":"Exogenous variables (features)\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p>"},{"location":"user_guides/exogenous-variables.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#prediction","title":"Prediction\u00b6","text":"<p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p>"},{"location":"user_guides/exogenous-variables.html#feature-importances","title":"Feature importances\u00b6","text":"<p>If exogenous variables are included as predictors, they have a value of feature importances.</p>"},{"location":"user_guides/exogenous-variables.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies available in skforecast can also be applied when incorporating exogenous variables in the forecasting model. Visit the Backtesting section for more information.</p>"},{"location":"user_guides/explainability.html","title":"Explainability","text":"<p>Machine learning explainability refers to the ability to understand, interpret, and explain the decisions or predictions made by machine learning models in a human-understandable way. It aims to shed light on how a model arrives at a particular result or decision.</p> <p>Due to the complex nature of many modern machine learning models, such as ensemble methods, they often function as black boxes, making it difficult to understand why a particular prediction was made. Explanability techniques aim to demystify these models, providing insight into their inner workings and helping to build trust, improve transparency, and meet regulatory requirements in various domains. Enhancing model explainability not only aids in understanding model behavior but also helps detect biases, improve model performance, and enables stakeholders to make more informed decisions based on machine learning insights.</p> <p> \ud83d\udca1 Tip </p> <p>To learn more about explainability, visit: Interpretable forecasting models</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport shap\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom lightgbm import LGBMRegressor\n\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt  import shap from sklearn.inspection import PartialDependenceDisplay from lightgbm import LGBMRegressor  from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg <p>The data used in this example has been obtained from the R tsibbledata package. The dataset contains 5 columns and 52,608 complete records. The information in each column is:</p> <ul> <li>Time: date and time of the record.</li> <li>Date: date of the record.</li> <li>Demand: electricity demand (MW).</li> <li>Temperature: temperature in Melbourne, the capital of Victoria.</li> <li>Holiday: indicates if the day is a public holiday.</li> </ul> In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"vic_electricity\")\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"vic_electricity\") data.head(3) <pre>vic_electricity\n---------------\nHalf-hourly electricity demand for Victoria, Australia\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse\nDatasets for 'tsibble'. https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/.\nhttps://tsibbledata.tidyverts.org/reference/vic_elec.html\nShape of the dataset: (52608, 4)\n</pre> Out[2]: Demand Temperature Date Holiday Time 2011-12-31 13:00:00 4382.825174 21.40 2012-01-01 True 2011-12-31 13:30:00 4263.365526 21.05 2012-01-01 True 2011-12-31 14:00:00 4048.966046 20.70 2012-01-01 True In\u00a0[3]: Copied! <pre># Aggregation to daily frequency\n# ==============================================================================\ndata = data.resample('D').agg({'Demand': 'sum', 'Temperature': 'mean'})\ndata.head(3)\n</pre> # Aggregation to daily frequency # ============================================================================== data = data.resample('D').agg({'Demand': 'sum', 'Temperature': 'mean'}) data.head(3) Out[3]: Demand Temperature Time 2011-12-31 82531.745918 21.047727 2012-01-01 227778.257304 26.578125 2012-01-02 275490.988882 31.751042 In\u00a0[16]: Copied! <pre># Split train-test\n# ==============================================================================\ndata_train = data.loc[: '2014-12-21']\ndata_test = data.loc['2014-12-22':]\n</pre> # Split train-test # ============================================================================== data_train = data.loc[: '2014-12-21'] data_test = data.loc['2014-12-22':] <p>A forecasting model is created to predict the energy demand using the past 7 values (last week) and the temperature as an exogenous variable.</p> In\u00a0[18]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 7\n             )\n\nforecaster.fit(\n    y    = data_train['Demand'],\n    exog = data_train['Temperature']\n)\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 7              )  forecaster.fit(     y    = data_train['Demand'],     exog = data_train['Temperature'] ) <p>Feature importance is a technique used in machine learning to determine the relevance or importance of each feature (or variable) in a model's prediction. In other words, it measures how much each feature contributes to the model's output.</p> <p>Feature importance can be used for several purposes, such as identifying the most relevant features for a given prediction, understanding the behavior of a model, and selecting the best set of features for a given task. It can also help to identify potential biases or errors in the data used to train the model. It is important to note that feature importance is not a definitive measure of causality. Just because a feature is identified as important does not necessarily mean that it causes the outcome. Other factors, such as confounding variables, may also be at play.</p> <p>The method used to calculate feature importance may vary depending on the type of machine learning model being used. Different machine learning models may have different assumptions and characteristics that affect the calculation of feature importance. For example, decision tree-based models such as Random Forest and Gradient Boosting typically use mean decrease impurity or permutation feature importance methods to calculate feature importance.</p> <p>Linear regression models typically use coefficients or standardized coefficients to determine the importance of a feature. The magnitude of the coefficient reflects the strength and direction of the relationship between the feature and the target variable.</p> <p>The importance of the predictors included in a forecaster can be obtained using the method <code>get_feature_importances()</code>. This method accesses the <code>coef_</code> and <code>feature_importances_</code> attributes of the internal regressor.</p> <p> \u26a0 Warning </p> <p>The <code>get_feature_importances()</code> method will only return values if the forecaster's regressor has either the <code>coef_</code> or <code>feature_importances_</code> attribute, which is the default in scikit-learn. If your regressor does not follow this naming convention, please consider opening an issue on GitHub and we will strive to include it in future updates.</p> In\u00a0[19]: Copied! <pre># Predictors importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Predictors importances # ============================================================================== forecaster.get_feature_importances() Out[19]: feature importance 7 Temperature 570 0 lag_1 470 2 lag_3 387 1 lag_2 362 6 lag_7 325 5 lag_6 313 4 lag_5 298 3 lag_4 275 <p>To properly retrieve the feature importances in the <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>, it is essential to specify the model from which to extract the feature importances are to be extracted. This is because Direct Strategy Forecasters fit one model per step, and each model may have different important features. Therefore, the user must explicitly specify which model's feature importances wish to extract to ensure that the correct features are used.</p> <p>SHAP (SHapley Additive exPlanations) values are a popular method for explaining machine learning models, as they help to understand how variables and values influence predictions visually and quantitatively.</p> <p>It is possible to generate SHAP-values explanations from Skforecast models with just two essential elements:</p> <ul> <li><p>The internal regressor of the forecaster.</p> </li> <li><p>The training matrices created from the time series and used to fit the forecaster.</p> </li> </ul> <p>By leveraging these two components, users can create insightful and interpretable explanations for their skforecast models. These explanations can be used to verify the reliability of the model, identify the most significant factors that contribute to model predictions, and gain a deeper understanding of the underlying relationship between the input variables and the target variable.</p> In\u00a0[20]: Copied! <pre># Training matrices used by the forecaster to fit the internal regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data_train['Demand'],\n                       exog = data_train['Temperature']\n                   )\n\ndisplay(X_train.head(3))\ndisplay(y_train.head(3))\n</pre> # Training matrices used by the forecaster to fit the internal regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data_train['Demand'],                        exog = data_train['Temperature']                    )  display(X_train.head(3)) display(y_train.head(3)) lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 Temperature Time 2012-01-07 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 227778.257304 82531.745918 24.098958 2012-01-08 200693.270298 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 227778.257304 20.223958 2012-01-09 200061.614738 200693.270298 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 19.161458 <pre>Time\n2012-01-07    200693.270298\n2012-01-08    200061.614738\n2012-01-09    216201.836844\nFreq: D, Name: y, dtype: float64</pre> <p>The python implementation of SHAP is built along the explainers. These explainers are appropriate only for certain types or classes of algorithms. For example, the TreeExplainer is used for tree-based models.</p> In\u00a0[21]: Copied! <pre># Create SHAP explainer\n# ==============================================================================\nshap.initjs()\nexplainer = shap.TreeExplainer(forecaster.regressor)\nshap_values = explainer.shap_values(X_train)\n</pre> # Create SHAP explainer # ============================================================================== shap.initjs() explainer = shap.TreeExplainer(forecaster.regressor) shap_values = explainer.shap_values(X_train) <p>The SHAP summary plot typically displays the feature importance or contribution of each feature to the model's output across multiple data points. It shows how much each feature contributes to pushing the model's prediction away from a base value (often the model's average prediction). By examining a SHAP summary plot, one can gain insights into which features have the most significant impact on predictions, whether they positively or negatively influence the outcome, and how different feature values contribute to specific predictions.</p> In\u00a0[22]: Copied! <pre>shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n</pre> shap.summary_plot(shap_values, X_train, plot_type=\"bar\") In\u00a0[23]: Copied! <pre>shap.summary_plot(shap_values, X_train)\n</pre> shap.summary_plot(shap_values, X_train) <p>A <code>shap.force_plot</code> is a specific type of visualization that provides an interactive and detailed view of how individual features contribute to a particular prediction made by a machine learning model. It's a local interpretation tool that helps understand why a model made a specific prediction for a given instance.</p> <p>Visualize a single prediction</p> In\u00a0[24]: Copied! <pre># Force plot for the first observation\n# ==============================================================================\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n</pre> # Force plot for the first observation # ============================================================================== shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:]) Out[24]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>Visualize many predictions</p> In\u00a0[25]: Copied! <pre># Force plot for the first 200 observations in the training set\n# ==============================================================================\nshap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:])\n</pre> # Force plot for the first 200 observations in the training set # ============================================================================== shap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:]) Out[25]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>SHAP dependence plots are visualizations used to understand the relationship between a feature and the model output by displaying how the value of a single feature affects predictions made by the model while considering interactions with other features. These plots are particularly useful for examining how a certain feature impacts the model's predictions across its range of values while considering interactions with other variables.</p> In\u00a0[26]: Copied! <pre># Dependence plot for Temperature\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 4))\nshap.dependence_plot(\"Temperature\", shap_values, X_train, ax=ax)\n</pre> # Dependence plot for Temperature # ============================================================================== fig, ax = plt.subplots(figsize=(7, 4)) shap.dependence_plot(\"Temperature\", shap_values, X_train, ax=ax) <p>It is also possible to use SHAP values to explain the forecasted values. This can be done by using the input matrices used by the forecaster's <code>predict</code> method.</p> In\u00a0[30]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=10, exog=data_test['Temperature'])\npredictions\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=10, exog=data_test['Temperature']) predictions Out[30]: <pre>2014-12-22    241514.532543\n2014-12-23    226165.936559\n2014-12-24    220506.468700\n2014-12-25    209260.948991\n2014-12-26    184885.145832\n2014-12-27    195623.591810\n2014-12-28    222766.340659\n2014-12-29    223112.716406\n2014-12-30    219103.891733\n2014-12-31    217948.965404\nFreq: D, Name: pred, dtype: float64</pre> In\u00a0[31]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=10, exog=data_test['Temperature'])\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=10, exog=data_test['Temperature']) X_predict Out[31]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 Temperature 2014-12-22 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 226093.767670 231923.044018 22.950000 2014-12-23 241514.532543 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 226093.767670 18.829167 2014-12-24 226165.936559 241514.532543 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 18.312500 2014-12-25 220506.468700 226165.936559 241514.532543 216483.631690 186486.896670 197129.766534 214934.022460 16.933333 2014-12-26 209260.948991 220506.468700 226165.936559 241514.532543 216483.631690 186486.896670 197129.766534 16.429167 2014-12-27 184885.145832 209260.948991 220506.468700 226165.936559 241514.532543 216483.631690 186486.896670 18.189583 2014-12-28 195623.591810 184885.145832 209260.948991 220506.468700 226165.936559 241514.532543 216483.631690 24.539583 2014-12-29 222766.340659 195623.591810 184885.145832 209260.948991 220506.468700 226165.936559 241514.532543 17.677083 2014-12-30 223112.716406 222766.340659 195623.591810 184885.145832 209260.948991 220506.468700 226165.936559 17.391667 2014-12-31 219103.891733 223112.716406 222766.340659 195623.591810 184885.145832 209260.948991 220506.468700 21.034615 In\u00a0[33]: Copied! <pre># Force plot for a specific prediction\n# ==============================================================================\npredicted_date = '2014-12-22'\niloc_predicted_date = X_predict.index.get_loc(predicted_date)\nshap_values = explainer.shap_values(X_predict)\nshap.force_plot(\n    explainer.expected_value,\n    shap_values[iloc_predicted_date,:],\n    X_predict.iloc[iloc_predicted_date,:]\n)\n</pre> # Force plot for a specific prediction # ============================================================================== predicted_date = '2014-12-22' iloc_predicted_date = X_predict.index.get_loc(predicted_date) shap_values = explainer.shap_values(X_predict) shap.force_plot(     explainer.expected_value,     shap_values[iloc_predicted_date,:],     X_predict.iloc[iloc_predicted_date,:] ) Out[33]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>Partial dependence plots (PDPs) are a useful tool for understanding the relationship between a feature and the target outcome in a machine learning model. In scikit-learn, you can create partial dependence plots using the <code>plot_partial_dependence</code> function. This function visualizes the effect of one or two features on the predicted outcome, while marginalizing the effect of all other features.</p> <p>The resulting plots show how changes in the selected feature(s) affect the predicted outcome while holding other features constant on average. Remember that these plots should be interpreted in the context of your model and data. They provide insight into the relationship between specific features and the model's predictions.</p> <p>A more detailed description of the Partial Dependency Plot can be found in Scikitlearn's User Guides.</p> In\u00a0[\u00a0]: Copied! <pre># Scikit-learn partial dependence plots\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 4))\nax.set_title(\"Decision Tree\")\npd.plots = PartialDependenceDisplay.from_estimator(\n    estimator = forecaster.regressor,\n    X         = X_train,\n    features  = [\"Temperature\", \"lag_1\"],\n    kind      = 'both',\n    ax        = ax,\n)\nax.set_title(\"Partial Dependence Plot\")\nfig.tight_layout();\n</pre> # Scikit-learn partial dependence plots # ============================================================================== fig, ax = plt.subplots(figsize=(9, 4)) ax.set_title(\"Decision Tree\") pd.plots = PartialDependenceDisplay.from_estimator(     estimator = forecaster.regressor,     X         = X_train,     features  = [\"Temperature\", \"lag_1\"],     kind      = 'both',     ax        = ax, ) ax.set_title(\"Partial Dependence Plot\") fig.tight_layout();"},{"location":"user_guides/explainability.html#forecaster-explainability-feature-importance-shap-values-and-partial-dependence-plots","title":"Forecaster explainability: Feature importance, SHAP Values and Partial Dependence Plots\u00b6","text":""},{"location":"user_guides/explainability.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/explainability.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/explainability.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/explainability.html#model-specific-feature-importances","title":"Model-specific feature importances\u00b6","text":""},{"location":"user_guides/explainability.html#extract-feature-importances-from-trained-forecaster","title":"Extract feature importances from trained forecaster\u00b6","text":""},{"location":"user_guides/explainability.html#shap-values","title":"Shap Values\u00b6","text":""},{"location":"user_guides/explainability.html#trainig-matrices","title":"Trainig matrices\u00b6","text":""},{"location":"user_guides/explainability.html#shap-explainer","title":"Shap explainer\u00b6","text":""},{"location":"user_guides/explainability.html#shap-summary-plot","title":"SHAP Summary Plot\u00b6","text":""},{"location":"user_guides/explainability.html#explain-individual-observations","title":"Explain individual observations\u00b6","text":""},{"location":"user_guides/explainability.html#shap-dependence-plots","title":"SHAP Dependence Plots\u00b6","text":""},{"location":"user_guides/explainability.html#explain-forecasted-values","title":"Explain forecasted values\u00b6","text":""},{"location":"user_guides/explainability.html#scikit-learn-partial-dependence-plots","title":"Scikit-learn partial dependence plots\u00b6","text":""},{"location":"user_guides/feature-selection.html","title":"Feature selection","text":"<p>Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: to simplify models to make them easier to interpret, to reduce training time, to avoid the curse of dimensionality, to improve generalization by reducing overfitting (formally, variance reduction), and others.</p> <p>Skforecast is compatible with the feature selection methods implemented in the scikit-learn library. There are several methods for feature selection, but the most common are:</p> <p>Recursive feature elimination</p> <p>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features, and the importance of each feature is obtained either by a specific attribute (such as <code>coef_</code>, <code>feature_importances_</code>) or by a <code>callable</code>. Then, the least important features are pruned from the current set of features. This procedure is repeated recursively on the pruned set until the desired number of features to select is eventually reached. RFECV performs RFE in a cross-validation loop to find the optimal number of features.</p> <p>Sequential Feature Selection</p> <p>Sequential Feature Selection (SFS) can be either forward or backward, with the <code>direction</code> parameter controlling whether forward or backward SFS is used.</p> <ul> <li><p>Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. It starts with zero features and finds the one that maximizes a cross-validated score when an estimator is trained on that single feature. Once this first feature is selected, the procedure is repeated, adding one new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the <code>n_features_to_select</code> parameter.</p> </li> <li><p>Backward-SFS follows the same idea, but works in the opposite direction. Instead of starting with no features and greedily adding features, it starts with all features and greedily removes features from the set.</p> </li> </ul> <p>In general, forward and backward selection do not produce equivalent results. Also, one can be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.</p> <p>SFS differs does not require the underlying model to expose a <code>coef_</code> or <code>feature_importances_</code> attribute. However, it may be slower compared to the other approaches, considering that more models have to be evaluated. For example in backward selection, the iteration going from $m$ features to $m - 1$ features using k-fold cross-validation requires fitting $m * k$ models to be evaluated.</p> <p>Feature selection based on threshold (SelectFromModel)</p> <p>SelectFromModel can be used along with any estimator that has a <code>coef_</code> or <code>feature_importances_</code> attribute after fitting. Features are considered unimportant and removed, if the corresponding <code>coef_</code> or <code>feature_importances_</code> values are below the given <code>threshold</code> parameter. In addition to specifying the <code>threshold</code> numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are <code>'mean'</code>, <code>'median'</code> and float multiples of these, such as <code>'0.1*mean'</code>.</p> <p>This method is very fast compared to the others because it does not require any additional model training. However, it does not evaluate the impact of feature removal on the model. It is often used for an initial selection before applying another more computationally expensive feature selection method.</p> <p> \ud83d\udca1 Tip </p> <p>Feature selection is a powerful tool for improving the performance of machine learning models. However, it is computationally expensive and can be time-consuming. Since the goal is to find the best subset of features, not the best model, it is not necessary to use the entire data set or a highly complex model. Instead, it is recommended to use a small subset of the data and a simple model. Once the best subset of features has been identified, the model can then be trained using the entire dataset and a more complex configuration.  For example, in this use case, the model is an <code>LGMBRegressor</code> with 900 trees and a maximum depth of 7. However, to find the best subset of features, only 100 trees and a maximum depth of 5 are used.</p> <p>The <code>select_features</code> and <code>select_features_multiseries</code> functions can be used to select the best subset of features (autoregressive and exogenous variables). These functions are compatible with the feature selection methods implemented in the scikit-learn library. The available parameters are:</p> <ul> <li><p><code>forecaster</code>: Forecaster of type <code>ForecasterAutoreg</code>,  <code>ForecasterAutoregDirect</code>, <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li><p><code>selector</code>: Feature selector from <code>sklearn.feature_selection</code>. For example, <code>RFE</code> or <code>RFECV</code>.</p> </li> <li><p><code>y</code> or <code>series</code>: Target time series to which the feature selection will be applied.</p> </li> <li><p><code>exog</code>: Exogenous variables.</p> </li> <li><p><code>select_only</code>: Decide what type of features to include in the selection process.</p> <ul> <li><p>If <code>'autoreg'</code>, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (<code>selected_exog</code>).</p> </li> <li><p>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the output (<code>selected_autoreg</code>).</p> </li> <li><p>If <code>None</code>, all features are evaluated by the selector.</p> </li> </ul> </li> <li><p><code>force_inclusion</code>: Features to force include in the final list of selected features.</p> <ul> <li><p>If <code>list</code>, list of feature names to force include.</p> </li> <li><p>If <code>str</code>, regular expression to identify features to force include. For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin with \"sun_\" will be included in the final list of selected features.</p> </li> </ul> </li> <li><p><code>subsample</code>: Proportion of records to use for feature selection.</p> </li> <li><p><code>random_state</code>: Sets a seed for the random subsample so that the subsampling process is always deterministic.</p> </li> <li><p><code>verbose</code>: Print information about feature selection process.</p> </li> </ul> <p>These functions return two <code>list</code>:</p> <ul> <li><p><code>selected_autoreg</code>: List of selected autoregressive features.</p> </li> <li><p><code>selected_exog</code>: List of selected exogenous features.</p> </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import ShuffleSplit\nfrom skforecast.datasets import fetch_dataset\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import select_features\n\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import select_features_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from sklearn.feature_selection import RFECV from sklearn.feature_selection import SequentialFeatureSelector from sklearn.feature_selection import SelectFromModel from sklearn.model_selection import ShuffleSplit from skforecast.datasets import fetch_dataset  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import select_features  from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import select_features_multiseries In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing_extended_features\")\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"bike_sharing_extended_features\") data.head(3) <pre>bike_sharing_extended_features\n------------------------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, the dataset\nwas enriched by introducing supplementary features. Addition includes calendar-\nbased variables (day of the week, hour of the day, month, etc.), indicators for\nsunlight, incorporation of rolling temperature averages, and the creation of\npolynomial features generated from variable pairs. All cyclic variables are\nencoded using sine and cosine functions to ensure accurate representation.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17352, 90)\n</pre> Out[2]: users weather month_sin month_cos week_of_year_sin week_of_year_cos week_day_sin week_day_cos hour_day_sin hour_day_cos ... temp_roll_mean_1_day temp_roll_mean_7_day temp_roll_max_1_day temp_roll_min_1_day temp_roll_max_7_day temp_roll_min_7_day holiday_previous_day holiday_next_day temp holiday date_time 2011-01-08 00:00:00 25.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.258819 0.965926 ... 8.063334 10.127976 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 2011-01-08 01:00:00 16.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.500000 0.866025 ... 8.029166 10.113334 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 2011-01-08 02:00:00 16.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.707107 0.707107 ... 7.995000 10.103572 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 <p>3 rows \u00d7 90 columns</p> In\u00a0[3]: Copied! <pre># Data selection (reduce data size to speed up the example)\n# ==============================================================================\ndata = data.drop(columns=\"weather\")\ndata = data.loc[\"2012-01-01 00:00:00\":]\n</pre> # Data selection (reduce data size to speed up the example) # ============================================================================== data = data.drop(columns=\"weather\") data = data.loc[\"2012-01-01 00:00:00\":] In\u00a0[4]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(\n                                 n_estimators = 900,\n                                 random_state = 15926,\n                                 max_depth    = 7,\n                                 verbose      = -1\n                             ),\n                 lags      = 48,\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(                                  n_estimators = 900,                                  random_state = 15926,                                  max_depth    = 7,                                  verbose      = -1                              ),                  lags      = 48,              ) In\u00a0[5]: Copied! <pre># Feature selection (autoregressive and exog) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = RFECV(\n    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n)\n\nselected_autoreg, selected_exog = select_features(\n    forecaster      = forecaster,\n    selector        = selector,\n    y               = data[\"users\"],\n    exog            = data.drop(columns=\"users\"),\n    select_only     = None,\n    force_inclusion = None,\n    subsample       = 0.5,\n    random_state    = 123,\n    verbose         = True,\n)\n</pre> # Feature selection (autoregressive and exog) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = RFECV(     estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1 )  selected_autoreg, selected_exog = select_features(     forecaster      = forecaster,     selector        = selector,     y               = data[\"users\"],     exog            = data.drop(columns=\"users\"),     select_only     = None,     force_inclusion = None,     subsample       = 0.5,     random_state    = 123,     verbose         = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 136\n    Autoreg (n=48)\n    Exog    (n=88)\nNumber of features selected: 39\n    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=12) : ['week_of_year_sin', 'hour_day_sin', 'hour_day_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'temp_roll_mean_1_day', 'temp']\n</pre> <p>Then, the Forecaster model is trained with the selected features.</p> In\u00a0[6]: Copied! <pre># Train forecaster with selected features\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(\n                                 n_estimators = 900,\n                                 random_state = 15926,\n                                 max_depth    = 7,\n                                 verbose      = -1\n                             ),\n                 lags      = selected_autoreg,\n             )\n\nforecaster.fit(y=data[\"users\"], exog=data[selected_exog])\n</pre> # Train forecaster with selected features # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(                                  n_estimators = 900,                                  random_state = 15926,                                  max_depth    = 7,                                  verbose      = -1                              ),                  lags      = selected_autoreg,              )  forecaster.fit(y=data[\"users\"], exog=data[selected_exog]) In\u00a0[7]: Copied! <pre># Feature selection (only autoregressive) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = RFECV(\n    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n)\n\nselected_autoreg, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = 'autoreg',\n    subsample   = 0.5,\n    verbose     = True,\n)\n</pre> # Feature selection (only autoregressive) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = RFECV(     estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1 )  selected_autoreg, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = 'autoreg',     subsample   = 0.5,     verbose     = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 27\n    Autoreg (n=27)\n    Exog    (n=88)\nNumber of features selected: 26\n    Autoreg (n=26) : [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=88) : ['month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos', 'sunset_hour_sin', 'sunset_hour_cos', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_sin', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_sin__sunset_hour_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_month_cos__sunrise_hour_sin', 'poly_month_cos__sunrise_hour_cos', 'poly_month_cos__sunset_hour_sin', 'poly_month_cos__sunset_hour_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'poly_sunrise_hour_sin__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_sin', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunrise_hour_cos__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday']\n</pre> In\u00a0[8]: Copied! <pre># Check all exogenous features are selected\n# ==============================================================================\nlen(selected_exog) == data.drop(columns=\"users\").shape[1]\n</pre> # Check all exogenous features are selected # ============================================================================== len(selected_exog) == data.drop(columns=\"users\").shape[1] Out[8]: <pre>True</pre> In\u00a0[9]: Copied! <pre># Feature selection (only exog) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = RFECV(\n    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n)\n\nselected_autoreg, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = 'exog',\n    subsample   = 0.5,\n    verbose     = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = RFECV(     estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1 )  selected_autoreg, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = 'exog',     subsample   = 0.5,     verbose     = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 88\n    Autoreg (n=27)\n    Exog    (n=88)\nNumber of features selected: 66\n    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=66) : ['week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'temp', 'holiday']\n</pre> In\u00a0[10]: Copied! <pre># Check all autoregressive features are selected\n# ==============================================================================\nlen(selected_autoreg) == len(forecaster.lags)\n</pre> # Check all autoregressive features are selected # ============================================================================== len(selected_autoreg) == len(forecaster.lags) Out[10]: <pre>True</pre> <p>The <code>force_inclusion</code> argument can be used to force the selection of certain features. To illustrate this, a non-informative feature is added to the data set, <code>noise</code>. This feature contains no information about the target variable and therefore should not be selected by the feature selector. However, if we force the inclusion of this feature, it will be included in the final list of selected features.</p> In\u00a0[11]: Copied! <pre># Add non-informative feature\n# ==============================================================================\ndata['noise'] = np.random.normal(size=len(data))\n</pre> # Add non-informative feature # ============================================================================== data['noise'] = np.random.normal(size=len(data)) In\u00a0[12]: Copied! <pre># Feature selection (only exog) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = RFECV(\n    estimator=regressor, step=1, cv=3, min_features_to_select=10, n_jobs=-1\n)\n\nselected_autoreg, selected_exog = select_features(\n    forecaster      = forecaster,\n    selector        = selector,\n    y               = data[\"users\"],\n    exog            = data.drop(columns=\"users\"),\n    select_only     = 'exog',\n    force_inclusion = [\"noise\"],\n    subsample       = 0.5,\n    verbose         = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = RFECV(     estimator=regressor, step=1, cv=3, min_features_to_select=10, n_jobs=-1 )  selected_autoreg, selected_exog = select_features(     forecaster      = forecaster,     selector        = selector,     y               = data[\"users\"],     exog            = data.drop(columns=\"users\"),     select_only     = 'exog',     force_inclusion = [\"noise\"],     subsample       = 0.5,     verbose         = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 89\n    Autoreg (n=27)\n    Exog    (n=89)\nNumber of features selected: 17\n    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=18) : ['week_of_year_sin', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_cos__week_day_sin', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp', 'noise']\n</pre> In\u00a0[13]: Copied! <pre># Check if \"noise\" is in selected_exog\n# ==============================================================================\n\"noise\" in selected_exog\n</pre> # Check if \"noise\" is in selected_exog # ============================================================================== \"noise\" in selected_exog Out[13]: <pre>True</pre> In\u00a0[14]: Copied! <pre># Feature selection (only exog) with scikit-learn SequentialFeatureSelector\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = SequentialFeatureSelector(\n               estimator            = forecaster.regressor,\n               n_features_to_select = 25,\n               direction            = \"forward\",\n               cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n               scoring              = \"neg_mean_absolute_error\",\n               n_jobs               = -1,\n           )\n\nselected_autoreg, selected_exog = select_features(\n    forecaster   = forecaster,\n    selector     = selector,\n    y            = data[\"users\"],\n    exog         = data.drop(columns=\"users\"),\n    select_only  = 'exog',\n    subsample    = 0.2,\n    random_state = 123,\n    verbose      = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn SequentialFeatureSelector # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = SequentialFeatureSelector(                estimator            = forecaster.regressor,                n_features_to_select = 25,                direction            = \"forward\",                cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),                scoring              = \"neg_mean_absolute_error\",                n_jobs               = -1,            )  selected_autoreg, selected_exog = select_features(     forecaster   = forecaster,     selector     = selector,     y            = data[\"users\"],     exog         = data.drop(columns=\"users\"),     select_only  = 'exog',     subsample    = 0.2,     random_state = 123,     verbose      = True, ) <pre>Recursive feature elimination (SequentialFeatureSelector)\n---------------------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 89\n    Autoreg (n=27)\n    Exog    (n=89)\nNumber of features selected: 25\n    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=25) : ['week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunset_hour_cos', 'poly_month_sin__week_day_cos', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__sunrise_hour_cos', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_min_1_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday']\n</pre> <p>Combining feature selection methods can help speed up the process. An effective approach is to first use <code>SelectFromModel</code> to eliminate the less important features, and then use <code>SequentialFeatureSelector</code> to determine the best subset of features from this reduced list. This two-step method often improves efficiency by focusing on the most important features.</p> In\u00a0[15]: Copied! <pre># Feature selection (autoregressive and exog) with SelectFromModel + SequentialFeatureSelector\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\n# Step 1: Select the 70% most important features with SelectFromModel\nselector_1 = SelectFromModel(\n                 estimator    = regressor,\n                 max_features = int(data.shape[1] * 0.7),\n                 threshold    = -np.inf\n             )\nselected_autoreg_1, selected_exog_1 = select_features(\n    forecaster  = forecaster,\n    selector    = selector_1,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = None,\n    subsample   = 0.2,\n    verbose     = True,\n)\nprint(\"\")\n\n# Step 2: Select the 25 most important features with SequentialFeatureSelector\nforecaster.set_lags(lags=selected_autoreg_1)\nselector_2 = SequentialFeatureSelector(\n                 estimator            = regressor,\n                 n_features_to_select = 25,\n                 direction            = \"forward\",\n                 cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n                 scoring              = \"neg_mean_absolute_error\",\n                 n_jobs               = -1,\n             )\nselected_autoreg, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector_2,\n    y           = data[\"users\"],\n    exog        = data[selected_exog_1],\n    select_only = None,\n    subsample   = 0.2,\n    verbose     = True,\n)\n</pre> # Feature selection (autoregressive and exog) with SelectFromModel + SequentialFeatureSelector # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  # Step 1: Select the 70% most important features with SelectFromModel selector_1 = SelectFromModel(                  estimator    = regressor,                  max_features = int(data.shape[1] * 0.7),                  threshold    = -np.inf              ) selected_autoreg_1, selected_exog_1 = select_features(     forecaster  = forecaster,     selector    = selector_1,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = None,     subsample   = 0.2,     verbose     = True, ) print(\"\")  # Step 2: Select the 25 most important features with SequentialFeatureSelector forecaster.set_lags(lags=selected_autoreg_1) selector_2 = SequentialFeatureSelector(                  estimator            = regressor,                  n_features_to_select = 25,                  direction            = \"forward\",                  cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),                  scoring              = \"neg_mean_absolute_error\",                  n_jobs               = -1,              ) selected_autoreg, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector_2,     y           = data[\"users\"],     exog        = data[selected_exog_1],     select_only = None,     subsample   = 0.2,     verbose     = True, ) <pre>Recursive feature elimination (SelectFromModel)\n-----------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 116\n    Autoreg (n=27)\n    Exog    (n=89)\nNumber of features selected: 62\n    Autoreg (n=27) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 21, 22, 23, 24, 25, 26, 29, 30, 32, 35, 36, 42, 48]\n    Exog    (n=35) : ['week_day_sin', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__hour_day_sin', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'holiday_previous_day', 'temp', 'noise']\n\nRecursive feature elimination (SequentialFeatureSelector)\n---------------------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 62\n    Autoreg (n=27)\n    Exog    (n=35)\nNumber of features selected: 25\n    Autoreg (n=8) : [1, 2, 4, 10, 21, 24, 32, 35]\n    Exog    (n=17) : ['hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_min_1_day', 'holiday_previous_day']\n</pre> <p>As with univariate forecasting models, feature selection can be applied to global forecasting models (multi-series). In this case, the <code>select_features_multiseries</code> function is used. This function has the same parameters as <code>select_features</code>, but the <code>y</code> parameter is replaced by <code>series</code>.</p> <ul> <li><p><code>forecaster</code>: Forecaster of type <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li><p><code>selector</code>: Feature selector from <code>sklearn.feature_selection</code>. For example, <code>RFE</code> or <code>RFECV</code>.</p> </li> <li><p><code>series</code>: Target time series to which the feature selection will be applied.</p> </li> <li><p><code>exog</code>: Exogenous variables.</p> </li> <li><p><code>select_only</code>: Decide what type of features to include in the selection process.</p> <ul> <li><p>If <code>'autoreg'</code>, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (<code>selected_exog</code>).</p> </li> <li><p>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the output (<code>selected_autoreg</code>).</p> </li> <li><p>If <code>None</code>, all features are evaluated by the selector.</p> </li> </ul> </li> <li><p><code>force_inclusion</code>: Features to force include in the final list of selected features.</p> <ul> <li><p>If <code>list</code>, list of feature names to force include.</p> </li> <li><p>If <code>str</code>, regular expression to identify features to force include. For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin with \"sun_\" will be included in the final list of selected features.</p> </li> </ul> </li> <li><p><code>subsample</code>: Proportion of records to use for feature selection.</p> </li> <li><p><code>random_state</code>: Sets a seed for the random subsample so that the subsampling process is always deterministic.</p> </li> <li><p><code>verbose</code>: Print information about feature selection process.</p> </li> </ul> In\u00a0[16]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"items_sales\") <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> In\u00a0[17]: Copied! <pre># Create exogenous features based on the calendar\n# ==============================================================================\ndata[\"month\"] = data.index.month\ndata[\"day_of_week\"] = data.index.dayofweek\ndata[\"day_of_month\"] = data.index.day\ndata[\"week_of_year\"] = data.index.isocalendar().week\ndata[\"quarter\"] = data.index.quarter\ndata[\"is_month_start\"] = data.index.is_month_start.astype(int)\ndata[\"is_month_end\"] = data.index.is_month_end.astype(int)\ndata[\"is_quarter_start\"] = data.index.is_quarter_start.astype(int)\ndata[\"is_quarter_end\"] = data.index.is_quarter_end.astype(int)\ndata[\"is_year_start\"] = data.index.is_year_start.astype(int)\ndata[\"is_year_end\"] = data.index.is_year_end.astype(int)\ndata.head()\n</pre> # Create exogenous features based on the calendar # ============================================================================== data[\"month\"] = data.index.month data[\"day_of_week\"] = data.index.dayofweek data[\"day_of_month\"] = data.index.day data[\"week_of_year\"] = data.index.isocalendar().week data[\"quarter\"] = data.index.quarter data[\"is_month_start\"] = data.index.is_month_start.astype(int) data[\"is_month_end\"] = data.index.is_month_end.astype(int) data[\"is_quarter_start\"] = data.index.is_quarter_start.astype(int) data[\"is_quarter_end\"] = data.index.is_quarter_end.astype(int) data[\"is_year_start\"] = data.index.is_year_start.astype(int) data[\"is_year_end\"] = data.index.is_year_end.astype(int) data.head() Out[17]: item_1 item_2 item_3 month day_of_week day_of_month week_of_year quarter is_month_start is_month_end is_quarter_start is_quarter_end is_year_start is_year_end date 2012-01-01 8.253175 21.047727 19.429739 1 6 1 52 1 1 0 1 0 1 0 2012-01-02 22.777826 26.578125 28.009863 1 0 2 1 1 0 0 0 0 0 0 2012-01-03 27.549099 31.751042 32.078922 1 1 3 1 1 0 0 0 0 0 0 2012-01-04 25.895533 24.567708 27.252276 1 2 4 1 1 0 0 0 0 0 0 2012-01-05 21.379238 18.191667 20.357737 1 3 5 1 1 0 0 0 0 0 0 In\u00a0[18]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n    regressor = LGBMRegressor(n_estimators=900,random_state=159, max_depth=7, verbose=-1),\n    lags      = 24,\n)\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeries(     regressor = LGBMRegressor(n_estimators=900,random_state=159, max_depth=7, verbose=-1),     lags      = 24, ) In\u00a0[19]: Copied! <pre># Feature selection (autoregressive and exog) with scikit-learn RFECV\n# ==============================================================================\nseries_columns = [\"item_1\", \"item_2\", \"item_3\"]\nexog_columns = [col for col in data.columns if col not in series_columns]\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\nselector = RFECV(\n    estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1\n)\n\nselected_autoreg, selected_exog = select_features_multiseries(\n    forecaster      = forecaster,\n    selector        = selector,\n    series          = data[series_columns],\n    exog            = data[exog_columns],\n    select_only     = None,\n    force_inclusion = None,\n    subsample       = 0.5,\n    random_state    = 123,\n    verbose         = True,\n)\n</pre> # Feature selection (autoregressive and exog) with scikit-learn RFECV # ============================================================================== series_columns = [\"item_1\", \"item_2\", \"item_3\"] exog_columns = [col for col in data.columns if col not in series_columns] regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  selector = RFECV(     estimator=regressor, step=1, cv=3, min_features_to_select=25, n_jobs=-1 )  selected_autoreg, selected_exog = select_features_multiseries(     forecaster      = forecaster,     selector        = selector,     series          = data[series_columns],     exog            = data[exog_columns],     select_only     = None,     force_inclusion = None,     subsample       = 0.5,     random_state    = 123,     verbose         = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 3219\nTotal number of records used for feature selection: 4827\nNumber of features available: 35\n    Autoreg (n=24)\n    Exog    (n=11)\nNumber of features selected: 28\n    Autoreg (n=24) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n    Exog    (n=4) : ['month', 'day_of_week', 'day_of_month', 'week_of_year']\n</pre> <p>Once the best subset of features has been selected, the global forecasting model is trained with the selected features.</p> In\u00a0[20]: Copied! <pre># Train forecaster with selected features\n# ==============================================================================\nforecaster.set_lags(lags=selected_autoreg)\nforecaster.fit(series=data[series_columns], exog=data[selected_exog])\nforecaster\n</pre> # Train forecaster with selected features # ============================================================================== forecaster.set_lags(lags=selected_autoreg) forecaster.fit(series=data[series_columns], exog=data[selected_exog]) forecaster Out[20]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(max_depth=7, n_estimators=900, random_state=159, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal_category \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['month', 'day_of_week', 'day_of_month', 'week_of_year'] \nTraining range: [\"'item_1': ['2012-01-01', '2015-01-01']\", \"'item_2': ['2012-01-01', '2015-01-01']\", \"'item_3': ['2012-01-01', '2015-01-01']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-07-29 17:03:27 \nLast fit date: 2024-07-29 17:03:29 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre>"},{"location":"user_guides/feature-selection.html#feature-selection","title":"Feature selection\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-with-skforecast","title":"Feature selection with skforecast\u00b6","text":""},{"location":"user_guides/feature-selection.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/feature-selection.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/feature-selection.html#create-forecaster","title":"Create forecaster\u00b6","text":"<p>A forecasting model is created to predict the number of users using the last 48 values (last two days) and the exogenous features available in the dataset.</p>"},{"location":"user_guides/feature-selection.html#feature-selection-with-recursive-feature-elimination-rfecv","title":"Feature selection with Recursive Feature Elimination (RFECV)\u00b6","text":""},{"location":"user_guides/feature-selection.html#selection-of-autoregressive-and-exogenous-features","title":"Selection of autoregressive and exogenous features\u00b6","text":"<p>By default, the <code>select_features</code> function selects the best subset of autoregressive and exogenous features.</p>"},{"location":"user_guides/feature-selection.html#selection-on-a-subset-of-features","title":"Selection on a subset of features\u00b6","text":"<ul> <li><p>If <code>select_only = 'autoreg'</code>, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output (<code>selected_exog</code>).</p> </li> <li><p>If <code>select_only = 'exog'</code>, exogenous features are evaluated by the selector in the absence of autoregressive features. All autoregressive features are included in the output (<code>selected_autoreg</code>).</p> </li> </ul>"},{"location":"user_guides/feature-selection.html#force-selection-of-specific-features","title":"Force selection of specific features\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-with-sequential-feature-selection-sfs","title":"Feature selection with Sequential Feature Selection (SFS)\u00b6","text":"<p>Sequential Feature Selection is a robust method for selecting features, but it is computationally expensive. When the data set is very large, one way to reduce the computational cost is to use a single validation split to evaluate each candidate model instead of cross-validation (default).</p>"},{"location":"user_guides/feature-selection.html#combination-of-feature-selection-methods","title":"Combination of feature selection methods\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-in-global-forecasting-models","title":"Feature Selection in Global Forecasting Models\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html","title":"Forecaster in production","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata_train = data.loc[:'2005-01-01']\ndata_train.tail()\n</pre> # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data_train = data.loc[:'2005-01-01'] data_train.tail() Out[3]: y date 2004-09-01 1.134432 2004-10-01 1.181011 2004-11-01 1.216037 2004-12-01 1.257238 2005-01-01 1.170690 In\u00a0[4]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata_train = data.loc[:'2005-01-01']\n# data_train\n</pre> # Data preprocessing # ============================================================================== data_train = data.loc[:'2005-01-01'] # data_train In\u00a0[5]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) In\u00a0[6]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster.predict(steps=3) Out[6]: <pre>2005-02-01    0.927480\n2005-03-01    0.756215\n2005-04-01    0.692595\nFreq: MS, Name: pred, dtype: float64</pre> <p>As expected, predictions follow directly from the end of training data.</p> <p>When the <code>last_window</code> argument is provided, the forecaster uses this data to generate the necessary lags as predictors and starts the prediction thereafter.</p> In\u00a0[7]: Copied! <pre># Predict with last_window\n# ==============================================================================\nlast_window = data['y'].tail(5)\nforecaster.predict(steps=3, last_window=last_window)\n</pre> # Predict with last_window # ============================================================================== last_window = data['y'].tail(5) forecaster.predict(steps=3, last_window=last_window) Out[7]: <pre>2008-07-01    0.803853\n2008-08-01    0.870858\n2008-09-01    0.905003\nFreq: MS, Name: pred, dtype: float64</pre> <p>Since the provided <code>last_window</code> contains values from 2008-02-01 to 2008-06-01, the forecaster can create the needed lags and predict the next 5 steps.</p> <p> \u26a0 Warning </p> <p>When using the <code>last_window</code> argument, it is crucial to ensure that the length of <code>last_window</code> is sufficient to include the maximum lag (or custom predictor) used by the forecaster. For instance, if the forecaster employs lags 1, 24, and 48, <code>last_window</code> must include the most recent 48 values of the series. Failing to include the required number of past observations may result in an error or incorrect predictions.</p> In\u00a0[8]: Copied! <pre># Split data\n# ==============================================================================\ndata_train = data.loc[:'2005-01-01'].copy()\ndata_last_window = data.loc['2005-08-01':'2005-12-01'].copy()\n\nprint(\n    f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"\n    f\"  (n={len(last_window)})\"\n)\n</pre> # Split data # ============================================================================== data_train = data.loc[:'2005-01-01'].copy() data_last_window = data.loc['2005-08-01':'2005-12-01'].copy()  print(     f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"     f\"  (n={len(last_window)})\" ) <pre>Train dates       : 1991-07-01 00:00:00 --- 2005-01-01 00:00:00  (n=163)\nLast window dates : 2005-08-01 00:00:00 --- 2005-12-01 00:00:00  (n=5)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(label='train', ax=ax)\ndata_last_window['y'].plot(label='last window', ax=ax)\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(label='train', ax=ax) data_last_window['y'].plot(label='last window', ax=ax) ax.set_xlabel('') ax.legend(); In\u00a0[10]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) <p>The size of the window needed to make predictions and the last window stored in the forecaster to make predictions immediately after the training data can be observed with the <code>window_size</code> and <code>last_window</code> attributes.</p> In\u00a0[11]: Copied! <pre># Forecaster Attributes\n# ==============================================================================\nprint('Window size:', forecaster.window_size)\nprint(\"Forecaster last window:\")\nprint(forecaster.last_window)\n</pre> # Forecaster Attributes # ============================================================================== print('Window size:', forecaster.window_size) print(\"Forecaster last window:\") print(forecaster.last_window) <pre>Window size: 5\nForecaster last window:\ndate\n2004-09-01    1.134432\n2004-10-01    1.181011\n2004-11-01    1.216037\n2004-12-01    1.257238\n2005-01-01    1.170690\nFreq: MS, Name: y, dtype: float64\n</pre> <p>The model is saved for future use.</p> <p> \u270e Note </p> <p>Learn more about how to Save and load forecasters.</p> In\u00a0[12]: Copied! <pre># Save Forecaster\n# ==============================================================================\nsave_forecaster(forecaster, file_name='forecaster_001.py', verbose=False)\n</pre> # Save Forecaster # ============================================================================== save_forecaster(forecaster, file_name='forecaster_001.py', verbose=False) <p> \ud83d\udca1 Tip </p> <p>Since the Forecaster has already been trained, there is no need to re-fit the model.</p> In\u00a0[13]: Copied! <pre># Load Forecaster\n# ==============================================================================\nforecaster_loaded = load_forecaster('forecaster_001.py', verbose=True)\n</pre> # Load Forecaster # ============================================================================== forecaster_loaded = load_forecaster('forecaster_001.py', verbose=True) <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2024-07-29 17:07:50 \nLast fit date: 2024-07-29 17:07:50 \nSkforecast version: 0.11.0 \nPython version: 3.10.11 \nForecaster id: forecasting_series_y \n\n</pre> <p>The forecaster's training range ends at '2005-01-01'. Using a <code>last_window</code>, the forecaster will be able to make predictions for '2006-01-01', 1 year later, without having to re-fit the model.</p> In\u00a0[14]: Copied! <pre># 1 year later last window\n# ==============================================================================\ndata_last_window\n</pre> # 1 year later last window # ============================================================================== data_last_window Out[14]: y date 2005-08-01 1.006497 2005-09-01 1.094736 2005-10-01 1.027043 2005-11-01 1.149232 2005-12-01 1.160712 In\u00a0[15]: Copied! <pre># Predict with last_window\n# ==============================================================================\nforecaster.predict(steps=3, last_window=data_last_window['y'])\n</pre> # Predict with last_window # ============================================================================== forecaster.predict(steps=3, last_window=data_last_window['y']) Out[15]: <pre>2006-01-01    0.979303\n2006-02-01    0.760421\n2006-03-01    0.634806\nFreq: MS, Name: pred, dtype: float64</pre>"},{"location":"user_guides/forecaster-in-production.html#using-forecaster-models-in-production","title":"Using forecaster models in production\u00b6","text":"<p>When using a trained model in production, regular predictions need to be generated, for example, on a weekly basis every Monday. By default, the <code>predict</code> method on a trained forecaster object generates predictions starting right after the last training observation. Therefore, the model could be retrained weekly, just before the first prediction is needed, and its predict method called. However, this approach may not be practical due to reasons such as: expensive model training, unavailability of the training data history, or high prediction frequency.</p> <p>In such scenarios, the model must be able to predict at any time, even if it has not been recently trained. Fortunately, every model generated using skforecast has the <code>last_window</code> argument in its <code>predict</code> method. This argument allows providing only the past values needed to create the autoregressive predictors (lags or custom predictors), enabling prediction without the need to retrain the model. This feature is particularly useful when there are limitations in retraining the model regularly or when dealing with high-frequency predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#predicting-with-last-window","title":"Predicting with last window\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#real-use-case","title":"Real Use Case\u00b6","text":"<p>The main advantage of using the <code>last_window</code> argument is that it can be used to predict at any time, even if the Forecaster has not been trained recently.</p> <p>Imagine a use case where a model is trained, stored, and 1 year later the company wants to use it to make some predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#data","title":"Data\u00b6","text":"<p>A gap is created between the end of the training data and the last window data to simulate this behavior.</p>"},{"location":"user_guides/forecaster-in-production.html#forecaster-initial-train","title":"Forecaster initial train\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#future-predictions","title":"Future predictions\u00b6","text":"<p>The model is loaded to make new predictions.</p>"},{"location":"user_guides/forecasting-baseline.html","title":"Foreasting baseline","text":"<p>In forecasting modeling, a baseline serves as a basic, often simplistic model that acts as a fundamental reference for evaluating the performance of more complex models. It provides a baseline forecast using simple techniques that do not rely on sophisticated algorithms or extensive data analysis. Common examples of baseline strategies include:</p> <ul> <li><p>Last Observed Value: Uses the last observed value as a forecast for all future periods.</p> </li> <li><p>Moving Average: In this technique, the average of the last few observations is calculated and used as a forecast for the next period. For example, a 3-period moving average would use the average of the last three observations.</p> </li> <li><p>Last Equivalent Date (Seasonal Naive Forecasting): Extends the concept of the last observed value by considering the corresponding period in the previous season (e.g., the same working day from the previous week) as the forecast for the current period.</p> </li> </ul> <p>The primary goal of establishing a baseline is to provide a benchmark against which the performance of more advanced predictive models can be evaluated. If the model does not outperform the baseline, it may indicate that there is a fundamental problem with the approach or that the added complexity is not justified by the available data. This underscores the importance of carefully evaluating the appropriateness of complex models relative to the simplicity and effectiveness of baseline models.</p> <p>The <code>ForecasterEquivalentDate</code> class from <code>skforecast.ForecasterBaseline</code> allows the creation of a baseline forecast based on the concept of equivalent dates. In this context, an equivalent date is a historical date that has similar characteristics to the target date. The forecast for a given date is based on the value observed on the last n equivalent dates.</p> <p>The behavior of the forecast is primarily controlled by two arguments:</p> <ul> <li><p><code>offset</code>: This parameter determines how many steps back in time to go to find the most recent equivalent date for the target period. When given as an integer, <code>offset</code> represents the number of steps to go back in time. For example, if the frequency of the time series is daily, <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Additionally, it is possible to use Pandas DateOffsets to move forward a given number of valid dates. For example, <code>Bday(2)</code> can be used to move back two business days.</p> </li> <li><p><code>n_offsets</code>: This parameter determines the number of equivalent dates to use in the prediction. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the specified aggregation function, <code>agg_func</code>. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code>, and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> </li> </ul> <p> \u270e Note </p> <p><code>ForecasterEquivalentDate</code> is designed to integrate seamlessly with other functionality offered by skforecast, such as backtesting. This makes it easy to obtain the baseline for a given period.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterBaseline import ForecasterEquivalentDate\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.datasets import fetch_dataset\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterBaseline import ForecasterEquivalentDate from skforecast.model_selection import backtesting_forecaster from skforecast.datasets import fetch_dataset from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ======================================================================================\ndata = fetch_dataset('fuel_consumption')\ndata = data.rename(columns={'Gasolinas':'litters'})\ndata.index.name = 'date'\ndata = data.loc[:'1985-01-01 00:00:00', 'litters']\ndisplay(data.head(4))\n\n# Train-test dates\n# ======================================================================================\nend_train = '1980-01-01 23:59:59'\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n\n# Plot\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.set_title('Monthly fuel consumption in Spain')\nax.legend();\n</pre> # Download data # ====================================================================================== data = fetch_dataset('fuel_consumption') data = data.rename(columns={'Gasolinas':'litters'}) data.index.name = 'date' data = data.loc[:'1985-01-01 00:00:00', 'litters'] display(data.head(4))  # Train-test dates # ====================================================================================== end_train = '1980-01-01 23:59:59' data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")  # Plot # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.set_title('Monthly fuel consumption in Spain') ax.legend(); <pre>fuel_consumption\n----------------\nMonthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.\nObtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos Petrol\u00edferos and\nCorporaci\u00f3n de Derecho P\u00fablico tutelada por el Ministerio para la Transici\u00f3n\nEcol\u00f3gica y el Reto Demogr\u00e1fico. https://www.cores.es/es/estadisticas\nShape of the dataset: (644, 5)\n</pre> <pre>date\n1969-01-01    166875.2129\n1969-02-01    155466.8105\n1969-03-01    184983.6699\n1969-04-01    202319.8164\nFreq: MS, Name: litters, dtype: float64</pre> <pre>Train dates : 1969-01-01 00:00:00 --- 1980-01-01 00:00:00  (n=133)\nTest dates  : 1980-02-01 00:00:00 --- 1985-01-01 00:00:00  (n=60)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterEquivalentDate(\n                 offset    = pd.DateOffset(months=12),\n                 n_offsets = 2,\n                 agg_func  = np.mean\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterEquivalentDate(                  offset    = pd.DateOffset(months=12),                  n_offsets = 2,                  agg_func  = np.mean              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>======================== \nForecasterEquivalentDate \n======================== \nOffset: &lt;DateOffset: months=12&gt; \nNumber of offsets: 2 \nAggregation function: mean \nWindow size: 24 \nTraining range: [Timestamp('1969-01-01 00:00:00'), Timestamp('1980-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2024-07-29 17:10:06 \nLast fit date: 2024-07-29 17:10:06 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data_test))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data_test)) predictions.head(3) Out[4]: <pre>1980-02-01    385298.35315\n1980-03-01    472815.89325\n1980-04-01    462944.81705\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n            \nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )              print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 446931887.96547604\n</pre> In\u00a0[7]: Copied! <pre># Backtesting\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster         = forecaster,\n                          y                  = data,\n                          initial_train_size = len(data_train),\n                          steps              = 15,\n                          metric             = 'mean_absolute_error',\n                          refit              = True,\n                          verbose            = False,\n                          n_jobs             = 'auto'\n                      )\n\nprint(\"Backtest error:\")\nmetric\n</pre> # Backtesting # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster         = forecaster,                           y                  = data,                           initial_train_size = len(data_train),                           steps              = 15,                           metric             = 'mean_absolute_error',                           refit              = True,                           verbose            = False,                           n_jobs             = 'auto'                       )  print(\"Backtest error:\") metric  <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error:\n</pre> Out[7]: mean_absolute_error 0 18575.076906 In\u00a0[8]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions.head(4)\n</pre> # Backtesting predictions # ============================================================================== predictions.head(4) Out[8]: pred 1980-02-01 385298.35315 1980-03-01 472815.89325 1980-04-01 462944.81705 1980-05-01 477889.17740 In\u00a0[9]: Copied! <pre># Plot backtesting predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot backtesting predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend();"},{"location":"user_guides/forecasting-baseline.html#baseline-forecaster","title":"Baseline forecaster\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#forecasterequivalentdate","title":"ForecasterEquivalentDate\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#backtesting","title":"Backtesting\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html","title":"ARIMA and SARIMAX forecasting","text":"<p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series with ARIMA models, visit our example: ARIMA and SARIMAX models with Python.</p> <p> \u26a0 Warning </p> <p>The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the <code>skforecast.Sarimax.Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.  User guide: https://skforecast.org/0.13.0/user_guides/forecasting-sarimax-arima#forecastersarimax</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.Sarimax import Sarimax\nfrom skforecast.ForecasterSarimax import ForecasterSarimax\nfrom skforecast.model_selection_sarimax import backtesting_sarimax\nfrom skforecast.model_selection_sarimax import grid_search_sarimax\nfrom skforecast.datasets import fetch_dataset\nfrom pmdarima import ARIMA\nfrom pmdarima import auto_arima\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.Sarimax import Sarimax from skforecast.ForecasterSarimax import ForecasterSarimax from skforecast.model_selection_sarimax import backtesting_sarimax from skforecast.model_selection_sarimax import grid_search_sarimax from skforecast.datasets import fetch_dataset from pmdarima import ARIMA from pmdarima import auto_arima from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"h2o_exog\")\ndata.index.name = 'datetime'\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"h2o_exog\") data.index.name = 'datetime' <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Train-test dates\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nprint(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\")\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax)\nax.legend();\n</pre> # Train-test dates # ============================================================================== end_train = '2005-06-01 23:59:59' print(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\") data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.plot(ax=ax) ax.legend(); <pre>Train dates : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> <p>The following section focus on how to train an ARIMA model and forecast future values with each of the three libraries.</p> <p>statsmodels</p> In\u00a0[4]: Copied! <pre># ARIMA model with statsmodels.Sarimax\n# ==============================================================================\narima = SARIMAX(endog = data_train['y'], order = (1, 1, 1))\narima_res = arima.fit(disp=0)\narima_res.summary()\n</pre> # ARIMA model with statsmodels.Sarimax # ============================================================================== arima = SARIMAX(endog = data_train['y'], order = (1, 1, 1)) arima_res = arima.fit(disp=0) arima_res.summary() Out[4]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Mon, 29 Jul 2024   AIC                 -173.869 Time: 17:39:12   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.420  0.000     0.352     0.912 ma.L1    -0.9535     0.054   -17.817  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.05 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[5]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima_res.get_forecast(steps=12)\npredictions.predicted_mean.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima_res.get_forecast(steps=12) predictions.predicted_mean.head(4) Out[5]: <pre>2005-07-01    0.859452\n2005-08-01    0.870310\n2005-09-01    0.877167\n2005-10-01    0.881499\nFreq: MS, Name: predicted_mean, dtype: float64</pre> <p>pmdarima</p> In\u00a0[6]: Copied! <pre># ARIMA model with pmdarima.ARIMA\n# ==============================================================================\narima = ARIMA(order=(1, 1, 1), suppress_warnings=True)\narima.fit(y=data_train['y'])\narima.summary()\n</pre> # ARIMA model with pmdarima.ARIMA # ============================================================================== arima = ARIMA(order=(1, 1, 1), suppress_warnings=True) arima.fit(y=data_train['y']) arima.summary() Out[6]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      93.643 Date: Mon, 29 Jul 2024   AIC                 -179.286 Time: 17:39:12   BIC                 -167.036 Sample: 04-01-1992   HQIC                -174.311 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] intercept     0.0011     0.000     2.188  0.029     0.000     0.002 ar.L1     0.6023     0.108     5.584  0.000     0.391     0.814 ma.L1    -0.9995     2.322    -0.431  0.667    -5.550     3.551 sigma2     0.0175     0.041     0.432  0.666    -0.062     0.097 Ljung-Box (L1) (Q): 1.57   Jarque-Bera (JB):   130.09 Prob(Q): 0.21   Prob(JB):           0.00 Heteroskedasticity (H): 2.19   Skew:               -1.49 Prob(H) (two-sided): 0.01   Kurtosis:           6.29 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[7]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima.predict(n_periods=12)\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima.predict(n_periods=12) predictions.head(4) Out[7]: <pre>2005-07-01    0.891939\n2005-08-01    0.922945\n2005-09-01    0.942706\n2005-10-01    0.955694\nFreq: MS, dtype: float64</pre> <p>skforecast</p> In\u00a0[8]: Copied! <pre># ARIMA model with skforecast.Sarimax\n# ==============================================================================\narima = Sarimax(order=(1, 1, 1))\narima.fit(y=data_train['y'])\narima.summary()\n</pre> # ARIMA model with skforecast.Sarimax # ============================================================================== arima = Sarimax(order=(1, 1, 1)) arima.fit(y=data_train['y']) arima.summary() Out[8]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Mon, 29 Jul 2024   AIC                 -173.869 Time: 17:39:12   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.420  0.000     0.352     0.912 ma.L1    -0.9535     0.054   -17.817  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.05 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[9]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima.predict(steps=12)\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima.predict(steps=12) predictions.head(4) Out[9]: pred 2005-07-01 0.859452 2005-08-01 0.870310 2005-09-01 0.877167 2005-10-01 0.881499 <p>The previous section introduced the construction of ARIMA-SARIMAX models using three different implementations. In order to seamlessly integrate these models with the various functionalities provided by skforecast, the next step is to encapsulate these models within a <code>ForecasterSarimax</code> object. This encapsulation harmonizes the intricacies of the model and allows for the coherent use of skforecast's extensive capabilities.</p> <p>The following code is done using the skforecast <code>Sarimax</code> model, but the same code can be applied to the pmdarima <code>ARIMA</code> model.</p> In\u00a0[10]: Copied! <pre># Create and fit ForecasterSarimax\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\nforecaster\n</pre> # Create and fit ForecasterSarimax # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(y=data_train['y'], suppress_warnings=True) forecaster Out[10]: <pre>================= \nForecasterSarimax \n================= \nRegressor: Sarimax(12,1,1)(0,0,0)[0] \nRegressor parameters: {'concentrate_scale': False, 'dates': None, 'disp': False, 'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None, 'hamilton_representation': False, 'maxiter': 200, 'measurement_error': False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True, 'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing': False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {}, 'start_params': None, 'time_varying_regression': False, 'trend': None, 'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2024-07-29 17:39:12 \nLast fit date: 2024-07-29 17:39:14 \nIndex seen by the forecaster: DatetimeIndex(['1992-04-01', '1992-05-01', '1992-06-01', '1992-07-01',\n               '1992-08-01', '1992-09-01', '1992-10-01', '1992-11-01',\n               '1992-12-01', '1993-01-01',\n               ...\n               '2004-09-01', '2004-10-01', '2004-11-01', '2004-12-01',\n               '2005-01-01', '2005-02-01', '2005-03-01', '2005-04-01',\n               '2005-05-01', '2005-06-01'],\n              dtype='datetime64[ns]', name='datetime', length=159, freq='MS') \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[11]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[11]: <pre>2005-07-01    0.957727\n2005-08-01    0.960103\n2005-09-01    1.108399\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[13]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.07123026078782618\n</pre> In\u00a0[14]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, alpha=0.05)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, alpha=0.05) predictions.head(3) Out[14]: pred lower_bound upper_bound 2005-07-01 0.957727 0.857672 1.057781 2005-08-01 0.960103 0.853857 1.066350 2005-09-01 1.108399 0.997995 1.218803 <p> \ud83d\udca1 Tip </p> <p>To learn more about exogenous variables and how to correctly manage them with skforecast visit: Exogenous variables (features) user guide.</p> In\u00a0[15]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps = 36,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog # ============================================================================== predictions = forecaster.predict(                   steps = 36,                   exog  = data_test[['exog_1', 'exog_2']]               ) predictions.head(3) Out[15]: <pre>2005-07-01    0.904069\n2005-08-01    0.931670\n2005-09-01    1.089182\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[16]: Copied! <pre># Split data Train - Last window - Test\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nend_last_window = '2007-06-01 23:59:59'\n\nprint(\n    f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"\n    f\"(n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"\n    f\"(n={len(data.loc[end_train:end_last_window])})\"\n)\nprint(\n    f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_last_window:])})\"\n)\ndata_train       = data.loc[:end_train]\ndata_last_window = data.loc[end_train:end_last_window]\ndata_test        = data.loc[end_last_window:]\n\n# Plot\n# ======================================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Split data Train - Last window - Test # ============================================================================== end_train = '2005-06-01 23:59:59' end_last_window = '2007-06-01 23:59:59'  print(     f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"     f\"(n={len(data.loc[:end_train])})\" ) print(     f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"     f\"(n={len(data.loc[end_train:end_last_window])})\" ) print(     f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_last_window:])})\" ) data_train       = data.loc[:end_train] data_last_window = data.loc[end_train:end_last_window] data_test        = data.loc[end_last_window:]  # Plot # ====================================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates       : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nLast window dates : 2005-07-01 00:00:00 --- 2007-06-01 00:00:00  (n=24)\nTest dates        : 2007-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=12)\n</pre> <p>Since exogenous variables have been included in the Forecaster tuning, it is necessary to pass both past values and their future values to the <code>predict</code> method using the <code>last_window_exog</code> and <code>exog</code> parameters when making predictions.</p> In\u00a0[17]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog and last window\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps            = 12,\n                  exog             = data_test[['exog_1', 'exog_2']],\n                  last_window      = data_last_window['y'],\n                  last_window_exog = data_last_window[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog and last window # ============================================================================== predictions = forecaster.predict(                   steps            = 12,                   exog             = data_test[['exog_1', 'exog_2']],                   last_window      = data_last_window['y'],                   last_window_exog = data_last_window[['exog_1', 'exog_2']]               ) predictions.head(3) Out[17]: <pre>2007-07-01    0.883806\n2007-08-01    1.041050\n2007-09-01    1.071687\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[18]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.06280550767395036\n</pre> In\u00a0[19]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[20]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[20]: feature importance 1 exog_2 1.523893 13 ar.L12 0.674807 15 sigma2 0.001585 12 ar.L11 -0.148613 2 ar.L1 -0.159481 6 ar.L5 -0.188342 4 ar.L3 -0.190626 7 ar.L6 -0.207007 10 ar.L9 -0.208029 9 ar.L8 -0.210240 11 ar.L10 -0.226946 3 ar.L2 -0.234267 8 ar.L7 -0.245370 5 ar.L4 -0.282110 0 exog_1 -0.539596 14 ma.L1 -0.979138 In\u00a0[21]: Copied! <pre># Backtest forecaster\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nmetric, predictions = backtesting_sarimax(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          steps                 = 12,\n                          metric                = 'mean_absolute_error',\n                          refit                 = True,\n                          n_jobs                = 'auto',\n                          suppress_warnings_fit = True,\n                          verbose               = True,\n                          show_progress         = True\n                      )\n\nmetric\n</pre> # Backtest forecaster # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  metric, predictions = backtesting_sarimax(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           steps                 = 12,                           metric                = 'mean_absolute_error',                           refit                 = True,                           n_jobs                = 'auto',                           suppress_warnings_fit = True,                           verbose               = True,                           show_progress         = True                       )  metric <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 159\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number skipped folds: 0 \n    Number of steps per fold: 12\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2005-06-01 00:00:00  (n=159)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2006-06-01 00:00:00  (n=171)\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2007-06-01 00:00:00  (n=183)\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[21]: mean_absolute_error 0 0.056369 In\u00a0[22]: Copied! <pre># Backtest predictions\n# ======================================================================================\npredictions.head(4)\n</pre> # Backtest predictions # ====================================================================================== predictions.head(4) Out[22]: pred 2005-07-01 0.904069 2005-08-01 0.931670 2005-09-01 1.089182 2005-10-01 1.113034 <p> \ud83d\udca1 Tip </p> <p>In summary, while the statistical criteria approach offers speed and efficiency, validation techniques provide a more comprehensive and insightful evaluation, albeit at a slower pace due to their reliance on new data for testing. Fortunately, for sufficiently large data sets, they all lead to the same model.</p> <p> \u26a0 Warning </p> <p>When evaluating ARIMA-SARIMAX models, it is important to note that AIC assumes that all models are trained on the same data. Thus, using AIC to decide between different orders of differencing is technically invalid, since one data point is lost with each order of differencing. Therefore, the Auto Arima algorithm uses a unit root test to select the order of differencing, and only uses the AIC to select the order of the AR and MA components.</p> <p> \u270e Note </p> <p>For a detailed explanation of Akaike's Information Criterion (AIC) see Rob J Hyndman's blog and AIC Myths and Misunderstandings by Anderson and Burnham.</p> In\u00a0[23]: Copied! <pre># Train-validation-test data\n# ======================================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\")\nprint(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")\n\n# Plot\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation')\ndata.loc[end_val:, 'y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Train-validation-test data # ====================================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\") print(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")  # Plot # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation') data.loc[end_val:, 'y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1992-04-01 00:00:00 --- 2001-01-01 00:00:00  (n=106)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=29)\n</pre> In\u00a0[24]: Copied! <pre># Grid search hyperparameter\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), maxiter=200),\n             )\n\nparam_grid = {\n    'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],\n    'trend': [None, 'n', 'c']\n}\n\nresults_grid = grid_search_sarimax(\n                   forecaster            = forecaster,\n                   y                     = data.loc[:end_val, 'y'],\n                   param_grid            = param_grid,\n                   steps                 = 12,\n                   refit                 = False,\n                   metric                = 'mean_absolute_error',\n                   initial_train_size    = len(data_train),\n                   fixed_train_size      = False,\n                   return_best           = True,\n                   n_jobs                = 'auto',\n                   suppress_warnings_fit = True,\n                   verbose               = False,\n                   show_progress         = True\n               )\n\nresults_grid.head(5)\n</pre> # Grid search hyperparameter # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), maxiter=200),              )  param_grid = {     'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],     'trend': [None, 'n', 'c'] }  results_grid = grid_search_sarimax(                    forecaster            = forecaster,                    y                     = data.loc[:end_val, 'y'],                    param_grid            = param_grid,                    steps                 = 12,                    refit                 = False,                    metric                = 'mean_absolute_error',                    initial_train_size    = len(data_train),                    fixed_train_size      = False,                    return_best           = True,                    n_jobs                = 'auto',                    suppress_warnings_fit = True,                    verbose               = False,                    show_progress         = True                )  results_grid.head(5) <pre>Number of models compared: 9.\n</pre> <pre>params grid:   0%|          | 0/9 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found parameters, and the whole data set: \n  Parameters: {'order': (12, 1, 1), 'trend': None}\n  Backtesting metric: 0.06859275472246462\n\n</pre> Out[24]: params mean_absolute_error order trend 6 {'order': (12, 1, 1), 'trend': None} 0.068593 (12, 1, 1) None 7 {'order': (12, 1, 1), 'trend': 'n'} 0.068593 (12, 1, 1) n 2 {'order': (12, 0, 0), 'trend': 'c'} 0.077191 (12, 0, 0) c 0 {'order': (12, 0, 0), 'trend': None} 0.077694 (12, 0, 0) None 1 {'order': (12, 0, 0), 'trend': 'n'} 0.077694 (12, 0, 0) n <p>Since <code>return_best = True</code>, the Forecaster object is updated with the most optimal configuration found and trained with the entire dataset. This means that the grid search will yield the lowest error model with the best hyperparameters that lead to the highest performance metric. This last model can subsequently be utilized for forecasts on new data.</p> In\u00a0[25]: Copied! <pre>forecaster\n</pre> forecaster Out[25]: <pre>================= \nForecasterSarimax \n================= \nRegressor: Sarimax(12,1,1)(0,0,0)[0] \nRegressor parameters: {'concentrate_scale': False, 'dates': None, 'disp': False, 'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None, 'hamilton_representation': False, 'maxiter': 200, 'measurement_error': False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True, 'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing': False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {}, 'start_params': None, 'time_varying_regression': False, 'trend': None, 'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2024-07-29 17:39:40 \nLast fit date: 2024-07-29 17:40:10 \nIndex seen by the forecaster: DatetimeIndex(['1992-04-01', '1992-05-01', '1992-06-01', '1992-07-01',\n               '1992-08-01', '1992-09-01', '1992-10-01', '1992-11-01',\n               '1992-12-01', '1993-01-01',\n               ...\n               '2005-04-01', '2005-05-01', '2005-06-01', '2005-07-01',\n               '2005-08-01', '2005-09-01', '2005-10-01', '2005-11-01',\n               '2005-12-01', '2006-01-01'],\n              dtype='datetime64[ns]', name='datetime', length=166, freq='MS') \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[26]: Copied! <pre># Auto arima: selection base on AIC\n# ==============================================================================\nmodel = auto_arima(\n            y                 = data.loc[:end_val, 'y'],\n            start_p           = 12,\n            start_q           = 0,\n            max_p             = 12,\n            max_q             = 1,\n            seasonal          = True,\n            test              = 'adf',\n            m                 = 12, # Seasonal period\n            d                 = None, # The algorithm will determine 'd'\n            D                 = None, # The algorithm will determine 'D'\n            trace             = True,\n            error_action      = 'ignore',\n            suppress_warnings = True,\n            stepwise          = True\n        )\n</pre> # Auto arima: selection base on AIC # ============================================================================== model = auto_arima(             y                 = data.loc[:end_val, 'y'],             start_p           = 12,             start_q           = 0,             max_p             = 12,             max_q             = 1,             seasonal          = True,             test              = 'adf',             m                 = 12, # Seasonal period             d                 = None, # The algorithm will determine 'd'             D                 = None, # The algorithm will determine 'D'             trace             = True,             error_action      = 'ignore',             suppress_warnings = True,             stepwise          = True         ) <pre>Performing stepwise search to minimize aic\n ARIMA(11,1,0)(1,1,1)[12]             : AIC=-465.630, Time=3.32 sec\n ARIMA(0,1,0)(0,1,0)[12]             : AIC=-383.254, Time=0.04 sec\n ARIMA(1,1,0)(1,1,0)[12]             : AIC=-439.401, Time=0.21 sec\n ARIMA(0,1,1)(0,1,1)[12]             : AIC=-463.505, Time=0.32 sec\n ARIMA(11,1,0)(0,1,1)[12]             : AIC=-467.479, Time=1.53 sec\n ARIMA(11,1,0)(0,1,0)[12]             : AIC=-439.864, Time=0.57 sec\n ARIMA(11,1,0)(0,1,2)[12]             : AIC=-465.752, Time=3.32 sec\n ARIMA(11,1,0)(1,1,0)[12]             : AIC=-450.415, Time=1.56 sec\n ARIMA(11,1,0)(1,1,2)[12]             : AIC=-464.761, Time=3.65 sec\n ARIMA(10,1,0)(0,1,1)[12]             : AIC=-466.868, Time=1.38 sec\n ARIMA(11,1,1)(0,1,1)[12]             : AIC=-464.317, Time=1.55 sec\n ARIMA(10,1,1)(0,1,1)[12]             : AIC=-464.448, Time=1.53 sec\n ARIMA(11,1,0)(0,1,1)[12] intercept   : AIC=-465.810, Time=1.47 sec\n\nBest model:  ARIMA(11,1,0)(0,1,1)[12]          \nTotal fit time: 20.469 seconds\n</pre> In\u00a0[27]: Copied! <pre># Create and fit ForecasterSarimax (skforecast)\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), maxiter=200),\n             )\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\n\n# In-sample Predictions\n# ==============================================================================\nforecaster.regressor.sarimax_res.fittedvalues\n</pre> # Create and fit ForecasterSarimax (skforecast) # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), maxiter=200),              ) forecaster.fit(y=data_train['y'], suppress_warnings=True)  # In-sample Predictions # ============================================================================== forecaster.regressor.sarimax_res.fittedvalues Out[27]: <pre>datetime\n1992-04-01    0.000000\n1992-05-01    0.379808\n1992-06-01    0.361444\n1992-07-01    0.413136\n1992-08-01    0.480509\n                ...   \n2005-02-01    0.739239\n2005-03-01    0.750984\n2005-04-01    0.716174\n2005-05-01    0.749955\n2005-06-01    0.817745\nFreq: MS, Length: 159, dtype: float64</pre> <p>When using the <code>pmdarima.ARIMA</code> model, the statsmodels <code>SARIMAXResults</code> object is stored inside the <code>arima_res_</code> attribute:</p> In\u00a0[28]: Copied! <pre># Create and fit ForecasterSarimax (pmdarima)\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=ARIMA(order=(12, 1, 1), maxiter=200),\n             )\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\n\n# In-sample Predictions\n# ==============================================================================\nforecaster.regressor.arima_res_.fittedvalues\n</pre> # Create and fit ForecasterSarimax (pmdarima) # ============================================================================== forecaster = ForecasterSarimax(                  regressor=ARIMA(order=(12, 1, 1), maxiter=200),              ) forecaster.fit(y=data_train['y'], suppress_warnings=True)  # In-sample Predictions # ============================================================================== forecaster.regressor.arima_res_.fittedvalues Out[28]: <pre>datetime\n1992-04-01    0.002790\n1992-05-01    0.382598\n1992-06-01    0.363529\n1992-07-01    0.417322\n1992-08-01    0.486638\n                ...   \n2005-02-01    0.740412\n2005-03-01    0.755329\n2005-04-01    0.724481\n2005-05-01    0.762504\n2005-06-01    0.832624\nFreq: MS, Length: 159, dtype: float64</pre>"},{"location":"user_guides/forecasting-sarimax-arima.html#arima-and-sarimax","title":"ARIMA and SARIMAX\u00b6","text":"<p>ARIMA (AutoRegressive Integrated Moving Average) and SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) are prominent and widely used statistical forecasting models. While ARIMA models are more widely known, SARIMAX models extend the ARIMA framework by seamlessly integrating seasonal patterns and exogenous variables.</p> <p>In the ARIMA-SARIMAX model notation, the parameters $p$, $d$, and $q$ represent the autoregressive, differencing, and moving-average components, respectively. $P$, $D$, and $Q$ denote the same components for the seasonal part of the model, with $m$ representing the number of periods in each season.</p> <ul> <li><p>$p$ is the order (number of time lags) of the autoregressive part of the model.</p> </li> <li><p>$d$ is the degree of differencing (the number of times that past values have been subtracted from the data).</p> </li> <li><p>$q$ is the order of the moving average part of the model.</p> </li> <li><p>$P$ is the order (number of time lags) of the seasonal part of the model.</p> </li> <li><p>$D$ is the degree of differencing (the number of times the data have had past values subtracted) of the seasonal part of the model.</p> </li> <li><p>$Q$ is the order of the moving average of the seasonal part of the model.</p> </li> <li><p>$m$ refers to the number of periods in each season.</p> </li> </ul> <p>When the terms $P$, $D$, $Q$, and $m$ are zero and no exogenous variables are included in the model, the SARIMAX model is equivalent to an ARIMA.</p> <p>When two out of the three terms are zero, the model can be referred to based on the non-zero parameter, dropping \"AR\", \"I\" or \"MA\" from the acronym describing the model. For example, $ARIMA(1,0,0)$ is $AR(1)$, $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$.</p> <p>ARIMA implementations</p> <p>Several Python libraries implement ARIMA-SARIMAX models. Three of them are:</p> <ul> <li><p>statsmodels: this is one of the most complete libraries for statistical modeling. While the functional paradigm may be intuitive for those coming from the R environment, those accustomed to the object-oriented API of scikit-learn may need a short period of adaptation.</p> </li> <li><p>pmdarima: This is a wrapper for <code>statsmodels SARIMAX</code>. Its distinguishing feature is its seamless integration with the scikit-learn API, allowing users familiar with scikit-learn's conventions to seamlessly dive into time series modeling.</p> </li> <li><p>skforecast: a novel wrapper for <code>statsmodels SARIMAX</code> that also follows the scikit-learn API. This implementation is very similar to that of pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p> </li> </ul> <p>ForecasterSarimax</p> <p>The <code>ForecasterSarimax</code> class allows training and validation of ARIMA and SARIMAX models using the skforecast API. <code>ForecasterSarimax</code> is compatible with the <code>Sarimax</code> from skforecast implementation, a novel wrapper for statsmodels SARIMAX that also follows the sklearn API. This implementation is very similar to pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#statsmodels-pmdarima-and-skforecast","title":"Statsmodels, pmdarima and skforecast\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#forecastersarimax","title":"ForecasterSarimax\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#training","title":"Training\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#interval-prediction","title":"Interval prediction\u00b6","text":"<p>Either <code>alpha</code> or <code>interval</code> can be used to indicate the confidence of the estimated prediction interval.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#exogenous-variables","title":"Exogenous variables\u00b6","text":"<p>The addition of exogenous variables is done using the <code>exog</code> argument. The only requirement for including an exogenous variable is the need to know the value of the variable also during the forecast period.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#using-an-already-trained-arima","title":"Using an already trained ARIMA\u00b6","text":"<p>Forecasting with an ARIMA model becomes challenging when the forecast horizon data does not immediately follow the last observed value during the training phase. This complexity is due to the moving average (MA) component, which relies on past forecast errors as predictors. Thus, to predict at time 't', the error of the 't-1' prediction becomes a necessity. In situations where this prediction isn't available, the corresponding error remains unavailable.</p> <p>For this reason, in most cases, ARIMA models are retrained each time predictions need to be made. Despite considerable efforts and advances to speed up the training process for these models, it is not always feasible to retrain the model between predictions, either due to time constraints or insufficient computational resources for repeated access to historical data. An intermediate approach is to feed the model with data from the last training observation to the start of the prediction phase. This technique enables the estimation of intermediate predictions and, as a result, the necessary errors.</p> <p>For example, imagine a situation where a model was trained 20 days ago with daily data from the past three years. When generating new predictions, only the 20 most recent values would be needed, rather than the complete historical dataset (365 * 3 + 20).</p> <p>Integrating new data into the model can be complex, but the <code>ForecasterSarimax</code> class simplifies this considerably by automating the process through the <code>last_window</code> argument in its <code>predict</code> method.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Returns the parameters of the model.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#backtesting","title":"Backtesting\u00b6","text":"<p>SARIMAX models can be evaluated using any of the backtesting strategies implemented in skforecast.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#model-tunning","title":"Model tunning\u00b6","text":"<p>To find the optimal hyperparameters for the SARIMAX model, the use of strategic search methods is essential. Among these methods, two widely used approaches are:</p> <ul> <li><p>Statistical Criteria: Information criterion metrics, such as Akaike's Information Criterion (AIC) or Bayesian Information Criterion (BIC), use different penalties on the maximum likelihood (log-likelihood) estimate of the model as a measure of fit. The advantage of using such criteria is that they are computed only on the training data, eliminating the need for predictions on new data. As a result, the optimization process is greatly accelerated. The well-known Auto Arima algorithm uses this approach.</p> </li> <li><p>Validation Techniques: The use of validation techniques, especially backtesting, is another effective strategy. Backtesting involves evaluating the performance of the model using historical data to simulate real-world conditions. This helps to validate the effectiveness of the hyperparameters under different scenarios, providing a practical assessment of their viability.</p> </li> </ul> <p>In the first approach, calculations are based solely on training data, eliminating the need for predictions on new data. This makes the optimization process very fast. However, it is important to note that information criteria metrics only measure the relative quality of models. This means that all tested models could still be poor fits. Therefore, the final selected model must undergo a backtesting phase. This phase calculates a metric (such as MAE, MSE, MAPE, etc.) that validates its performance on a meaningful scale.</p> <p>On the other hand, the second approach - validation techniques - tends to be more time-consuming, since the model must be trained and then evaluated on new data. However, the results generated are often more robust, and the metrics derived can provide deeper insights.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#grid-search-with-backtesting","title":"Grid search with backtesting\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#auto-arima","title":"Auto arima\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction-on-training-data-in-sample-predictions","title":"Prediction on training data (In-sample Predictions)\u00b6","text":"<p>Predictions on the training data are crucial for evaluating the accuracy and effectiveness of the model. By comparing the predicted values wtih the actual observed values in the training dataset, you can assess how well the model has learned the underlying patterns and trends in the data. This comparison helps in understanding the model's performance and identify areas where it may need improvement or adjustment. In essence, they act as a mirror, reflecting how the model interprets and reconstructs the historical data on which it was trained.</p> <p>The predictions of the fitted values are stored in the <code>fittedvalues</code> attribute of the <code>SARIMAXResults</code> object. This object is stored within the <code>sarimax_res</code> attribute of the <code>skforecast.Sarimax</code> model:</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html","title":"Deep learning Recurrent Neural Networks","text":"<p>Recurrent Neural Networks (RNN) are a type of neural networks designed to process data that follows a sequential order. In conventional neural networks, such as feedforward networks, information flows in one direction, from input to output through hidden layers, without considering the sequential structure of the data. In contrast, RNNs maintain internal states or memories, which allow them to remember past information and use it to predict future data in the sequence.</p> <p>The basic unit of an RNN is the recurrent cell. This cell takes two inputs: the current input and the previous hidden state. The hidden state can be understood as a \"memory\" that retains information from previous iterations. The current input and the previous hidden state are combined to calculate the current output and the new hidden state. This output is used as input for the next iteration, along with the next input in the data sequence.</p> <p>Despite the advances that have been achieved with RNN architectures, they have limitations to capture long-term patterns. This is why variants such as  Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been developed, which address these problems and allow long-term information to be retained more effectively.</p> <p></p> RNN diagram. Fuente: James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (1st ed.) [PDF]. Springer. <p>Long Short-Term Memory (LSTM) neural networks are a specialized type of RNNs designed to overcome the limitations associated with capturing long-term temporal dependencies. Unlike traditional RNNs, LSTMs incorporate a more complex architecture, introducing memory units and gate mechanisms to improve information management over time.</p> <p>Structure of LSTMs</p> <p>LSTMs have a modular structure consisting of three fundamental gates: the forget gate, the input gate, and the output gate. These gates work together to regulate the flow of information through the memory unit, allowing for more precise control over what information to retain and what to forget.</p> <ul> <li><p>Forget Gate: Regulates how much information should be forgotten and how much should be retained, combining the current input and the previous output through a sigmoid function.</p> </li> <li><p>Input Gate: Decides how much new information should be added to long-term memory.</p> </li> <li><p>Output Gate: Determines how much information from the current memory will be used for the final output, combining the current input and memory information through a sigmoid function.</p> </li> </ul> <p></p> Diagram of the inputs and outputs of an LSTM. Fuente: codificandobits https://databasecamp.de/wp-content/uploads/lstm-architecture-1024x709.png. <p> \ud83d\udca1 Tip </p> <p>To learn more about forecasting with deep learning  models visit our examples:</p> <ul> <li> Deep Learning for time series forecasting: Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM).     </li> </ul> <p> \ud83d\udca1 Tip: Configuring your backend </p> <p>As of Skforecast version <code>0.13.0</code>, PyTorch backend support is available. You can configure the backend by exporting the <code>KERAS_BACKEND</code> environment variable or by editing your local configuration file at <code>~/.keras/keras.json</code>. The available backend options are: \"tensorflow\" and \"torch\". Example:</p> <pre>import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\nimport keras\n</pre> <p> \u26a0 Warning </p> <p>Note: The backend must be configured before importing Keras, and the backend cannot be changed after the package has been imported.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"torch\" # 'tensorflow', 'jax\u00b4 or 'torch'\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport skforecast\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.ForecasterRnn import ForecasterRnn\nfrom skforecast.ForecasterRnn.utils import create_and_compile_model\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n\nfrom keras.optimizers import Adam\nfrom keras.losses import MeanSquaredError\nfrom keras.callbacks import EarlyStopping\n\nimport warnings\nwarnings.filterwarnings('once')\n\nprint(f\"skforecast version: {skforecast.__version__}\")\nprint(f\"keras version: {keras.__version__}\")\n\nif keras.__version__ &gt; \"3.0\":\n    print(f\"Using backend: {keras.backend.backend()}\")\n    if keras.backend.backend() == \"tensorflow\":\n        import tensorflow\n        print(f\"tensorflow version: {tensorflow.__version__}\")\n    elif keras.backend.backend() == \"torch\":\n        import torch\n        print(f\"torch version: {torch.__version__}\")\n    else:\n        print(\"Backend not recognized. Please use 'tensorflow' or 'torch'.\")\n</pre> # Libraries # ============================================================================== import os os.environ[\"KERAS_BACKEND\"] = \"torch\" # 'tensorflow', 'jax\u00b4 or 'torch' import keras  import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler  import skforecast from skforecast.datasets import fetch_dataset from skforecast.plot import set_dark_theme from skforecast.ForecasterRnn import ForecasterRnn from skforecast.ForecasterRnn.utils import create_and_compile_model from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries  from keras.optimizers import Adam from keras.losses import MeanSquaredError from keras.callbacks import EarlyStopping  import warnings warnings.filterwarnings('once')  print(f\"skforecast version: {skforecast.__version__}\") print(f\"keras version: {keras.__version__}\")  if keras.__version__ &gt; \"3.0\":     print(f\"Using backend: {keras.backend.backend()}\")     if keras.backend.backend() == \"tensorflow\":         import tensorflow         print(f\"tensorflow version: {tensorflow.__version__}\")     elif keras.backend.backend() == \"torch\":         import torch         print(f\"torch version: {torch.__version__}\")     else:         print(\"Backend not recognized. Please use 'tensorflow' or 'torch'.\") <pre>skforecast version: 0.13.0\nkeras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nair_quality = fetch_dataset(name=\"air_quality_valencia\")\n\n# Data preparation\n# ==============================================================================\nair_quality = air_quality.interpolate(method=\"linear\")\nair_quality = air_quality.sort_index()\nair_quality.head()\n</pre> # Data download # ============================================================================== air_quality = fetch_dataset(name=\"air_quality_valencia\")  # Data preparation # ============================================================================== air_quality = air_quality.interpolate(method=\"linear\") air_quality = air_quality.sort_index() air_quality.head() <pre>air_quality_valencia\n--------------------\nHourly measures of several air chemical pollutant (pm2.5, co, no, no2, pm10,\nnox, o3, so2) at Valencia city.\n Red de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250054-Val\u00e8ncia\n- Centre, https://mediambient.gva.es/es/web/calidad-ambiental/datos-historicos.\nShape of the dataset: (26304, 10)\n</pre> Out[2]: pm2.5 co no no2 pm10 nox o3 veloc. direc. so2 datetime 2019-01-01 00:00:00 19.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 8.0 2019-01-01 01:00:00 26.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 8.0 2019-01-01 02:00:00 31.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 8.0 2019-01-01 03:00:00 30.0 0.1 15.0 41.0 35.0 63.0 3.0 0.2 220.0 10.0 2019-01-01 04:00:00 30.0 0.1 16.0 39.0 36.0 63.0 3.0 0.4 221.0 11.0 In\u00a0[3]: Copied! <pre># Checking the frequency of the time series\n# ==============================================================================\nprint(f\"Index: {air_quality.index.dtype}\")\nprint(f\"Frequency: {air_quality.index.freq}\")\n</pre> # Checking the frequency of the time series # ============================================================================== print(f\"Index: {air_quality.index.dtype}\") print(f\"Frequency: {air_quality.index.freq}\") <pre>Index: datetime64[ns]\nFrequency: &lt;Hour&gt;\n</pre> In\u00a0[4]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = \"2021-03-31 23:59:00\"\nend_validation = \"2021-09-30 23:59:00\"\nair_quality_train = air_quality.loc[:end_train, :].copy()\nair_quality_val = air_quality.loc[end_train:end_validation, :].copy()\nair_quality_test = air_quality.loc[end_validation:, :].copy()\n\nprint(\n    f\"Dates train      : {air_quality_train.index.min()} --- \" \n    f\"{air_quality_train.index.max()}  (n={len(air_quality_train)})\"\n)\nprint(\n    f\"Dates validation : {air_quality_val.index.min()} --- \" \n    f\"{air_quality_val.index.max()}  (n={len(air_quality_val)})\"\n)\nprint(\n    f\"Dates test       : {air_quality_test.index.min()} --- \" \n    f\"{air_quality_test.index.max()}  (n={len(air_quality_test)})\"\n)\n</pre> # Split train-validation-test # ============================================================================== end_train = \"2021-03-31 23:59:00\" end_validation = \"2021-09-30 23:59:00\" air_quality_train = air_quality.loc[:end_train, :].copy() air_quality_val = air_quality.loc[end_train:end_validation, :].copy() air_quality_test = air_quality.loc[end_validation:, :].copy()  print(     f\"Dates train      : {air_quality_train.index.min()} --- \"      f\"{air_quality_train.index.max()}  (n={len(air_quality_train)})\" ) print(     f\"Dates validation : {air_quality_val.index.min()} --- \"      f\"{air_quality_val.index.max()}  (n={len(air_quality_val)})\" ) print(     f\"Dates test       : {air_quality_test.index.min()} --- \"      f\"{air_quality_test.index.max()}  (n={len(air_quality_test)})\" ) <pre>Dates train      : 2019-01-01 00:00:00 --- 2021-03-31 23:00:00  (n=19704)\nDates validation : 2021-04-01 00:00:00 --- 2021-09-30 23:00:00  (n=4392)\nDates test       : 2021-10-01 00:00:00 --- 2021-12-31 23:00:00  (n=2208)\n</pre> In\u00a0[5]: Copied! <pre># Plotting one feature\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(8, 3))\nair_quality_train[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"train\")\nair_quality_val[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"validation\")\nair_quality_test[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"test\")\nax.set_title(\"pm2.5\")\nax.legend();\n</pre> # Plotting one feature # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(8, 3)) air_quality_train[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"train\") air_quality_val[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"validation\") air_quality_test[\"pm2.5\"].rolling(100).mean().plot(ax=ax, label=\"test\") ax.set_title(\"pm2.5\") ax.legend(); <p>This type of problem involves modeling a time series using only its own past. It is a typical autoregressive problem.</p> <p>Although tensorflow-keras facilitates the process of creating deep learning architectures, it is not always trivial to determine the Xtrain and Ytrain dimensions requiered to run an LSTM model. The dimensions depend on how many time series are being modeled, how many of them are are used as predictors, and the length of the prediction horizon.</p> <p> \u270e Note </p> <p>The <code>create_and_compile_model</code> function is designed to facilitate the creation of the Tensorflow model. Advanced users can create their own architectures and pass them to the skforecast RNN Forecaster. Input and output dimensions must match the use case to which the model will be applied. the Annex at the end of the document for more details.</p> In\u00a0[6]: Copied! <pre># Create model\n# ==============================================================================\nseries = [\"o3\"] #\u00a0Series used as predictors\nlevels = [\"o3\"] # Target serie to predict\nlags = 32 # Past time steps to be used to predict the target\nsteps = 1 #\u00a0Future time steps to be predicted\n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=4,\n    dense_units=16,\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Create model # ============================================================================== series = [\"o3\"] #\u00a0Series used as predictors levels = [\"o3\"] # Target serie to predict lags = 32 # Past time steps to be used to predict the target steps = 1 #\u00a0Future time steps to be predicted  data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=4,     dense_units=16,     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer (InputLayer)        \u2502 (None, 32, 1)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm (LSTM)                     \u2502 (None, 4)              \u2502            96 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                   \u2502 (None, 16)             \u2502            80 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 1)              \u2502            17 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 1, 1)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 193 (772.00 B)\n</pre> <pre> Trainable params: 193 (772.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[7]: Copied! <pre># Forecaster Definition\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 3,  # Number of epochs to train the model.\n        \"batch_size\": 32,  # Batch size to train the model.\n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=5)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)    \n\nforecaster\n</pre> # Forecaster Definition # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 3,  # Number of epochs to train the model.         \"batch_size\": 32,  # Batch size to train the model.         \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=5)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )      forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterRnn\\ForecasterRnn.py:228: UserWarning: Setting `lags` = 'auto'. `lags` are inferred from the regressor architecture. Avoid the warning with lags=lags.\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterRnn\\ForecasterRnn.py:264: UserWarning: `steps` default value = 'auto'. `steps` inferred from regressor architecture. Avoid the warning with steps=steps.\n  warnings.warn(\n</pre> Out[7]: <pre>============= \nForecasterRnn \n============= \nRegressor: &lt;Functional name=functional, built=True&gt; \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32] \nTransformer for series: MinMaxScaler() \nWindow size: 32 \nTarget series, levels: ['o3'] \nMultivariate series (names): None \nMaximum steps predicted: [1] \nTraining range: None \nTraining index type: None \nTraining index frequency: None \nModel parameters: {'name': 'functional', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32, 1), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None, 'name': 'input_layer', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 4, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 1)}, 'name': 'lstm', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 1), 'dtype': 'float32', 'keras_history': ['input_layer', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 16, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 4)}, 'name': 'dense', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 4), 'dtype': 'float32', 'keras_history': ['lstm', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 16)}, 'name': 'dense_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 16), 'dtype': 'float32', 'keras_history': ['dense', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (1, 1)}, 'registered_name': None, 'build_config': {'input_shape': (None, 1)}, 'name': 'reshape', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 1), 'dtype': 'float32', 'keras_history': ['dense_1', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer', 0, 0]], 'output_layers': [['reshape', 0, 0]]} \nCompile parameters: {'optimizer': {'module': 'keras.src.backend.torch.optimizers.torch_adam', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': 'Adam'}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False} \nfit_kwargs: {'epochs': 3, 'batch_size': 32, 'callbacks': [&lt;keras.src.callbacks.early_stopping.EarlyStopping object at 0x000002B51782FA50&gt;]} \nCreation date: 2024-07-30 13:05:10 \nLast fit date: None \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> <p> \u26a0 Warning </p> <p>The output warning indicates that the number of lags has been inferred from the model architecture. In this case, the model has an LSTM layer with 32 neurons, so the number of lags is 32. If a different number of lags is desired, the <code>lags</code> argument can be specified in the <code>create_and_compile_model</code> function. To omit the warning, set <code>lags=lags</code> and <code>steps=steps</code> arguments can be specified in the initialization of the <code>ForecasterRnn</code>.</p> In\u00a0[8]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>Epoch 1/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18s 30ms/step - loss: 0.0551 - val_loss: 0.0382\nEpoch 2/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 19s 31ms/step - loss: 0.0412 - val_loss: 0.0364\nEpoch 3/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 26s 42ms/step - loss: 0.0408 - val_loss: 0.0388\n</pre> <p>Overfitting can be tracked by moniting the loss function on the validation set. Metrics are automatically stored in the <code>history</code> attribute of the <code>ForecasterRnn</code> object. The method <code>plot_history</code> can be used to visualize the training and validation loss.</p> In\u00a0[9]: Copied! <pre># Track training and overfitting\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 2.5))\nforecaster.plot_history(ax=ax)\n</pre> # Track training and overfitting # ============================================================================== fig, ax = plt.subplots(figsize=(6, 2.5)) forecaster.plot_history(ax=ax) In\u00a0[10]: Copied! <pre># Predictions\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Predictions # ============================================================================== predictions = forecaster.predict() predictions Out[10]: o3 2021-04-01 52.653137 In\u00a0[11]: Copied! <pre># Backtesting with test data\n# ==============================================================================\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster=forecaster,\n    steps=forecaster.max_step,\n    series=data,\n    levels=forecaster.levels,\n    initial_train_size=len(data.loc[:end_validation, :]), # Training + Validation Data\n    metric=\"mean_absolute_error\",\n    verbose=False, # Set to True to print detailed information\n    refit=False,\n)\n</pre> # Backtesting with test data # ============================================================================== metrics, predictions = backtesting_forecaster_multiseries(     forecaster=forecaster,     steps=forecaster.max_step,     series=data,     levels=forecaster.levels,     initial_train_size=len(data.loc[:end_validation, :]), # Training + Validation Data     metric=\"mean_absolute_error\",     verbose=False, # Set to True to print detailed information     refit=False, ) <pre>Epoch 1/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 30s 40ms/step - loss: 0.0398 - val_loss: 0.0390\nEpoch 2/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 33ms/step - loss: 0.0402 - val_loss: 0.0341\nEpoch 3/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21s 27ms/step - loss: 0.0399 - val_loss: 0.0359\n</pre> <pre>  0%|          | 0/2208 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions\n</pre> # Backtesting predictions # ============================================================================== predictions Out[12]: o3 2021-10-01 00:00:00 54.899395 2021-10-01 01:00:00 54.899395 2021-10-01 02:00:00 54.899395 2021-10-01 03:00:00 54.899395 2021-10-01 04:00:00 54.899395 ... ... 2021-12-31 19:00:00 54.899395 2021-12-31 20:00:00 54.899395 2021-12-31 21:00:00 54.899395 2021-12-31 22:00:00 54.899395 2021-12-31 23:00:00 54.899395 <p>2208 rows \u00d7 1 columns</p> In\u00a0[13]: Copied! <pre># % Error vs series mean\n# ==============================================================================\nrel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"])\nprint(f\"Serie mean: {np.mean(data['o3']):0.2f}\")\nprint(f\"Relative error (mae): {rel_mse:0.2f} %\")\n</pre> # % Error vs series mean # ============================================================================== rel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"]) print(f\"Serie mean: {np.mean(data['o3']):0.2f}\") print(f\"Relative error (mae): {rel_mse:0.2f} %\") <pre>Serie mean: 54.52\nRelative error (mae): 33.31 %\n</pre> In\u00a0[14]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions[\"o3\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions[\"o3\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend(); <p>The next case is similar to the previous one, but now the goal is to predict multiple future values. In this scenario multiple future steps of a single time series are modeled using only its past values.</p> <p>A similar architecture to the previous one will be used, but with a greater number of neurons in the LSTM layer and in the first dense layer. This will allow the model to have greater flexibility to predict the time series.</p> In\u00a0[15]: Copied! <pre># Model creation\n# ==============================================================================\nseries = [\"o3\"] #\u00a0Series used as predictors\nlevels = [\"o3\"] # Target serie to predict\nlags = 32 # Past time steps to be used to predict the target\nsteps = 5 #\u00a0Future time steps to be predicted\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=50,\n    dense_units=32,\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Model creation # ============================================================================== series = [\"o3\"] #\u00a0Series used as predictors levels = [\"o3\"] # Target serie to predict lags = 32 # Past time steps to be used to predict the target steps = 5 #\u00a0Future time steps to be predicted  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=50,     dense_units=32,     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_1 (InputLayer)      \u2502 (None, 32, 1)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)                   \u2502 (None, 50)             \u2502        10,400 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                 \u2502 (None, 32)             \u2502         1,632 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_3 (Dense)                 \u2502 (None, 5)              \u2502           165 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape_1 (Reshape)             \u2502 (None, 5, 1)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 12,197 (47.64 KB)\n</pre> <pre> Trainable params: 12,197 (47.64 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[16]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 3,  # Number of epochs to train the model.\n        \"batch_size\": 32,  # Batch size to train the model.\n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=5)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)    \n\nforecaster\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 3,  # Number of epochs to train the model.         \"batch_size\": 32,  # Batch size to train the model.         \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=5)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )      forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterRnn\\ForecasterRnn.py:228: UserWarning: Setting `lags` = 'auto'. `lags` are inferred from the regressor architecture. Avoid the warning with lags=lags.\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterRnn\\ForecasterRnn.py:264: UserWarning: `steps` default value = 'auto'. `steps` inferred from regressor architecture. Avoid the warning with steps=steps.\n  warnings.warn(\n</pre> Out[16]: <pre>============= \nForecasterRnn \n============= \nRegressor: &lt;Functional name=functional_1, built=True&gt; \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32] \nTransformer for series: MinMaxScaler() \nWindow size: 32 \nTarget series, levels: ['o3'] \nMultivariate series (names): None \nMaximum steps predicted: [1 2 3 4 5] \nTraining range: None \nTraining index type: None \nTraining index frequency: None \nModel parameters: {'name': 'functional_1', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32, 1), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_1'}, 'registered_name': None, 'name': 'input_layer_1', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 50, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 1)}, 'name': 'lstm_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 1), 'dtype': 'float32', 'keras_history': ['input_layer_1', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 50)}, 'name': 'dense_2', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 50), 'dtype': 'float32', 'keras_history': ['lstm_1', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_3', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['dense_2', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (5, 1)}, 'registered_name': None, 'build_config': {'input_shape': (None, 5)}, 'name': 'reshape_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 5), 'dtype': 'float32', 'keras_history': ['dense_3', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_1', 0, 0]], 'output_layers': [['reshape_1', 0, 0]]} \nCompile parameters: {'optimizer': {'module': 'keras.src.backend.torch.optimizers.torch_adam', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': 'Adam'}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False} \nfit_kwargs: {'epochs': 3, 'batch_size': 32, 'callbacks': [&lt;keras.src.callbacks.early_stopping.EarlyStopping object at 0x000002B51E58C990&gt;]} \nCreation date: 2024-07-30 13:08:24 \nLast fit date: None \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> <p> \u270e Note </p> <p>The <code>fit_kwargs</code> parameter allows the user to set any Tensorflow-based configuration in the model. In the example, 10 training epochs are defined with a batch size of 32. An <code>EarlyStopping</code> callback is configured to stop training when the validation loss stops decreasing for 5 epochs (<code>patience=5</code>). Other callbacks can also be configured, such as <code>ModelCheckpoint</code> to save the model at each epoch, or even Tensorboard to visualize the training and validation loss in real time.</p> In\u00a0[17]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>Epoch 1/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 32s 52ms/step - loss: 0.0292 - val_loss: 0.0176\nEpoch 2/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 36s 59ms/step - loss: 0.0140 - val_loss: 0.0122\nEpoch 3/3\n615/615 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 37s 60ms/step - loss: 0.0123 - val_loss: 0.0128\n</pre> In\u00a0[18]: Copied! <pre># Train and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 2.5))\nforecaster.plot_history(ax=ax)\n</pre> # Train and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(6, 2.5)) forecaster.plot_history(ax=ax) In\u00a0[19]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Prediction # ============================================================================== predictions = forecaster.predict() predictions Out[19]: o3 2021-04-01 00:00:00 43.579914 2021-04-01 01:00:00 43.036030 2021-04-01 02:00:00 38.125568 2021-04-01 03:00:00 35.505390 2021-04-01 04:00:00 37.147205 In\u00a0[20]: Copied! <pre># Specific step predictions\n# ==============================================================================\npredictions = forecaster.predict(steps=[1, 3])\npredictions\n</pre> # Specific step predictions # ============================================================================== predictions = forecaster.predict(steps=[1, 3]) predictions Out[20]: o3 2021-04-01 00:00:00 43.579914 2021-04-01 02:00:00 38.125568 In\u00a0[21]: Copied! <pre># Backtesting \n# ==============================================================================\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster=forecaster,\n    steps=forecaster.max_step,\n    series=data,\n    levels=forecaster.levels,\n    initial_train_size=len(data.loc[:end_validation, :]),\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    refit=False,\n)\n</pre> # Backtesting  # ============================================================================== metrics, predictions = backtesting_forecaster_multiseries(     forecaster=forecaster,     steps=forecaster.max_step,     series=data,     levels=forecaster.levels,     initial_train_size=len(data.loc[:end_validation, :]),     metric=\"mean_absolute_error\",     verbose=False,     refit=False, ) <pre>Epoch 1/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41s 54ms/step - loss: 0.0121 - val_loss: 0.0115\nEpoch 2/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41s 55ms/step - loss: 0.0119 - val_loss: 0.0120\nEpoch 3/3\n752/752 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 46s 61ms/step - loss: 0.0116 - val_loss: 0.0114\n</pre> <pre>  0%|          | 0/442 [00:00&lt;?, ?it/s]</pre> In\u00a0[22]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions\n</pre> # Backtesting predictions # ============================================================================== predictions Out[22]: o3 2021-10-01 00:00:00 57.922863 2021-10-01 01:00:00 55.142830 2021-10-01 02:00:00 53.163837 2021-10-01 03:00:00 50.351650 2021-10-01 04:00:00 48.197075 ... ... 2021-12-31 19:00:00 21.249779 2021-12-31 20:00:00 23.083706 2021-12-31 21:00:00 18.573345 2021-12-31 22:00:00 16.224133 2021-12-31 23:00:00 16.216816 <p>2208 rows \u00d7 1 columns</p> In\u00a0[23]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions[\"o3\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions[\"o3\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend(); In\u00a0[24]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metrics Out[24]: levels mean_absolute_error 0 o3 9.561174 In\u00a0[25]: Copied! <pre># % Error vs series mean\n# ==============================================================================\nrel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"])\nprint(f\"Serie mean: {np.mean(data['o3']):0.2f}\")\nprint(f\"Relative error (mae): {rel_mse:0.2f} %\")\n</pre> # % Error vs series mean # ============================================================================== rel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"]) print(f\"Serie mean: {np.mean(data['o3']):0.2f}\") print(f\"Relative error (mae): {rel_mse:0.2f} %\") <pre>Serie mean: 54.52\nRelative error (mae): 17.54 %\n</pre> <p>In this case, a single series will be predicted, but using multiple time series as predictors. Now, the past values of multiple time series will influence the prediction of a single time series.</p> In\u00a0[26]: Copied! <pre># Model creation\n# ==============================================================================\n# Time series used in the training. Now, it is multiseries\nseries = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2'] \nlevels = [\"o3\"] \nlags = 32 \nsteps = 5 \n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=[100, 50],\n    dense_units=[64, 32],\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Model creation # ============================================================================== # Time series used in the training. Now, it is multiseries series = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2']  levels = [\"o3\"]  lags = 32  steps = 5   data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=[100, 50],     dense_units=[64, 32],     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional_2\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_2 (InputLayer)      \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)                   \u2502 (None, 32, 100)        \u2502        44,400 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_3 (LSTM)                   \u2502 (None, 50)             \u2502        30,200 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_4 (Dense)                 \u2502 (None, 64)             \u2502         3,264 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_5 (Dense)                 \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_6 (Dense)                 \u2502 (None, 5)              \u2502           165 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape_2 (Reshape)             \u2502 (None, 5, 1)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 80,109 (312.93 KB)\n</pre> <pre> Trainable params: 80,109 (312.93 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[27]: Copied! <pre># Forecaster creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    steps=steps,\n    lags=lags,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 4,  \n        \"batch_size\": 128,  \n        \"series_val\": data_val,\n    },\n)\nforecaster\n</pre> # Forecaster creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     steps=steps,     lags=lags,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 4,           \"batch_size\": 128,           \"series_val\": data_val,     }, ) forecaster Out[27]: <pre>============= \nForecasterRnn \n============= \nRegressor: &lt;Functional name=functional_2, built=True&gt; \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32] \nTransformer for series: MinMaxScaler() \nWindow size: 32 \nTarget series, levels: ['o3'] \nMultivariate series (names): None \nMaximum steps predicted: [1 2 3 4 5] \nTraining range: None \nTraining index type: None \nTraining index frequency: None \nModel parameters: {'name': 'functional_2', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32, 10), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_2'}, 'registered_name': None, 'name': 'input_layer_2', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 100, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 10)}, 'name': 'lstm_2', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 10), 'dtype': 'float32', 'keras_history': ['input_layer_2', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_3', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 50, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 100)}, 'name': 'lstm_3', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 100), 'dtype': 'float32', 'keras_history': ['lstm_2', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 50)}, 'name': 'dense_4', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 50), 'dtype': 'float32', 'keras_history': ['lstm_3', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}, 'name': 'dense_5', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['dense_4', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_6', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['dense_5', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (5, 1)}, 'registered_name': None, 'build_config': {'input_shape': (None, 5)}, 'name': 'reshape_2', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 5), 'dtype': 'float32', 'keras_history': ['dense_6', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_2', 0, 0]], 'output_layers': [['reshape_2', 0, 0]]} \nCompile parameters: {'optimizer': {'module': 'keras.src.backend.torch.optimizers.torch_adam', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': 'Adam'}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False} \nfit_kwargs: {'epochs': 4, 'batch_size': 128} \nCreation date: 2024-07-30 13:12:37 \nLast fit date: None \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[28]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>Epoch 1/4\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23s 152ms/step - loss: 0.0470 - val_loss: 0.0178\nEpoch 2/4\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 153ms/step - loss: 0.0134 - val_loss: 0.0122\nEpoch 3/4\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 145ms/step - loss: 0.0110 - val_loss: 0.0127\nEpoch 4/4\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 162ms/step - loss: 0.0100 - val_loss: 0.0127\n</pre> In\u00a0[29]: Copied! <pre># Trainig and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\nforecaster.plot_history(ax=ax)\n</pre> # Trainig and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) forecaster.plot_history(ax=ax) In\u00a0[30]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Prediction # ============================================================================== predictions = forecaster.predict() predictions Out[30]: o3 2021-04-01 00:00:00 43.277061 2021-04-01 01:00:00 40.084240 2021-04-01 02:00:00 37.378689 2021-04-01 03:00:00 34.141743 2021-04-01 04:00:00 30.391361 In\u00a0[31]: Copied! <pre># Backtesting with test data\n# ==============================================================================\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster=forecaster,\n    steps=forecaster.max_step,\n    series=data,\n    levels=forecaster.levels,\n    initial_train_size=len(data.loc[:end_validation, :]), # Datos de entrenamiento + validaci\u00f3n\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    refit=False,\n)\n</pre> # Backtesting with test data # ============================================================================== metrics, predictions = backtesting_forecaster_multiseries(     forecaster=forecaster,     steps=forecaster.max_step,     series=data,     levels=forecaster.levels,     initial_train_size=len(data.loc[:end_validation, :]), # Datos de entrenamiento + validaci\u00f3n     metric=\"mean_absolute_error\",     verbose=False,     refit=False, ) <pre>Epoch 1/4\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 30s 157ms/step - loss: 0.0098 - val_loss: 0.0113\nEpoch 2/4\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 28s 150ms/step - loss: 0.0095 - val_loss: 0.0111\nEpoch 3/4\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 30s 159ms/step - loss: 0.0093 - val_loss: 0.0121\nEpoch 4/4\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 34s 182ms/step - loss: 0.0093 - val_loss: 0.0113\n</pre> <pre>  0%|          | 0/442 [00:00&lt;?, ?it/s]</pre> In\u00a0[32]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metrics Out[32]: levels mean_absolute_error 0 o3 10.363694 In\u00a0[33]: Copied! <pre># % Error vs series mean\n# ==============================================================================\nrel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"])\nprint(f\"Serie mean: {np.mean(data['o3']):0.2f}\")\nprint(f\"Relative error (mae): {rel_mse:0.2f} %\")\n</pre> # % Error vs series mean # ============================================================================== rel_mse = 100 * metrics.loc[0, 'mean_absolute_error'] / np.mean(data[\"o3\"]) print(f\"Serie mean: {np.mean(data['o3']):0.2f}\") print(f\"Relative error (mae): {rel_mse:0.2f} %\") <pre>Serie mean: 54.52\nRelative error (mae): 19.01 %\n</pre> In\u00a0[34]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions\n</pre> # Backtesting predictions # ============================================================================== predictions Out[34]: o3 2021-10-01 00:00:00 49.819893 2021-10-01 01:00:00 46.665585 2021-10-01 02:00:00 42.649746 2021-10-01 03:00:00 37.113007 2021-10-01 04:00:00 32.271500 ... ... 2021-12-31 19:00:00 17.417406 2021-12-31 20:00:00 11.689775 2021-12-31 21:00:00 3.200252 2021-12-31 22:00:00 4.635203 2021-12-31 23:00:00 6.826915 <p>2208 rows \u00d7 1 columns</p> In\u00a0[35]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions[\"o3\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions[\"o3\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend();  <p>This is a more complex scenario in which multiple time series are predicted using multiple time series as predictors. It is therefore a scenario in which multiple series are modeled simultaneously using a single model. This has special application in many real scenarios, such as the prediction of stock values for several companies based on the stock history, the price of energy and commodities. Or the case of forecasting multiple products in an online store, based on the sales of other products, the price of the products, etc.</p> In\u00a0[36]: Copied! <pre># Model creation\n# ==============================================================================\n# Now, we have multiple series and multiple targets\nseries = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2'] \nlevels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less\nlags = 32 \nsteps = 5 \n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=[100, 50],\n    dense_units=[64, 32],\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Model creation # ============================================================================== # Now, we have multiple series and multiple targets series = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2']  levels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less lags = 32  steps = 5   data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=[100, 50],     dense_units=[64, 32],     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional_3\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_3 (InputLayer)      \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_4 (LSTM)                   \u2502 (None, 32, 100)        \u2502        44,400 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_5 (LSTM)                   \u2502 (None, 50)             \u2502        30,200 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_7 (Dense)                 \u2502 (None, 64)             \u2502         3,264 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_8 (Dense)                 \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_9 (Dense)                 \u2502 (None, 20)             \u2502           660 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape_3 (Reshape)             \u2502 (None, 5, 4)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 80,604 (314.86 KB)\n</pre> <pre> Trainable params: 80,604 (314.86 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[37]: Copied! <pre># Forecaster creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    steps=steps,\n    lags=lags,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 10, \n        \"batch_size\": 128, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3)\n        ],  \n        \"series_val\": data_val,\n    },\n)\nforecaster\n</pre> # Forecaster creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     steps=steps,     lags=lags,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 10,          \"batch_size\": 128,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3)         ],           \"series_val\": data_val,     }, ) forecaster Out[37]: <pre>============= \nForecasterRnn \n============= \nRegressor: &lt;Functional name=functional_3, built=True&gt; \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32] \nTransformer for series: MinMaxScaler() \nWindow size: 32 \nTarget series, levels: ['pm2.5', 'co', 'no', 'o3'] \nMultivariate series (names): None \nMaximum steps predicted: [1 2 3 4 5] \nTraining range: None \nTraining index type: None \nTraining index frequency: None \nModel parameters: {'name': 'functional_3', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32, 10), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_3'}, 'registered_name': None, 'name': 'input_layer_3', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_4', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 100, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 10)}, 'name': 'lstm_4', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 10), 'dtype': 'float32', 'keras_history': ['input_layer_3', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_5', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 50, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32, 100)}, 'name': 'lstm_5', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32, 100), 'dtype': 'float32', 'keras_history': ['lstm_4', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 50)}, 'name': 'dense_7', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 50), 'dtype': 'float32', 'keras_history': ['lstm_5', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}, 'name': 'dense_8', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['dense_7', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 20, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_9', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['dense_8', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape_3', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (5, 4)}, 'registered_name': None, 'build_config': {'input_shape': (None, 20)}, 'name': 'reshape_3', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 20), 'dtype': 'float32', 'keras_history': ['dense_9', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_3', 0, 0]], 'output_layers': [['reshape_3', 0, 0]]} \nCompile parameters: {'optimizer': {'module': 'keras.src.backend.torch.optimizers.torch_adam', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': 'Adam'}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False} \nfit_kwargs: {'epochs': 10, 'batch_size': 128, 'callbacks': [&lt;keras.src.callbacks.early_stopping.EarlyStopping object at 0x000002B51CA05910&gt;]} \nCreation date: 2024-07-30 13:16:58 \nLast fit date: None \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[38]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>Epoch 1/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 160ms/step - loss: 0.0138 - val_loss: 0.0100\nEpoch 2/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 166ms/step - loss: 0.0050 - val_loss: 0.0094\nEpoch 3/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27s 175ms/step - loss: 0.0043 - val_loss: 0.0084\nEpoch 4/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 26s 172ms/step - loss: 0.0038 - val_loss: 0.0076\nEpoch 5/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27s 174ms/step - loss: 0.0036 - val_loss: 0.0073\nEpoch 6/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23s 149ms/step - loss: 0.0035 - val_loss: 0.0072\nEpoch 7/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 163ms/step - loss: 0.0034 - val_loss: 0.0076\nEpoch 8/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 163ms/step - loss: 0.0034 - val_loss: 0.0069\nEpoch 9/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24s 158ms/step - loss: 0.0033 - val_loss: 0.0069\nEpoch 10/10\n154/154 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27s 175ms/step - loss: 0.0032 - val_loss: 0.0073\n</pre> <p>The prediction can also be made for specific <code>steps</code>, as long as they are within the prediction horizon defined in the model.</p> In\u00a0[39]: Copied! <pre># Specific step predictions\n# ==============================================================================\nforecaster.predict(steps=[1, 5], levels=\"o3\")\n</pre> # Specific step predictions # ============================================================================== forecaster.predict(steps=[1, 5], levels=\"o3\") Out[39]: o3 2021-04-01 00:00:00 36.963165 2021-04-01 04:00:00 24.514666 In\u00a0[40]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Prediction # ============================================================================== predictions = forecaster.predict() predictions Out[40]: pm2.5 co no o3 2021-04-01 00:00:00 14.250182 0.114879 1.290489 36.963165 2021-04-01 01:00:00 14.238721 0.102153 1.658386 34.069336 2021-04-01 02:00:00 14.098439 0.116550 0.644113 30.392138 2021-04-01 03:00:00 13.381383 0.113979 1.317875 25.742840 2021-04-01 04:00:00 13.152705 0.117441 1.632742 24.514666 In\u00a0[41]: Copied! <pre># Backtesting with test data\n# ==============================================================================\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster=forecaster,\n    steps=forecaster.max_step,\n    series=data,\n    levels=forecaster.levels,\n    initial_train_size=len(data.loc[:end_validation, :]),\n    metric=\"mean_absolute_error\",\n    verbose=False,\n    refit=False,\n)\n</pre> # Backtesting with test data # ============================================================================== metrics, predictions = backtesting_forecaster_multiseries(     forecaster=forecaster,     steps=forecaster.max_step,     series=data,     levels=forecaster.levels,     initial_train_size=len(data.loc[:end_validation, :]),     metric=\"mean_absolute_error\",     verbose=False,     refit=False, ) <pre>Epoch 1/10\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 30s 161ms/step - loss: 0.0030 - val_loss: 0.0066\nEpoch 2/10\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 31s 167ms/step - loss: 0.0030 - val_loss: 0.0067\nEpoch 3/10\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 29s 153ms/step - loss: 0.0030 - val_loss: 0.0067\nEpoch 4/10\n188/188 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 34s 181ms/step - loss: 0.0029 - val_loss: 0.0067\n</pre> <pre>  0%|          | 0/442 [00:00&lt;?, ?it/s]</pre> In\u00a0[42]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metrics Out[42]: levels mean_absolute_error 0 pm2.5 3.521863 1 co 0.025072 2 no 2.873905 3 o3 11.518277 4 average 4.484779 5 weighted_average 4.484779 6 pooling 4.484779 In\u00a0[43]: Copied! <pre># Plot all the predicted variables as rows in the plot\n# ==============================================================================\nfig, ax = plt.subplots(len(levels), 1, figsize=(8, 3*len(levels)), sharex=True)\nfor i, level in enumerate(levels):\n    data_test[level].plot(ax=ax[i], label=\"test\")\n    predictions[level].plot(ax=ax[i], label=\"predictions\")\n    ax[i].set_title(level)\n    ax[i].legend()\n</pre> # Plot all the predicted variables as rows in the plot # ============================================================================== fig, ax = plt.subplots(len(levels), 1, figsize=(8, 3*len(levels)), sharex=True) for i, level in enumerate(levels):     data_test[level].plot(ax=ax[i], label=\"test\")     predictions[level].plot(ax=ax[i], label=\"predictions\")     ax[i].set_title(level)     ax[i].legend() <p>To improve the user experience and speed up the prototyping, development, and production process, skforecast has the <code>create_and_compile_model</code> function, with which, by indicating just a few arguments, the architecture is inferred and the model is created.</p> <ul> <li><p><code>series</code>: Time series to be used to train the model</p> </li> <li><p><code>levels</code>: Time series to be predicted.</p> </li> <li><p><code>lags</code>: Number of time steps to be used to predict the next value.</p> </li> <li><p><code>steps</code>: Number of time steps to be predicted.</p> </li> <li><p><code>recurrent_layer</code>: Type of recurrent layer to use. By default, an LSTM layer is used.</p> </li> <li><p><code>recurrent_units</code>: Number of units in the recurrent layer. By default, 100 is used. If a list is passed, a recurrent layer will be created for each element in the list.</p> </li> <li><p><code>dense_units</code>: Number of units in the dense layer. By default, 64 is used. If a list is passed, a dense layer will be created for each element in the list.</p> </li> <li><p><code>optimizer</code>: Optimizer to use. By default, Adam with a learning rate of 0.01 is used.</p> </li> <li><p><code>loss</code>: Loss function to use. By default, Mean Squared Error is used.</p> </li> </ul> <p> \u270e Note </p> <p>The following examples use <code>recurrent_layer=\"LSTM\"</code> but it is also possible to use <code>\"RNN\"</code> layers.</p> In\u00a0[44]: Copied! <pre># Create model\n# ==============================================================================\nseries = [\"o3\"] #\u00a0Series used as predictors\nlevels = [\"o3\"] # Target series to predict\nlags = 32 # Past time steps to be used to predict the target\nsteps = 1 #\u00a0Future time steps to be predicted\n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=4,\n    dense_units=16,\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Create model # ============================================================================== series = [\"o3\"] #\u00a0Series used as predictors levels = [\"o3\"] # Target series to predict lags = 32 # Past time steps to be used to predict the target steps = 1 #\u00a0Future time steps to be predicted  data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=4,     dense_units=16,     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional_4\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_4 (InputLayer)      \u2502 (None, 32, 1)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_6 (LSTM)                   \u2502 (None, 4)              \u2502            96 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_10 (Dense)                \u2502 (None, 16)             \u2502            80 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_11 (Dense)                \u2502 (None, 1)              \u2502            17 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape_4 (Reshape)             \u2502 (None, 1, 1)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 193 (772.00 B)\n</pre> <pre> Trainable params: 193 (772.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>In this case, a simple LSTM network is used, with a single recurrent layer with 4 neurons and a hidden dense layer with 16 neurons. The following table shows a detailed description of each layer:</p> Layer Type Output Shape Parameters Description Input Layer (InputLayer) <code>InputLayer</code> <code>(None, 32, 1)</code> 0 This is the input layer of the model. It receives sequences of length 32, corresponding to the number of lags with a dimension at each time step. LSTM Layer (Long Short-Term Memory) <code>LSTM</code> <code>(None, 4)</code> 96 The LSTM layer is a long and short-term memory layer that processes the input sequence. It has 4 LSTM units and connects to the next layer. First Dense Layer (Dense) <code>Dense</code> <code>(None, 16)</code> 80 This is a fully connected layer with 16 units and uses a default activation function (relu) in the provided architecture. Second Dense Layer (Dense) <code>Dense</code> <code>(None, 1)</code> 17 Another fully connected dense layer, this time with a single output unit. It also uses a default activation function. Reshape Layer (Reshape) <code>Reshape</code> <code>(None, 1, 1)</code> 0 This layer reshapes the output of the previous dense layer to have a specific shape <code>(None, 1, 1)</code>. This layer is not strictly necessary, but is included to make the module generalizable to other multi-output forecasting problems. The dimension of this output layer is <code>(None, steps_to_predict_future, series_to_predict)</code>. In this case, <code>steps=1 and levels=\"o3\"</code>, so the dimension is <code>(None, 1, 1)</code> Total Parameters and Trainable - - 193 Total Parameters: 193, Trainable Parameters: 193, Non-Trainable Parameters: 0 <p>More complex models can be created including:</p> <ul> <li>Multiple series to be modeled (levels)</li> <li>Multiple series to be used as predictors (series)</li> <li>Multiple steps to be predicted (steps)</li> <li>Multiple lags to be used as predictors (lags)</li> <li>Multiple recurrent layers (recurrent_units)</li> <li>Multiple dense layers (dense_units)</li> </ul> In\u00a0[45]: Copied! <pre># Model creation\n# ==============================================================================\n# Now, we have multiple series and multiple targets\nseries = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2'] \nlevels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less\nlags = 32 \nsteps = 5 \n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=[100, 50],\n    dense_units=[64, 32],\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\nmodel.summary()\n</pre> # Model creation # ============================================================================== # Now, we have multiple series and multiple targets series = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2']  levels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less lags = 32  steps = 5   data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=[100, 50],     dense_units=[64, 32],     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() ) model.summary() <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> <pre>Model: \"functional_5\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_5 (InputLayer)      \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_7 (LSTM)                   \u2502 (None, 32, 100)        \u2502        44,400 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_8 (LSTM)                   \u2502 (None, 50)             \u2502        30,200 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_12 (Dense)                \u2502 (None, 64)             \u2502         3,264 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_13 (Dense)                \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_14 (Dense)                \u2502 (None, 20)             \u2502           660 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape_5 (Reshape)             \u2502 (None, 5, 4)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 80,604 (314.86 KB)\n</pre> <pre> Trainable params: 80,604 (314.86 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[46]: Copied! <pre># Model creation\n# ==============================================================================\nseries = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2'] \nlevels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less\nlags = 10 \nsteps = 5 \n\ndata = air_quality[series].copy()\ndata_train = air_quality_train[series].copy()\ndata_val = air_quality_val[series].copy()\ndata_test = air_quality_test[series].copy()\n\nmodel = create_and_compile_model(\n    series=data_train,\n    levels=levels, \n    lags=lags,\n    steps=steps,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=[100, 50],\n    dense_units=[64, 32],\n    optimizer=Adam(learning_rate=0.01), \n    loss=MeanSquaredError()\n)\n\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    steps=steps,\n    lags=lags,\n)\nforecaster\n</pre> # Model creation # ============================================================================== series = ['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.','so2']  levels = ['pm2.5', 'co', 'no', \"o3\"] #  Features to predict. It can be all the series or less lags = 10  steps = 5   data = air_quality[series].copy() data_train = air_quality_train[series].copy() data_val = air_quality_val[series].copy() data_test = air_quality_test[series].copy()  model = create_and_compile_model(     series=data_train,     levels=levels,      lags=lags,     steps=steps,     recurrent_layer=\"LSTM\",     recurrent_units=[100, 50],     dense_units=[64, 32],     optimizer=Adam(learning_rate=0.01),      loss=MeanSquaredError() )  forecaster = ForecasterRnn(     regressor=model,     levels=levels,     steps=steps,     lags=lags, ) forecaster <pre>keras version: 3.4.1\nUsing backend: torch\ntorch version: 2.2.2+cpu\n</pre> Out[46]: <pre>============= \nForecasterRnn \n============= \nRegressor: &lt;Functional name=functional_6, built=True&gt; \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for series: MinMaxScaler() \nWindow size: 10 \nTarget series, levels: ['pm2.5', 'co', 'no', 'o3'] \nMultivariate series (names): None \nMaximum steps predicted: [1 2 3 4 5] \nTraining range: None \nTraining index type: None \nTraining index frequency: None \nModel parameters: {'name': 'functional_6', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 10, 10), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_6'}, 'registered_name': None, 'name': 'input_layer_6', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_9', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 100, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 10, 10)}, 'name': 'lstm_9', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 10, 10), 'dtype': 'float32', 'keras_history': ['input_layer_6', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_10', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 50, 'activation': 'relu', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 10, 100)}, 'name': 'lstm_10', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 10, 100), 'dtype': 'float32', 'keras_history': ['lstm_9', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 50)}, 'name': 'dense_15', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 50), 'dtype': 'float32', 'keras_history': ['lstm_10', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_16', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}, 'name': 'dense_16', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['dense_15', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_17', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 20, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_17', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['dense_16', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape_6', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (5, 4)}, 'registered_name': None, 'build_config': {'input_shape': (None, 20)}, 'name': 'reshape_6', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 20), 'dtype': 'float32', 'keras_history': ['dense_17', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_6', 0, 0]], 'output_layers': [['reshape_6', 0, 0]]} \nCompile parameters: {'optimizer': {'module': 'keras.src.backend.torch.optimizers.torch_adam', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': 'Adam'}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False} \nfit_kwargs: {} \nCreation date: 2024-07-30 13:24:02 \nLast fit date: None \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[47]: Copied! <pre>forecaster.create_train_X_y(data_train)[2][\"X_train\"][2]\n</pre> forecaster.create_train_X_y(data_train)[2][\"X_train\"][2] Out[47]: <pre>['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'so2']</pre> <p>The RNN Forecaster can also be used to generate the X_train and y_train matrices that will be used to train the model. The <code>create_train_X_y</code> method allows the user to obtain the training and test matrices that will be used to train the model. The method return 3 elements: X_train, y_train, and dimension_information. The X_train matrix has dimensions <code>(n_samples, n_lags, n_series)</code>, while the y_train matrix has dimensions <code>(n_samples, n_steps, n_levels)</code>.</p> In\u00a0[48]: Copied! <pre>X_train, y_train, dimension_info = forecaster.create_train_X_y(data_train)\n</pre> X_train, y_train, dimension_info = forecaster.create_train_X_y(data_train) In\u00a0[49]: Copied! <pre>X_train.shape, y_train.shape\n</pre> X_train.shape, y_train.shape Out[49]: <pre>((19690, 10, 10), (19690, 5, 4))</pre> <p>The dimension_information dictionary contains the information of each dimension value for both X_train and y_train.</p> In\u00a0[50]: Copied! <pre>dimension_info[\"X_train\"][0][:10]\n</pre> dimension_info[\"X_train\"][0][:10] Out[50]: <pre>[Timestamp('2019-01-01 10:00:00'),\n Timestamp('2019-01-01 11:00:00'),\n Timestamp('2019-01-01 12:00:00'),\n Timestamp('2019-01-01 13:00:00'),\n Timestamp('2019-01-01 14:00:00'),\n Timestamp('2019-01-01 15:00:00'),\n Timestamp('2019-01-01 16:00:00'),\n Timestamp('2019-01-01 17:00:00'),\n Timestamp('2019-01-01 18:00:00'),\n Timestamp('2019-01-01 19:00:00')]</pre> In\u00a0[51]: Copied! <pre>dimension_info[\"X_train\"][1]\n</pre> dimension_info[\"X_train\"][1] Out[51]: <pre>['lag_1',\n 'lag_2',\n 'lag_3',\n 'lag_4',\n 'lag_5',\n 'lag_6',\n 'lag_7',\n 'lag_8',\n 'lag_9',\n 'lag_10']</pre> In\u00a0[52]: Copied! <pre>dimension_info[\"X_train\"][2]\n</pre> dimension_info[\"X_train\"][2] Out[52]: <pre>['pm2.5', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'so2']</pre> In\u00a0[53]: Copied! <pre>dimension_info[\"y_train\"][0][:10]\n</pre> dimension_info[\"y_train\"][0][:10] Out[53]: <pre>[Timestamp('2019-01-01 10:00:00'),\n Timestamp('2019-01-01 11:00:00'),\n Timestamp('2019-01-01 12:00:00'),\n Timestamp('2019-01-01 13:00:00'),\n Timestamp('2019-01-01 14:00:00'),\n Timestamp('2019-01-01 15:00:00'),\n Timestamp('2019-01-01 16:00:00'),\n Timestamp('2019-01-01 17:00:00'),\n Timestamp('2019-01-01 18:00:00'),\n Timestamp('2019-01-01 19:00:00')]</pre> In\u00a0[54]: Copied! <pre>dimension_info[\"y_train\"][1]\n</pre> dimension_info[\"y_train\"][1] Out[54]: <pre>['step_1', 'step_2', 'step_3', 'step_4', 'step_5']</pre> In\u00a0[55]: Copied! <pre>dimension_info[\"y_train\"][2]\n</pre> dimension_info[\"y_train\"][2] Out[55]: <pre>['pm2.5', 'co', 'no', 'o3']</pre>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#forecaster-with-deep-learning","title":"Forecaster with Deep Learning\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#recurrent-neural-networks-rnn-and-long-short-term-memory-lstm","title":"Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#types-of-problems-in-time-series-modeling","title":"Types of problems in time series modeling\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#11-single-step-forecasting-predict-one-step-ahead-of-a-single-series-using-the-same-series-as-predictor","title":"1:1 Single-Step Forecasting - Predict one step ahead of a single series using the same series as predictor.\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#11-multiple-step-forecasting-predict-a-single-series-using-the-same-series-as-predictor-multiple-steps-ahead","title":"1:1 Multiple-Step Forecasting - Predict a single series using the same series as predictor. Multiple steps ahead.\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#n1-multiple-step-forecasting-predict-one-series-using-multiple-series-as-predictors","title":"N:1 Multiple-Step Forecasting - Predict one series using multiple series as predictors.\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#nm-multiple-step-forecasting-multiple-time-series-with-multiple-outputs","title":"N:M Multiple-Step Forecasting - Multiple time series with multiple outputs\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#create-and-compile-tensorflow-models","title":"Create and compile Tensorflow models\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#get-training-and-test-matrices","title":"Get training and test matrices\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html","title":"Forecasting with XGBoost and LightGBM","text":"<p> \u270e Note </p> <p>All of the gradient boosting libraries mentioned above - XGBoost, Lightgbm, HistGradientBoostingRegressor, and CatBoost - can handle categorical features natively, but they require specific encoding techniques that may not be entirely intuitive. Detailed information can be found in categorical features and in this two use cases:</p> <ul> <li>Forecasting time series with gradient boosting</li> <li>Forecasting with XGBoost</li> <li>Forecasting energy demand with machine learning</li> </ul> <p> \ud83d\udca1 Tip </p> <p>Tree-based models, including decision trees, random forests and gradient boosting machines (GBMs) have limitations when it comes to extrapolation, i.e. making predictions or estimates beyond the range of observed data. This limitation becomes particularly critical when forecasting time series data with a trend. Because these models cannot predict values beyond the observed range during training, their predicted values will deviate from the underlying trend.</p> <p>Several strategies have been proposed to address this challenge, with one of the most common approaches being the use of differentiation. The differentiation process involves computing the differences between consecutive observations in the time series. Instead of directly modeling the absolute values, the focus shifts to modeling the relative change ratios. Skforecast, version 0.10.0 or higher, introduces a novel <code>differentiation</code> parameter within its forecaster classes to indicate that a differentiation process must be applied before training the model. It is worth noting that the entire differentiation process has been automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p>For more details in this topic visit: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd from xgboost import XGBRegressor from lightgbm import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\"h2o_exog\")\n</pre> # Download data # ============================================================================== data = fetch_dataset(\"h2o_exog\") <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata.index.name = 'date'\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Data preprocessing # ============================================================================== data.index.name = 'date' steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 8,\n                 differentiation = None\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 8,                  differentiation = None              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-08-09 11:11:32 \nLast fit date: 2024-08-09 11:11:32 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[5]: <pre>2005-07-01    0.939158\n2005-08-01    0.931943\n2005-09-01    1.072937\n2005-10-01    1.090429\n2005-11-01    1.087492\n2005-12-01    1.170073\n2006-01-01    0.964073\n2006-02-01    0.760841\n2006-03-01    0.829831\n2006-04-01    0.800095\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[6]: feature importance 9 exog_2 127 1 lag_2 91 0 lag_1 61 5 lag_6 49 8 exog_1 43 3 lag_4 38 4 lag_5 35 7 lag_8 26 6 lag_7 25 2 lag_3 14 In\u00a0[7]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = XGBRegressor(random_state=123, enable_categorical=True),\n                 lags            = 8,\n                 differentiation = None\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = XGBRegressor(random_state=123, enable_categorical=True),                  lags            = 8,                  differentiation = None              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[7]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=True, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=123, ...) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': True, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None} \nfit_kwargs: {} \nCreation date: 2024-08-09 11:11:32 \nLast fit date: 2024-08-09 11:11:32 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> In\u00a0[8]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[8]: <pre>2005-07-01    0.879593\n2005-08-01    0.988315\n2005-09-01    1.069755\n2005-10-01    1.107012\n2005-11-01    1.114354\n2005-12-01    1.212606\n2006-01-01    0.980519\n2006-02-01    0.682377\n2006-03-01    0.743607\n2006-04-01    0.738725\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[9]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[9]: feature importance 0 lag_1 0.288422 9 exog_2 0.266819 1 lag_2 0.118983 6 lag_7 0.101393 4 lag_5 0.082091 7 lag_8 0.061150 8 exog_1 0.050711 3 lag_4 0.015282 5 lag_6 0.011015 2 lag_3 0.004135 <p> \u26a0 Warning </p> <p>Starting with version <code>0.13.0</code>, the <code>ForecasterAutoregMultiSeries</code> class defaults to using <code>encoding='ordinal_category'</code> for encoding time series identifiers. This approach creates a new column (_level_skforecast) of type pandas <code>category</code>. Consequently, the regressors must be able to handle categorical variables. If the regressors do not support categorical variables, the user should set the encoding to <code>'ordinal'</code> or <code>'onehot'</code> for compatibility.</p> <p>Some examples of regressors that support categorical variables and how to enable them are:</p> <ul> <li> HistGradientBoostingRegressor: <code>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")</code> </li> <li> LightGBM: <code>LGBMRegressor</code> does not allow configuration of categorical features during initialization, but rather in its <code>fit</code> method. Therefore, use <code>fit_kwargs = {'categorical_feature':'auto'}</code>. This is the default behavior of <code>LGBMRegressor</code> if no indication is given.          </li> <li> XGBoost: <code>XGBRegressor(enable_categorical=True)</code> </li> </ul>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecasting-with-xgboost-lightgbm-and-other-gradient-boosting-models","title":"Forecasting with XGBoost, LightGBM and other Gradient Boosting models\u00b6","text":"<p>Gradient boosting models have gained popularity in the machine learning community due to their ability to achieve excellent results in a wide range of use cases, including both regression and classification. Although these models have traditionally been less common in forecasting, recent research has shown that they can be highly effective in this domain. Some of the key advantages of using gradient boosting models for forecasting include:</p> <ul> <li><p>The ease with which exogenous variables, in addition to autoregressive variables, can be incorporated into the model.</p> </li> <li><p>The ability to capture non-linear relationships between variables.</p> </li> <li><p>High scalability, which enables the models to handle large volumes of data.</p> </li> </ul> <p>There are several popular implementations of gradient boosting in Python, with four of the most popular being XGBoost, LightGBM, scikit-learn HistGradientBoostingRegressor and CatBoost. All of these libraries follow the scikit-learn API, making them compatible with skforecast.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-lightgbm","title":"Forecaster LightGBM\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-xgboost","title":"Forecaster XGBoost\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html","title":"Hyperparameter tuning and lags selection","text":"<p> \ud83d\udca1 Tip </p> <p>The computational cost of hyperparameter tuning depends heavily on the backtesting approach chosen to evaluate each hyperparameter combination. In general, the duration of the tuning process increases with the number of re-trains involved in the backtesting.</p> <p>To effectively speed up the prototyping phase, it is highly recommended to adopt a two-step strategy. First, use <code>refit=False</code> during the initial search to narrow down the range of values. Then, focus on the identified region of interest and apply a tailored backtesting strategy that meets the specific requirements of the use case.</p> <p>For additional tips on backtesting strategies, refer to the following resource: Which backtesting strategy should I use?.</p> <p> \u270e Note </p> <p>Since skforecast 0.9.0, all backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance. This applies to all functions of the different <code>model_selection</code> modules.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import random_search_forecaster\nfrom skforecast.model_selection import bayesian_search_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error  from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import random_search_forecaster from skforecast.model_selection import bayesian_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} ) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-val-test dates\n# ==============================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"\n    f\"  (n={len(data.loc[end_train:end_val])})\"\n)\nprint(\n    f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"\n    f\" (n={len(data.loc[end_val:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:end_val].plot(ax=ax, label='validation')\ndata.loc[end_val:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-val-test dates # ============================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"     f\"  (n={len(data.loc[end_train:end_val])})\" ) print(     f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"     f\" (n={len(data.loc[end_val:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:end_val].plot(ax=ax, label='validation') data.loc[end_val:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00 (n=29)\n</pre> In\u00a0[4]: Copied! <pre># Grid search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid ={\n    'lags_1': 3,\n    'lags_2': 10,\n    'lags_3': [1, 2, 3, 20]\n}\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\nresults = grid_search_forecaster(\n              forecaster         = forecaster,\n              y                  = data.loc[:end_val, 'y'],\n              param_grid         = param_grid,\n              lags_grid          = lags_grid,\n              steps              = 12,\n              refit              = False,\n              metric             = 'mean_squared_error',\n              initial_train_size = len(data.loc[:end_train]),\n              fixed_train_size   = False,\n              return_best        = True,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid ={     'lags_1': 3,     'lags_2': 10,     'lags_3': [1, 2, 3, 20] }  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  results = grid_search_forecaster(               forecaster         = forecaster,               y                  = data.loc[:end_val, 'y'],               param_grid         = param_grid,               lags_grid          = lags_grid,               steps              = 12,               refit              = False,               metric             = 'mean_squared_error',               initial_train_size = len(data.loc[:end_train]),               fixed_train_size   = False,               return_best        = True,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.04387531272712768\n\n</pre> Out[4]: lags lags_label params mean_squared_error max_depth n_estimators 1 [1, 2, 3] lags_1 {'max_depth': 5, 'n_estimators': 100} 0.043875 5 100 3 [1, 2, 3] lags_1 {'max_depth': 10, 'n_estimators': 100} 0.043875 10 100 5 [1, 2, 3] lags_1 {'max_depth': 15, 'n_estimators': 100} 0.043875 15 100 17 [1, 2, 3, 20] lags_3 {'max_depth': 15, 'n_estimators': 100} 0.044074 15 100 15 [1, 2, 3, 20] lags_3 {'max_depth': 10, 'n_estimators': 100} 0.044074 10 100 13 [1, 2, 3, 20] lags_3 {'max_depth': 5, 'n_estimators': 100} 0.044074 5 100 0 [1, 2, 3] lags_1 {'max_depth': 5, 'n_estimators': 50} 0.045423 5 50 4 [1, 2, 3] lags_1 {'max_depth': 15, 'n_estimators': 50} 0.045423 15 50 2 [1, 2, 3] lags_1 {'max_depth': 10, 'n_estimators': 50} 0.045423 10 50 16 [1, 2, 3, 20] lags_3 {'max_depth': 15, 'n_estimators': 50} 0.046221 15 50 12 [1, 2, 3, 20] lags_3 {'max_depth': 5, 'n_estimators': 50} 0.046221 5 50 14 [1, 2, 3, 20] lags_3 {'max_depth': 10, 'n_estimators': 50} 0.046221 10 50 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 5, 'n_estimators': 100} 0.047896 5 100 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 10, 'n_estimators': 100} 0.047896 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 15, 'n_estimators': 100} 0.047896 15 100 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 15, 'n_estimators': 50} 0.051399 15 50 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 5, 'n_estimators': 50} 0.051399 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 10, 'n_estimators': 50} 0.051399 10 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[5]: Copied! <pre>forecaster\n</pre> forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(max_depth=5, random_state=123, verbose=-1) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 17:46:08 \nLast fit date: 2024-07-29 17:46:09 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[6]: Copied! <pre># Random search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),\n    'max_depth': np.arange(start=5, stop=30, step=1, dtype=int)\n}\n\nresults = random_search_forecaster(\n              forecaster           = forecaster,\n              y                    = data.loc[:end_val, 'y'],\n              steps                = 12,\n              lags_grid            = lags_grid,\n              param_distributions  = param_distributions,\n              n_iter               = 5,\n              metric               = 'mean_squared_error',\n              refit                = False,\n              initial_train_size   = len(data.loc[:end_train]),\n              fixed_train_size     = False,\n              return_best          = True,\n              random_state         = 123,\n              n_jobs               = 'auto',\n              verbose              = False,\n              show_progress        = True\n          )\n\nresults.head(4)\n</pre> # Random search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters param_distributions = {     'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),     'max_depth': np.arange(start=5, stop=30, step=1, dtype=int) }  results = random_search_forecaster(               forecaster           = forecaster,               y                    = data.loc[:end_val, 'y'],               steps                = 12,               lags_grid            = lags_grid,               param_distributions  = param_distributions,               n_iter               = 5,               metric               = 'mean_squared_error',               refit                = False,               initial_train_size   = len(data.loc[:end_train]),               fixed_train_size     = False,               return_best          = True,               random_state         = 123,               n_jobs               = 'auto',               verbose              = False,               show_progress        = True           )  results.head(4) <pre>Number of models compared: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'n_estimators': 96, 'max_depth': 19}\n  Backtesting metric: 0.04313147793349785\n\n</pre> Out[6]: lags lags_label params mean_squared_error n_estimators max_depth 5 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 96, 'max_depth': 19} 0.043131 96 19 8 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 94, 'max_depth': 28} 0.043171 94 28 9 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 77, 'max_depth': 17} 0.043663 77 17 0 [1, 2, 3] [1, 2, 3] {'n_estimators': 96, 'max_depth': 19} 0.043868 96 19 <p> \u26a0 Warning </p> <p><code>lags_grid</code> is no longer required when using <code>bayesian_search_forecaster</code> since skforecast 0.12.0. The <code>lags</code> argument is now included in the <code>search_space</code>. This allows the lags to be optimized along with the other hyperparameters of the regressor in the bayesian search.</p> In\u00a0[7]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'             : trial.suggest_categorical('lags', [3, 5]),\n        'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n    \n    return search_space\n\nresults, best_trial = bayesian_search_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data.loc[:end_val, 'y'],\n                          search_space          = search_space,\n                          steps                 = 12,\n                          metric                = 'mean_absolute_error',\n                          refit                 = False,\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = True,\n                          n_trials              = 10,\n                          random_state          = 123,\n                          return_best           = False,\n                          n_jobs                = 'auto',\n                          verbose               = False,\n                          show_progress         = True,\n                          engine                = 'optuna',\n                          kwargs_create_study   = {},\n                          kwargs_study_optimize = {}\n                      )\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Search space def search_space(trial):     search_space  = {         'lags'             : trial.suggest_categorical('lags', [3, 5]),         'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10),         'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }          return search_space  results, best_trial = bayesian_search_forecaster(                           forecaster            = forecaster,                           y                     = data.loc[:end_val, 'y'],                           search_space          = search_space,                           steps                 = 12,                           metric                = 'mean_absolute_error',                           refit                 = False,                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = True,                           n_trials              = 10,                           random_state          = 123,                           return_best           = False,                           n_jobs                = 'auto',                           verbose               = False,                           show_progress         = True,                           engine                = 'optuna',                           kwargs_create_study   = {},                           kwargs_study_optimize = {}                       )  results.head(4) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[7]: lags params mean_absolute_error n_estimators min_samples_leaf max_features 7 [1, 2, 3, 4, 5] {'n_estimators': 19, 'min_samples_leaf': 3, 'm... 0.126995 19 3 sqrt 1 [1, 2, 3] {'n_estimators': 15, 'min_samples_leaf': 4, 'm... 0.153278 15 4 sqrt 4 [1, 2, 3] {'n_estimators': 13, 'min_samples_leaf': 3, 'm... 0.160396 13 3 sqrt 5 [1, 2, 3, 4, 5] {'n_estimators': 14, 'min_samples_leaf': 5, 'm... 0.172366 14 5 log2 <p><code>best_trial</code> contains information of the trial which achived the best results. See more in Study class.</p> In\u00a0[8]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[8]: <pre>FrozenTrial(number=7, state=1, values=[0.1269945910624239], datetime_start=datetime.datetime(2024, 7, 29, 17, 46, 11, 17921), datetime_complete=datetime.datetime(2024, 7, 29, 17, 46, 11, 80857), params={'lags': 5, 'n_estimators': 19, 'min_samples_leaf': 3, 'max_features': 'sqrt'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(3, 5)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=7, value=None)</pre> <p> \u26a0 Warning </p> <p>This engine is deprecated since skforecast 0.7.0 in favor of optuna engine. To continue using it, install skforecast 0.6.0.  User guide: https://joaquinamatrodrigo.github.io/skforecast/0.6.0/user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize</p> In\u00a0[9]: Copied! <pre># Custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n</pre> # Custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric In\u00a0[10]: Copied! <pre># Grid search hyperparameter and lags with custom metric\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults = grid_search_forecaster(\n              forecaster         = forecaster,\n              y                  = data.loc[:end_val, 'y'],\n              param_grid         = param_grid,\n              lags_grid          = lags_grid,\n              steps              = 12,\n              refit              = False,\n              metric             = custom_metric,\n              initial_train_size = len(data.loc[:end_train]),\n              fixed_train_size   = False,\n              return_best        = True,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults.head(4)\n</pre> # Grid search hyperparameter and lags with custom metric # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results = grid_search_forecaster(               forecaster         = forecaster,               y                  = data.loc[:end_val, 'y'],               param_grid         = param_grid,               lags_grid          = lags_grid,               steps              = 12,               refit              = False,               metric             = custom_metric,               initial_train_size = len(data.loc[:end_train]),               fixed_train_size   = False,               return_best        = True,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3 20] \n  Parameters: {'max_depth': 15, 'n_estimators': 100}\n  Backtesting metric: 0.0681822427249296\n\n</pre> Out[10]: lags lags_label params custom_metric max_depth n_estimators 17 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.068182 15 100 15 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.068182 10 100 13 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.068182 5 100 1 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.070472 5 100 In\u00a0[11]: Copied! <pre># Grid search hyperparameter and lags with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults = grid_search_forecaster(\n              forecaster         = forecaster,\n              y                  = data.loc[:end_val, 'y'],\n              param_grid         = param_grid,\n              lags_grid          = lags_grid,\n              steps              = 12,\n              refit              = False,\n              metric             = ['mean_absolute_error', mean_squared_error, custom_metric],\n              initial_train_size = len(data.loc[:end_train]),\n              fixed_train_size   = False,\n              return_best        = True,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults.head(4)\n</pre> # Grid search hyperparameter and lags with multiple metrics # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results = grid_search_forecaster(               forecaster         = forecaster,               y                  = data.loc[:end_val, 'y'],               param_grid         = param_grid,               lags_grid          = lags_grid,               steps              = 12,               refit              = False,               metric             = ['mean_absolute_error', mean_squared_error, custom_metric],               initial_train_size = len(data.loc[:end_train]),               fixed_train_size   = False,               return_best        = True,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.18359367014650177\n\n</pre> Out[11]: lags lags_label params mean_absolute_error mean_squared_error custom_metric max_depth n_estimators 1 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.183594 0.043875 0.070472 5 100 3 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.183594 0.043875 0.070472 10 100 5 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.183594 0.043875 0.070472 15 100 17 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.184901 0.044074 0.068182 15 100 In\u00a0[12]: Copied! <pre># Models to compare\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\n\nmodels = [\n    RandomForestRegressor(random_state=123), \n    LGBMRegressor(random_state=123, verbose=-1),\n    Ridge(random_state=123)\n]\n\n# Hyperparameter to search for each model\nparam_grids = {\n    'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},\n    'LGBMRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},\n    'Ridge': {'alpha': [0.01, 0.1, 1]}\n}\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\ndf_results = pd.DataFrame()\nfor i, model in enumerate(models):\n\n    print(f\"Grid search for regressor: {model}\")\n    print(f\"-------------------------\")\n\n    forecaster = ForecasterAutoreg(\n                     regressor = model,\n                     lags      = 3\n                 )\n\n    # Regressor hyperparameters\n    param_grid = param_grids[list(param_grids)[i]]\n\n    results = grid_search_forecaster(\n                  forecaster         = forecaster,\n                  y                  = data.loc[:end_val, 'y'],\n                  param_grid         = param_grid,\n                  lags_grid          = lags_grid,\n                  steps              = 3,\n                  refit              = False,\n                  metric             = 'mean_squared_error',\n                  initial_train_size = len(data.loc[:end_train]),\n                  fixed_train_size   = True,\n                  return_best        = False,\n                  n_jobs             = 'auto',\n                  verbose            = False,\n                  show_progress      = True\n              )\n    \n    # Create a column with model name\n    results['model'] = list(param_grids)[i]\n    \n    df_results = pd.concat([df_results, results])\n\ndf_results = df_results.sort_values(by='mean_squared_error')\ndf_results.head(10)\n</pre> # Models to compare from sklearn.ensemble import RandomForestRegressor from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge  models = [     RandomForestRegressor(random_state=123),      LGBMRegressor(random_state=123, verbose=-1),     Ridge(random_state=123) ]  # Hyperparameter to search for each model param_grids = {     'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},     'LGBMRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},     'Ridge': {'alpha': [0.01, 0.1, 1]} }  # Lags used as predictors lags_grid = [3, 5]  df_results = pd.DataFrame() for i, model in enumerate(models):      print(f\"Grid search for regressor: {model}\")     print(f\"-------------------------\")      forecaster = ForecasterAutoreg(                      regressor = model,                      lags      = 3                  )      # Regressor hyperparameters     param_grid = param_grids[list(param_grids)[i]]      results = grid_search_forecaster(                   forecaster         = forecaster,                   y                  = data.loc[:end_val, 'y'],                   param_grid         = param_grid,                   lags_grid          = lags_grid,                   steps              = 3,                   refit              = False,                   metric             = 'mean_squared_error',                   initial_train_size = len(data.loc[:end_train]),                   fixed_train_size   = True,                   return_best        = False,                   n_jobs             = 'auto',                   verbose            = False,                   show_progress      = True               )          # Create a column with model name     results['model'] = list(param_grids)[i]          df_results = pd.concat([df_results, results])  df_results = df_results.sort_values(by='mean_squared_error') df_results.head(10) <pre>Grid search for regressor: RandomForestRegressor(random_state=123)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: LGBMRegressor(random_state=123, verbose=-1)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: Ridge(random_state=123)\n-------------------------\nNumber of models compared: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[12]: lags lags_label params mean_squared_error max_depth n_estimators model alpha 5 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 50} 0.050180 5.0 50.0 LGBMRegressor NaN 7 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 50} 0.050180 10.0 50.0 LGBMRegressor NaN 1 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.050907 5.0 50.0 LGBMRegressor NaN 3 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.050907 10.0 50.0 LGBMRegressor NaN 2 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 20} 0.056990 10.0 20.0 LGBMRegressor NaN 0 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 20} 0.056990 5.0 20.0 LGBMRegressor NaN 6 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 20} 0.057542 10.0 20.0 LGBMRegressor NaN 4 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 20} 0.057542 5.0 20.0 LGBMRegressor NaN 0 [1, 2, 3] [1, 2, 3] {'alpha': 0.01} 0.059814 NaN NaN Ridge 0.01 1 [1, 2, 3] [1, 2, 3] {'alpha': 0.1} 0.060078 NaN NaN Ridge 0.10 In\u00a0[13]: Copied! <pre># Save results to file\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\nresults = grid_search_forecaster(\n              forecaster         = forecaster,\n              y                  = data.loc[:end_val, 'y'],\n              param_grid         = param_grid,\n              lags_grid          = lags_grid,\n              steps              = 12,\n              refit              = False,\n              metric             = 'mean_squared_error',\n              initial_train_size = len(data.loc[:end_train]),\n              fixed_train_size   = False,\n              return_best        = True,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True,\n              output_file        = \"results_grid_search.txt\"\n          )\n</pre> # Save results to file # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  results = grid_search_forecaster(               forecaster         = forecaster,               y                  = data.loc[:end_val, 'y'],               param_grid         = param_grid,               lags_grid          = lags_grid,               steps              = 12,               refit              = False,               metric             = 'mean_squared_error',               initial_train_size = len(data.loc[:end_train]),               fixed_train_size   = False,               return_best        = True,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True,               output_file        = \"results_grid_search.txt\"           ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.04387531272712768\n\n</pre> In\u00a0[14]: Copied! <pre># Read results file\n# ==============================================================================\npd.read_csv(\"results_grid_search.txt\", sep=\"\\t\")\n</pre> # Read results file # ============================================================================== pd.read_csv(\"results_grid_search.txt\", sep=\"\\t\") Out[14]: lags lags_label params mean_squared_error max_depth n_estimators 0 [1 2 3] [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.045423 5 50 1 [1 2 3] [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.043875 5 100 2 [1 2 3] [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.045423 10 50 3 [1 2 3] [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.043875 10 100 4 [1 2 3] [1 2 3] {'max_depth': 15, 'n_estimators': 50} 0.045423 15 50 5 [1 2 3] [1 2 3] {'max_depth': 15, 'n_estimators': 100} 0.043875 15 100 6 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 5, 'n_estimators': 50} 0.051399 5 50 7 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 5, 'n_estimators': 100} 0.047896 5 100 8 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 10, 'n_estimators': 50} 0.051399 10 50 9 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 10, 'n_estimators': 100} 0.047896 10 100 10 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 15, 'n_estimators': 50} 0.051399 15 50 11 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 15, 'n_estimators': 100} 0.047896 15 100 12 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 5, 'n_estimators': 50} 0.046221 5 50 13 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 5, 'n_estimators': 100} 0.044074 5 100 14 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 10, 'n_estimators': 50} 0.046221 10 50 15 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 10, 'n_estimators': 100} 0.044074 10 100 16 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 15, 'n_estimators': 50} 0.046221 15 50 17 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 15, 'n_estimators': 100} 0.044074 15 100"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search, that can be combined with backtesting to identify the optimal combination of lags and hyperparameters that achieve the best prediction performance.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#grid-search","title":"Grid search\u00b6","text":"<p>Grid search is a popular hyperparameter tuning technique that evaluate an exaustive list of combinations of hyperparameters and lags to find the optimal configuration for a forecasting model. To perform a grid search with the Skforecast library, two grids are needed: one with different lags (<code>lags_grid</code>) and another with the hyperparameters (<code>param_grid</code>).</p> <p>The grid search process involves the following steps:</p> <ol> <li><p><code>grid_search_forecaster</code> creates a copy of the forecaster object and replaces the <code>lags</code> argument with the first option appearing in <code>lags_grid</code>.</p> </li> <li><p>The function validates all combinations of hyperparameters presented in <code>param_grid</code> using backtesting.</p> </li> <li><p>The function repeats these two steps until it has evaluated all possible combinations of lags and hyperparameters.</p> </li> <li><p>If <code>return_best = True</code>, the original forecaster is trained with the best lags and hyperparameters configuration found during the grid search process.</p> </li> </ol>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#random-search","title":"Random search\u00b6","text":"<p>Random search is another hyperparameter tuning strategy available in the Skforecast library. In contrast to grid search, which tries out all possible combinations of hyperparameters and lags, randomized search samples a fixed number of values from the specified possibilities. The number of combinations that are evaluated is given by <code>n_iter</code>.</p> <p>It is important to note that random sampling is only applied to the model hyperparameters, but not to the lags. All lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#bayesian-search","title":"Bayesian search\u00b6","text":"<p>Grid and random search can generate good results, especially when the search range is narrowed down. However, neither of them takes into account the results obtained so far, which prevents them from focusing the search on the regions of greatest interest while avoiding unnecessary ones.</p> <p>An alternative is to use Bayesian optimization methods to search for hyperparameters. In general terms, bayesian hyperparameter optimization consists of creating a probabilistic model in which the objective function is the model validation metric (RMSE, AUC, accuracy...). With this strategy, the search is redirected at each iteration to the regions of greatest interest. The ultimate goal is to reduce the number of hyperparameter combinations with which the model is evaluated, choosing only the best candidates. This approach is particularly advantageous when the search space is very large or the model evaluation is very slow.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#optuna","title":"Optuna\u00b6","text":"<p>In skforecast, Bayesian optimization with Optuna is performed using its Study object. The objective of the optimization is to minimize the metric generated by backtesting.</p> <p>Additional parameters can be included by passing a dictionary to <code>kwargs_create_study</code> and <code>kwargs_study_optimize</code> arguments to create_study and optimize method, respectively. These arguments are used to configure the study object and optimization algorithm.</p> <p>To use Optuna in skforecast, the <code>search_space</code> argument must be a python function that defines the hyperparameters to optimize over. Optuna uses the Trial object object to generate each search space.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize","title":"Scikit-optimize\u00b6","text":"<p>Skopt performs bayesian optimization with Gaussian processes. This is done with the function gp_minimize where the objective value to be minimized is calculated by backtesting.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-with-custom-metric","title":"Hyperparameter tuning with custom metric\u00b6","text":"<p>Besides to the commonly used metrics such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p> <p>The example below demonstrates how to use hyperparameter optimization to find the optimal parameters for a custom metric that considers only the last three months of each year.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p> <p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) enable users to calculate multiple metrics for each forecaster configuration if a list is provided. This list may include any combination of built-in metrics, such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, as well as user-defined custom metrics.</p> <p>Note that if multiple metrics are specified, these functions will select the best model based on the first metric in the list.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-regressors","title":"Compare multiple regressors\u00b6","text":"<p>The grid search process can be easily extended to compare several machine learning models. This can be achieved by using a simple for loop that iterates over each regressor and applying the <code>grid_search_forecaster</code> function. This approach allows for a more thorough exploration and can help you select the best model.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#saving-results-to-file","title":"Saving results to file\u00b6","text":"<p>The results of the hyperparameter search process can be saved to a file by setting the <code>output_file</code> argument to the desired path. The results will be saved in a tab-separated values (TSV) format containing the hyperparameters, lags, and metrics of each configuration evaluated during the search.</p> <p>The saving process occurs after each hyperparameter evaluation, which means that if the optimization is stopped in the middle of the process, the logs of the first part of the evaluation have already been stored in the file. This can be useful for further analysis or to keep a record of the tuning process.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html","title":"Independent multi-time series forecasting\"","text":"<p> \u270e Note </p> <p>Skforecast offers additional approaches to building global forecasting models:</p> <ul> <li> Global Forecasting Models: Independent multi-series forecasting using window and custom features </li> <li> Global Forecasting Models: Time series with different lengths and different exogenous variables </li> <li> Global Forecasting Models: Dependent multi-series forecasting (Multivariate forecasting) </li> </ul> <p>To learn more about global forecasting models visit our examples:</p> <ul> <li> Global Forecasting Models: Multi-series forecasting with Python and skforecast </li> <li> Global Forecasting Models: Comparative Analysis of Single and Multi-Series Forecasting Modeling </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import MinMaxScaler from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error  from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[2]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00   (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00   (n=170)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); <p> \u26a0 Warning </p> <p>Starting with version <code>0.13.0</code>, the <code>ForecasterAutoregMultiSeries</code> class defaults to using <code>encoding='ordinal_category'</code> for encoding time series identifiers. This approach creates a new column (_level_skforecast) of type pandas <code>category</code>. Consequently, the regressors must be able to handle categorical variables. If the regressors do not support categorical variables, the user should set the encoding to <code>'ordinal'</code> or <code>'onehot'</code> for compatibility.</p> <p>Some examples of regressors that support categorical variables and how to enable them are:</p> <ul> <li> HistGradientBoostingRegressor: <code>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")</code> </li> <li> LightGBM: <code>LGBMRegressor</code> does not allow configuration of categorical features during initialization, but rather in its <code>fit</code> method. Therefore, use <code>fit_kwargs = {'categorical_feature':'auto'}</code>. This is the default behavior of <code>LGBMRegressor</code> if no indication is given.          </li> <li> XGBoost: <code>XGBRegressor(enable_categorical=True)</code> </li> </ul> In\u00a0[5]: Copied! <pre># Create and train ForecasterAutoregMultiSeries\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and train ForecasterAutoregMultiSeries # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [\"'item_1': ['2012-01-01', '2014-07-15']\", \"'item_2': ['2012-01-01', '2014-07-15']\", \"'item_3': ['2012-01-01', '2014-07-15']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-08-26 13:00:41 \nLast fit date: 2024-08-26 13:00:41 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>Two methods can be use to predict the next n steps: <code>predict()</code> or <code>predict_interval()</code>. The argument <code>levels</code> is used to indicate for which series estimate predictions. If <code>None</code> all series will be predicted.</p> In\u00a0[6]: Copied! <pre># Predictions and prediction intervals\n# ==============================================================================\nsteps = 24\n\n# Predictions for item_1\npredictions_item_1 = forecaster.predict(steps=steps, levels='item_1')\ndisplay(predictions_item_1.head(3))\n\n# Interval predictions for item_1 and item_2\npredictions_intervals = forecaster.predict_interval(\n    steps  = steps,\n    levels = ['item_1', 'item_2'],\n    n_boot = 200\n)\ndisplay(predictions_intervals.head(3))\n</pre> # Predictions and prediction intervals # ============================================================================== steps = 24  # Predictions for item_1 predictions_item_1 = forecaster.predict(steps=steps, levels='item_1') display(predictions_item_1.head(3))  # Interval predictions for item_1 and item_2 predictions_intervals = forecaster.predict_interval(     steps  = steps,     levels = ['item_1', 'item_2'],     n_boot = 200 ) display(predictions_intervals.head(3)) item_1 2014-07-16 25.906323 2014-07-17 25.807194 2014-07-18 25.127355 item_1 item_1_lower_bound item_1_upper_bound item_2 item_2_lower_bound item_2_upper_bound 2014-07-16 25.906323 24.753460 27.545243 10.522491 8.740665 12.718655 2014-07-17 25.807194 24.221439 26.801298 10.623789 8.112683 13.123220 2014-07-18 25.127355 23.515433 26.569362 11.299802 8.887296 13.098867 In\u00a0[7]: Copied! <pre># Backtesting multiple time series\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = data,\n    exog                  = None,\n    levels                = None,\n    steps                 = 24,\n    metric                = 'mean_absolute_error',\n    add_aggregated_metric = True,\n    initial_train_size    = len(data_train),\n    fixed_train_size      = True,\n    gap                   = 0,\n    allow_incomplete_fold = True,\n    refit                 = True,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True,\n    suppress_warnings     = False\n)\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting multiple time series # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = data,     exog                  = None,     levels                = None,     steps                 = 24,     metric                = 'mean_absolute_error',     add_aggregated_metric = True,     initial_train_size    = len(data_train),     fixed_train_size      = True,     gap                   = 0,     allow_incomplete_fold = True,     refit                 = True,     n_jobs                = 'auto',     verbose               = False,     show_progress         = True,     suppress_warnings     = False )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.158312 1 item_2 2.563145 2 item_3 3.322265 3 average 2.347908 4 weighted_average 2.347908 5 pooling 2.347908 <pre>\nBacktest predictions\n</pre> Out[7]: item_1 item_2 item_3 2014-07-16 25.906323 10.522491 12.034587 2014-07-17 25.807194 10.623789 10.503966 2014-07-18 25.127355 11.299802 12.206434 2014-07-19 23.902609 11.441606 12.618740 In\u00a0[8]: Copied! <pre># Create Forecaster Multi-Series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n</pre> # Create Forecaster Multi-Series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              ) In\u00a0[9]: Copied! <pre># Grid search Multi-Series\n# ==============================================================================\nlags_grid ={\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\nlevels = ['item_1', 'item_2', 'item_3']\n\nresults = grid_search_forecaster_multiseries(\n              forecaster         = forecaster,\n              series             = data,\n              exog               = None,\n              levels             = levels,\n              lags_grid          = lags_grid,\n              param_grid         = param_grid,\n              steps              = 24,\n              metric             = 'mean_absolute_error',\n              aggregate_metric   = 'weighted_average',\n              initial_train_size = len(data_train),\n              refit              = False,\n              fixed_train_size   = False,\n              return_best        = False,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search Multi-Series # ============================================================================== lags_grid ={     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  levels = ['item_1', 'item_2', 'item_3']  results = grid_search_forecaster_multiseries(               forecaster         = forecaster,               series             = data,               exog               = None,               levels             = levels,               lags_grid          = lags_grid,               param_grid         = param_grid,               steps              = 24,               metric             = 'mean_absolute_error',               aggregate_metric   = 'weighted_average',               initial_train_size = len(data_train),               refit              = False,               fixed_train_size   = False,               return_best        = False,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>8 models compared for 3 level(s). Number of iterations: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> Out[9]: levels lags lags_label params mean_absolute_error__weighted_average max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 7, 'n_estimators': 20} 2.240665 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 3, 'n_estimators': 20} 2.379334 3 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 7, 'n_estimators': 20} 2.432997 7 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 3, 'n_estimators': 20} 2.457422 3 20 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 7, 'n_estimators': 10} 3.048544 7 10 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 3, 'n_estimators': 10} 3.245697 3 10 6 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 3, 'n_estimators': 10} 3.267566 3 10 7 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 7, 'n_estimators': 10} 3.331623 7 10 <p>It is also possible to perform a bayesian optimization with <code>optuna</code> using the <code>bayesian_search_forecaster_multiseries</code> function. For more information about this type of optimization, see the user guide here.</p> In\u00a0[10]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\nlevels = ['item_1', 'item_2', 'item_3']\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'             : trial.suggest_categorical('lags', [24, 48]),\n        'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n\n    return search_space\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = data,\n    exog                  = None,\n    levels                = levels,\n    search_space          = search_space,\n    steps                 = 24,\n    metric                = 'mean_absolute_error',\n    aggregate_metric      = ['weighted_average', 'average', 'pooling'],\n    refit                 = False,\n    initial_train_size    = len(data_train),\n    fixed_train_size      = False,\n    n_trials              = 5,\n    random_state          = 123,\n    return_best           = False,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True,\n    suppress_warnings     = False,\n    engine                = 'optuna',\n    kwargs_create_study   = {},\n    kwargs_study_optimize = {}\n)\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )  levels = ['item_1', 'item_2', 'item_3']  # Search space def search_space(trial):     search_space  = {         'lags'             : trial.suggest_categorical('lags', [24, 48]),         'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10),         'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  results, best_trial = bayesian_search_forecaster_multiseries(     forecaster            = forecaster,     series                = data,     exog                  = None,     levels                = levels,     search_space          = search_space,     steps                 = 24,     metric                = 'mean_absolute_error',     aggregate_metric      = ['weighted_average', 'average', 'pooling'],     refit                 = False,     initial_train_size    = len(data_train),     fixed_train_size      = False,     n_trials              = 5,     random_state          = 123,     return_best           = False,     n_jobs                = 'auto',     verbose               = False,     show_progress         = True,     suppress_warnings     = False,     engine                = 'optuna',     kwargs_create_study   = {},     kwargs_study_optimize = {} )  results.head(4) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[10]: levels lags params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling n_estimators min_samples_leaf max_features 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 16, 'min_samples_leaf': 9, 'm... 2.544932 2.544932 2.544932 16 9 log2 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 15, 'min_samples_leaf': 4, 'm... 2.712388 2.712388 2.712388 15 4 sqrt 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 14, 'min_samples_leaf': 8, 'm... 2.824843 2.824843 2.824843 14 8 log2 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 13, 'min_samples_leaf': 3, 'm... 2.984116 2.984116 2.984116 13 3 sqrt <p><code>best_trial</code> contains information of the trial which achived the best results. See more in Study class.</p> In\u00a0[11]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[11]: <pre>FrozenTrial(number=3, state=1, values=[2.544932138435008], datetime_start=datetime.datetime(2024, 8, 26, 13, 0, 49, 841516), datetime_complete=datetime.datetime(2024, 8, 26, 13, 0, 50, 92174), params={'lags': 48, 'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(24, 48)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=3, value=None)</pre> <p> \u270e Note </p> <p>     As of version <code>0.12.0</code>, the <code>ForecasterAutoregMultiSeries</code> now supports the use of distinct exogenous variables for each individual series. For a comprehensive guide on handling time series with varying lengths and exogenous variables, refer to the      Global Forecasting Models: Time Series with Different Lengths and Different Exogenous Variables.      Additionally, for a more general overview of using exogenous variables in forecasting, please consult the      Exogenous Variables User Guide. </p> In\u00a0[12]: Copied! <pre># Generate the exogenous variable month\n# ==============================================================================\ndata_exog = data.copy()\ndata_exog['month'] = data_exog.index.month\n\n# Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_exog_train = data_exog.loc[:end_train, :].copy()\ndata_exog_test  = data_exog.loc[end_train:, :].copy()\n\ndata_exog_train.head(3)\n</pre> # Generate the exogenous variable month # ============================================================================== data_exog = data.copy() data_exog['month'] = data_exog.index.month  # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_exog_train = data_exog.loc[:end_train, :].copy() data_exog_test  = data_exog.loc[end_train:, :].copy()  data_exog_train.head(3) Out[12]: item_1 item_2 item_3 month date 2012-01-01 8.253175 21.047727 19.429739 1 2012-01-02 22.777826 26.578125 28.009863 1 2012-01-03 27.549099 31.751042 32.078922 1 In\u00a0[13]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\nforecaster.fit(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = data_exog_train[['month']]\n)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )  forecaster.fit(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = data_exog_train[['month']] ) forecaster Out[13]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['month'] \nTraining range: [\"'item_1': ['2012-01-01', '2014-07-15']\", \"'item_2': ['2012-01-01', '2014-07-15']\", \"'item_3': ['2012-01-01', '2014-07-15']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-08-26 13:00:50 \nLast fit date: 2024-08-26 13:00:50 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p> In\u00a0[14]: Copied! <pre># Predict with exogenous variables\n# ==============================================================================\npredictions = forecaster.predict(steps=24, exog=data_exog_test[['month']])\npredictions.head(3)\n</pre> # Predict with exogenous variables # ============================================================================== predictions = forecaster.predict(steps=24, exog=data_exog_test[['month']]) predictions.head(3) Out[14]: item_1 item_2 item_3 2014-07-16 25.730285 11.557816 11.293334 2014-07-17 25.495978 10.603252 11.438098 2014-07-18 25.238433 11.095160 11.399880 <p>As mentioned earlier, the <code>month</code> exogenous variable is replicated for each of the series. This can be easily demonstrated using the <code>create_train_X_y</code> method, which returns the matrix used in the <code>fit</code> method.</p> In\u00a0[15]: Copied! <pre># X_train matrix\n# ==============================================================================\nX_train = forecaster.create_train_X_y(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = data_exog_train[['month']]\n)[0]\n</pre> # X_train matrix # ============================================================================== X_train = forecaster.create_train_X_y(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = data_exog_train[['month']] )[0] In\u00a0[16]: Copied! <pre># X_train slice for item_1\n# ==============================================================================\nX_train.loc[X_train['_level_skforecast'] == 0].head(3)\n</pre> # X_train slice for item_1 # ============================================================================== X_train.loc[X_train['_level_skforecast'] == 0].head(3) Out[16]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 ... lag_17 lag_18 lag_19 lag_20 lag_21 lag_22 lag_23 lag_24 _level_skforecast month date 2012-01-25 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 ... 20.069327 20.533871 21.106643 21.379238 25.895533 27.549099 22.777826 8.253175 0 1 2012-01-26 28.747482 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 ... 20.006161 20.069327 20.533871 21.106643 21.379238 25.895533 27.549099 22.777826 0 1 2012-01-27 23.908368 28.747482 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 ... 21.620184 20.006161 20.069327 20.533871 21.106643 21.379238 25.895533 27.549099 0 1 <p>3 rows \u00d7 26 columns</p> In\u00a0[17]: Copied! <pre># X_train slice for item_2\n# ==============================================================================\nX_train.loc[X_train['_level_skforecast'] == 1].head(3)\n</pre> # X_train slice for item_2 # ============================================================================== X_train.loc[X_train['_level_skforecast'] == 1].head(3) Out[17]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 ... lag_17 lag_18 lag_19 lag_20 lag_21 lag_22 lag_23 lag_24 _level_skforecast month date 2012-01-25 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 19.255208 17.096875 ... 24.098958 19.510417 17.812500 18.191667 24.567708 31.751042 26.578125 21.047727 1 1 2012-01-26 26.611458 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 19.255208 ... 20.223958 24.098958 19.510417 17.812500 18.191667 24.567708 31.751042 26.578125 1 1 2012-01-27 19.759375 26.611458 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 ... 19.161458 20.223958 24.098958 19.510417 17.812500 18.191667 24.567708 31.751042 1 1 <p>3 rows \u00d7 26 columns</p> <p>To use exogenous variables in backtesting or hyperparameter tuning, they must be specified with the <code>exog</code> argument.</p> In\u00a0[18]: Copied! <pre># Backtesting Multi-Series with exog\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = data_exog[['item_1', 'item_2', 'item_3']],\n    exog                  = data_exog[['month']],\n    levels                = None,\n    steps                 = 24,\n    metric                = 'mean_absolute_error',\n    add_aggregated_metric = True,\n    initial_train_size    = len(data_exog_train),\n    fixed_train_size      = True,\n    gap                   = 0,\n    allow_incomplete_fold = True,\n    refit                 = True,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True,\n    suppress_warnings     = False\n)\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions with exogenous variables\")\nbacktest_predictions.head(4)\n</pre> # Backtesting Multi-Series with exog # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = data_exog[['item_1', 'item_2', 'item_3']],     exog                  = data_exog[['month']],     levels                = None,     steps                 = 24,     metric                = 'mean_absolute_error',     add_aggregated_metric = True,     initial_train_size    = len(data_exog_train),     fixed_train_size      = True,     gap                   = 0,     allow_incomplete_fold = True,     refit                 = True,     n_jobs                = 'auto',     verbose               = False,     show_progress         = True,     suppress_warnings     = False )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions with exogenous variables\") backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.227286 1 item_2 2.600569 2 item_3 3.250053 3 average 2.359303 4 weighted_average 2.359303 5 pooling 2.359303 <pre>\nBacktest predictions with exogenous variables\n</pre> Out[18]: item_1 item_2 item_3 2014-07-16 25.730285 11.557816 11.293334 2014-07-17 25.495978 10.603252 11.438098 2014-07-18 25.238433 11.095160 11.399880 2014-07-19 23.986693 12.170443 12.525807 <p><code>ForecasterAutoregMultiSeries</code> allows to transform series before training the model using the argument <code>transformer_series</code>, three diferent options are available:</p> <ul> <li><p><code>transformer_series</code> is a single transformer: When a single transformer is provided, it is automatically cloned for each individual series. Each cloned transformer is then trained separately on one of the series.</p> </li> <li><p><code>transformer_series</code> is a dictionary: A different transformer can be specified for each series by passing a dictionary where the keys correspond to the series names and the values are the transformers. Each series is transformed according to its designated transformer. When this option is used, it is mandatory to include a transformer for unknown series, which is indicated by the key <code>'_unknown_level'</code>.</p> </li> <li><p><code>transformer_series</code> is None: no transformations are applied to any of the series.</p> </li> </ul> <p>Regardless of the configuration, each series is transformed independently. Even when using a single transformer, it is cloned internally and applied separately to each series.</p> In\u00a0[19]: Copied! <pre># Series transformation: same transformation for all series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Series transformation: same transformation for all series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  forecaster.fit(series=data_train) forecaster Out[19]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: StandardScaler() \nTransformer for exog: None \nSeries encoding: ordinal \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [\"'item_1': ['2012-01-01', '2014-07-15']\", \"'item_2': ['2012-01-01', '2014-07-15']\", \"'item_3': ['2012-01-01', '2014-07-15']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-08-26 13:00:51 \nLast fit date: 2024-08-26 13:00:51 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>It is possible to access the fitted transformers for each series through the <code>transformers_series_</code> attribute. This allows verification that each transformer has been trained independently.</p> In\u00a0[20]: Copied! <pre># Mean and scale of the transformer for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\")\n</pre> # Mean and scale of the transformer for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\") <pre>Series item_1: StandardScaler() mean=[22.47606719], scale=[2.56240321]\nSeries item_2: StandardScaler() mean=[16.41739687], scale=[5.00145466]\nSeries item_3: StandardScaler() mean=[17.30064109], scale=[5.53439225]\nSeries _unknown_level: StandardScaler() mean=[18.73136838], scale=[5.2799675]\n</pre> In\u00a0[21]: Copied! <pre># Series transformation: different transformation for each series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=data_train)\n</pre> # Series transformation: different transformation for each series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},                  transformer_exog   = None              )  forecaster.fit(series=data_train) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\utils.py:255: IgnoredArgumentWarning: {'item_3'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> In\u00a0[22]: Copied! <pre># Transformer trained for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    if v is not None:\n        print(f\"Series {k}: {v.get_params()}\")\n    else:\n        print(f\"Series {k}: {v}\")\n</pre> # Transformer trained for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     if v is not None:         print(f\"Series {k}: {v.get_params()}\")     else:         print(f\"Series {k}: {v}\") <pre>Series item_1: {'copy': True, 'with_mean': True, 'with_std': True}\nSeries item_2: {'clip': False, 'copy': True, 'feature_range': (0, 1)}\nSeries item_3: None\nSeries _unknown_level: {'copy': True, 'with_mean': True, 'with_std': True}\n</pre> <p>Starting from version <code>0.12.0</code>, the classes <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> allow the simultaneous modeling of time series of different lengths and using different exogenous variables. Various scenarios are possible:</p> <ul> <li><p>If <code>series</code> is a <code>pandas DataFrame</code> and <code>exog</code> is a <code>pandas Series</code> or <code>DataFrame</code>, each exog is duplicated for each series. <code>exog</code> must have the same index as <code>series</code> (type, length and frequency).</p> </li> <li><p>If <code>series</code> is a <code>pandas DataFrame</code> and <code>exog</code> is a dict of <code>pandas Series</code> or <code>DataFrames</code>. Each key in <code>exog</code> must be a column in <code>series</code> and the values are the exog for each series. <code>exog</code> must have the same index as <code>series</code> (type, length and frequency).</p> </li> <li><p>If <code>series</code> is a <code>dict</code> of <code>pandas Series</code>, <code>exog</code> must be a dict of <code>pandas Series</code> or <code>DataFrames</code>. The keys in <code>series</code> and <code>exog</code> must be the same. All series and exog must have a <code>pandas DatetimeIndex</code> with the same frequency.</p> </li> </ul> Series type Exog type Requirements <code>DataFrame</code> <code>Series</code> or <code>DataFrame</code> Same index (type, length and frequency) <code>DataFrame</code> <code>dict</code> Same index (type, length and frequency) <code>dict</code> <code>dict</code> Both <code>pandas DatetimeIndex</code> (same frequency) In\u00a0[23]: Copied! <pre># Series and exog as DataFrames \n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 4,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler()\n             )\n\nX, y = forecaster.create_train_X_y(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = data_exog_train[['month']]\n)\nX.head(3)\n</pre> # Series and exog as DataFrames  # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 4,                  encoding           = 'ordinal',                  transformer_series = StandardScaler()              )  X, y = forecaster.create_train_X_y(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = data_exog_train[['month']] ) X.head(3) Out[23]: lag_1 lag_2 lag_3 lag_4 _level_skforecast month date 2012-01-05 1.334476 1.979794 0.117764 -5.550607 0 1 2012-01-06 -0.428047 1.334476 1.979794 0.117764 0 1 2012-01-07 -0.534430 -0.428047 1.334476 1.979794 0 1 <p>When <code>exog</code> is a dictionary of <code>pandas Series</code> or <code>DataFrames</code>, different exogenous variables can be used for each series or the same exogenous variable can have different values for each series.</p> In\u00a0[24]: Copied! <pre># Ilustrative example of different values for the same exogenous variable\n# ==============================================================================\nexog_1_item_1_train = pd.Series([1]*len(data_exog_train), name='exog_1', index=data_exog_train.index)\nexog_1_item_2_train = pd.Series([10]*len(data_exog_train), name='exog_1', index=data_exog_train.index)\nexog_1_item_3_train = pd.Series([100]*len(data_exog_train), name='exog_1', index=data_exog_train.index)\n\nexog_1_item_1_test = pd.Series([1]*len(data_exog_test), name='exog_1', index=data_exog_test.index)\nexog_1_item_2_test = pd.Series([10]*len(data_exog_test), name='exog_1', index=data_exog_test.index)\nexog_1_item_3_test = pd.Series([100]*len(data_exog_test), name='exog_1', index=data_exog_test.index)\n</pre> # Ilustrative example of different values for the same exogenous variable # ============================================================================== exog_1_item_1_train = pd.Series([1]*len(data_exog_train), name='exog_1', index=data_exog_train.index) exog_1_item_2_train = pd.Series([10]*len(data_exog_train), name='exog_1', index=data_exog_train.index) exog_1_item_3_train = pd.Series([100]*len(data_exog_train), name='exog_1', index=data_exog_train.index)  exog_1_item_1_test = pd.Series([1]*len(data_exog_test), name='exog_1', index=data_exog_test.index) exog_1_item_2_test = pd.Series([10]*len(data_exog_test), name='exog_1', index=data_exog_test.index) exog_1_item_3_test = pd.Series([100]*len(data_exog_test), name='exog_1', index=data_exog_test.index) In\u00a0[25]: Copied! <pre># Series as DataFrame and exog as dict\n# ==============================================================================\nexog_train_as_dict = {\n    'item_1': exog_1_item_1_train,\n    'item_2': exog_1_item_2_train,\n    'item_3': exog_1_item_3_train\n}\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 4,\n                 encoding  = 'ordinal'\n             )\n\nX, y = forecaster.create_train_X_y(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = exog_train_as_dict\n)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(\"Column `exog_1` as different values for each item (_level_skforecast id):\")\nX['exog_1'].value_counts()\n</pre> # Series as DataFrame and exog as dict # ============================================================================== exog_train_as_dict = {     'item_1': exog_1_item_1_train,     'item_2': exog_1_item_2_train,     'item_3': exog_1_item_3_train }  forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 4,                  encoding  = 'ordinal'              )  X, y = forecaster.create_train_X_y(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = exog_train_as_dict )  display(X.head(3)) print(\"\") print(\"Column `exog_1` as different values for each item (_level_skforecast id):\") X['exog_1'].value_counts() lag_1 lag_2 lag_3 lag_4 _level_skforecast exog_1 date 2012-01-05 25.895533 27.549099 22.777826 8.253175 0 1 2012-01-06 21.379238 25.895533 27.549099 22.777826 0 1 2012-01-07 21.106643 21.379238 25.895533 27.549099 0 1 <pre>\nColumn `exog_1` as different values for each item (_level_skforecast id):\n</pre> Out[25]: <pre>exog_1\n1      923\n10     923\n100    923\nName: count, dtype: int64</pre> In\u00a0[26]: Copied! <pre># Predict with series as DataFrame and exog as dict\n# ==============================================================================\nforecaster.fit(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = exog_train_as_dict\n)\n\nexog_pred_as_dict = {\n    'item_1': exog_1_item_1_test,\n    'item_2': exog_1_item_2_test,\n    'item_3': exog_1_item_3_test\n}\n\npredictions = forecaster.predict(steps=24, exog=exog_pred_as_dict)\npredictions.head(3)\n</pre> # Predict with series as DataFrame and exog as dict # ============================================================================== forecaster.fit(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = exog_train_as_dict )  exog_pred_as_dict = {     'item_1': exog_1_item_1_test,     'item_2': exog_1_item_2_test,     'item_3': exog_1_item_3_test }  predictions = forecaster.predict(steps=24, exog=exog_pred_as_dict) predictions.head(3) Out[26]: item_1 item_2 item_3 2014-07-16 25.648910 10.677388 12.402632 2014-07-17 25.370120 11.401488 11.974233 2014-07-18 25.168771 12.805684 11.657917 <p> \ud83d\udca1 Tip </p> <p>When using series with different lengths and different exogenous variables, it is recommended to use <code>series</code> and <code>exog</code> as dictionaries. This way, it is easier to manage the data and avoid errors.</p> <p>Visit Global Forecasting Models: Time series with different lengths and different exogenous variables for more information.</p> <p> \u26a0 Warning </p> <p>When using <code>'ordinal_category'</code> a new categorical column is created (<code>_level_skforecast</code>). Therefore, the regressor must be able to handle categorical variables, otherwise use <code>'ordinal'</code> or <code>'onehot'</code> encoding.</p> <p> HistGradientBoostingRegressor </p> <pre>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")\n</pre> <p> LightGBM </p> <p>LGBMRegressor automatically handles categorical variables.</p> <p> XGBoost</p> <pre>XGBRegressor(enable_categorical=True)\n</pre> In\u00a0[27]: Copied! <pre># Ordinal_category encoding\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal_category'\n             )\n\nX, y = forecaster.create_train_X_y(series=data_train)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['_level_skforecast'].value_counts())\n</pre> # Ordinal_category encoding # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal_category'              )  X, y = forecaster.create_train_X_y(series=data_train)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['_level_skforecast'].value_counts()) lag_1 lag_2 lag_3 _level_skforecast date 2012-01-04 27.549099 22.777826 8.253175 0 2012-01-05 25.895533 27.549099 22.777826 0 2012-01-06 21.379238 25.895533 27.549099 0 <pre>\nlag_1                 float64\nlag_2                 float64\nlag_3                 float64\n_level_skforecast    category\ndtype: object\n\n_level_skforecast\n0    924\n1    924\n2    924\nName: count, dtype: int64\n</pre> In\u00a0[28]: Copied! <pre># Ordinal encoding\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nX, y = forecaster.create_train_X_y(series=data_train)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['_level_skforecast'].value_counts())\n</pre> # Ordinal encoding # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  X, y = forecaster.create_train_X_y(series=data_train)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['_level_skforecast'].value_counts()) lag_1 lag_2 lag_3 _level_skforecast date 2012-01-04 27.549099 22.777826 8.253175 0 2012-01-05 25.895533 27.549099 22.777826 0 2012-01-06 21.379238 25.895533 27.549099 0 <pre>\nlag_1                float64\nlag_2                float64\nlag_3                float64\n_level_skforecast      int64\ndtype: object\n\n_level_skforecast\n0    924\n1    924\n2    924\nName: count, dtype: int64\n</pre> In\u00a0[29]: Copied! <pre># Onehot encoding (one column per series)\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'onehot'\n             )\n\nX, y = forecaster.create_train_X_y(series=data_train)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['item_1'].value_counts())\n</pre> # Onehot encoding (one column per series) # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'onehot'              )  X, y = forecaster.create_train_X_y(series=data_train)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['item_1'].value_counts()) lag_1 lag_2 lag_3 item_1 item_2 item_3 date 2012-01-04 27.549099 22.777826 8.253175 1 0 0 2012-01-05 25.895533 27.549099 22.777826 1 0 0 2012-01-06 21.379238 25.895533 27.549099 1 0 0 <pre>\nlag_1     float64\nlag_2     float64\nlag_3     float64\nitem_1      int64\nitem_2      int64\nitem_3      int64\ndtype: object\n\nitem_1\n0    1848\n1     924\nName: count, dtype: int64\n</pre> In\u00a0[30]: Copied! <pre># Onehot encoding (one column per series)\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = None\n             )\n\nX, y = forecaster.create_train_X_y(series=data_train)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\n</pre> # Onehot encoding (one column per series) # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = None              )  X, y = forecaster.create_train_X_y(series=data_train)  display(X.head(3)) print(\"\") print(X.dtypes) lag_1 lag_2 lag_3 date 2012-01-04 27.549099 22.777826 8.253175 2012-01-05 25.895533 27.549099 22.777826 2012-01-06 21.379238 25.895533 27.549099 <pre>\nlag_1    float64\nlag_2    float64\nlag_3    float64\ndtype: object\n</pre> <p> \u26a0 Warning </p> <p>Since the unknown series are encoded as NaN when the forecaster uses the <code>'ordinal_category'</code> or <code>'ordinal'</code> encoding, only regressors that can handle missing values can be used, otherwise an error will be raised.</p> In\u00a0[31]: Copied! <pre># Forecaster trainied with series item_1, item_2 and item_3\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nforecaster.fit(series=data_train)\nprint(f\"Series seen by during training: {forecaster.series_col_names}\")\n</pre> # Forecaster trainied with series item_1, item_2 and item_3 # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  forecaster.fit(series=data_train) print(f\"Series seen by during training: {forecaster.series_col_names}\") <pre>Series seen by during training: ['item_1', 'item_2', 'item_3']\n</pre> In\u00a0[32]: Copied! <pre># Forecasting a new series not seen in the training\n# ==============================================================================\nlast_window_item_4 = pd.DataFrame(\n    data    = [23.46, 22.3587, 29.348],\n    columns = ['item_4'],\n    index   = pd.date_range(start=\"2014-07-13\", periods=3, freq=\"D\"),\n)\n\nforecaster.predict(\n    levels            = 'item_4', \n    steps             = 3, \n    last_window       = last_window_item_4,\n    suppress_warnings = False\n)\n</pre> # Forecasting a new series not seen in the training # ============================================================================== last_window_item_4 = pd.DataFrame(     data    = [23.46, 22.3587, 29.348],     columns = ['item_4'],     index   = pd.date_range(start=\"2014-07-13\", periods=3, freq=\"D\"), )  forecaster.predict(     levels            = 'item_4',      steps             = 3,      last_window       = last_window_item_4,     suppress_warnings = False ) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\utils.py:778: UnknownLevelWarning: `levels` {'item_4'} were not included in training. Unknown levels are encoded as NaN, which may cause the prediction to fail if the regressor does not accept NaN values. \n You can suppress this warning using: warnings.simplefilter('ignore', category=UnknownLevelWarning)\n  warnings.warn(\n</pre> Out[32]: item_4 2014-07-16 24.351416 2014-07-17 25.779253 2014-07-18 25.637366 <p>If the forecaster <code>encoding</code> is <code>None</code>, then the forecaster does not take the series id into account, therefore it can predict unknown series without any problem as long as the last window of the series is provided.</p> In\u00a0[33]: Copied! <pre># Forecaster trainied with series item_1, item_2 and item_3 without encoding\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = None\n             )\n\nforecaster.fit(series=data_train)\n</pre> # Forecaster trainied with series item_1, item_2 and item_3 without encoding # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = None              )  forecaster.fit(series=data_train) In\u00a0[34]: Copied! <pre># Forecasting a new series not seen in the training\n# ==============================================================================\nforecaster.predict(\n    levels            = 'item_4', \n    steps             = 3, \n    last_window       = last_window_item_4,\n    suppress_warnings = False\n)\n</pre> # Forecasting a new series not seen in the training # ============================================================================== forecaster.predict(     levels            = 'item_4',      steps             = 3,      last_window       = last_window_item_4,     suppress_warnings = False ) Out[34]: item_4 2014-07-16 21.897883 2014-07-17 19.992323 2014-07-18 19.847593 <p>Forecasting intervals for unknown series are also possible. In this case, a random sample of residuals (<code>_unknown_level</code> key) is drawn from the residuals of the known series. The quality of the intervals depends on the similarity of the unknown series to the known series.</p> In\u00a0[35]: Copied! <pre># Forecasting intervals for an unknown series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nforecaster.fit(series=data_train)\n\n# Number of in-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.in_sample_residuals.items():\n    print(f\"Residuals for {k}: n={len(v)}\")\n</pre> # Forecasting intervals for an unknown series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  forecaster.fit(series=data_train)  # Number of in-sample residuals by bin # ============================================================================== for k, v in forecaster.in_sample_residuals.items():     print(f\"Residuals for {k}: n={len(v)}\") <pre>Residuals for item_1: n=924\nResiduals for item_2: n=924\nResiduals for item_3: n=924\nResiduals for _unknown_level: n=1000\n</pre> In\u00a0[36]: Copied! <pre># Forecasting intervals for an unknown series\n# ==============================================================================\nforecaster.predict_interval(\n    levels              = 'item_4',\n    steps               = 3,\n    last_window         = last_window_item_4,\n    in_sample_residuals = True,\n    suppress_warnings   = False\n)\n</pre> # Forecasting intervals for an unknown series # ============================================================================== forecaster.predict_interval(     levels              = 'item_4',     steps               = 3,     last_window         = last_window_item_4,     in_sample_residuals = True,     suppress_warnings   = False ) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\utils.py:778: UnknownLevelWarning: `levels` {'item_4'} were not included in training. Unknown levels are encoded as NaN, which may cause the prediction to fail if the regressor does not accept NaN values. \n You can suppress this warning using: warnings.simplefilter('ignore', category=UnknownLevelWarning)\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\utils.py:2820: UnknownLevelWarning: `levels` {'item_4'} are not present in `forecaster.in_sample_residuals`, most likely because they were not present in the training data. A random sample of the residuals from other levels will be used. This can lead to inaccurate intervals for the unknown levels. \n You can suppress this warning using: warnings.simplefilter('ignore', category=UnknownLevelWarning)\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\utils.py:778: UnknownLevelWarning: `levels` {'item_4'} were not included in training. Unknown levels are encoded as NaN, which may cause the prediction to fail if the regressor does not accept NaN values. \n You can suppress this warning using: warnings.simplefilter('ignore', category=UnknownLevelWarning)\n  warnings.warn(\n</pre> Out[36]: item_4 item_4_lower_bound item_4_upper_bound 2014-07-16 24.351416 21.079711 28.337472 2014-07-17 25.779253 18.261362 28.758783 2014-07-18 25.637366 18.515235 28.943720 <p>For the use of out-of-sample residuals (<code>in_sample_residuals = False</code>), the user can provide the residuals using the <code>set_out_sample_residuals</code> method and a random sample of residuals will be drawn to predict the unknown series.</p> In\u00a0[37]: Copied! <pre># Weights in Multi-Series\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = custom_weights,\n                 series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.}  # Same as {'item_2': 2.}\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=24).head(3)\n</pre> # Weights in Multi-Series # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = custom_weights,                  series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.}  # Same as {'item_2': 2.}              )  forecaster.fit(series=data_train) forecaster.predict(steps=24).head(3) Out[37]: item_1 item_2 item_3 2014-07-16 26.073036 10.891763 12.142271 2014-07-17 26.457289 10.728702 10.170189 2014-07-18 25.811176 10.730621 12.808785 <p> \u26a0 Warning </p> <p>The <code>weight_func</code> and <code>series_weights</code> arguments will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>. If <code>weight_func</code> is a <code>dict</code>, it will be a <code>dict</code> of the form <code>{'series_column_name': source_code_weight_func}</code> .</p> In\u00a0[38]: Copied! <pre># Source code weight function\n# ==============================================================================\nprint(forecaster.source_code_weight_func)\n</pre> # Source code weight function # ============================================================================== print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[39]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 24,\n                 differentiation = 1\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 24,                  differentiation = 1              )  forecaster.fit(series=data_train) forecaster Out[39]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal_category \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: 1 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [\"'item_1': ['2012-01-01', '2014-07-15']\", \"'item_2': ['2012-01-01', '2014-07-15']\", \"'item_3': ['2012-01-01', '2014-07-15']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-08-26 13:00:53 \nLast fit date: 2024-08-26 13:00:53 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> In\u00a0[40]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=24)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=24) predictions.head(3) Out[40]: item_1 item_2 item_3 2014-07-16 26.415599 9.382244 9.197428 2014-07-17 26.542300 9.863923 8.721811 2014-07-18 26.638615 10.387266 11.806588 <p> \ud83d\udca1 Tip </p> <p>To learn more about metrics, visit our user guide: Time Series Forecasting Metrics.</p> In\u00a0[41]: Copied! <pre># Grid search Multi-Series with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [24, 48]\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster         = forecaster,\n              series             = data,\n              lags_grid          = lags_grid,\n              param_grid         = param_grid,\n              steps              = 24,\n              metric             = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n              initial_train_size = len(data_train),\n              fixed_train_size   = True,\n              levels             = None,\n              exog               = None,\n              refit              = True,\n              return_best        = False,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search Multi-Series with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )  def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [24, 48] param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  results = grid_search_forecaster_multiseries(               forecaster         = forecaster,               series             = data,               lags_grid          = lags_grid,               param_grid         = param_grid,               steps              = 24,               metric             = [mean_absolute_error, custom_metric, 'mean_squared_error'],               aggregate_metric   = ['weighted_average', 'average', 'pooling'],               initial_train_size = len(data_train),               fixed_train_size   = True,               levels             = None,               exog               = None,               refit              = True,               return_best        = False,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>8 models compared for 3 level(s). Number of iterations: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> Out[41]: levels lags lags_label params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling custom_metric__weighted_average custom_metric__average custom_metric__pooling mean_squared_error__weighted_average mean_squared_error__average mean_squared_error__pooling max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 2.288448 2.288448 2.288448 2.314922 2.314922 2.314922 9.462712 9.462712 9.462712 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 2.357734 2.357734 2.357734 2.306112 2.306112 2.306112 10.121310 10.121310 10.121310 7 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 2.377537 2.377537 2.377537 2.302790 2.302790 2.302790 10.179578 10.179578 10.179578 3 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 2.474797 2.474797 2.474797 2.365732 2.365732 2.365732 10.708651 10.708651 10.708651 3 20 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 3.088904 3.088904 3.088904 2.642092 2.642092 2.642092 14.761587 14.761587 14.761587 7 10 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 3.134955 3.134955 3.134955 2.565650 2.565650 2.565650 15.509249 15.509249 15.509249 7 10 6 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 3.208173 3.208173 3.208173 2.600285 2.600285 2.600285 15.870433 15.870433 15.870433 3 10 7 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 3.240515 3.240515 3.240515 2.638598 2.638598 2.638598 16.016639 16.016639 16.016639 3 10 In\u00a0[42]: Copied! <pre># Create and fit a Forecaster Multi-Series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 3,\n                 encoding           = 'ordinal',\n                 dropna_from_series = False,\n             )\n</pre> # Create and fit a Forecaster Multi-Series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 3,                  encoding           = 'ordinal',                  dropna_from_series = False,              ) In\u00a0[43]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) X_train.head() Out[43]: lag_1 lag_2 lag_3 _level_skforecast date 2012-01-04 27.549099 22.777826 8.253175 0 2012-01-05 25.895533 27.549099 22.777826 0 2012-01-06 21.379238 25.895533 27.549099 0 2012-01-07 21.106643 21.379238 25.895533 0 2012-01-08 20.533871 21.106643 21.379238 0 In\u00a0[44]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[44]: <pre>date\n2012-01-04    25.895533\n2012-01-05    21.379238\n2012-01-06    21.106643\n2012-01-07    20.533871\n2012-01-08    20.069327\nName: y, dtype: float64</pre> In\u00a0[45]: Copied! <pre># Fit the forecaster\n# ==============================================================================\nforecaster.fit(series=data_train)\n</pre> # Fit the forecaster # ============================================================================== forecaster.fit(series=data_train) In\u00a0[46]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict_dict = forecaster.create_predict_X(steps=5)\n\n# Check 'item_1' matrix\nX_predict = X_predict_dict['item_1']\nX_predict.head()\n</pre> # Create input matrix for predict method # ============================================================================== X_predict_dict = forecaster.create_predict_X(steps=5)  # Check 'item_1' matrix X_predict = X_predict_dict['item_1'] X_predict.head() Out[46]: lag_1 lag_2 lag_3 _level_skforecast 2014-07-16 25.980745 23.307307 22.609388 0.0 2014-07-17 25.778936 25.980745 23.307307 0.0 2014-07-18 24.965677 25.778936 25.980745 0.0 2014-07-19 23.929113 24.965677 25.778936 0.0 2014-07-20 23.458191 23.929113 24.965677 0.0"},{"location":"user_guides/independent-multi-time-series-forecasting.html#global-forecasting-models-independent-multi-series-forecasting","title":"Global Forecasting Models: Independent multi-series forecasting\u00b6","text":"<p>Univariate time series forecasting models a single time series as a linear or nonlinear combination of its lags, using past values of the series to predict its future. Global forecasting, involves building a single predictive model that considers all time series simultaneously. It attempts to capture the core patterns that govern the series, thereby mitigating the potential noise that each series might introduce. This approach is computationally efficient, easy to maintain, and can yield more robust generalizations across time series.</p> <p>In independent multi-series forecasting a single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p> Internal Forecaster transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied, with the only difference being that the series name for which to estimate the predictions needs to be indicated.</p> <p> Diagram of recursive forecasting with multiple independent time series. </p> <p>Using the <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes, it is possible to easily build machine learning models for independent multi-series forecasting.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#forecasterautoregmultiseries","title":"ForecasterAutoregMultiSeries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#backtesting-multiple-series","title":"Backtesting multiple series\u00b6","text":"<p>As in the <code>predict</code> method, the <code>levels</code> at which backtesting is performed must be indicated. The argument can also be set to <code>None</code> to perform backtesting at all levels. In addition to the individual metric(s) for each series, the aggregated value is calculated using the following methods:</p> <ul> <li><p>average: the average (arithmetic mean) of all levels.</p> </li> <li><p>weighted_average: the average of the metrics weighted by the number of predicted values of each level.</p> </li> <li><p>pooling: the values of all levels are pooled and then the metric is calculated.</p> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#hyperparameter-search-and-lags-selection","title":"Hyperparameter search and lags selection\u00b6","text":"<p>The <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> functions in the <code>model_selection_multiseries</code> module allow for lags and hyperparameter optimization. It is performed using the backtesting strategy for validation as in other Forecasters, see the user guide here, except for the <code>levels</code> argument:</p> <p><code>levels</code>: level(s) at which the forecaster is optimized, for example:</p> <ul> <li><p>If <code>levels</code> is a list, the function will search for the lags and hyperparameters that minimize the aggregated error of the predictions of the selected time series. The available aggregation methods are the same as for backtesting (<code>average</code>, <code>weighted_average</code>, <code>pooling</code>). If the <code>aggregate_metric</code> argument is a list, all aggregation methods will be calculated for each metric.</p> </li> <li><p>If <code>levels = None</code>, the function will search for the lags and hyperparameters that minimize the aggregated error of the predictions of all time series.</p> </li> <li><p>If <code>levels = 'item_1'</code> (Same as <code>levels = ['item_1']</code>), the function will search for the lags and hyperparameters that minimize the error of the <code>item_1</code> predictions. The resulting metric will be the one calculated for <code>item_1</code>.</p> </li> </ul> <p>The following example shows how to use <code>grid_search_forecaster_multiseries</code> to find the best lags and model hyperparameters for all time series (all <code>levels</code>).</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#exogenous-variables-in-multi-series-forecasting","title":"Exogenous variables in multi-series forecasting\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-transformations","title":"Series transformations\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-with-different-lengths-and-different-exogenous-variables","title":"Series with different lengths and different exogenous variables\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-encoding-in-multi-series","title":"Series Encoding in multi-series\u00b6","text":"<p>When creating the training matrices, the <code>ForecasterAutoregMultiSeries</code> class encodes the series names to identify to which series the observations belong. Different encoding methods can be used:</p> <ul> <li><p><code>'ordinal_category'</code> (default), a single column (<code>_level_skforecast</code>) is created with integer values from 0 to n_series - 1. Then, the column is transformed into <code>pandas.category</code> dtype so that it can be used as a categorical variable.</p> </li> <li><p><code>'ordinal'</code>, a single column (<code>_level_skforecast</code>) is created with integer values from 0 to n_series - 1.</p> </li> <li><p><code>'onehot'</code>, a binary column is created for each series.</p> </li> <li><p><code>None</code>, no encoding is performed (no column is created).</p> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#forecasting-unknown-series","title":"Forecasting unknown series\u00b6","text":"<p><code>ForecasterAutoregMultiSeries</code> allows the prediction of unknown series (levels). If a series not seen during training is found during the prediction phase, the forecaster will encode the series according to the following rules:</p> <ul> <li><p>If <code>encoding</code> is <code>'onehot'</code>, all dummy columns are set to 0.</p> </li> <li><p>If <code>encoding</code> is <code>'ordinal_category'</code> or <code>'ordinal'</code>, the value of the column <code>_level_skforecast</code> is set to <code>NaN</code>.</p> </li> </ul> <p>Since the series was not present during training, the last window of the series must be provided when calling the <code>predict</code> method.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#weights-in-multi-series","title":"Weights in multi-series\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights:</p> <ul> <li><p><code>series_weights</code> controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.</p> </li> <li><p><code>weight_func</code> controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.</p> </li> </ul> <p>If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <p> Weights in multi-series. </p> <ul> <li><p><code>series_weights</code> is a dict of the form <code>{'series_column_name': float}</code>. If a series is used during <code>fit</code> and is not present in <code>series_weights</code>, it will have a weight of 1.</p> </li> <li><p><code>weight_func</code> is a function that defines the individual weights of each sample based on the index.</p> <ul> <li><p>If it is a <code>callable</code>, the same function will apply to all series.</p> </li> <li><p>If it is a <code>dict</code> of the form <code>{'series_column_name': callable}</code>, a different function can be used for each series. A weight of 1 is given to all series not present in <code>weight_func</code>.</p> </li> </ul> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#feature-selection-in-multi-series","title":"Feature selection in multi-series\u00b6","text":"<p>Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: to simplify models to make them easier to interpret, to reduce training time, to avoid the curse of dimensionality, to improve generalization by reducing overfitting (formally, variance reduction), and others.</p> <p>Skforecast is compatible with the feature selection methods implemented in the scikit-learn library. Visit Global Forecasting Models: Feature Selection for more information.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All four functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>,  <code>random_search_forecaster_multiseries</code>, and <code>bayesian_search_forecaster_multiseries</code>) allow the calculation of multiple metrics, including custom metrics, for each forecaster configuration if a list is provided.</p> <p>The best model is selected based on the first metric in the list and the first aggregation method (if more than one is indicated).</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#extract-prediction-matrices","title":"Extract prediction matrices\u00b6","text":"<p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p>"},{"location":"user_guides/input-data.html","title":"Input data","text":"<p>Skforecast only allows pandas series and dataframes as input (although numpy arrays are used internally for better performance). The type of pandas index is used to determine how the data is processed:</p> <ul> <li><p>If the index is not of type DatetimeIndex, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex but has no frequency, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex and has a frequency, it remains unchanged.</p> </li> </ul> <p> \u270e Note </p> <p>Although it is possible to use data without an associated date/time index, when using a pandas series with an associated frequency prediction results will have a more useful index.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\ndata[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y-%m-%d\")\ndata = data.set_index(\"date\")\ndata = data.asfreq(\"MS\")\ndata\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) data[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y-%m-%d\") data = data.set_index(\"date\") data = data.asfreq(\"MS\") data <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> Out[2]: y date 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 1991-11-01 0.502369 ... ... 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'])\n\n# Predictions\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags = 5              )  forecaster.fit(y=data['y'])  # Predictions # ============================================================================== forecaster.predict(steps=5) Out[3]: <pre>2008-07-01    0.861239\n2008-08-01    0.871102\n2008-09-01    0.835840\n2008-10-01    0.938713\n2008-11-01    1.004192\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Data without datetime index\n# ==============================================================================\ndata = data.reset_index(drop=True)\ndata\n</pre> # Data without datetime index # ============================================================================== data = data.reset_index(drop=True) data Out[4]: y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 ... ... 199 0.761822 200 0.649435 201 0.827887 202 0.816255 203 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[5]: Copied! <pre># Fit - Predict\n# ==============================================================================\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=5)\n</pre> # Fit - Predict # ============================================================================== forecaster.fit(y=data['y']) forecaster.predict(steps=5) Out[5]: <pre>204    0.861239\n205    0.871102\n206    0.835840\n207    0.938713\n208    1.004192\nName: pred, dtype: float64</pre>"},{"location":"user_guides/input-data.html#input-data","title":"Input data\u00b6","text":""},{"location":"user_guides/input-data.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/input-data.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-with-datetime-and-frequency-index","title":"Train and predict using input with datetime and frequency index\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-without-datetime-index","title":"Train and predict using input without datetime index\u00b6","text":""},{"location":"user_guides/metrics.html","title":"Metrics","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.metrics import mean_absolute_scaled_error\n</pre> # Libraries # ============================================================================== import pandas as pd from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_percentage_error from sklearn.metrics import mean_absolute_error from skforecast.metrics import mean_absolute_scaled_error In\u00a0[2]: Copied! <pre># Dowlnoad dataset\n# ==============================================================================\ndata = fetch_dataset('website_visits', raw=True)\ndata['date'] = pd.to_datetime(data['date'], format='%d/%m/%y')\ndata = data.set_index('date')\ndata = data.asfreq('1D')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Dowlnoad dataset # ============================================================================== data = fetch_dataset('website_visits', raw=True) data['date'] = pd.to_datetime(data['date'], format='%d/%m/%y') data = data.set_index('date') data = data.asfreq('1D') data = data.sort_index() data.head(3) <pre>website_visits\n--------------\nDaily visits to the cienciadedatos.net website registered with the google\nanalytics service.\nAmat Rodrigo, J. (2021). cienciadedatos.net (1.0.0). Zenodo.\nhttps://doi.org/10.5281/zenodo.10006330\nShape of the dataset: (421, 2)\n</pre> Out[2]: users date 2020-07-01 2324 2020-07-02 2201 2020-07-03 2146 In\u00a0[3]: Copied! <pre># Backtesting to generate predictions over the test set and calculate metrics\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                regressor= LGBMRegressor(random_state=123, verbose=-1),\n                lags= 7\n            )\n\nmetrics = [\n    'mean_absolute_error',\n    'mean_squared_error',\n    'mean_absolute_percentage_error',\n    'mean_absolute_scaled_error'\n]\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster         = forecaster,\n                                    y                  = data['users'],\n                                    steps              = 7,\n                                    initial_train_size = len(data) // 2,\n                                    metric             = metrics,\n                                    show_progress      = True\n                                )\n\nbacktest_metrics\n</pre> # Backtesting to generate predictions over the test set and calculate metrics # ============================================================================== forecaster = ForecasterAutoreg(                 regressor= LGBMRegressor(random_state=123, verbose=-1),                 lags= 7             )  metrics = [     'mean_absolute_error',     'mean_squared_error',     'mean_absolute_percentage_error',     'mean_absolute_scaled_error' ]  backtest_metrics, predictions = backtesting_forecaster(                                     forecaster         = forecaster,                                     y                  = data['users'],                                     steps              = 7,                                     initial_train_size = len(data) // 2,                                     metric             = metrics,                                     show_progress      = True                                 )  backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[3]: mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 298.298903 165698.930884 0.176136 0.704935 <p>It is possible to pass a list of functions instead of names.</p> In\u00a0[4]: Copied! <pre># Backtesting with callable metrics\n# ==============================================================================\nmetrics = [\n    mean_absolute_error,\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    mean_absolute_scaled_error\n]\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster         = forecaster,\n                                    y                  = data['users'],\n                                    steps              = 7,\n                                    initial_train_size = len(data) // 2,\n                                    metric             = metrics,\n                                    show_progress      = True\n                                )\n\nbacktest_metrics\n</pre> # Backtesting with callable metrics # ============================================================================== metrics = [     mean_absolute_error,     mean_squared_error,     mean_absolute_percentage_error,     mean_absolute_scaled_error ]  backtest_metrics, predictions = backtesting_forecaster(                                     forecaster         = forecaster,                                     y                  = data['users'],                                     steps              = 7,                                     initial_train_size = len(data) // 2,                                     metric             = metrics,                                     show_progress      = True                                 )  backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[4]: mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 298.298903 165698.930884 0.176136 0.704935 <p>Since functions can be passed as arguments, it is possible to define custom metrics. The custom metric must be a function that takes two arguments: the true values (<code>y_true</code>) and the predicted values (<code>y_pred</code>), and returns a single value representing the metric. Optionally, it can take additional arguments <code>y_train</code> with the training data.</p> In\u00a0[5]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error excluding weekends.\n    \"\"\"\n    mask = y_true.index.weekday &lt; 5\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n\n    return metric\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster         = forecaster,\n                                    y                  = data['users'],\n                                    steps              = 7,\n                                    initial_train_size = len(data) // 2,\n                                    metric             = custom_metric,\n                                    show_progress      = True\n                                )\n\nbacktest_metrics\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error excluding weekends.     \"\"\"     mask = y_true.index.weekday &lt; 5     metric = mean_absolute_error(y_true[mask], y_pred[mask])      return metric  backtest_metrics, predictions = backtesting_forecaster(                                     forecaster         = forecaster,                                     y                  = data['users'],                                     steps              = 7,                                     initial_train_size = len(data) // 2,                                     metric             = custom_metric,                                     show_progress      = True                                 )  backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[5]: custom_metric 0 298.375447 <p>When working with multiple time series, it is common to calculate metrics across all series to get an overall measure of performance, in addition to individual values. Skforecast provides 3 different aggregation methods:</p> <ul> <li><p>average: the average (arithmetic mean) of all levels.</p> </li> <li><p>weighted_average: the average of the metrics weighted by the number of predicted values of each level.</p> </li> <li><p>pooling: the values of all levels are pooled and then the metric is calculated.</p> </li> </ul> <p>To include the aggregated metrics in the output of <code>backtesting_forecaster_multiseries</code>, the <code>add_aggregated_metric</code> argument must be set to <code>True</code>.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries In\u00a0[7]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[7]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[8]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00   (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00   (n=170)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True)\n\nfor i, item in enumerate(['item_1', 'item_2', 'item_3'], start=0):\n    data_train[item].plot(label='train', ax=axes[i])\n    data_test[item].plot(label='test', ax=axes[i])\n    axes[i].set_ylabel('sales')\n    axes[i].set_title(f'Item {i + 1}')\n    if i == 2:  # Only set xlabel for the last plot\n        axes[i].set_xlabel('')\n    axes[i].legend(loc='upper left')\n\nfig.tight_layout()\nplt.show()\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True)  for i, item in enumerate(['item_1', 'item_2', 'item_3'], start=0):     data_train[item].plot(label='train', ax=axes[i])     data_test[item].plot(label='test', ax=axes[i])     axes[i].set_ylabel('sales')     axes[i].set_title(f'Item {i + 1}')     if i == 2:  # Only set xlabel for the last plot         axes[i].set_xlabel('')     axes[i].legend(loc='upper left')  fig.tight_layout() plt.show() In\u00a0[10]: Copied! <pre># Create and fit a Forecaster Multi-Series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n             )\n\nforecaster.fit(series=data_train)\n</pre> # Create and fit a Forecaster Multi-Series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',              )  forecaster.fit(series=data_train) In\u00a0[11]: Copied! <pre># Backtesting forecaster on multiple time series\n# ==============================================================================\nmetrics = [\n    'mean_absolute_error',\n    'mean_squared_error',\n    'mean_absolute_percentage_error',\n    'mean_absolute_scaled_error'\n]\nbacktest_metrics, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = data,\n    levels                = None,\n    steps                 = 24,\n    metric                = metrics,\n    add_aggregated_metric = True,\n    initial_train_size    = len(data_train),\n    verbose               = False,\n    show_progress         = True,\n    suppress_warnings     = False\n)\n\nprint(\"Backtest metrics\")\nbacktest_metrics\n</pre> # Backtesting forecaster on multiple time series # ============================================================================== metrics = [     'mean_absolute_error',     'mean_squared_error',     'mean_absolute_percentage_error',     'mean_absolute_scaled_error' ] backtest_metrics, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = data,     levels                = None,     steps                 = 24,     metric                = metrics,     add_aggregated_metric = True,     initial_train_size    = len(data_train),     verbose               = False,     show_progress         = True,     suppress_warnings     = False )  print(\"Backtest metrics\") backtest_metrics <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> Out[11]: levels mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 item_1 1.163837 2.748612 0.057589 0.751106 1 item_2 2.634182 12.358668 0.171335 1.110702 2 item_3 3.111351 16.096772 0.200664 0.835620 3 average 2.303123 10.401350 0.143196 0.899143 4 weighted_average 2.303123 10.401350 0.143196 0.899143 5 pooling 2.303123 10.401350 0.143196 0.903831 In\u00a0[12]: Copied! <pre># Backtest predictions\n# ==============================================================================\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtest predictions # ============================================================================== print(\"Backtest predictions\") backtest_predictions.head(4) <pre>Backtest predictions\n</pre> Out[12]: item_1 item_2 item_3 2014-07-16 25.906323 10.522491 12.034587 2014-07-17 25.807194 10.623789 10.503966 2014-07-18 25.127355 11.299802 12.206434 2014-07-19 23.902609 11.441606 12.618740 <p>When using the hyperparameter optimization functions, the metrics are computed for each level and then aggregated. The <code>aggregate_metric</code> argument determines the aggregation method(s) to be used. Some considerations when choosing the aggregation method are:</p> <ul> <li><p>'average' and 'weighted_average' will be different if the number of predicted values is different for each series (level).</p> </li> <li><p>When all series have the same number of predicted values, 'average', 'weighted_average' and 'pooling' are equivalent except for the case of scaled metrics (<code>mean_absolute_scaled_error</code>, <code>root_mean_squared_scaled_error</code>) as the naive forecast is calculated individually for each series.</p> </li> </ul> <p>In this example, <code>metric = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']</code> and <code>aggregate_metric = ['weighted_average', 'average', 'pooling']</code> so that the following metrics are calculated:</p> <ul> <li><p>mean_absolute_scaled_error__weighted_average, mean_absolute_scaled_error__average, mean_absolute_scaled_error__pooling</p> </li> <li><p>root_mean_squared_scaled_error__weighted_average, root_mean_squared_scaled_error__average, root_mean_squared_scaled_error__pooling</p> </li> </ul> <p>Since <code>return_best</code> is set to <code>True</code>, the best model is returned. The best model is the one that minimizes the metric obtained by the first specified metric and aggregation method, <code>mean_absolute_scaled_error__weighted_average</code>.</p> In\u00a0[13]: Copied! <pre># Grid search Multi-Series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal',\n             )\n\nlags_grid = [24, 48]\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\nlevels = ['item_1', 'item_2', 'item_3']\n\nresults = grid_search_forecaster_multiseries(\n              forecaster         = forecaster,\n              series             = data,\n              levels             = levels, # Same as levels=None\n              lags_grid          = lags_grid,\n              param_grid         = param_grid,\n              steps              = 24,\n              metric             = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error'],\n              aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n              initial_train_size = len(data_train),\n              refit              = False,\n              fixed_train_size   = False,\n              return_best        = True,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search Multi-Series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal',              )  lags_grid = [24, 48] param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  levels = ['item_1', 'item_2', 'item_3']  results = grid_search_forecaster_multiseries(               forecaster         = forecaster,               series             = data,               levels             = levels, # Same as levels=None               lags_grid          = lags_grid,               param_grid         = param_grid,               steps              = 24,               metric             = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error'],               aggregate_metric   = ['weighted_average', 'average', 'pooling'],               initial_train_size = len(data_train),               refit              = False,               fixed_train_size   = False,               return_best        = True,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>8 models compared for 3 level(s). Number of iterations: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n  Parameters: {'max_depth': 7, 'n_estimators': 20}\n  Backtesting metric: 0.8832159475915824\n  Levels: ['item_1', 'item_2', 'item_3']\n\n</pre> Out[13]: levels lags lags_label params mean_absolute_scaled_error__weighted_average mean_absolute_scaled_error__average mean_absolute_scaled_error__pooling root_mean_squared_scaled_error__weighted_average root_mean_squared_scaled_error__average root_mean_squared_scaled_error__pooling max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 0.883216 0.883216 0.879320 0.855980 0.855980 0.850240 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 0.954187 0.954187 0.933739 0.908577 0.908577 0.883190 3 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 0.961524 0.961524 0.954798 0.916509 0.916509 0.913176 7 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 0.983487 0.983487 0.964384 0.922949 0.922949 0.902035 3 20 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 1.275773 1.275773 1.196362 1.147386 1.147386 1.061439 7 10 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 1.356463 1.356463 1.282314 1.216944 1.216944 1.132679 3 10 6 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 1.357693 1.357693 1.273732 1.217487 1.217487 1.123731 3 10 7 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 1.396803 1.396803 1.307453 1.256564 1.256564 1.161849 7 10"},{"location":"user_guides/metrics.html#time-series-forecasting-metrics","title":"Time Series Forecasting Metrics\u00b6","text":"<p>In time series forecasting, evaluating the performance of predictive models is crucial to ensure accurate and reliable forecasts. Forecasting metrics are quantitative measures used to assess the accuracy and effectiveness of these models. They help in comparing different models, diagnosing errors, and making informed decisions based on forecast results.</p> <p>Skforecast is compatible with most of the regression metrics from scikit-learn and includes additional metrics specifically designed for time series forecasting.</p> <ul> <li><p>Mean Squared Error (\"mean_squared_error\")</p> </li> <li><p>Mean Absolute Error (\"mean_absolute_error\")</p> </li> <li><p>Mean Absolute Percentage Error (\"mean_absolute_percentage_error\")</p> </li> <li><p>Mean Squared Log Error (\"mean_squared_log_error\")</p> </li> <li><p>Median Absolute Error (\"median_absolute_error\")</p> </li> <li><p>Mean Absolute Scaled Error (\"mean_absolute_scaled_error\")</p> </li> <li><p>Root Mean Squared Scaled Error (\"root_mean_squared_scaled_error\")</p> </li> </ul> <p>In addition, Skforecast allows the user to define their own custom metrics. This can be done by creating a function that takes two arguments: the true values (<code>y_true</code>) and the predicted values (<code>y_pred</code>), and returns a single value representing the metric. The custom metric, optionally takes an additional argument <code>y_train</code> with the training data used to fit the model.</p> <p>In most cases, the metrics are calculated using the predictions generated in a backtesting process. For this, the <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> functions take the <code>metrics</code> argument to specify the metric(s) to be calculated in addition to the predictions.</p>"},{"location":"user_guides/metrics.html#metrics-for-single-series-forecasting","title":"Metrics for single series forecasting\u00b6","text":"<p>The following code shows how to calculate metrics when a single series is forecasted. The example uses the <code>backtesting_forecaster</code> function to generate predictions and calculate the metrics.</p>"},{"location":"user_guides/metrics.html#metrics-for-multiple-time-series-forecasting","title":"Metrics for multiple time series forecasting\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html","title":"Series with different lengths and different exogenous variables\"","text":"<p>When faced with a multi-series forecasting problem, it is common for the series to have varying lengths due to differences in the starting times of data recording. To address this scenario, the classes ForecasterAutoregMultiSeries and ForecasterAutoregMultiSeriesCustom allow the simultaneous modeling of time series of different lengths and using different exogenous variables.</p> <ul> <li>When the modeled series have different lengths, they must be stored in a Python dictionary. The keys of the dictionary are the names of the series and the values are the series themselves. All series must be of type <code>pandas.Series</code>, have a <code>pandas.DatetimeIndex</code> with the same frequency.</li> </ul> Series values Allowed <code>[NaN, NaN, NaN, NaN, 4, 5, 6, 7, 8, 9]</code> \u2714\ufe0f <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, NaN]</code> \u2714\ufe0f <code>[0, 1, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u2714\ufe0f <code>[NaN, NaN, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u2714\ufe0f <ul> <li>When different exogenous variables are used for each series, or if the exogenous variables are the same but have different values for each series, they must be stored in a dictionary. The keys of the dictionary are the names of the series and the values are the exogenous variables themselves. All exogenous variables must be of type <code>pandas.DataFrame</code> or <code>pandas.Series</code> and have a <code>pandas.DatetimeIndex</code> with the same frequency.</li> </ul> <p> \ud83d\udca1 Tip </p> <p>API Reference:</p> <ul> <li> ForecasterAutoregMultiSeries </li> <li> ForecasterAutoregMultiSeriesCustom </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\n\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.preprocessing import series_long_to_dict\nfrom skforecast.preprocessing import exog_long_to_dict\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from lightgbm import LGBMRegressor  from skforecast.plot import set_dark_theme from skforecast.preprocessing import series_long_to_dict from skforecast.preprocessing import exog_long_to_dict from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries <p>The data for this example is stored in \"long format\" in a single <code>DataFrame</code>. The <code>series_id</code> column identifies the series to which each observation belongs. The <code>timestamp</code> column contains the date of the observation, and the <code>value</code> column contains the value of the series at that date. Each time series is of a different length.</p> <p>The exogenous variables are stored in a separate <code>DataFrame</code>, also in \"long format\". The column <code>series_id</code> identifies the series to which each observation belongs. The column <code>timestamp</code> contains the date of the observation, and the remaining columns contain the values of the exogenous variables at that date.</p> In\u00a0[2]: Copied! <pre># Load time series of multiple lengths and exogenous variables\n# ==============================================================================\nseries = pd.read_csv(\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series.csv'\n)\nexog = pd.read_csv(\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series_exog.csv'\n)\n\nseries['timestamp'] = pd.to_datetime(series['timestamp'])\nexog['timestamp'] = pd.to_datetime(exog['timestamp'])\n\ndisplay(series.head())\nprint(\"\")\ndisplay(exog.head())\n</pre> # Load time series of multiple lengths and exogenous variables # ============================================================================== series = pd.read_csv(     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series.csv' ) exog = pd.read_csv(     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast-datasets/main/data/demo_multi_series_exog.csv' )  series['timestamp'] = pd.to_datetime(series['timestamp']) exog['timestamp'] = pd.to_datetime(exog['timestamp'])  display(series.head()) print(\"\") display(exog.head()) series_id timestamp value 0 id_1000 2016-01-01 1012.500694 1 id_1000 2016-01-02 1158.500099 2 id_1000 2016-01-03 983.000099 3 id_1000 2016-01-04 1675.750496 4 id_1000 2016-01-05 1586.250694 <pre>\n</pre> series_id timestamp sin_day_of_week cos_day_of_week air_temperature wind_speed 0 id_1000 2016-01-01 -0.433884 -0.900969 6.416639 4.040115 1 id_1000 2016-01-02 -0.974928 -0.222521 6.366474 4.530395 2 id_1000 2016-01-03 -0.781831 0.623490 6.555272 3.273064 3 id_1000 2016-01-04 0.000000 1.000000 6.704778 4.865404 4 id_1000 2016-01-05 0.781831 0.623490 2.392998 5.228913 <p>When series have different lengths, the data must be transformed into a dictionary. The keys of the dictionary are the names of the series and the values are the series themselves. To do this, the <code>series_long_to_dict</code> function is used, which takes the <code>DataFrame</code> in \"long format\" and returns a <code>dict</code> of <code>pandas Series</code>.</p> <p>Similarly, when the exogenous variables are different (values or variables) for each series, the data must be transformed into a dictionary. The keys of the dictionary are the names of the series and the values are the exogenous variables themselves. The <code>exog_long_to_dict</code> function is used, which takes the <code>DataFrame</code> in \"long format\" and returns a <code>dict</code> of exogenous variables (<code>pandas Series</code> or <code>pandas DataFrame</code>).</p> In\u00a0[3]: Copied! <pre># Transform series and exog to dictionaries\n# ==============================================================================\nseries_dict = series_long_to_dict(\n    data      = series,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    values    = 'value',\n    freq      = 'D'\n)\n\nexog_dict = exog_long_to_dict(\n    data      = exog,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    freq      = 'D'\n)\n</pre> # Transform series and exog to dictionaries # ============================================================================== series_dict = series_long_to_dict(     data      = series,     series_id = 'series_id',     index     = 'timestamp',     values    = 'value',     freq      = 'D' )  exog_dict = exog_long_to_dict(     data      = exog,     series_id = 'series_id',     index     = 'timestamp',     freq      = 'D' ) <p>Some exogenous variables are omitted for series 1 and 3 to illustrate that different exogenous variables can be used for each series.</p> In\u00a0[4]: Copied! <pre># Drop some exogenous variables for series 'id_1000' and 'id_1003'\n# ==============================================================================\nexog_dict['id_1000'] = exog_dict['id_1000'].drop(columns=['air_temperature', 'wind_speed'])\nexog_dict['id_1003'] = exog_dict['id_1003'].drop(columns=['cos_day_of_week'])\n</pre> # Drop some exogenous variables for series 'id_1000' and 'id_1003' # ============================================================================== exog_dict['id_1000'] = exog_dict['id_1000'].drop(columns=['air_temperature', 'wind_speed']) exog_dict['id_1003'] = exog_dict['id_1003'].drop(columns=['cos_day_of_week']) In\u00a0[5]: Copied! <pre># Partition data in train and test\n# ==============================================================================\nend_train = '2016-07-31 23:59:00'\n\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n</pre> # Partition data in train and test # ============================================================================== end_train = '2016-07-31 23:59:00'  series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()} In\u00a0[6]: Copied! <pre># Plot series\n# ==============================================================================\nset_dark_theme()\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nfig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)\n\nfor i, s in enumerate(series_dict.values()):\n    axs[i].plot(s, label=s.name, color=colors[i])\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train\n\nfig.suptitle('Series in `series_dict`', fontsize=15)\nplt.tight_layout()\n</pre> # Plot series # ============================================================================== set_dark_theme() colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] fig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)  for i, s in enumerate(series_dict.values()):     axs[i].plot(s, label=s.name, color=colors[i])     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train  fig.suptitle('Series in `series_dict`', fontsize=15) plt.tight_layout() In\u00a0[7]: Copied! <pre># Description of each partition\n# ==============================================================================\nfor k in series_dict.keys():\n    print(f\"{k}:\")\n    try:\n        print(\n            f\"\\tTrain: len={len(series_dict_train[k])}, {series_dict_train[k].index[0]}\"\n            f\" --- {series_dict_train[k].index[-1]}\"\n        )\n    except:\n        print(f\"\\tTrain: len=0\")\n    try:\n        print(\n            f\"\\tTest : len={len(series_dict_test[k])}, {series_dict_test[k].index[0]}\"\n            f\" --- {series_dict_test[k].index[-1]}\"\n        )\n    except:\n        print(f\"\\tTest : len=0\")\n</pre> # Description of each partition # ============================================================================== for k in series_dict.keys():     print(f\"{k}:\")     try:         print(             f\"\\tTrain: len={len(series_dict_train[k])}, {series_dict_train[k].index[0]}\"             f\" --- {series_dict_train[k].index[-1]}\"         )     except:         print(f\"\\tTrain: len=0\")     try:         print(             f\"\\tTest : len={len(series_dict_test[k])}, {series_dict_test[k].index[0]}\"             f\" --- {series_dict_test[k].index[-1]}\"         )     except:         print(f\"\\tTest : len=0\") <pre>id_1000:\n\tTrain: len=213, 2016-01-01 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1001:\n\tTrain: len=30, 2016-07-02 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1002:\n\tTrain: len=183, 2016-01-01 00:00:00 --- 2016-07-01 00:00:00\n\tTest : len=0\nid_1003:\n\tTrain: len=213, 2016-01-01 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1004:\n\tTrain: len=91, 2016-05-02 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=31, 2016-08-01 00:00:00 --- 2016-08-31 00:00:00\n</pre> In\u00a0[8]: Copied! <pre># Exogenous variables for each series\n# ==============================================================================\nfor k in series_dict.keys():\n    print(f\"{k}:\")\n    try:\n        print(f\"\\t{exog_dict[k].columns.to_list()}\")\n    except:\n        print(f\"\\tNo exogenous variables\")\n</pre> # Exogenous variables for each series # ============================================================================== for k in series_dict.keys():     print(f\"{k}:\")     try:         print(f\"\\t{exog_dict[k].columns.to_list()}\")     except:         print(f\"\\tNo exogenous variables\") <pre>id_1000:\n\t['sin_day_of_week', 'cos_day_of_week']\nid_1001:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\nid_1002:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\nid_1003:\n\t['sin_day_of_week', 'air_temperature', 'wind_speed']\nid_1004:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\n</pre> <p> \u26a0 Warning </p> <p>Starting with version <code>0.13.0</code>, the <code>ForecasterAutoregMultiSeries</code> class defaults to using <code>encoding='ordinal_category'</code> for encoding time series identifiers. This approach creates a new column (_level_skforecast) of type pandas <code>category</code>. Consequently, the regressors must be able to handle categorical variables. If the regressors do not support categorical variables, the user should set the encoding to <code>'ordinal'</code> or <code>'onehot'</code> for compatibility.</p> <p>Some examples of regressors that support categorical variables and how to enable them are:</p> <ul> <li> HistGradientBoostingRegressor: <code>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")</code> </li> <li> LightGBM: <code>LGBMRegressor</code> does not allow configuration of categorical features during initialization, but rather in its <code>fit</code> method. Therefore, use <code>fit_kwargs = {'categorical_feature':'auto'}</code>. This is the default behavior of <code>LGBMRegressor</code> if no indication is given.          </li> <li> XGBoost: <code>XGBRegressor(enable_categorical=True)</code> </li> </ul> In\u00a0[9]: Copied! <pre># Fit forecaster\n# ==============================================================================\nregressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5)\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = regressor, \n                 lags               = 14, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\nforecaster.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\nforecaster\n</pre> # Fit forecaster # ============================================================================== regressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5) forecaster = ForecasterAutoregMultiSeries(                  regressor          = regressor,                   lags               = 14,                   encoding           = \"ordinal\",                   dropna_from_series = False              )  forecaster.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True) forecaster Out[9]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(max_depth=5, random_state=123, verbose=-1) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal \nWindow size: 14 \nSeries levels (names): ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'dict'&gt; \nExogenous variables names: ['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed'] \nTraining range: 'id_1000': ['2016-01-01', '2016-07-31'], 'id_1001': ['2016-07-02', '2016-07-31'], 'id_1002': ['2016-01-01', '2016-07-01'], ... \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-07-29 18:05:00 \nLast fit date: 2024-07-29 18:05:00 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> <p>Only series whose last window of data ends at the same datetime index can be predicted together. If <code>levels = None</code>, series that do not reach the maximum index are excluded from prediction. In this example, series <code>'id_1002'</code> is excluded because it does not reach the maximum index.</p> In\u00a0[10]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=5, exog=exog_dict_test, suppress_warnings=True)\npredictions\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=5, exog=exog_dict_test, suppress_warnings=True) predictions Out[10]: id_1000 id_1001 id_1003 id_1004 2016-08-01 1469.450739 3021.628418 2949.305530 7408.161532 2016-08-02 1461.934467 3308.658430 2657.179559 8495.680202 2016-08-03 1417.709761 3095.373322 2148.531262 8795.777535 2016-08-04 1341.888287 3395.436028 1995.329983 8863.318002 2016-08-05 1330.593757 2802.032726 2024.038548 8881.080283 In\u00a0[11]: Copied! <pre># Sample data with interspersed NaNs\n# ==============================================================================\nseries_dict_nan = {\n    'id_1000': series_dict['id_1000'].copy(),\n    'id_1003': series_dict['id_1003'].copy()\n}\n\n# Create NaNs\nseries_dict_nan['id_1000'].loc['2016-03-01':'2016-04-01',] = np.nan\nseries_dict_nan['id_1000'].loc['2016-05-01':'2016-05-07',] = np.nan\nseries_dict_nan['id_1003'].loc['2016-07-01',] = np.nan\n\n# Plot series\n# ==============================================================================\nfig, axs = plt.subplots(2, 1, figsize=(8, 2.5), sharex=True)\n\nfor i, s in enumerate(series_dict_nan.values()):\n    axs[i].plot(s, label=s.name, color=colors[i])\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train\n\nfig.suptitle('Series in `series_dict_nan`', fontsize=15)\nplt.tight_layout()\n</pre> # Sample data with interspersed NaNs # ============================================================================== series_dict_nan = {     'id_1000': series_dict['id_1000'].copy(),     'id_1003': series_dict['id_1003'].copy() }  # Create NaNs series_dict_nan['id_1000'].loc['2016-03-01':'2016-04-01',] = np.nan series_dict_nan['id_1000'].loc['2016-05-01':'2016-05-07',] = np.nan series_dict_nan['id_1003'].loc['2016-07-01',] = np.nan  # Plot series # ============================================================================== fig, axs = plt.subplots(2, 1, figsize=(8, 2.5), sharex=True)  for i, s in enumerate(series_dict_nan.values()):     axs[i].plot(s, label=s.name, color=colors[i])     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train  fig.suptitle('Series in `series_dict_nan`', fontsize=15) plt.tight_layout() <p>When <code>dropna_from_series = False</code>, the NaNs in <code>X_train</code> are kept and the user is warned. This is useful if the user wants to keep the NaNs in the series and use a regressor that can handle them.</p> In\u00a0[12]: Copied! <pre># Create Matrices, dropna_from_series = False\n# ==============================================================================\nregressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5)\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = regressor, \n                 lags               = 3, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\nX, y = forecaster.create_train_X_y(series=series_dict_nan)\n\ndisplay(X.head(3))\nprint(\"Observations per series:\")\nprint(X['_level_skforecast'].value_counts())\nprint(\"\")\nprint(\"NaNs per series:\")\nprint(X.isnull().sum())\n</pre> # Create Matrices, dropna_from_series = False # ============================================================================== regressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5) forecaster = ForecasterAutoregMultiSeries(                  regressor          = regressor,                   lags               = 3,                   encoding           = \"ordinal\",                   dropna_from_series = False              )  X, y = forecaster.create_train_X_y(series=series_dict_nan)  display(X.head(3)) print(\"Observations per series:\") print(X['_level_skforecast'].value_counts()) print(\"\") print(\"NaNs per series:\") print(X.isnull().sum()) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:834: MissingValuesWarning: NaNs detected in `y_train`. They have been dropped because the target variable cannot have NaN values. Same rows have been dropped from `X_train` to maintain alignment. This is caused by series with interspersed NaNs. \n You can suppress this warning using: warnings.simplefilter('ignore', category=MissingValuesWarning)\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:856: MissingValuesWarning: NaNs detected in `X_train`. Some regressors do not allow NaN values during training. If you want to drop them, set `forecaster.dropna_from_series = True`. \n You can suppress this warning using: warnings.simplefilter('ignore', category=MissingValuesWarning)\n  warnings.warn(\n</pre> lag_1 lag_2 lag_3 _level_skforecast 2016-01-04 983.000099 1158.500099 1012.500694 0 2016-01-05 1675.750496 983.000099 1158.500099 0 2016-01-06 1586.250694 1675.750496 983.000099 0 <pre>Observations per series:\n_level_skforecast\n0    324\n1    216\nName: count, dtype: int64\n\nNaNs per series:\nlag_1                 5\nlag_2                 9\nlag_3                13\n_level_skforecast     0\ndtype: int64\n</pre> <p>When <code>dropna_from_series = True</code>, the NaNs in <code>X_train</code> are removed and the user is warned. This is useful if the user chooses a regressor that cannot handle missing values.</p> In\u00a0[13]: Copied! <pre># Create Matrices, dropna_from_series = False\n# ==============================================================================\nregressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5)\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = regressor, \n                 lags               = 3, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = True\n             )\n\nX, y = forecaster.create_train_X_y(series=series_dict_nan)\n\ndisplay(X.head(3))\nprint(\"Observations per series:\")\nprint(X['_level_skforecast'].value_counts())\nprint(\"\")\nprint(\"NaNs per series:\")\nprint(X.isnull().sum())\n</pre> # Create Matrices, dropna_from_series = False # ============================================================================== regressor = LGBMRegressor(random_state=123, verbose=-1, max_depth=5) forecaster = ForecasterAutoregMultiSeries(                  regressor          = regressor,                   lags               = 3,                   encoding           = \"ordinal\",                   dropna_from_series = True              )  X, y = forecaster.create_train_X_y(series=series_dict_nan)  display(X.head(3)) print(\"Observations per series:\") print(X['_level_skforecast'].value_counts()) print(\"\") print(\"NaNs per series:\") print(X.isnull().sum()) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:834: MissingValuesWarning: NaNs detected in `y_train`. They have been dropped because the target variable cannot have NaN values. Same rows have been dropped from `X_train` to maintain alignment. This is caused by series with interspersed NaNs. \n You can suppress this warning using: warnings.simplefilter('ignore', category=MissingValuesWarning)\n  warnings.warn(\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11\\Lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:847: MissingValuesWarning: NaNs detected in `X_train`. They have been dropped. If you want to keep them, set `forecaster.dropna_from_series = False`. Same rows have been removed from `y_train` to maintain alignment. This caused by series with interspersed NaNs. \n You can suppress this warning using: warnings.simplefilter('ignore', category=MissingValuesWarning)\n  warnings.warn(\n</pre> lag_1 lag_2 lag_3 _level_skforecast 2016-01-04 983.000099 1158.500099 1012.500694 0 2016-01-05 1675.750496 983.000099 1158.500099 0 2016-01-06 1586.250694 1675.750496 983.000099 0 <pre>Observations per series:\n_level_skforecast\n0    318\n1    207\nName: count, dtype: int64\n\nNaNs per series:\nlag_1                0\nlag_2                0\nlag_3                0\n_level_skforecast    0\ndtype: int64\n</pre> <p>During the training process, the warnings can be suppressed by setting <code>suppress_warnings = True</code>.</p> In\u00a0[14]: Copied! <pre># Suppress warnings during fit method\n# ==============================================================================\nforecaster.fit(series=series_dict_nan, suppress_warnings=True)\nforecaster\n</pre> # Suppress warnings during fit method # ============================================================================== forecaster.fit(series=series_dict_nan, suppress_warnings=True) forecaster Out[14]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: LGBMRegressor(max_depth=5, random_state=123, verbose=-1) \nLags: [1 2 3] \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal \nWindow size: 3 \nSeries levels (names): ['id_1000', 'id_1003'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [\"'id_1000': ['2016-01-01', '2016-12-31']\", \"'id_1003': ['2016-01-01', '2016-12-31']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-07-29 18:05:01 \nLast fit date: 2024-07-29 18:05:01 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[15]: Copied! <pre># Backtesting\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = regressor, \n                 lags               = 14, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = series_dict,\n    exog                  = exog_dict,\n    levels                = None,\n    steps                 = 24,\n    metric                = \"mean_absolute_error\",\n    add_aggregated_metric = True,\n    initial_train_size    = len(series_dict_train[\"id_1000\"]),\n    fixed_train_size      = True,\n    gap                   = 0,\n    allow_incomplete_fold = True,\n    refit                 = False,\n    n_jobs                =\"auto\",\n    verbose               = True,\n    show_progress         = True,\n    suppress_warnings     = True\n)\n\ndisplay(metrics_levels)\nprint(\"\")\ndisplay(backtest_predictions)\n</pre> # Backtesting # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = regressor,                   lags               = 14,                   encoding           = \"ordinal\",                   dropna_from_series = False              )  metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = series_dict,     exog                  = exog_dict,     levels                = None,     steps                 = 24,     metric                = \"mean_absolute_error\",     add_aggregated_metric = True,     initial_train_size    = len(series_dict_train[\"id_1000\"]),     fixed_train_size      = True,     gap                   = 0,     allow_incomplete_fold = True,     refit                 = False,     n_jobs                =\"auto\",     verbose               = True,     show_progress         = True,     suppress_warnings     = True )  display(metrics_levels) print(\"\") display(backtest_predictions) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 213\nNumber of observations used for backtesting: 153\n    Number of folds: 7\n    Number skipped folds: 0 \n    Number of steps per fold: 24\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 9 observations.\n\nFold: 0\n    Training:   2016-01-01 00:00:00 -- 2016-07-31 00:00:00  (n=213)\n    Validation: 2016-08-01 00:00:00 -- 2016-08-24 00:00:00  (n=24)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2016-08-25 00:00:00 -- 2016-09-17 00:00:00  (n=24)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2016-09-18 00:00:00 -- 2016-10-11 00:00:00  (n=24)\nFold: 3\n    Training:   No training in this fold\n    Validation: 2016-10-12 00:00:00 -- 2016-11-04 00:00:00  (n=24)\nFold: 4\n    Training:   No training in this fold\n    Validation: 2016-11-05 00:00:00 -- 2016-11-28 00:00:00  (n=24)\nFold: 5\n    Training:   No training in this fold\n    Validation: 2016-11-29 00:00:00 -- 2016-12-22 00:00:00  (n=24)\nFold: 6\n    Training:   No training in this fold\n    Validation: 2016-12-23 00:00:00 -- 2016-12-31 00:00:00  (n=9)\n\n</pre> <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> levels mean_absolute_error 0 id_1000 170.087276 1 id_1001 963.948336 2 id_1002 NaN 3 id_1003 329.189269 4 id_1004 787.969280 5 average 562.798540 6 weighted_average 537.816877 7 pooling 537.816877 <pre>\n</pre> id_1000 id_1001 id_1003 id_1004 2016-08-01 1469.450739 3021.628418 2949.305530 7408.161532 2016-08-02 1461.934467 3308.658430 2657.179559 8495.680202 2016-08-03 1417.709761 3095.373322 2148.531262 8795.777535 2016-08-04 1341.888287 3395.436028 1995.329983 8863.318002 2016-08-05 1330.593757 2802.032726 2024.038548 8881.080283 ... ... ... ... ... 2016-12-27 1655.064790 1049.095377 1799.599477 NaN 2016-12-28 1550.616672 1002.617817 1807.259072 NaN 2016-12-29 1463.626212 1061.122584 1810.838622 NaN 2016-12-30 1501.660276 1046.970317 1768.303003 NaN 2016-12-31 1347.881411 961.760005 1735.950240 NaN <p>153 rows \u00d7 4 columns</p> <p>Note that if a series has no observations in the test set, the backtesting process will not return any predictions for that series and the metric will be <code>NaN</code>, as happened with series <code>'id_1002'</code>.</p> In\u00a0[16]: Copied! <pre># Plot backtesting predictions\n# ==============================================================================\nfig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)\n\nfor i, s in enumerate(series_dict.keys()):\n    axs[i].plot(series_dict[s], label=series_dict[s].name, color=colors[i])\n    axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train\n    try:\n        axs[i].plot(backtest_predictions[s], label='prediction', color=\"white\")\n    except:\n        pass\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n\nfig.suptitle('Backtest Predictions', fontsize=15)\nplt.tight_layout()\n</pre> # Plot backtesting predictions # ============================================================================== fig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)  for i, s in enumerate(series_dict.keys()):     axs[i].plot(series_dict[s], label=series_dict[s].name, color=colors[i])     axs[i].axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1) # End train     try:         axs[i].plot(backtest_predictions[s], label='prediction', color=\"white\")     except:         pass     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)  fig.suptitle('Backtest Predictions', fontsize=15) plt.tight_layout() In\u00a0[17]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 14,\n                 encoding  = 'ordinal'\n             )\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'             : trial.suggest_categorical('lags', [7, 14]),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 10, 30),\n        'max_depth'        : trial.suggest_int('max_depth', 3, 7)\n    }\n\n    return search_space\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = series_dict,\n    exog                  = exog_dict,\n    levels                = None,\n    search_space          = search_space,\n    steps                 = 24,\n    metric                = 'mean_absolute_error',\n    refit                 = False,\n    initial_train_size    = len(series_dict_train[\"id_1000\"]),\n    fixed_train_size      = False,\n    n_trials              = 10,\n    random_state          = 123,\n    return_best           = False,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True,\n    suppress_warnings     = True\n)\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 14,                  encoding  = 'ordinal'              )  # Search space def search_space(trial):     search_space  = {         'lags'             : trial.suggest_categorical('lags', [7, 14]),         'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 10, 30),         'max_depth'        : trial.suggest_int('max_depth', 3, 7)     }      return search_space  results, best_trial = bayesian_search_forecaster_multiseries(     forecaster            = forecaster,     series                = series_dict,     exog                  = exog_dict,     levels                = None,     search_space          = search_space,     steps                 = 24,     metric                = 'mean_absolute_error',     refit                 = False,     initial_train_size    = len(series_dict_train[\"id_1000\"]),     fixed_train_size      = False,     n_trials              = 10,     random_state          = 123,     return_best           = False,     n_jobs                = 'auto',     verbose               = False,     show_progress         = True,     suppress_warnings     = True )  results.head(4) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[17]: levels lags params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling min_samples_leaf max_depth 0 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 29, 'max_depth': 5} 534.931524 502.669593 502.669593 29 5 1 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 18, 'max_depth': 4} 543.922801 552.114034 552.114034 18 4 2 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 25, 'max_depth': 6} 550.004326 535.677251 535.677251 25 6 3 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 11, 'max_depth': 5} 551.347091 552.172102 552.172102 11 5"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#global-forecasting-models-time-series-with-different-lengths-and-different-exogenous-variables","title":"Global Forecasting Models: Time series with different lengths and different exogenous variables\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#train-and-predict","title":"Train and predict\u00b6","text":"<p>The <code>fit</code> method is used to train the model, it is passed the dictionary of series and the dictionary of exogenous variables where the keys of each dictionary are the names of the series.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#missing-values-in-the-series","title":"Missing values in the series\u00b6","text":"<p>When working with time series of different lengths, it is common for some series to have missing values. As not all the regressors allow missing values, the argument <code>dropna_from_series</code> can be used to remove the missing values from the training matrices.</p> <ul> <li><p>If <code>False</code>, leave NaNs in <code>X_train</code> and warn the user. (default)</p> </li> <li><p>If <code>True</code>, drop NaNs in <code>X_train</code> and same rows in <code>y_train</code> and warn the user.</p> </li> </ul> <p><code>y_train</code> NaNs (and same rows from <code>X_train</code>) are always dropped because the target variable cannot have NaN values.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#backtesting","title":"Backtesting\u00b6","text":"<p>As in the <code>predict</code> method, the <code>levels</code> at which backtesting is performed must be indicated.</p> <p>When series have different lengths, the backtesting process only returns predictions for the date-times that are present in the series.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>The <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> functions in the <code>model_selection_multiseries</code> module allow for lags and hyperparameter optimization. It is performed using the backtesting strategy for validation as in other Forecasters.</p> <p>When using series of different lengths, the metric is calculated as a weighted average of the backtesting metrics of the selected series (levels) and their number of predictions.</p> <p>More information about: Hyperparameter tuning and lags selection Multi-Series.</p>"},{"location":"user_guides/plotting.html","title":"Plotting","text":"<p>Analyzing the residuals (errors) of predictions is useful to understand the behavior of a forecaster. The function <code>skforecast.plot.plot_residuals</code> creates 3 plots:</p> <ul> <li><p>A time-ordered plot of residual values</p> </li> <li><p>A distribution plot that showcases the distribution of residuals</p> </li> <li><p>A plot showcasing the autocorrelation of residuals</p> </li> </ul> <p>By examining the residual values over time, you can determine whether there is a pattern in the errors made by the forecast model. The distribution plot helps you understand whether the residuals are normally distributed, and the autocorrelation plot helps you identify whether there are any dependencies or relationships between the residuals.</p> In\u00a0[10]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import plot_residuals\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.plot import plot_residuals from skforecast.datasets import fetch_dataset from skforecast.plot import set_dark_theme In\u00a0[11]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\n# Plot data\n# ==============================================================================\nset_dark_theme()\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  # Plot data # ============================================================================== set_dark_theme() fig, ax=plt.subplots(figsize=(7, 3)) data.plot(ax=ax); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[12]: Copied! <pre># Train and backtest forecaster\n# ==============================================================================\nn_backtest = 36*3\ndata_train = data[:-n_backtest]\ndata_test  = data[-n_backtest:]\n\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(),\n                 lags      = 5 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.y,\n                        initial_train_size = len(data_train),\n                        steps              = 36,\n                        metric             = 'mean_squared_error',\n                        verbose            = True\n                     )\n \npredictions.head()\n</pre> # Train and backtest forecaster # ============================================================================== n_backtest = 36*3 data_train = data[:-n_backtest] data_test  = data[-n_backtest:]  forecaster = ForecasterAutoreg(                  regressor = Ridge(),                  lags      = 5               )  metric, predictions = backtesting_forecaster(                         forecaster         = forecaster,                         y                  = data.y,                         initial_train_size = len(data_train),                         steps              = 36,                         metric             = 'mean_squared_error',                         verbose            = True                      )   predictions.head() <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 96\nNumber of observations used for backtesting: 108\n    Number of folds: 3\n    Number of steps per fold: 36\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 1999-07-01 00:00:00 -- 2002-06-01 00:00:00  (n=36)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2002-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=36)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2005-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=36)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[12]: pred 1999-07-01 0.667651 1999-08-01 0.655759 1999-09-01 0.652177 1999-10-01 0.641377 1999-11-01 0.635245 <p>The <code>plot_residuals</code> function can be used in two ways: with pre-calculated residuals or by passing the predicted and actual values of the series.</p> In\u00a0[13]: Copied! <pre># Plot residuals\n# ======================================================================================\nresiduals = predictions['pred'] - data_test['y']\n_ = plot_residuals(residuals=residuals, figsize=(7, 3.5))\n</pre> # Plot residuals # ====================================================================================== residuals = predictions['pred'] - data_test['y'] _ = plot_residuals(residuals=residuals, figsize=(7, 3.5)) In\u00a0[14]: Copied! <pre>_ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'], figsize=(7, 3.5))\n</pre> _ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'], figsize=(7, 3.5)) <p>It is possible to customize the plot by by either passing a pre-existing matplotlib figure object or using additional keyword arguments that are passed to <code>matplotlib.pyplot.figure()</code>.</p> In\u00a0[15]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.plot import plot_prediction_intervals\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.datasets import fetch_dataset from skforecast.plot import set_dark_theme from skforecast.plot import plot_prediction_intervals In\u00a0[16]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=True, verbose=False)\n\n# Data preparation\n# ==============================================================================\ndata = data.rename(columns={'fecha': 'date'})\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Split data into train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=True, verbose=False)  # Data preparation # ============================================================================== data = data.rename(columns={'fecha': 'date'}) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data = data.sort_index()  # Split data into train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:] print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Train dates : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[17]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(alpha=0.1, random_state=765),\n                 lags      = 15\n             )\nforecaster.fit(y=data_train['y'])\n\n# Prediction intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                  steps    = steps,\n                  interval = [1, 99],\n                  n_boot   = 500\n              )\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(alpha=0.1, random_state=765),                  lags      = 15              ) forecaster.fit(y=data_train['y'])  # Prediction intervals # ============================================================================== predictions = forecaster.predict_interval(                   steps    = steps,                   interval = [1, 99],                   n_boot   = 500               ) In\u00a0[18]: Copied! <pre># Plot forecasts with prediction intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    target_variable = \"y\",\n    title           = \"Real value vs predicted in test data\"\n)\n</pre> # Plot forecasts with prediction intervals # ============================================================================== plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     target_variable = \"y\",     title           = \"Real value vs predicted in test data\" ) <p>When training a Global Forecasting Model of type Dependent multi-series, it is useful to analyze the correlation between the lags of the different time series and the target series. The function <code>skforecast.plot.plot_correlation_lags</code> creates a heatmap that shows the correlation between the lags of the different time series.</p> In\u00a0[19]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import plot_multivariate_time_series_corr\nfrom skforecast.utils import multivariate_time_series_corr\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.datasets import fetch_dataset from skforecast.plot import plot_multivariate_time_series_corr from skforecast.utils import multivariate_time_series_corr In\u00a0[20]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='air_quality_valencia', raw=False, verbose=True)\ndata\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='air_quality_valencia', raw=False, verbose=True) data <pre>air_quality_valencia\n--------------------\nHourly measures of several air quimical pollutant (pm2.5, co, no, no2, pm10,\nnox, o3, so2) at Valencia city.\n Red de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250054-Val\u00e8ncia\n- Centre, https://mediambient.gva.es/es/web/calidad-ambiental/datos-historicos.\nShape of the dataset: (26304, 10)\n</pre> Out[20]: pm2.5 co no no2 pm10 nox o3 veloc. direc. so2 datetime 2019-01-01 00:00:00 19.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 8.0 2019-01-01 01:00:00 26.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 8.0 2019-01-01 02:00:00 31.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 8.0 2019-01-01 03:00:00 30.0 0.1 15.0 41.0 35.0 63.0 3.0 0.2 220.0 10.0 2019-01-01 04:00:00 30.0 0.1 16.0 39.0 36.0 63.0 3.0 0.4 221.0 11.0 ... ... ... ... ... ... ... ... ... ... ... 2021-12-31 19:00:00 31.0 0.1 20.0 36.0 34.0 66.0 13.0 0.1 87.0 3.0 2021-12-31 20:00:00 33.0 0.1 25.0 38.0 39.0 75.0 13.0 0.1 67.0 4.0 2021-12-31 21:00:00 34.0 0.1 24.0 36.0 42.0 73.0 13.0 0.1 84.0 5.0 2021-12-31 22:00:00 43.0 0.1 22.0 32.0 50.0 66.0 14.0 0.1 112.0 5.0 2021-12-31 23:00:00 50.0 0.1 22.0 28.0 57.0 61.0 13.0 0.1 74.0 3.0 <p>26304 rows \u00d7 10 columns</p> In\u00a0[21]: Copied! <pre># Correlation between target series and the lags of the other series\n# ======================================================================================\ncorr = multivariate_time_series_corr(\n           time_series = data['pm2.5'],\n           other       = data,\n           lags        = 24\n       )\ncorr.head(3)\n</pre> # Correlation between target series and the lags of the other series # ====================================================================================== corr = multivariate_time_series_corr(            time_series = data['pm2.5'],            other       = data,            lags        = 24        ) corr.head(3) Out[21]: pm2.5 co no no2 pm10 nox o3 veloc. direc. so2 lag 0 1.000000 0.162926 0.354571 0.526002 0.742242 0.490614 -0.349984 -0.207674 0.015281 0.156599 1 0.947931 0.166266 0.333870 0.515745 0.697067 0.472367 -0.334106 -0.221082 0.002978 0.141813 2 0.885050 0.159302 0.290982 0.485289 0.643001 0.429932 -0.300287 -0.223318 -0.011300 0.120706 In\u00a0[22]: Copied! <pre>_ = plot_multivariate_time_series_corr(corr, figsize=(7, 7), )\n</pre> _ = plot_multivariate_time_series_corr(corr, figsize=(7, 7), )"},{"location":"user_guides/plotting.html#plotting","title":"Plotting\u00b6","text":""},{"location":"user_guides/plotting.html#plot-forecasting-residuals","title":"Plot forecasting residuals\u00b6","text":""},{"location":"user_guides/plotting.html#plot-prediction-intervals","title":"Plot prediction intervals\u00b6","text":""},{"location":"user_guides/plotting.html#plot-correlation-between-lags-of-multiple-time-series","title":"Plot correlation between lags of multiple time series\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html","title":"Probabilistic forecasting","text":"<p>All forecasters in skforecast have four different methods that allow for probabilistic forecasting:</p> <ul> <li><p><code>predict_bootstrapping</code>: this method generates multiple forecasting predictions through a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each bootstrapping iteration generates a different set of predictions. The output is a <code>pandas DataFrame</code> with one row for each predicted step and one column for each bootstrapping iteration.</p> </li> <li><p><code>predict_intervals</code>: this method estimates quantile prediction intervals using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_quantiles</code>: this method estimates a list of quantile predictions using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_dist</code>: this method fits a parametric distribution using the values generated with <code>predict_bootstrapping</code>. Any of the continuous distributions available in scipy.stats can be used.</p> </li> </ul> <p>The four can use in-sample residuals (default) or out-sample residuals. In both cases, the residuals can be conditioned on the predicted value to try to account for the existence of a correlation between the predicted values and the residuals.</p> <p> \u26a0 Warning </p> <p>As Rob J Hyndman explains in his blog, in real-world problems, almost all prediction intervals are too narrow. For example, nominal 95% intervals may only provide coverage between 71% and 87%. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. With forecasting models, there are at least four sources of uncertainty:</p> <ul> <li>The random error term</li> <li>The parameter estimates</li> <li>The choice of model for the historical data</li> <li>The continuation of the historical data generating process into the future</li> </ul> <p>When producing prediction intervals for time series models, generally only the first of these sources is taken into account. Therefore, it is advisable to use test data to validate the empirical coverage of the interval and not solely rely on the expected coverage.</p> <p> \u270e Note </p> <p>Conformal prediction is a relatively new framework that allows for the creation of confidence measures for predictions made by machine learning models. This method is on the roadmap of skforecast, but not yet available.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom skforecast.plot import plot_residuals\nfrom skforecast.plot import plot_prediction_distribution\nfrom skforecast.plot import plot_prediction_intervals\nfrom pprint import pprint\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom scipy.stats import norm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_pinball_loss\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import backtesting_forecaster\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt import matplotlib.ticker as ticker from skforecast.plot import plot_residuals from skforecast.plot import plot_prediction_distribution from skforecast.plot import plot_prediction_intervals from pprint import pprint plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from scipy.stats import norm from lightgbm import LGBMRegressor from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.metrics import mean_pinball_loss from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.model_selection import backtesting_forecaster  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') <p> \u26a0 Warning </p> <p>To create a sufficiently illustrative user guide, the data download process takes about 1 minute, and some functions called during the guide may take a few seconds to run. We appreciate your patience.</p> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing_extended_features')\ndata.head(2)\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='bike_sharing_extended_features') data.head(2) <pre>bike_sharing_extended_features\n------------------------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, the dataset\nwas enriched by introducing supplementary features. Addition includes calendar-\nbased variables (day of the week, hour of the day, month, etc.), indicators for\nsunlight, incorporation of rolling temperature averages, and the creation of\npolynomial features generated from variable pairs. All cyclic variables are\nencoded using sine and cosine functions to ensure accurate representation.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17352, 90)\n</pre> Out[2]: users weather month_sin month_cos week_of_year_sin week_of_year_cos week_day_sin week_day_cos hour_day_sin hour_day_cos ... temp_roll_mean_1_day temp_roll_mean_7_day temp_roll_max_1_day temp_roll_min_1_day temp_roll_max_7_day temp_roll_min_7_day holiday_previous_day holiday_next_day temp holiday date_time 2011-01-08 00:00:00 25.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.258819 0.965926 ... 8.063334 10.127976 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 2011-01-08 01:00:00 16.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.500000 0.866025 ... 8.029166 10.113334 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 <p>2 rows \u00d7 90 columns</p> In\u00a0[3]: Copied! <pre># One hot encoding of categorical variables\n# ==============================================================================\nencoder = ColumnTransformer(\n              [('one_hot_encoder', OneHotEncoder(sparse_output=False), ['weather'])],\n              remainder='passthrough',\n              verbose_feature_names_out=False\n          ).set_output(transform=\"pandas\")\ndata = encoder.fit_transform(data)\n</pre> # One hot encoding of categorical variables # ============================================================================== encoder = ColumnTransformer(               [('one_hot_encoder', OneHotEncoder(sparse_output=False), ['weather'])],               remainder='passthrough',               verbose_feature_names_out=False           ).set_output(transform=\"pandas\") data = encoder.fit_transform(data) In\u00a0[4]: Copied! <pre># Select exogenous variables to be included in the model\n# ==============================================================================\nexog_features = [\n    'weather_clear', 'weather_mist', 'weather_rain', 'month_sin', 'month_cos',\n    'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos',\n    'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',\n    'sunset_hour_sin', 'sunset_hour_cos', 'temp', 'holiday'\n]\ndata = data[['users'] + exog_features]\n</pre> # Select exogenous variables to be included in the model # ============================================================================== exog_features = [     'weather_clear', 'weather_mist', 'weather_rain', 'month_sin', 'month_cos',     'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos',     'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',     'sunset_hour_sin', 'sunset_hour_cos', 'temp', 'holiday' ] data = data[['users'] + exog_features] In\u00a0[5]: Copied! <pre># Split train-validation-test\n# ==============================================================================\ndata = data.loc['2011-05-30 23:59:00':, :]\nend_train = '2012-08-30 23:59:00'\nend_validation = '2012-11-15 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validacion : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== data = data.loc['2011-05-30 23:59:00':, :] end_train = '2012-08-30 23:59:00' end_validation = '2012-11-15 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validacion : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2011-05-31 00:00:00 --- 2012-08-30 23:00:00  (n=10992)\nDates validacion : 2012-08-31 00:00:00 --- 2012-11-15 23:00:00  (n=1848)\nDates test       : 2012-11-16 00:00:00 --- 2012-12-30 23:00:00  (n=1080)\n</pre> In\u00a0[6]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 3))\ndata_train['users'].plot(label='train', ax=ax)\ndata_val['users'].plot(label='validation', ax=ax)\ndata_test['users'].plot(label='test', ax=ax)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_title('Number of users')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(9, 3)) data_train['users'].plot(label='train', ax=ax) data_val['users'].plot(label='validation', ax=ax) data_test['users'].plot(label='test', ax=ax) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_title('Number of users') ax.legend(); In\u00a0[7]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nparams = {\n    \"n_estimators\": 600,\n    \"max_depth\": 6,\n    \"min_data_in_leaf\": 88,\n    \"learning_rate\": 0.2520098236227423,\n    \"feature_fraction\": 0.6,\n    \"max_bin\": 75,\n    \"reg_alpha\": 1.0,\n    \"reg_lambda\": 0.8,\n}\nlags = 48\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),\n                 lags      = lags\n             )\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== params = {     \"n_estimators\": 600,     \"max_depth\": 6,     \"min_data_in_leaf\": 88,     \"learning_rate\": 0.2520098236227423,     \"feature_fraction\": 0.6,     \"max_bin\": 75,     \"reg_alpha\": 1.0,     \"reg_lambda\": 0.8, } lags = 48  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),                  lags      = lags              ) forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features] ) In\u00a0[8]: Copied! <pre># In-sample residuals stored during fit\n# ==============================================================================\nprint(\"Amount of residuals stored:\", len(forecaster.in_sample_residuals))\nforecaster.in_sample_residuals\n</pre> # In-sample residuals stored during fit # ============================================================================== print(\"Amount of residuals stored:\", len(forecaster.in_sample_residuals)) forecaster.in_sample_residuals <pre>Amount of residuals stored: 2000\n</pre> Out[8]: <pre>array([ -2.20887533,  -5.90417032,   1.3012363 , ..., -18.71892632,\n        26.46825175,   0.18927409])</pre> In\u00a0[9]: Copied! <pre># Predict interval with in-sample residuals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                  exog     = data_test[exog_features],\n                  steps    = 7,\n                  interval = [10, 90]\n              )\npredictions\n</pre> # Predict interval with in-sample residuals # ============================================================================== predictions = forecaster.predict_interval(                   exog     = data_test[exog_features],                   steps    = 7,                   interval = [10, 90]               ) predictions Out[9]: pred lower_bound upper_bound 2012-11-16 00:00:00 70.009970 54.853980 84.575658 2012-11-16 01:00:00 45.679914 27.872421 63.867664 2012-11-16 02:00:00 19.225220 1.202892 36.277884 2012-11-16 03:00:00 -0.039409 -13.523202 15.024639 2012-11-16 04:00:00 0.154831 -15.181007 13.631743 2012-11-16 05:00:00 37.330998 24.193142 52.502698 2012-11-16 06:00:00 116.737843 62.597391 138.526045 <p>The <code>backtesting_forecaster()</code> function is used to generate the prediction intervals for the entire test set and calculate coverage of a given interval.</p> <ul> <li><p><code>in_sample_residuals=True</code> is used to compute the intervals using in-sample residuals.</p> </li> <li><p>The <code>interval</code> argument indicates the desired coverage probability of the prediction intervals. In this case, <code>interval</code> is set to <code>[10, 90]</code>, which means that the prediction intervals are calculated for the 10th and 90th percentiles, resulting in a theoretical coverage probability of 80%.</p> </li> <li><p>The <code>n_boot</code> argument is used to specify the number of bootstrap samples to be used in estimating the prediction intervals. The larger the number of samples, the more accurate the prediction intervals will be, but the longer the calculation will take.</p> </li> </ul> In\u00a0[10]: Copied! <pre># Backtesting with prediction intervals in test data using in-sample residuals\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster          = forecaster,\n                          y                   = data['users'],\n                          exog                = data[exog_features],\n                          steps               = 24,\n                          metric              = 'mean_absolute_error',\n                          initial_train_size  = len(data.loc[:end_validation]),\n                          refit               = False,\n                          interval            = [10, 90],  # 80% prediction interval\n                          n_boot              = 250,\n                          in_sample_residuals = True,  # Use in-sample residuals\n                          binned_residuals    = False,\n                          n_jobs              = 'auto',\n                          verbose             = False,\n                          show_progress       = True\n                      )\n\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using in-sample residuals # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster          = forecaster,                           y                   = data['users'],                           exog                = data[exog_features],                           steps               = 24,                           metric              = 'mean_absolute_error',                           initial_train_size  = len(data.loc[:end_validation]),                           refit               = False,                           interval            = [10, 90],  # 80% prediction interval                           n_boot              = 250,                           in_sample_residuals = True,  # Use in-sample residuals                           binned_residuals    = False,                           n_jobs              = 'auto',                           verbose             = False,                           show_progress       = True                       )  predictions.head(5) <pre>  0%|          | 0/45 [00:00&lt;?, ?it/s]</pre> Out[10]: pred lower_bound upper_bound 2012-11-16 00:00:00 70.009970 54.853980 84.575658 2012-11-16 01:00:00 45.679914 27.872421 63.867664 2012-11-16 02:00:00 19.225220 1.202892 36.277884 2012-11-16 03:00:00 -0.039409 -13.523202 15.024639 2012-11-16 04:00:00 0.154831 -15.181007 13.631743 In\u00a0[11]: Copied! <pre># Function to calculate coverage of a given interval\n# ======================================================================================\ndef empirical_coverage(y, lower_bound, upper_bound):\n    \"\"\"\n    Calculate coverage of a given interval\n    \"\"\"\n        \n    return np.mean(np.logical_and(y &gt;= lower_bound, y &lt;= upper_bound))\n</pre> # Function to calculate coverage of a given interval # ====================================================================================== def empirical_coverage(y, lower_bound, upper_bound):     \"\"\"     Calculate coverage of a given interval     \"\"\"              return np.mean(np.logical_and(y &gt;= lower_bound, y &lt;= upper_bound)) In\u00a0[12]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    target_variable = \"users\",\n    initial_x_zoom  = ['2012-12-01', '2012-12-20'],\n    title           = \"Real value vs predicted in test data\",\n    xaxis_title     = \"Date time\",\n    yaxis_title     = \"users\",\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = empirical_coverage(\n    y = data.loc[end_validation:, 'users'],\n    lower_bound = predictions[\"lower_bound\"], \n    upper_bound = predictions[\"upper_bound\"]\n)\nprint(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     target_variable = \"users\",     initial_x_zoom  = ['2012-12-01', '2012-12-20'],     title           = \"Real value vs predicted in test data\",     xaxis_title     = \"Date time\",     yaxis_title     = \"users\", )  # Predicted interval coverage (on test data) # ============================================================================== coverage = empirical_coverage(     y = data.loc[end_validation:, 'users'],     lower_bound = predictions[\"lower_bound\"],      upper_bound = predictions[\"upper_bound\"] ) print(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 62.31 %\nArea of the interval: 89201.03\n</pre> <p>The prediction intervals exhibit overconfidence as they tend to be excessively narrow, resulting in a true coverage that falls below the nominal coverage (80 %). This phenomenon arises from the tendency of in-sample residuals to often overestimate the predictive capacity of the model.</p> In\u00a0[13]: Copied! <pre># Backtesting on validation data to obtain out-sample residuals\n# ==============================================================================\n_, predictions_val = backtesting_forecaster(\n                         forecaster         = forecaster,\n                         y                  = data.loc[:end_validation, 'users'],\n                         exog               = data.loc[:end_validation, exog_features],\n                         steps              = 24,\n                         metric             = 'mean_absolute_error',\n                         initial_train_size = len(data.loc[:end_train]),\n                         refit              = False,\n                         n_jobs             = 'auto',\n                         verbose            = False,\n                         show_progress      = True\n                     )\n\nresiduals = data.loc[predictions_val.index, 'users'] - predictions_val['pred']\nresiduals.head(3)\n</pre> # Backtesting on validation data to obtain out-sample residuals # ============================================================================== _, predictions_val = backtesting_forecaster(                          forecaster         = forecaster,                          y                  = data.loc[:end_validation, 'users'],                          exog               = data.loc[:end_validation, exog_features],                          steps              = 24,                          metric             = 'mean_absolute_error',                          initial_train_size = len(data.loc[:end_train]),                          refit              = False,                          n_jobs             = 'auto',                          verbose            = False,                          show_progress      = True                      )  residuals = data.loc[predictions_val.index, 'users'] - predictions_val['pred'] residuals.head(3) <pre>  0%|          | 0/77 [00:00&lt;?, ?it/s]</pre> Out[13]: <pre>2012-08-31 00:00:00   -2.100991\n2012-08-31 01:00:00   -9.776291\n2012-08-31 02:00:00    6.562946\nFreq: h, dtype: float64</pre> In\u00a0[14]: Copied! <pre># Out-sample residuals distribution\n# ==============================================================================\nprint(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts())\nplt.rcParams.update({'font.size': 8})\n_ = plot_residuals(residuals=residuals, figsize=(7, 4))\n</pre> # Out-sample residuals distribution # ============================================================================== print(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts()) plt.rcParams.update({'font.size': 8}) _ = plot_residuals(residuals=residuals, figsize=(7, 4)) <pre>positive    1118\nnegative     730\nName: count, dtype: int64\n</pre> <p>Then, <code>set_out_sample_residuals()</code> method is used to specify the computed out-sample residuals.</p> In\u00a0[15]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.set_out_sample_residuals(residuals=residuals)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.set_out_sample_residuals(residuals=residuals) <p>Now that the new residuals have been added to the forecaster, the prediction intervals can be calculated using <code>in_sample_residuals = False</code>.</p> In\u00a0[16]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster          = forecaster,\n                          y                   = data['users'],\n                          exog                = data[exog_features],\n                          steps               = 24,\n                          metric              = 'mean_absolute_error',\n                          initial_train_size  = len(data.loc[:end_validation]),\n                          refit               = False,\n                          interval            = [10, 90], # 80% prediction interval\n                          n_boot              = 250,\n                          in_sample_residuals = False, # Use out-sample residuals\n                          binned_residuals    = False,\n                          n_jobs              = 'auto',\n                          verbose             = False,\n                          show_progress       = True\n                      )\n\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster          = forecaster,                           y                   = data['users'],                           exog                = data[exog_features],                           steps               = 24,                           metric              = 'mean_absolute_error',                           initial_train_size  = len(data.loc[:end_validation]),                           refit               = False,                           interval            = [10, 90], # 80% prediction interval                           n_boot              = 250,                           in_sample_residuals = False, # Use out-sample residuals                           binned_residuals    = False,                           n_jobs              = 'auto',                           verbose             = False,                           show_progress       = True                       )  predictions.head(5) <pre>  0%|          | 0/45 [00:00&lt;?, ?it/s]</pre> Out[16]: pred lower_bound upper_bound 2012-11-16 00:00:00 70.009970 35.232983 195.389471 2012-11-16 01:00:00 45.679914 -9.233033 209.921993 2012-11-16 02:00:00 19.225220 -8.006518 202.804502 2012-11-16 03:00:00 -0.039409 -17.532126 235.975562 2012-11-16 04:00:00 0.154831 -30.978673 228.508684 In\u00a0[17]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    target_variable = \"users\",\n    initial_x_zoom  = ['2012-12-01', '2012-12-20'],\n    title           = \"Real value vs predicted in test data\",\n    xaxis_title     = \"Date time\",\n    yaxis_title     = \"users\",\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = empirical_coverage(\n    y = data.loc[end_validation:, 'users'],\n    lower_bound = predictions[\"lower_bound\"], \n    upper_bound = predictions[\"upper_bound\"]\n)\nprint(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     target_variable = \"users\",     initial_x_zoom  = ['2012-12-01', '2012-12-20'],     title           = \"Real value vs predicted in test data\",     xaxis_title     = \"Date time\",     yaxis_title     = \"users\", )  # Predicted interval coverage (on test data) # ============================================================================== coverage = empirical_coverage(     y = data.loc[end_validation:, 'users'],     lower_bound = predictions[\"lower_bound\"],      upper_bound = predictions[\"upper_bound\"] ) print(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 75.46 %\nArea of the interval: 316412.72\n</pre> <p>The resulting prediction intervals derived from the out-sample residuals are wider than those generated using the in-sample residuals. This results in an empirical coverage that is closer to the nominal coverage (80 %), although still lower. Examining the plot, it is easy to see that the intervals are particularly wide when the predicted values are low, indicating that the model is not able to properly locate the uncertainty of its predictions.</p> <p> \ud83d\udca1 Tip </p> <p>Since this is a new feature, intervals conditioned on predicted values (binned residuals) are only available for the <code>ForecasterAutoreg</code> class. Let us know via GitHub issues if you want us to extend this feature to all Forecasters.</p> In\u00a0[18]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = LGBMRegressor(random_state=15926, verbose=-1, **params),\n                 lags          = lags,\n                 binner_kwargs = {'n_bins': 10}   \n             )\n\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features]\n)\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = LGBMRegressor(random_state=15926, verbose=-1, **params),                  lags          = lags,                  binner_kwargs = {'n_bins': 10}                 )  forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features] ) <p>During the training process, the forecaster uses the in-sample predictions to define the intervals at which the residuals are stored depending on the predicted value to which they are related (<code>binner_intervals</code> attribut).</p> <p>Although not used in this example, the in-sample residuals are divided into bins and stored in the <code>in_sample_residuals_by_bin</code> attribute.</p> In\u00a0[19]: Copied! <pre># Intervals of the residual bins\n# ==============================================================================\npprint(forecaster.binner_intervals)\n</pre> # Intervals of the residual bins # ============================================================================== pprint(forecaster.binner_intervals) <pre>{0: (-8.229467171553717, 11.116037535200665),\n 1: (11.116037535200665, 31.879155847370434),\n 2: (31.879155847370434, 75.9019071402224),\n 3: (75.9019071402224, 124.5691653220086),\n 4: (124.5691653220086, 170.35484312260417),\n 5: (170.35484312260417, 218.96823239624555),\n 6: (218.96823239624555, 278.6496576655771),\n 7: (278.6496576655771, 355.13229168292287),\n 8: (355.13229168292287, 486.1660497574729),\n 9: (486.1660497574729, 970.517259284916)}\n</pre> In\u00a0[20]: Copied! <pre># Number of in-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.in_sample_residuals_by_bin.items():\n    print(f\"Bin {k}: n={len(v)}\")\n</pre> # Number of in-sample residuals by bin # ============================================================================== for k, v in forecaster.in_sample_residuals_by_bin.items():     print(f\"Bin {k}: n={len(v)}\") <pre>Bin 0: n=200\nBin 1: n=200\nBin 2: n=200\nBin 3: n=200\nBin 4: n=200\nBin 5: n=200\nBin 6: n=200\nBin 7: n=200\nBin 8: n=200\nBin 9: n=200\n</pre> <p>To allow the forecaster to bin the out-sample residuals, the predicted values must be passed to the <code>set_out_sample_residuals()</code> method in addition to the residuals. The method will bin the residuals according to the intervals learned during fitting. To avoid using too much memory, a maximum of 200 residuals are stored per bin.</p> In\u00a0[21]: Copied! <pre># Store binned out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.set_out_sample_residuals(residuals=residuals, y_pred=predictions_val['pred'])\n</pre> # Store binned out-sample residuals in the forecaster # ============================================================================== forecaster.set_out_sample_residuals(residuals=residuals, y_pred=predictions_val['pred']) In\u00a0[22]: Copied! <pre># Number of out-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.out_sample_residuals_by_bin.items():\n    print(f\"Bin {k}: n={len(v)}\")\n</pre> # Number of out-sample residuals by bin # ============================================================================== for k, v in forecaster.out_sample_residuals_by_bin.items():     print(f\"Bin {k}: n={len(v)}\") <pre>Bin 0: n=114\nBin 1: n=190\nBin 2: n=183\nBin 3: n=162\nBin 4: n=126\nBin 5: n=164\nBin 6: n=175\nBin 7: n=200\nBin 8: n=200\nBin 9: n=200\n</pre> In\u00a0[23]: Copied! <pre># Distribution of the residual by bin\n# ==============================================================================\nout_sample_residuals_by_bin_df = pd.DataFrame(\n    dict(\n        [(k, pd.Series(v))\n         for k, v in forecaster.out_sample_residuals_by_bin.items()]\n    )\n)\n\nfig, ax = plt.subplots(figsize=(6, 3))\nout_sample_residuals_by_bin_df.boxplot(ax=ax)\nax.set_title(\"Distribution of residuals by bin\", fontsize=12)\nax.set_xlabel(\"Bin\", fontsize=10)\nax.set_ylabel(\"Residuals\", fontsize=10)\nplt.show();\n</pre> # Distribution of the residual by bin # ============================================================================== out_sample_residuals_by_bin_df = pd.DataFrame(     dict(         [(k, pd.Series(v))          for k, v in forecaster.out_sample_residuals_by_bin.items()]     ) )  fig, ax = plt.subplots(figsize=(6, 3)) out_sample_residuals_by_bin_df.boxplot(ax=ax) ax.set_title(\"Distribution of residuals by bin\", fontsize=12) ax.set_xlabel(\"Bin\", fontsize=10) ax.set_ylabel(\"Residuals\", fontsize=10) plt.show(); <p>The box plots show how the spread and magnitude of the residuals differ depending on the predicted value. For example, for bin 0, whose predicted value is in the interval (-8.2, 11.1), the residuals never exceed an absolute value of 100, while for bin 9, for predicted values in the interval (486.2, 970.5), they often do.</p> <p>Finally, the prediction intervals for the test data are estimated using the backtesting process, with out-sample residuals conditioned on the predicted values.</p> In\u00a0[24]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster          = forecaster,\n                          y                   = data['users'],\n                          exog                = data[exog_features],\n                          steps               = 24,\n                          metric              = 'mean_absolute_error',\n                          initial_train_size  = len(data.loc[:end_validation]),\n                          refit               = False,\n                          interval            = [10, 90], # 80% prediction interval\n                          n_boot              = 250,\n                          in_sample_residuals = False, # Use out-sample residuals\n                          binned_residuals    = True,  # Use binned residuals\n                          n_jobs              = 'auto',\n                          verbose             = False,\n                          show_progress       = True\n                     )\n\npredictions.head(3)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster          = forecaster,                           y                   = data['users'],                           exog                = data[exog_features],                           steps               = 24,                           metric              = 'mean_absolute_error',                           initial_train_size  = len(data.loc[:end_validation]),                           refit               = False,                           interval            = [10, 90], # 80% prediction interval                           n_boot              = 250,                           in_sample_residuals = False, # Use out-sample residuals                           binned_residuals    = True,  # Use binned residuals                           n_jobs              = 'auto',                           verbose             = False,                           show_progress       = True                      )  predictions.head(3) <pre>  0%|          | 0/45 [00:00&lt;?, ?it/s]</pre> Out[24]: pred lower_bound upper_bound 2012-11-16 00:00:00 70.009970 43.169483 111.284002 2012-11-16 01:00:00 45.679914 16.252003 124.574522 2012-11-16 02:00:00 19.225220 5.881439 67.542903 In\u00a0[25]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    target_variable = \"users\",\n    initial_x_zoom  = ['2012-12-01', '2012-12-20'],\n    title           = \"Real value vs predicted in test data\",\n    xaxis_title     = \"Date time\",\n    yaxis_title     = \"users\",\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = empirical_coverage(\n    y = data.loc[end_validation:, 'users'],\n    lower_bound = predictions[\"lower_bound\"], \n    upper_bound = predictions[\"upper_bound\"]\n)\nprint(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     target_variable = \"users\",     initial_x_zoom  = ['2012-12-01', '2012-12-20'],     title           = \"Real value vs predicted in test data\",     xaxis_title     = \"Date time\",     yaxis_title     = \"users\", )  # Predicted interval coverage (on test data) # ============================================================================== coverage = empirical_coverage(     y = data.loc[end_validation:, 'users'],     lower_bound = predictions[\"lower_bound\"],      upper_bound = predictions[\"upper_bound\"] ) print(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 84.44 %\nArea of the interval: 284549.02\n</pre> <p>When using out-sample residuals conditioned on the predicted value, the area of the interval is significantly reduced and the uncertainty is mainly allocated to the predictions with high values. The empirical coverage is slightly above the expected coverage, which means that the estimated intervals are conservative.</p> <p>The previous sections have demonstrated the use of the backtesting process to estimate the prediction interval over a given period of time. The goal is to mimic the behavior of the model in production by running predictions at regular intervals, incrementally updating the input data.</p> <p>Alternatively, it is possible to run a single prediction that forecasts N steps ahead without going through the entire backtesting process. In such cases, skforecast provides four different methods: <code>predict_bootstrapping</code>, <code>predict_interval</code>, <code>predict_quantile</code> and <code>predict_distribution</code>.</p> <p> \ud83d\udca1 Tip </p> <p>All of these methods can be used either with in-sample or out-sample residuals using the <code>in_sample_residuals</code> argument, and, in the case of the <code>ForecasterAutoreg</code>, with binned intervals conditioned on predicted values using the <code>binned_residuals</code> argument.</p> <p>Predict Bootstraping</p> <p>The <code>predict_bootstrapping</code> method performs the <code>n_boot</code> bootstrapping iterations that generate the alternative prediction paths. These are the underlying values used to compute the intervals, quantiles, and distributions.</p> In\u00a0[26]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),\n                 lags      = lags\n             )\n\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features]\n)\n</pre> # Fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),                  lags      = lags              )  forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features] ) In\u00a0[27]: Copied! <pre># Predict 10 different forecasting sequences of 7 steps each using bootstrapping\n# ==============================================================================\nboot_predictions = forecaster.predict_bootstrapping(\n                       exog   = data_test[exog_features],\n                       steps  = 7,\n                       n_boot = 25\n                   )\nboot_predictions\n</pre> # Predict 10 different forecasting sequences of 7 steps each using bootstrapping # ============================================================================== boot_predictions = forecaster.predict_bootstrapping(                        exog   = data_test[exog_features],                        steps  = 7,                        n_boot = 25                    ) boot_predictions Out[27]: pred_boot_0 pred_boot_1 pred_boot_2 pred_boot_3 pred_boot_4 pred_boot_5 pred_boot_6 pred_boot_7 pred_boot_8 pred_boot_9 ... pred_boot_15 pred_boot_16 pred_boot_17 pred_boot_18 pred_boot_19 pred_boot_20 pred_boot_21 pred_boot_22 pred_boot_23 pred_boot_24 2012-11-16 00:00:00 70.898024 69.873695 69.565263 61.857376 85.665439 66.467093 63.857676 69.212068 65.847223 66.556235 ... 62.585319 53.217628 65.327025 60.221925 57.991403 58.582670 61.824311 61.637199 37.511226 65.951935 2012-11-16 01:00:00 55.979274 55.068304 45.396548 44.287035 72.763300 36.002591 33.524357 54.668580 51.760307 52.862864 ... 48.803053 27.341747 30.489447 41.454845 16.347848 19.899535 26.816973 72.009503 15.384528 46.721738 2012-11-16 02:00:00 4.685382 29.137452 5.823476 12.731261 23.512352 18.064962 32.940457 28.855360 33.516604 24.740684 ... 26.879931 6.940376 -7.508078 38.266053 3.350771 6.951659 10.701724 23.387247 6.355835 29.828715 2012-11-16 03:00:00 -21.472685 5.713801 12.987916 5.931830 -10.509108 -3.890380 -8.155016 -57.395536 11.805676 -9.478093 ... -0.459709 -13.449848 12.932434 0.982905 3.709442 0.028574 8.109365 6.686400 -9.799510 0.795208 2012-11-16 04:00:00 -15.627414 11.940814 -13.344150 -22.901702 -5.829132 -1.587510 10.397912 -5.745829 4.587523 27.978311 ... -0.338277 -8.359792 0.981976 1.216587 1.335403 -5.779670 -6.718349 4.597700 3.497823 -10.896204 2012-11-16 05:00:00 34.112840 23.142163 35.361155 30.438594 38.277198 46.580395 61.971720 29.661283 40.288920 64.128520 ... 41.593024 41.164170 6.514000 39.361794 45.313482 33.712530 34.830050 29.797932 44.425095 46.296916 2012-11-16 06:00:00 131.662882 80.006263 76.758310 88.200642 108.838941 144.855933 108.554524 96.014433 106.963498 102.074819 ... 93.088546 142.148200 59.312544 110.899546 160.273533 121.783521 113.056479 71.936642 130.198038 129.479849 <p>7 rows \u00d7 25 columns</p> <p>A ridge plot is a useful way to visualize the uncertainty of a forecasting model. This plot estimates a kernel density for each step by using the bootstrapped predictions.</p> In\u00a0[28]: Copied! <pre># Ridge plot of bootstrapping predictions\n# ==============================================================================\n_ = plot_prediction_distribution(boot_predictions, figsize=(7, 4))\n</pre> # Ridge plot of bootstrapping predictions # ============================================================================== _ = plot_prediction_distribution(boot_predictions, figsize=(7, 4)) <p>Predict Interval</p> <p>In most cases, the user is interested in a specific interval rather than the entire bootstrapping simulation matrix. To address this need, skforecast provides the <code>predict_interval</code> method. This method internally uses <code>predict_bootstrapping</code> to obtain the bootstrapping matrix and estimates the upper and lower quantiles for each step, thus providing the user with the desired prediction intervals.</p> In\u00a0[29]: Copied! <pre># Predict intervals for next 7 steps, quantiles 10th and 90th\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                  exog     = data_test[exog_features],\n                  steps    = 7,\n                  interval = [10, 90],\n                  n_boot   = 150\n              )\npredictions\n</pre> # Predict intervals for next 7 steps, quantiles 10th and 90th # ============================================================================== predictions = forecaster.predict_interval(                   exog     = data_test[exog_features],                   steps    = 7,                   interval = [10, 90],                   n_boot   = 150               ) predictions Out[29]: pred lower_bound upper_bound 2012-11-16 00:00:00 70.009970 56.228078 84.625000 2012-11-16 01:00:00 45.679914 28.079998 62.560809 2012-11-16 02:00:00 19.225220 1.202892 36.291047 2012-11-16 03:00:00 -0.039409 -15.040924 14.159163 2012-11-16 04:00:00 0.154831 -16.075714 11.953596 2012-11-16 05:00:00 37.330998 23.134641 52.499718 2012-11-16 06:00:00 116.737843 62.597391 138.671717 <p>Predict Quantile</p> <p>This method operates identically to <code>predict_interval</code>, with the added feature of enabling users to define a specific list of quantiles for estimation at each step. It's important to remember that these quantiles should be specified within the range of 0 to 1.</p> In\u00a0[30]: Copied! <pre># Predict quantiles for next 7 steps, quantiles 5th, 25th, 75th and 95th\n# ==============================================================================\npredictions = forecaster.predict_quantiles(\n                  exog      = data_test[exog_features],\n                  steps     = 7,\n                  n_boot    = 150,\n                  quantiles = [0.05, 0.25, 0.75, 0.95],\n              )\npredictions\n</pre> # Predict quantiles for next 7 steps, quantiles 5th, 25th, 75th and 95th # ============================================================================== predictions = forecaster.predict_quantiles(                   exog      = data_test[exog_features],                   steps     = 7,                   n_boot    = 150,                   quantiles = [0.05, 0.25, 0.75, 0.95],               ) predictions Out[30]: q_0.05 q_0.25 q_0.75 q_0.95 2012-11-16 00:00:00 50.259804 62.357488 76.103135 88.342403 2012-11-16 01:00:00 19.756968 37.758711 55.037050 71.245169 2012-11-16 02:00:00 -5.351717 10.716533 28.009095 42.344104 2012-11-16 03:00:00 -22.251535 -6.111390 7.964113 21.803893 2012-11-16 04:00:00 -20.409406 -7.927528 4.777113 14.402585 2012-11-16 05:00:00 19.074869 29.313532 45.592435 56.174378 2012-11-16 06:00:00 52.797473 80.210779 122.888501 144.320813 <p>Predict Distribution</p> <p>The intervals estimated so far are distribution-free, which means that no assumptions are made about a particular distribution. The <code>predict_dist</code> method in skforecast allows fitting a parametric distribution to the bootstrapped prediction samples obtained with <code>predict_bootstrapping</code>. This is useful when there is reason to believe that the forecast errors follow a particular distribution, such as the normal distribution or the student's t-distribution. The <code>predict_dist</code> method allows the user to specify any continuous distribution from the scipy.stats module.</p> In\u00a0[31]: Copied! <pre># Predict the parameters of a normal distribution for the next 7 steps\n# ==============================================================================\npredictions = forecaster.predict_dist(\n                  exog         = data_test[exog_features],\n                  steps        = 7,\n                  n_boot       = 150,\n                  distribution = norm\n              )\npredictions\n</pre> # Predict the parameters of a normal distribution for the next 7 steps # ============================================================================== predictions = forecaster.predict_dist(                   exog         = data_test[exog_features],                   steps        = 7,                   n_boot       = 150,                   distribution = norm               ) predictions Out[31]: loc scale 2012-11-16 00:00:00 69.798829 11.975728 2012-11-16 01:00:00 46.127171 15.482351 2012-11-16 02:00:00 19.660019 15.875897 2012-11-16 03:00:00 0.209658 14.161571 2012-11-16 04:00:00 -2.036986 11.998902 2012-11-16 05:00:00 37.548196 12.250819 2012-11-16 06:00:00 101.109095 30.599564 <p> \u26a0 Warning </p> <p>Forecasters of type <code>ForecasterAutoregDirect</code> are slower than <code>ForecasterAutoregRecursive</code> because they require training one model per step. Although they can achieve better performance, their scalability is an important limitation when many steps need to be predicted. To limit the time required to run the following examples, the data is aggregated from hourly frequency to daily frequency and only 7 steps ahead (one week) are predicted.</p> In\u00a0[32]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='vic_electricity')\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='vic_electricity') <pre>vic_electricity\n---------------\nHalf-hourly electricity demand for Victoria, Australia\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse\nDatasets for 'tsibble'. https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/.\nhttps://tsibbledata.tidyverts.org/reference/vic_elec.html\nShape of the dataset: (52608, 4)\n</pre> In\u00a0[33]: Copied! <pre># Data preparation (aggregation at daily level)\n# ==============================================================================\ndata = data.resample(rule=\"D\", closed=\"left\", label=\"right\").agg(\n    {\"Demand\": \"sum\", \"Temperature\": \"mean\", \"Holiday\": \"max\"}\n)\ndata.head(3)\n</pre> # Data preparation (aggregation at daily level) # ============================================================================== data = data.resample(rule=\"D\", closed=\"left\", label=\"right\").agg(     {\"Demand\": \"sum\", \"Temperature\": \"mean\", \"Holiday\": \"max\"} ) data.head(3) Out[33]: Demand Temperature Holiday Time 2012-01-01 82531.745918 21.047727 True 2012-01-02 227778.257304 26.578125 True 2012-01-03 275490.988882 31.751042 True In\u00a0[34]: Copied! <pre># Split data into train-validation-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\nend_validation = '2014-9-30 23:59:00'\ndata_train = data.loc[: end_train, :].copy()\ndata_val   = data.loc[end_train:end_validation, :].copy()\ndata_test  = data.loc[end_validation:, :].copy()\n\nprint(\n    f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"\n    f\"  (n={len(data_val)})\"\n)\nprint(\n    f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split data into train-validation-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' end_validation = '2014-9-30 23:59:00' data_train = data.loc[: end_train, :].copy() data_val   = data.loc[end_train:end_validation, :].copy() data_test  = data.loc[end_validation:, :].copy()  print(     f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"     f\"  (n={len(data_val)})\" ) print(     f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Train dates      : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00  (n=731)\nValidation dates : 2014-01-01 00:00:00 --- 2014-09-30 00:00:00  (n=273)\nTest dates       : 2014-10-01 00:00:00 --- 2014-12-30 00:00:00  (n=91)\n</pre> In\u00a0[35]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['Demand'].plot(label='train', ax=ax)\ndata_val['Demand'].plot(label='validation', ax=ax)\ndata_test['Demand'].plot(label='test', ax=ax)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylim(bottom=160_000)\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['Demand'].plot(label='train', ax=ax) data_val['Demand'].plot(label='validation', ax=ax) data_test['Demand'].plot(label='test', ax=ax) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylim(bottom=160_000) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand') ax.legend(); In\u00a0[36]: Copied! <pre># Create forecasters: one for each bound of the interval\n# ==============================================================================\n# The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence\n# interval (90% - 10% = 80%).\n\n# Forecaster for quantile 10%\nforecaster_q10 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.1,\n                                     learning_rate = 0.01,\n                                     max_depth     = 3,\n                                     n_estimators  = 500,\n                                     verbose       = -1\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n                  \n# Forecaster for quantile 90%\nforecaster_q90 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.9,\n                                     learning_rate = 0.1,\n                                     max_depth     = 3,\n                                     n_estimators  = 100,\n                                     verbose       = -1\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n\nforecaster_q10.fit(y=data['Demand'])\nforecaster_q90.fit(y=data['Demand'])\n</pre> # Create forecasters: one for each bound of the interval # ============================================================================== # The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence # interval (90% - 10% = 80%).  # Forecaster for quantile 10% forecaster_q10 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.1,                                      learning_rate = 0.01,                                      max_depth     = 3,                                      n_estimators  = 500,                                      verbose       = -1                                  ),                      lags = 7,                      steps = 7                  )                    # Forecaster for quantile 90% forecaster_q90 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.9,                                      learning_rate = 0.1,                                      max_depth     = 3,                                      n_estimators  = 100,                                      verbose       = -1                                  ),                      lags = 7,                      steps = 7                  )  forecaster_q10.fit(y=data['Demand']) forecaster_q90.fit(y=data['Demand']) <p>When validating a quantile regression model, a custom metric must be provided depending on the quantile being estimated.</p> In\u00a0[37]: Copied! <pre># Loss function for each quantile (pinball_loss)\n# ==============================================================================\ndef mean_pinball_loss_q10(y_true, y_pred):\n    \"\"\"\n    Pinball loss for quantile 10.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.1)\n\n\ndef mean_pinball_loss_q90(y_true, y_pred):\n    \"\"\"\n    Pinball loss for quantile 90.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.9)\n</pre> # Loss function for each quantile (pinball_loss) # ============================================================================== def mean_pinball_loss_q10(y_true, y_pred):     \"\"\"     Pinball loss for quantile 10.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.1)   def mean_pinball_loss_q90(y_true, y_pred):     \"\"\"     Pinball loss for quantile 90.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.9) In\u00a0[38]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster          = forecaster_q10,\n                                  y                   = data['Demand'],\n                                  steps               = 7,\n                                  metric              = mean_pinball_loss_q10,\n                                  initial_train_size  = len(data.loc[:end_validation]),\n                                  refit               = False,\n                                  n_jobs              = 'auto',\n                                  verbose             = False,\n                                  show_progress       = True\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster          = forecaster_q90,\n                                  y                   = data['Demand'],\n                                  steps               = 7,\n                                  metric              = mean_pinball_loss_q90,\n                                  initial_train_size  = len(data.loc[:end_validation]),\n                                  refit               = False,\n                                  n_jobs              = 'auto',\n                                  verbose             = False,\n                                  show_progress       = True\n                              )\n</pre> # Backtesting on test data # ============================================================================== metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster          = forecaster_q10,                                   y                   = data['Demand'],                                   steps               = 7,                                   metric              = mean_pinball_loss_q10,                                   initial_train_size  = len(data.loc[:end_validation]),                                   refit               = False,                                   n_jobs              = 'auto',                                   verbose             = False,                                   show_progress       = True                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster          = forecaster_q90,                                   y                   = data['Demand'],                                   steps               = 7,                                   metric              = mean_pinball_loss_q90,                                   initial_train_size  = len(data.loc[:end_validation]),                                   refit               = False,                                   n_jobs              = 'auto',                                   verbose             = False,                                   show_progress       = True                               ) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[39]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Real value')\nax.fill_between(\n    data.loc[end_validation:].index,\n    predictions_q10['pred'],\n    predictions_q90['pred'],\n    color = '#444444',\n        alpha = 0.3,\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend();\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Real value') ax.fill_between(     data.loc[end_validation:].index,     predictions_q10['pred'],     predictions_q90['pred'],     color = '#444444',         alpha = 0.3, ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(); <p>Predictions generated for each model are used to define the upper and lower limits of the interval.</p> In\u00a0[40]: Copied! <pre># Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = empirical_coverage(\n    y = data.loc[end_validation:, 'Demand'],\n    lower_bound = predictions_q10[\"pred\"], \n    upper_bound = predictions_q90[\"pred\"]\n)\nprint(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions_q90[\"pred\"] - predictions_q10[\"pred\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Predicted interval coverage (on test data) # ============================================================================== coverage = empirical_coverage(     y = data.loc[end_validation:, 'Demand'],     lower_bound = predictions_q10[\"pred\"],      upper_bound = predictions_q90[\"pred\"] ) print(f\"Predicted interval coverage: {round(100*coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions_q90[\"pred\"] - predictions_q10[\"pred\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 74.73 %\nArea of the interval: 2832564.24\n</pre>"},{"location":"user_guides/probabilistic-forecasting.html#probabilistic-forecasting-prediction-intervals-and-prediction-distribution","title":"Probabilistic forecasting: prediction intervals and prediction distribution\u00b6","text":"<p>When trying to anticipate future values, most forecasting models try to predict what will be the most likely value. This is called point-forecasting. Although knowing in advance the expected value of a time series is useful in almost every business case, this kind of prediction does not provide any information about the confidence of the model nor the prediction uncertainty.</p> <p>Probabilistic forecasting, as opposed to point-forecasting, is a family of techniques that allow for predicting the expected distribution of the outcome instead of a single future value. This type of forecasting provides much rich information since it allows for creating prediction intervals, the range of likely values where the true value may fall. More formally, a prediction interval defines the interval within which the true value of the response variable is expected to be found with a given probability.</p> <p>There are multiple ways to estimate prediction intervals, most of which require that the residuals (errors) of the model follow a normal distribution. When this property cannot be assumed, two alternatives commonly used are bootstrapping and quantile regression. To illustrate how skforecast allows estimating prediction intervals for multi-step forecasting, the following examples are shown:</p> <ul> <li><p>Prediction intervals based on bootstrapped residuals and recursive-multi-step forecaster.</p> </li> <li><p>Prediction intervals based on quantile regression and direct-multi-step forecaster.</p> </li> </ul>"},{"location":"user_guides/probabilistic-forecasting.html#prediction-intervals-using-bootstrapped-residuals","title":"Prediction intervals using bootstrapped residuals\u00b6","text":"<p>The error of a one-step-ahead forecast is defined as the difference between the actual value and the predicted value ($e_t = y_t - \\hat{y}_{t|t-1}$). By assuming that future errors will be similar to past errors, it is possible to simulate different predictions by taking samples from the collection of errors previously seen in the past (i.e., the residuals) and adding them to the predictions.</p> <p> Diagram bootstrapping prediction process. </p> <p>Repeatedly performing this process creates a collection of slightly different predictions, which represent the distribution of possible outcomes due to the expected variance in the forecasting process.</p> <p> Bootstrapping predictions. </p> <p>Using the outcome of the bootstrapping process, prediction intervals can be computed by calculating the $\u03b1/2$ and $1 \u2212 \u03b1/2$ percentiles at each forecasting horizon.</p> <p> </p> <p>Alternatively, it is also possible to fit a parametric distribution for each forecast horizon.</p> <p>One of the main advantages of this strategy is that it requires only a single model to estimate any interval. However, performing hundreds or thousands of bootstrapping iterations can be computationally expensive and may not always be feasible.</p>"},{"location":"user_guides/probabilistic-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#intervals-with-in-sample-residuals","title":"Intervals with In-sample residuals\u00b6","text":"<p>By default, intervals can be computed using in-sample residuals (residuals from the training set), either by calling the <code>predict_interval()</code> method, or by performing a full backtesting procedure.</p> <p>However, this can result in intervals that are too narrow (overly optimistic).</p>"},{"location":"user_guides/probabilistic-forecasting.html#out-sample-residuals-non-conditioned-on-predicted-values","title":"Out-sample residuals (non-conditioned on predicted values)\u00b6","text":"<p>To address this issue, it is possible to use out-sample residuals (residuals from a validation set not seen during training) to estimate the prediction intervals. These residuals can be obtained through backtesting.</p>"},{"location":"user_guides/probabilistic-forecasting.html#intervals-conditioned-on-predicted-values-binned-residuals","title":"Intervals conditioned on predicted values (binned residuals)\u00b6","text":"<p>The bootstrapping process assumes that the residuals are independently distributed so that they can be used independently of the predicted value. In reality, this is rarely true; in most cases, the magnitude of the residuals is correlated with the magnitude of the predicted value. In this case, for example, one would hardly expect the error to be the same when the predicted number of users is close to zero as when it is in the hundreds.</p> <p>To account for the dependence between the residuals and the predicted values, skforecast allows to partition the residuals into K bins, where each bin is associated with a range of predicted values. Using this strategy, the bootstrapping process samples the residuals from different bins depending on the predicted value, which can improve the coverage of the interval while adjusting the width if necessary, allowing the model to better distribute the uncertainty of its predictions.</p> <p>Internally, skforecast uses a <code>sklearn.preprocessing.KBinsDiscretizer</code> with the parameters: <code>n_bins=15</code>, <code>encode=ordinal</code>, <code>strategy=quantile</code>, <code>subsample=10000</code>, <code>random_state=789654</code>, <code>dtype=np.float64</code>. The binning process can be adjusted using the argument <code>binner_kwargs</code> of the Forecaster object.</p>"},{"location":"user_guides/probabilistic-forecasting.html#predict-bootstraping-interval-quantile-and-distribution","title":"Predict bootstraping, interval, quantile and distribution\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#prediction-intervals-using-quantile-regression-models","title":"Prediction intervals using quantile regression models\u00b6","text":"<p>As opposed to linear regression, which is intended to estimate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating the conditional quantiles of the response variable. For a continuous distribution function, the $\\alpha$-quantile $Q_{\\alpha}(x)$ is defined such that the probability of $Y$ being smaller than $Q_{\\alpha}(x)$ is, for a given $X=x$, equal to $\\alpha$. For example, 36% of the population values are lower than the quantile  $Q=0.36$. The most known quantile is the 50%-quantile, more commonly called the median.</p> <p>By combining the predictions of two quantile regressors, it is possible to build an interval. Each model estimates one of the limits of the interval. For example, the models obtained for $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval (90% - 10% = 80%).</p> <p>Several machine learning algorithms are capable of modeling quantiles. Some of them are:</p> <ul> <li><p>LightGBM</p> </li> <li><p>XGBoost</p> </li> <li><p>CatBoost</p> </li> <li><p>Scikit-learn HistGradientBoostingRegressor</p> </li> <li><p>Scikit-learn QuantileRegressor</p> </li> <li><p>skranger quantile RandomForest</p> </li> </ul> <p>Just as the squared-error loss function is used to train models that predict the mean value, a specific loss function is needed in order to train models that predict quantiles. The most common metric used for quantile regression is calles quantile loss  or pinball loss:</p> <p>$$\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)$$</p> <p>where $\\alpha$ is the target quantile, $y$ the real value and $\\hat{y}$ the quantile prediction.</p> <p>It can be seen that loss differs depending on the evaluated quantile. The higher the quantile, the more the loss function penalizes underestimates, and the less it penalizes overestimates. As with MSE and MAE, the goal is to minimize its values (the lower loss, the better).</p> <p>Two disadvantages of quantile regression, compared to the bootstrap approach to prediction intervals, are that each quantile needs its regressor and quantile regression is not available for all types of regression models. However, once the models are trained, the inference is much faster since no iterative process is needed.</p> <p>This type of prediction intervals can be easily estimated using ForecasterAutoregDirect and ForecasterAutoregMultiVariate models.</p>"},{"location":"user_guides/probabilistic-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#quantile-regresion-models","title":"Quantile regresion models\u00b6","text":"<p>An 80% prediction interval is estimated for 7 steps-ahead predictions using quantile regression. A LightGBM gradient boosting model is trained in this example, however, the reader may use any other model just replacing the definition of the regressor.</p>"},{"location":"user_guides/probabilistic-forecasting.html#predictions-backtesting","title":"Predictions (backtesting)\u00b6","text":"<p>Once the quantile forecasters are trained, they can be used to predict each of the bounds of the forecasting interval.</p>"},{"location":"user_guides/save-load-forecaster.html","title":"Save and load forecaster","text":"<p> \u270e Note </p> <p>Learn how to use forecaster models in production.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = \"forecaster_001\"\n             )\n\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=3)\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = \"forecaster_001\"              )  forecaster.fit(y=data['y']) forecaster.predict(steps=3) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Save model\n# ==============================================================================\nsave_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False)\n</pre> # Save model # ============================================================================== save_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False) In\u00a0[5]: Copied! <pre># Load model\n# ==============================================================================\nforecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True)\n</pre> # Load model # ============================================================================== forecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True) <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2024-07-29 18:14:22 \nLast fit date: 2024-07-29 18:14:22 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: forecaster_001 \n\n</pre> In\u00a0[6]: Copied! <pre># Predict\n# ==============================================================================\nforecaster_loaded.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster_loaded.predict(steps=3) Out[6]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[7]: Copied! <pre># Forecaster identifier\n# ==============================================================================\nforecaster.forecaster_id\n</pre> # Forecaster identifier # ============================================================================== forecaster.forecaster_id Out[7]: <pre>'forecaster_001'</pre> In\u00a0[8]: Copied! <pre># Custom function to create predictors\n# ==============================================================================\ndef create_predictors(y):\n    \"\"\"\n    Create first 5 lags of a time series.\n    \"\"\"\n    \n    lags = y[-1:-6:-1]\n    \n    return lags\n</pre> # Custom function to create predictors # ============================================================================== def create_predictors(y):     \"\"\"     Create first 5 lags of a time series.     \"\"\"          lags = y[-1:-6:-1]          return lags In\u00a0[9]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor      = RandomForestRegressor(random_state=123),\n                 fun_predictors = create_predictors,\n                 window_size    = 5\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor      = RandomForestRegressor(random_state=123),                  fun_predictors = create_predictors,                  window_size    = 5              )  forecaster.fit(y=data['y']) <p>The <code>save_forecaster</code> function will save all the functions used to create it in different files. In this case, the created module is <code>create_predictors.py</code>.</p> In\u00a0[10]: Copied! <pre># Save model and custom function\n# ==============================================================================\nsave_forecaster(\n    forecaster, \n    file_name = 'forecaster_custom.joblib', \n    save_custom_functions = True, \n    verbose = False\n)\n</pre> # Save model and custom function # ============================================================================== save_forecaster(     forecaster,      file_name = 'forecaster_custom.joblib',      save_custom_functions = True,      verbose = False ) <p>These functions must be imported into the environment where the Forecaster is going to be loaded.</p> In\u00a0[11]: Copied! <pre># Load model and custom function\n# ==============================================================================\nfrom create_predictors import create_predictors\n\nforecaster_loaded = load_forecaster('forecaster_custom.joblib', verbose=True)\n</pre> # Load model and custom function # ============================================================================== from create_predictors import create_predictors  forecaster_loaded = load_forecaster('forecaster_custom.joblib', verbose=True) <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2024-07-29 18:14:23 \nLast fit date: 2024-07-29 18:14:23 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None \n\n</pre>"},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecasters","title":"Save and load forecasters\u00b6","text":"<p>Skforecast models can be easily saved and loaded from disk using the pickle or joblib library. Two handy functions, <code>save_forecaster</code> and <code>load_forecaster</code> are available to streamline this process. See below for a simple example.</p> <p>A <code>forecaster_id</code> has been included when initializing the Forecaster, this may help to identify the target of the model.</p>"},{"location":"user_guides/save-load-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecaster-model","title":"Save and load forecaster model\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-functions","title":"Saving and Loading a Forecaster Model with Custom Functions\u00b6","text":"<p>Sometimes external functions are needed when creating a Forecaster object. For example:</p> <ul> <li><p>A custom function to create window and custom features.</p> </li> <li><p>A function to reduce the impact of some dates on the model, Weighted Time Series Forecasting.</p> </li> </ul> <p>For your code to work properly, these functions must be available in the environment where the Forecaster is loaded.</p>"},{"location":"user_guides/skforecast-in-GPU.html","title":"Skforecast in GPU","text":"<p>Traditionally, machine learning algorithms are executed on CPUs (Central Processing Units), which are general-purpose processors that are designed to handle a wide range of tasks. However, CPUs are not optimized for the highly parallelized matrix operations that are required by many machine learning algorithms, which can result in slow training times and limited scalability. GPUs, on the other hand, are designed specifically for parallel processing and can perform thousands of mathematical operations simultaneously, making them ideal for training and deploying large-scale machine learning models.</p> <p>Three popular machine learning libraries that have implemented GPU acceleration are XGBoost, LightGBM and CatBoost. These libraries are used for building gradient boosting models, which are a type of machine learning algorithm that is highly effective for a wide range of tasks, including forecasting. With GPU acceleration, these libraries can significantly reduce the training time required to build these models and improve their scalability.</p> <p>Despite the significant advantages offered by GPUs (specifically Nvidia GPUs) in accelerating machine learning computations, access to them is often limited due to high costs or other practical constraints. Fortunatelly, Google Colaboratory (Colab), a free Jupyter notebook environment, allows users to run Python code in the cloud, with access to powerful hardware resources such as GPUs. This makes it an excellent platform for experimenting with machine learning models, especially those that require intensive computations.</p> <p>The following sections demonstrate how to install and use XGBoost and LightGBM with GPU acceleration to create powerful forecasting models.</p> <p> \u270e Note </p> <p>The following code assumes that the user is executing it in Google Colab with an activated GPU runtime.</p> <ul> <li>Skforecast in GPU: XGBoost</li> <li>Skforecast in GPU: LightGBM</li> <li>Skforecast in GPU: LightGBM</li> </ul> <p>When creating the model with XGBoost version &gt;= 2.0, two arguments are need to indicate XGBoost to run in GPU, if it available: <code>device='cuda'</code> and <code>tree_method='hist'</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom xgboost import XGBRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport torch\nimport os\nimport sys\nimport psutil\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg import torch import os import sys import psutil In\u00a0[\u00a0]: Copied! <pre># Print information abput the GPU and CPU\n# ==============================================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n\nprint(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n</pre> # Print information abput the GPU and CPU # ============================================================================== device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device)  if device.type == 'cuda':     print(torch.cuda.get_device_name(0))     print('Memory Usage:')     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')     print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')  print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\") In\u00a0[\u00a0]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a XGBRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 n_estimators = 5000,\n                                 tree_method  = 'hist',\n                                 device       = 'cuda'\n                             ),\n                 lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a XGBRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  n_estimators = 5000,                                  tree_method  = 'hist',                                  device       = 'cuda'                              ),                  lags = 20              )  forecaster.fit(y=data) <p>When creating the model with XGBoost version &lt; 2.0, two arguments are need to indicate XGBoost to run in GPU, if it available: <code>tree_method='gpu_hist'</code> and <code>gpu_id=0</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a XGBRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 n_estimators = 5000,\n                                 tree_method  = 'gpu_hist',\n                                 gpu_id       = 0\n                             ),\n                 lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a XGBRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  n_estimators = 5000,                                  tree_method  = 'gpu_hist',                                  gpu_id       = 0                              ),                  lags = 20              )  forecaster.fit(y=data) In\u00a0[\u00a0]: Copied! <pre>!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n</pre> !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm In\u00a0[\u00a0]: Copied! <pre>!git clone --recursive https://github.com/Microsoft/LightGBM\n</pre> !git clone --recursive https://github.com/Microsoft/LightGBM In\u00a0[\u00a0]: Copied! <pre>!apt-get install -y -qq libboost-all-dev\n</pre> !apt-get install -y -qq libboost-all-dev In\u00a0[\u00a0]: Copied! <pre>%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)\n</pre> %%bash cd LightGBM rm -r build mkdir build cd build cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc) In\u00a0[\u00a0]: Copied! <pre>!cd LightGBM/python-package/;python3 setup.py install --precompile\n</pre> !cd LightGBM/python-package/;python3 setup.py install --precompile In\u00a0[\u00a0]: Copied! <pre>!mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM\n</pre> !mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd !rm -r LightGBM <p>Once all the above installation has been executed, it is necessary to restart the runtime (kernel).</p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom lightgbm  import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from lightgbm  import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[\u00a0]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a LGBMRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),\n                 lags      = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a LGBMRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),                  lags      = 20              )  forecaster.fit(y=data)"},{"location":"user_guides/skforecast-in-GPU.html#skforecast-in-gpu","title":"Skforecast in GPU\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#version-20","title":"Version &gt;= 2.0\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#version-20","title":"Version &lt; 2.0\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html","title":"Data transformation","text":"In\u00a0[20]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection import grid_search_forecaster from skforecast.datasets import fetch_dataset In\u00a0[21]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\"h2o_exog\")\n</pre> # Download data # ============================================================================== data = fetch_dataset(\"h2o_exog\") <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[22]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata.index.name = 'date'\n# Add an extra categorical variable\ndata['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1))\ndata.head()\n</pre> # Data preprocessing # ============================================================================== data.index.name = 'date' # Add an extra categorical variable data['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1)) data.head() Out[22]: y exog_1 exog_2 exog_3 date 1992-04-01 0.379808 0.958792 1.166029 A 1992-05-01 0.361801 0.951993 1.117859 A 1992-06-01 0.410534 0.952955 1.067942 A 1992-07-01 0.483389 0.958078 1.097376 A 1992-08-01 0.475463 0.956370 1.122199 A In\u00a0[23]: Copied! <pre># Create and fit forecaster that scales the input series\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster that scales the input series # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[23]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: StandardScaler() \nTransformer for exog: None \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2024-08-13 20:55:49 \nLast fit date: 2024-08-13 20:55:49 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> In\u00a0[24]: Copied! <pre># Create and fit forecaster with same tranformation for all exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = StandardScaler()\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster with same tranformation for all exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = StandardScaler()              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[24]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: StandardScaler() \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2024-08-13 20:55:50 \nLast fit date: 2024-08-13 20:55:50 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>It is also possible to apply a different transformation to each exogenous variable making use of <code>ColumnTransformer</code>.</p> In\u00a0[25]: Copied! <pre># Create and fit forecaster with different transformations for each exog variable\n# ==============================================================================\ntransformer_exog = ColumnTransformer(\n                       [('scale_1', StandardScaler(), ['exog_1']),\n                        ('scale_2', StandardScaler(), ['exog_2']),\n                        ('onehot', OneHotEncoder(), ['exog_3']),\n                       ],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = transformer_exog\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']])\nforecaster\n</pre> # Create and fit forecaster with different transformations for each exog variable # ============================================================================== transformer_exog = ColumnTransformer(                        [('scale_1', StandardScaler(), ['exog_1']),                         ('scale_2', StandardScaler(), ['exog_2']),                         ('onehot', OneHotEncoder(), ['exog_3']),                        ],                        remainder = 'passthrough',                        verbose_feature_names_out = False                    )  forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = transformer_exog              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']]) forecaster Out[25]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('scale_1', StandardScaler(), ['exog_1']),\n                                ('scale_2', StandardScaler(), ['exog_2']),\n                                ('onehot', OneHotEncoder(), ['exog_3'])],\n                  verbose_feature_names_out=False) \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2', 'exog_3'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2024-08-13 20:55:50 \nLast fit date: 2024-08-13 20:55:50 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>It is possible to verify if the data transformation has been applied correctly by examining the training matrices. The training matrices should reflect the data transformation that was specified using the <code>transformer_y</code> or <code>transformer_exog</code> arguments.</p> In\u00a0[26]: Copied! <pre>X_train, y_train = forecaster.create_train_X_y(\n                       y    = data['y'],\n                       exog = data[['exog_1', 'exog_2', 'exog_3']]\n                   )\n</pre> X_train, y_train = forecaster.create_train_X_y(                        y    = data['y'],                        exog = data[['exog_1', 'exog_2', 'exog_3']]                    ) In\u00a0[27]: Copied! <pre>X_train.head(4)\n</pre> X_train.head(4) Out[27]: lag_1 lag_2 lag_3 exog_1 exog_2 exog_3_A exog_3_B date 1992-07-01 0.410534 0.361801 0.379808 -2.119529 -2.135088 1.0 0.0 1992-08-01 0.483389 0.410534 0.361801 -2.131024 -1.996017 1.0 0.0 1992-09-01 0.475463 0.483389 0.410534 -2.109222 -1.822392 1.0 0.0 1992-10-01 0.534761 0.475463 0.483389 -2.132137 -1.590667 1.0 0.0 In\u00a0[28]: Copied! <pre>y_train.head(4)\n</pre> y_train.head(4) Out[28]: <pre>date\n1992-07-01    0.483389\n1992-08-01    0.475463\n1992-09-01    0.534761\n1992-10-01    0.568606\nFreq: MS, Name: y, dtype: float64</pre> <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation.</p> <p>Scikit-learn's FunctionTransformer can be used to incorporate custom transformers, such as a logarithmic transformation, in the forecaster object. To implement this, a user-defined transformation function can be created and then passed to the <code>FunctionTransformer</code>. Detailed information on how to use FunctionTransformer can be found in the scikit-learn documentation.</p> <p> \u26a0 Warning </p> <p>For versions 1.1.0 &gt;= scikit-learn &lt;= 1.2.0 <code>sklearn.preprocessing.FunctionTransformer.inverse_transform</code> does not support DataFrames that are all numerical when <code>check_inverse=True</code>. It will raise an Exception which is fixed in scikit-learn 1.2.1.</p> <p>More info: https://scikit-learn.org/stable/whats_new/v1.2.html#version-1-2-1</p> In\u00a0[29]: Copied! <pre># Create custom transformer\n# =============================================================================\ndef log_transform(x):\n    \"\"\" \n    Calculate log adding 1 to avoid calculation errors if x is very close to 0.\n    \"\"\"\n    return np.log(x+1)\n\ndef exp_transform(x):\n    \"\"\"\n    Inverse of log_transform.\n    \"\"\"\n    return np.exp(x) - 1\n\ntransformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)\n\n# Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 lags          = 3,\n                 transformer_y = transformer_y\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create custom transformer # ============================================================================= def log_transform(x):     \"\"\"      Calculate log adding 1 to avoid calculation errors if x is very close to 0.     \"\"\"     return np.log(x+1)  def exp_transform(x):     \"\"\"     Inverse of log_transform.     \"\"\"     return np.exp(x) - 1  transformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)  # Create and train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  lags          = 3,                  transformer_y = transformer_y              )  forecaster.fit(y=data['y']) <p>If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[30]: Copied! <pre>forecaster.predict(steps=4)\n</pre> forecaster.predict(steps=4) Out[30]: <pre>2008-07-01    0.776206\n2008-08-01    0.775471\n2008-09-01    0.777200\n2008-10-01    0.777853\nFreq: MS, Name: pred, dtype: float64</pre> <p> \u26a0 Warning </p> <p>Starting from version 0.4.0, skforecast permits the usage of scikit-learn pipelines as regressors. It is important to note that ColumnTransformer cannot be included in the pipeline; thus, the same transformation will be applied to both the modeled series and all exogenous variables. However, if the preprocessing transformations only apply to specific columns, then they need to be applied separately using <code>transformer_y</code> and <code>transformer_exog</code>.</p> In\u00a0[31]: Copied! <pre>pipe = make_pipeline(StandardScaler(), Ridge())\npipe\n</pre> pipe = make_pipeline(StandardScaler(), Ridge()) pipe Out[31]: <pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> \u00a0Ridge?Documentation for Ridge<pre>Ridge()</pre> In\u00a0[32]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags      = 10\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags      = 10              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[32]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2024-08-13 20:55:51 \nLast fit date: 2024-08-13 20:55:51 \nSkforecast version: 0.13.0 \nPython version: 3.12.4 \nForecaster id: None </pre> <p>When performing a grid search over a scikit-learn pipeline, the model's name precedes the parameters' name.</p> In\u00a0[33]: Copied! <pre># Hyperparameter grid search using a scikit-learn pipeline\n# ==============================================================================\npipe = make_pipeline(StandardScaler(), Ridge())\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags = 10  # This value will be replaced in the grid search\n             )\n\n# Regressor's hyperparameters\nparam_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}\n\n# Lags used as predictors\nlags_grid = [5, 24, [1, 2, 3, 23, 24]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data['y'],\n                   exog               = data[['exog_1', 'exog_2']],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 5,\n                   metric             = 'mean_absolute_error',\n                   refit              = False,\n                   initial_train_size = len(data.loc[:'2000-04-01']),\n                   return_best        = True,\n                   verbose            = False,\n                   show_progress      = True\n               )\n\nresults_grid.head(4)\n</pre> # Hyperparameter grid search using a scikit-learn pipeline # ============================================================================== pipe = make_pipeline(StandardScaler(), Ridge()) forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags = 10  # This value will be replaced in the grid search              )  # Regressor's hyperparameters param_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}  # Lags used as predictors lags_grid = [5, 24, [1, 2, 3, 23, 24]]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data['y'],                    exog               = data[['exog_1', 'exog_2']],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 5,                    metric             = 'mean_absolute_error',                    refit              = False,                    initial_train_size = len(data.loc[:'2000-04-01']),                    return_best        = True,                    verbose            = False,                    show_progress      = True                )  results_grid.head(4) <pre>Number of models compared: 30.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'ridge__alpha': np.float64(0.001)}\n  Backtesting metric: 6.845311709559406e-05\n\n</pre> Out[33]: lags lags_label params mean_absolute_error ridge__alpha 0 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'ridge__alpha': 0.001} 0.000068 0.001000 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.001} 0.000188 0.001000 1 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'ridge__alpha': 0.007742636826811269} 0.000526 0.007743 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.007742636826811269} 0.001413 0.007743 In\u00a0[34]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[34]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[35]: Copied! <pre># Series transformation: same transformation for all series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\nforecaster.fit(series=data)\n</pre> # Series transformation: same transformation for all series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = None              ) forecaster.fit(series=data)  <p>It is possible to access the fitted transformers for each series through the <code>transformers_series_</code> attribute. This allows verification that each transformer has been trained independently.</p> In\u00a0[36]: Copied! <pre># Mean and scale of the transformer for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\")\n</pre> # Mean and scale of the transformer for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\") <pre>Series item_1: StandardScaler() mean=[22.37366364], scale=[2.54258317]\nSeries item_2: StandardScaler() mean=[16.26942518], scale=[4.89965692]\nSeries item_3: StandardScaler() mean=[17.19276546], scale=[5.43694388]\nSeries _unknown_level: StandardScaler() mean=[18.61195143], scale=[5.21803675]\n</pre> In\u00a0[37]: Copied! <pre># Series transformation: different transformation for each series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=data)\n</pre> # Series transformation: different transformation for each series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},                  transformer_exog   = None              )  forecaster.fit(series=data) <pre>/home/ubuntu/anaconda3/envs/skforecast_13_py12/lib/python3.12/site-packages/skforecast/utils/utils.py:255: IgnoredArgumentWarning: {'item_3'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> In\u00a0[38]: Copied! <pre># Transformer trained for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    if v is not None:\n        print(f\"Series {k}: {v.get_params()}\")\n    else:\n        print(f\"Series {k}: {v}\")\n</pre> # Transformer trained for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     if v is not None:         print(f\"Series {k}: {v.get_params()}\")     else:         print(f\"Series {k}: {v}\") <pre>Series item_1: {'copy': True, 'with_mean': True, 'with_std': True}\nSeries item_2: {'clip': False, 'copy': True, 'feature_range': (0, 1)}\nSeries item_3: None\nSeries _unknown_level: {'copy': True, 'with_mean': True, 'with_std': True}\n</pre>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data-transformation-and-pipelines","title":"Data transformation and pipelines\u00b6","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <ul> <li><p><code>transformer_y</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform. Scikit-learn ColumnTransformer is not allowed since they do not have the inverse_transform method.</p> </li> <li><p><code>transformer_exog</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. Scikit-learn ColumnTransformer can be used if the preprocessing transformations only apply to some specific columns or if different transformations are needed for different columns. For example, scale numeric features and one hot encode categorical ones.</p> </li> </ul> <p>Transformations are learned and applied before training the forecaster and are automatically used when calling <code>predict</code>. The output of <code>predict</code> is always on the same scale as the original series y.</p> <p>Although skforecast has allowed using scikit-learn pipelines as regressors since version 0.4.0, it is recommended to use <code>transformer_y</code> and <code>transformer_exog</code> instead.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-input-series","title":"Transforming input series\u00b6","text":"<p>The following example shows how to include a transformer that scales the input series y.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-exogenous-variables","title":"Transforming exogenous variables\u00b6","text":"<p>The following example shows how to apply the same transformation (scaling) to all exogenous variables.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#custom-transformers","title":"Custom transformers\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#scikit-learn-pipelines","title":"Scikit-learn pipelines\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-multiple-input-series-in-global-models","title":"Transforming multiple input series in global models\u00b6","text":"<p>When using global forecasting models (ForecasterAutoregMultiSeries or ForecasterAutoregMultiVariate) the <code>transformer_series</code> argument replaces <code>transformer_y</code>. Three diferent options are available:</p> <ul> <li><p><code>transformer_series</code> is a single transformer: When a single transformer is provided, it is automatically cloned for each individual series. Each cloned transformer is then trained separately on one of the series.</p> </li> <li><p><code>transformer_series</code> is a dictionary: A different transformer can be specified for each series by passing a dictionary where the keys correspond to the series names and the values are the transformers. Each series is transformed according to its designated transformer. When this option is used, it is mandatory to include a transformer for unknown series, which is indicated by the key <code>'_unknown_level'</code>.</p> </li> <li><p><code>transformer_series</code> is None: no transformations are applied to any of the series.</p> </li> </ul> <p>Regardless of the configuration, each series is transformed independently. Even when using a single transformer, it is cloned internally and applied separately to each series.</p>"},{"location":"user_guides/stacking-ensemble-models-forecasting.html","title":"Stacking multiple models","text":"<p>In machine learning, stacking is an ensemble technique that combines multiple models to reduce their biases and improve predictive performance. Specifically, the predictions of each model (base models) are stacked and used as input to a final model (metamodel) to compute the prediction.</p> <p>Stacking is effective because it leverages the strengths of different algorithms and attempts to mitigate their individual weaknesses. By combining multiple models, it can capture complex patterns in the data and improve prediction accuracy.</p> <p>However, stacking can be computationally expensive and requires careful tuning to avoid overfitting. To this end, it is highly recommended to train the final estimator through cross-validation. In addition, obtaining diverse and well-performing base models is critical to the success of the stacking technique.</p> <p>With scikit-learn, it is very easy to combine multiple regressors thanks to the StackingRegressor class. The <code>estimators</code> parameter corresponds to the list of the estimators (base learners) that will be stacked in parallel on the input data. The <code>final_estimator</code> (metamodel) will use the predictions of the estimators as input.</p> <p> \u270e Note </p> <p>See Stacking (ensemble) machine learning models to improve forecasting for a more detailed example of stacking models.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble  import StackingRegressor\nfrom sklearn.model_selection  import KFold\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.datasets import fetch_dataset\n\n# Warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.ensemble  import StackingRegressor from sklearn.model_selection  import KFold  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.datasets import fetch_dataset  # Warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name = 'fuel_consumption')\ndata = data.loc[:\"2019-01-01\", ['Gasolinas']]\ndata = data.rename(columns = {'Gasolinas':'consumption'})\ndata.index.name = 'date'\ndata['consumption'] = data['consumption']/100000\ndata.head(3)\n</pre> # Data # ============================================================================== data = fetch_dataset(name = 'fuel_consumption') data = data.loc[:\"2019-01-01\", ['Gasolinas']] data = data.rename(columns = {'Gasolinas':'consumption'}) data.index.name = 'date' data['consumption'] = data['consumption']/100000 data.head(3) <pre>fuel_consumption\n----------------\nMonthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.\nObtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos Petrol\u00edferos and\nCorporaci\u00f3n de Derecho P\u00fablico tutelada por el Ministerio para la Transici\u00f3n\nEcol\u00f3gica y el Reto Demogr\u00e1fico. https://www.cores.es/es/estadisticas\nShape of the dataset: (644, 5)\n</pre> Out[2]: consumption date 1969-01-01 1.668752 1969-02-01 1.554668 1969-03-01 1.849837 <p>In addition to the past values of the series (lags), an additional variable indicating the month of the year is added. This variable is included in the model to capture the seasonality of the series.</p> In\u00a0[3]: Copied! <pre># Calendar features\n# ==============================================================================\ndata['month_of_year'] = data.index.month\ndata.head(3)\n</pre> # Calendar features # ============================================================================== data['month_of_year'] = data.index.month data.head(3) Out[3]: consumption month_of_year date 1969-01-01 1.668752 1 1969-02-01 1.554668 2 1969-03-01 1.849837 3 <p>To facilitate the training of the models, the search for optimal hyperparameters, and the evaluation of their predictive accuracy, the data are divided into three separate sets: training, validation, and test.</p> In\u00a0[4]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2007-12-01 23:59:00'\nend_validation = '2012-12-01 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validacion : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2007-12-01 23:59:00' end_validation = '2012-12-01 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validacion : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 1969-01-01 00:00:00 --- 2007-12-01 00:00:00  (n=468)\nDates validacion : 2008-01-01 00:00:00 --- 2012-12-01 00:00:00  (n=60)\nDates test       : 2013-01-01 00:00:00 --- 2019-01-01 00:00:00  (n=73)\n</pre> In\u00a0[5]: Copied! <pre># Create stacking regressor\n# ==============================================================================\nparams_ridge = {'alpha': 0.001}\nparams_lgbm = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'verbose': -1}\n\nestimators = [\n    ('ridge', Ridge(**params_ridge)),\n    ('lgbm', LGBMRegressor(random_state=42, **params_lgbm)),\n]\n\nstacking_regressor = StackingRegressor(\n                         estimators = estimators,\n                         final_estimator = Ridge(),\n                         cv = KFold(n_splits=5, shuffle=False)\n                     )\nstacking_regressor\n</pre> # Create stacking regressor # ============================================================================== params_ridge = {'alpha': 0.001} params_lgbm = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'verbose': -1}  estimators = [     ('ridge', Ridge(**params_ridge)),     ('lgbm', LGBMRegressor(random_state=42, **params_lgbm)), ]  stacking_regressor = StackingRegressor(                          estimators = estimators,                          final_estimator = Ridge(),                          cv = KFold(n_splits=5, shuffle=False)                      ) stacking_regressor Out[5]: <pre>StackingRegressor(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n                  estimators=[('ridge', Ridge(alpha=0.001)),\n                              ('lgbm',\n                               LGBMRegressor(max_depth=5, n_estimators=500,\n                                             random_state=42, verbose=-1))],\n                  final_estimator=Ridge())</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0StackingRegressor?Documentation for StackingRegressoriNot fitted<pre>StackingRegressor(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n                  estimators=[('ridge', Ridge(alpha=0.001)),\n                              ('lgbm',\n                               LGBMRegressor(max_depth=5, n_estimators=500,\n                                             random_state=42, verbose=-1))],\n                  final_estimator=Ridge())</pre> ridge\u00a0Ridge?Documentation for Ridge<pre>Ridge(alpha=0.001)</pre> lgbmLGBMRegressor<pre>LGBMRegressor(max_depth=5, n_estimators=500, random_state=42, verbose=-1)</pre> final_estimator\u00a0Ridge?Documentation for Ridge<pre>Ridge()</pre> In\u00a0[6]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = stacking_regressor,\n                 lags = 12 # Last 12 months used as predictors\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = stacking_regressor,                  lags = 12 # Last 12 months used as predictors              ) In\u00a0[7]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster         = forecaster,\n                          y                  = data['consumption'],\n                          exog               = data['month_of_year'],\n                          initial_train_size = len(data.loc[:end_validation]),\n                          fixed_train_size   = False,\n                          steps              = 12, # Forecast horizon\n                          refit              = False,\n                          metric             = 'mean_squared_error',\n                          n_jobs             = 'auto',\n                          verbose            = False\n                      )        \n\nmetric\n</pre> # Backtesting on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster         = forecaster,                           y                  = data['consumption'],                           exog               = data['month_of_year'],                           initial_train_size = len(data.loc[:end_validation]),                           fixed_train_size   = False,                           steps              = 12, # Forecast horizon                           refit              = False,                           metric             = 'mean_squared_error',                           n_jobs             = 'auto',                           verbose            = False                       )          metric <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> Out[7]: mean_squared_error 0 0.053776 <p>When using <code>StackingRegressor</code>, the hyperparameters of each regressor must be preceded by the name of the regressor followed by two underscores. For example, the <code>alpha</code> hyperparameter of the ridge regressor must be specified as <code>ridge__alpha</code>. The hyperparameter of the final estimator must be specified with the prefix <code>final_estimator__</code>.</p> In\u00a0[8]: Copied! <pre># Grid search of hyperparameters and lags\n# ==============================================================================\nparam_grid = {\n    'ridge__alpha': [0.1, 1, 10],\n    'lgbm__n_estimators': [100, 500],\n    'lgbm__max_depth': [3, 5, 10],\n    'lgbm__learning_rate': [0.01, 0.1],\n    'final_estimator__alpha': [0.1, 1]\n}\n\n# Lags used as predictors\nlags_grid = [24]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data['consumption'],\n                   exog               = data['month_of_year'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 36,\n                   refit              = False,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   n_jobs             = 'auto',\n                   verbose            = False\n               )\n\nresults_grid.head()\n</pre> # Grid search of hyperparameters and lags # ============================================================================== param_grid = {     'ridge__alpha': [0.1, 1, 10],     'lgbm__n_estimators': [100, 500],     'lgbm__max_depth': [3, 5, 10],     'lgbm__learning_rate': [0.01, 0.1],     'final_estimator__alpha': [0.1, 1] }  # Lags used as predictors lags_grid = [24]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data['consumption'],                    exog               = data['month_of_year'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 36,                    refit              = False,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    n_jobs             = 'auto',                    verbose            = False                )  results_grid.head() <pre>Number of models compared: 72.\n</pre> <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/72 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \n  Parameters: {'final_estimator__alpha': 1, 'lgbm__learning_rate': 0.01, 'lgbm__max_depth': 3, 'lgbm__n_estimators': 100, 'ridge__alpha': 0.1}\n  Backtesting metric: 0.5080664092557889\n\n</pre> Out[8]: lags lags_label params mean_squared_error final_estimator__alpha lgbm__learning_rate lgbm__max_depth lgbm__n_estimators ridge__alpha 36 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.508066 1.0 0.01 3.0 100.0 0.1 37 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.519290 1.0 0.01 3.0 100.0 1.0 42 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.531137 1.0 0.01 5.0 100.0 0.1 48 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.532003 1.0 0.01 10.0 100.0 0.1 69 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.535757 1.0 0.10 10.0 500.0 0.1 <p>Once the best hyperparameters have been determined for each regressor in the ensemble, the test error is computed through back-testing.</p> In\u00a0[9]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster         = forecaster,\n                          y                  = data['consumption'],\n                          exog               = data['month_of_year'],\n                          initial_train_size = len(data.loc[:end_validation]),\n                          fixed_train_size   = False,\n                          steps              = 12, # Forecast horizon\n                          refit              = False,\n                          metric             = 'mean_squared_error',\n                          n_jobs             = 'auto',\n                          verbose            = False\n                      )        \n\nmetric\n</pre> # Backtesting on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster         = forecaster,                           y                  = data['consumption'],                           exog               = data['month_of_year'],                           initial_train_size = len(data.loc[:end_validation]),                           fixed_train_size   = False,                           steps              = 12, # Forecast horizon                           refit              = False,                           metric             = 'mean_squared_error',                           n_jobs             = 'auto',                           verbose            = False                       )          metric <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> Out[9]: mean_squared_error 0 0.013506 <p>When a regressor of type <code>StackingRegressor</code> is used as a regressor in a predictor, its <code>get_feature_importances</code> method will not work. This is because objects of type <code>StackingRegressor</code> do not have either the <code>feature_importances</code> or <code>coef_</code> attribute. Instead, it is necessary to inspect each of the regressors that are part of the stacking.</p> In\u00a0[10]: Copied! <pre># Feature importances for each regressor in the stacking\n# ==============================================================================\nif forecaster.regressor.__class__.__name__ == 'StackingRegressor':\n    importancia_pred = []\n    for regressor in forecaster.regressor.estimators_:\n        try:\n            importancia = pd.DataFrame(\n                data = {\n                    'feature': forecaster.regressor.feature_names_in_,\n                    f'importance_{type(regressor).__name__}': regressor.coef_,\n                    f'importance_abs_{type(regressor).__name__}': np.abs(regressor.coef_)\n                }\n            ).set_index('feature')\n        except:\n            importancia = pd.DataFrame(\n                data = {\n                    'feature': forecaster.regressor.feature_names_in_,\n                    f'importance_{type(regressor).__name__}': regressor.feature_importances_,\n                    f'importance_abs_{type(regressor).__name__}': np.abs(regressor.feature_importances_)\n                }\n            ).set_index('feature')\n        importancia_pred.append(importancia)\n    \n    importancia_pred = pd.concat(importancia_pred, axis=1)\n    \nelse:\n    importancia_pred = forecaster.get_feature_importances()\n    importancia_pred['importance_abs'] = importancia_pred['importance'].abs()\n    importancia_pred = importancia_pred.sort_values(by='importance_abs', ascending=False)\n\nimportancia_pred.head(5)\n</pre> # Feature importances for each regressor in the stacking # ============================================================================== if forecaster.regressor.__class__.__name__ == 'StackingRegressor':     importancia_pred = []     for regressor in forecaster.regressor.estimators_:         try:             importancia = pd.DataFrame(                 data = {                     'feature': forecaster.regressor.feature_names_in_,                     f'importance_{type(regressor).__name__}': regressor.coef_,                     f'importance_abs_{type(regressor).__name__}': np.abs(regressor.coef_)                 }             ).set_index('feature')         except:             importancia = pd.DataFrame(                 data = {                     'feature': forecaster.regressor.feature_names_in_,                     f'importance_{type(regressor).__name__}': regressor.feature_importances_,                     f'importance_abs_{type(regressor).__name__}': np.abs(regressor.feature_importances_)                 }             ).set_index('feature')         importancia_pred.append(importancia)          importancia_pred = pd.concat(importancia_pred, axis=1)      else:     importancia_pred = forecaster.get_feature_importances()     importancia_pred['importance_abs'] = importancia_pred['importance'].abs()     importancia_pred = importancia_pred.sort_values(by='importance_abs', ascending=False)  importancia_pred.head(5) Out[10]: importance_Ridge importance_abs_Ridge importance_LGBMRegressor importance_abs_LGBMRegressor feature lag_1 0.020984 0.020984 59 59 lag_2 0.216998 0.216998 1 1 lag_3 0.188519 0.188519 0 0 lag_4 0.200916 0.200916 0 0 lag_5 0.106734 0.106734 0 0"},{"location":"user_guides/stacking-ensemble-models-forecasting.html#stacking-ensemble-machine-learning-models-to-improve-forecasting","title":"Stacking ensemble machine learning models to improve forecasting\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#libraries-and-data","title":"Libraries and Data\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#stackingregressor","title":"StackingRegressor\u00b6","text":"<p>With scikit-learn it is very easy to combine multiple regressors thanks to the StackingRegressor class.</p> <p>The <code>estimators</code> parameter corresponds to the list of the estimators (base learners) that will be stacked in parallel on the input data. It should be specified as a list of names and estimators. The <code>final_estimator</code> (metamodel) will use the predictions of the estimators as input.</p>"},{"location":"user_guides/stacking-ensemble-models-forecasting.html#hiperparameters-search-of-stackingregressor","title":"Hiperparameters search of StackingRegressor\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#feature-importance-in-stackingregressor","title":"Feature importance in StackingRegressor\u00b6","text":""},{"location":"user_guides/table-of-contents.html","title":"Table of Contents","text":"<p>Welcome to the skforecast user guides! This comprehensive collection of guides is designed to help you navigate through the various features and functionalities of skforecast. Whether you are a beginner or an advanced user, you will find the necessary resources to master time series forecasting with skforecast. Below, you will find the user guides categorized by topic for easier navigation.</p>"},{"location":"user_guides/table-of-contents.html#single-series-forecasters","title":"Single series Forecasters","text":"<ul> <li>Recursive multi-step forecasting</li> <li>Direct multi-step forecasting</li> <li>Forecasting with window and custom features</li> <li>ARIMA and SARIMAX forecasting</li> <li>Foreasting baseline</li> </ul>"},{"location":"user_guides/table-of-contents.html#global-forecasters-multiple-series","title":"Global Forecasters (multiple series)","text":"<ul> <li>Independent multi-time series forecasting</li> <li>Series with different lengths and different exogenous variables</li> <li>Dependent multivariate series forecasting</li> <li>Deep learning Recurrent Neural Networks</li> </ul>"},{"location":"user_guides/table-of-contents.html#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Exogenous variables</li> <li>Feature selection</li> <li>Categorical features</li> <li>Calendars features</li> </ul>"},{"location":"user_guides/table-of-contents.html#model-evaluation-and-tuning","title":"Model Evaluation and Tuning","text":"<ul> <li>Backtesting forecaster</li> <li>Hyperparameter tuning and lags selection</li> <li>Time Series Forecasting Metrics</li> </ul>"},{"location":"user_guides/table-of-contents.html#probabilistic-forecasting","title":"Probabilistic Forecasting","text":"<ul> <li>Probabilistic forecasting</li> </ul>"},{"location":"user_guides/table-of-contents.html#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Data transformations</li> <li>Forecaster in production</li> <li>Save and load forecaster</li> <li>Explainability</li> <li>Forecasting with XGBoost and LightGBM</li> <li>Skforecast in GPU</li> <li>Plotting</li> </ul>"},{"location":"user_guides/table-of-contents.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Input data</li> <li>Datasets</li> <li>FAQ and forecasting tips</li> </ul> <p>We hope you find these guides helpful. If you have any questions or need further assistance, please don't hesitate to reach out to the skforecast community. Remember to visit the FAQ and forecasting tips page for answers to frequently asked questions and forecasting tips.</p>"},{"location":"user_guides/time-series-differentiation.html","title":"Differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p> <p>This methodology is very useful when dealing with time series that exhibit trends, as not all models can capture such trends properly. Among the many machine learning techniques, tree-based models, including decision trees, random forests, and gradient boosting machines (GBMs), stand out for their effectiveness and broad applicability to various domains. Nonetheless, these models are limited in their ability to extrapolate. Their inability to project values outside the observed range during training inevitably results in predicted values that deviate from the underlying trend.</p> <p>Skforecast, version 0.10.0 or higher, introduces a novel <code>differentiation</code> parameter within its forecasters. This parameter indicates that a differentiation process must be applied before training the model, and this task is performed through the internal use of a new class named <code>skforecast.preprocessing.TimeSeriesDifferentiator</code>. It is important to note that the entire differentiation process is automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from skforecast.preprocessing import TimeSeriesDifferentiator In\u00a0[2]: Copied! <pre># Differentiation with TimeSeriesDifferentiator\n# ==============================================================================\ny = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float)\ndiffenciator = TimeSeriesDifferentiator()\ndiffenciator.fit(y)\ny_diff = diffenciator.transform(y)\n\nprint(f\"Original time series   : {y}\")\nprint(f\"Differenced time series: {y_diff}\")\n</pre> # Differentiation with TimeSeriesDifferentiator # ============================================================================== y = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float) diffenciator = TimeSeriesDifferentiator() diffenciator.fit(y) y_diff = diffenciator.transform(y)  print(f\"Original time series   : {y}\") print(f\"Differenced time series: {y_diff}\") <pre>Original time series   : [ 5.  8. 12. 10. 14. 17. 21. 19.]\nDifferenced time series: [nan  3.  4. -2.  4.  3.  4. -2.]\n</pre> <p>The process of differencing can be reversed (integration) using the <code>inverse_transform</code> method.</p> In\u00a0[3]: Copied! <pre>diffenciator.inverse_transform(y_diff)\n</pre> diffenciator.inverse_transform(y_diff) Out[3]: <pre>array([ 5.,  8., 12., 10., 14., 17., 21., 19.])</pre> <p> \u26a0 Warning </p> <p>The inverse transformation process, <code>inverse_transform</code>, is applicable only to the same time series that was previously differentiated using the same <code>TimeSeriesDifferentiator</code> object. This limitation arises from the need to use the initial n values of the time series (n equals the order of differentiation) to successfully reverse the differentiation. These values are stored when the <code>fit</code> method is executed.</p> <p> \u270e Note </p> <p>An additional method <code>inverse_transform_next_window</code> is available in the <code>TimeSeriesDifferentiator</code>. This method is designed to be used inside the Forecasters to reverse the differentiation of the predicted values. If the Forecaster regressor is trained with a differentiated time series, then the predicted values will be differentiated as well. The <code>inverse_transform_next_window</code> method allows to return the predictions to the original scale, with the assumption that they start immediately after the last values observed (<code>last_window</code>).</p> In\u00a0[4]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import StandardScaler from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.preprocessing import TimeSeriesDifferentiator from skforecast.model_selection import backtesting_forecaster In\u00a0[5]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'\n    'master/data/AirPassengers.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y-%m')\ndata = data.set_index('Date')\ndata = data.asfreq('MS')\ndata = data['Passengers']\ndata = data.sort_index()\ndata.head(4)\n\n# Data partition train-test\n# ==============================================================================\nend_train = '1956-12-01 23:59:59'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 2.5))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'     'master/data/AirPassengers.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m') data = data.set_index('Date') data = data.asfreq('MS') data = data['Passengers'] data = data.sort_index() data.head(4)  # Data partition train-test # ============================================================================== end_train = '1956-12-01 23:59:59' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 2.5)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1949-01-01 00:00:00 --- 1956-12-01 00:00:00  (n=96)\nTest dates  : 1957-01-01 00:00:00 --- 1960-12-01 00:00:00  (n=48)\n</pre> <p>Two autoregressive forecasters are created, one with a scikit-learn <code>RandomForestRegressor</code> and the other with an <code>XGBoost</code>. Both are trained on data from 1949-01-01 to 1956-12-01 and produce forecasts for the next 48 months (4 years).</p> In\u00a0[6]: Copied! <pre># Forecasting without differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterAutoreg(\n                    regressor = RandomForestRegressor(random_state=963),\n                    lags      = 12\n                )\nforecaster_gb = ForecasterAutoreg(\n                    regressor = XGBRegressor(random_state=963),\n                    lags      = 12\n                )\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title(f'Forecasting without differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting without differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterAutoreg(                     regressor = RandomForestRegressor(random_state=963),                     lags      = 12                 ) forecaster_gb = ForecasterAutoreg(                     regressor = XGBRegressor(random_state=963),                     lags      = 12                 ) # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title(f'Forecasting without differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 54.79\nError (MAE) Gradient Boosting: 49.16\n</pre> <p>The plot shows that none of the models is capable of accurately predicting the trend. After a few steps, the predictions become nearly constant, close to the maximum values observed in the training data.</p> <p>Next, two new forecasters are trained using the same configuration, but with the argument <code>differentiation = 1</code>. This activates the internal process of differencing (order 1) the time series before training the model, and reverses the differentiation (also known as integration) for the predicted values.</p> In\u00a0[7]: Copied! <pre># Forecasting with differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterAutoreg(\n                    regressor       = RandomForestRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\nforecaster_gb = ForecasterAutoreg(\n                    regressor       = XGBRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title(f'Forecasting with differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting with differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterAutoreg(                     regressor       = RandomForestRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 ) forecaster_gb = ForecasterAutoreg(                     regressor       = XGBRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 ) # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title(f'Forecasting with differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 24.91\nError (MAE) Gradient Boosting: 25.49\n</pre> <p>This time, both models are able to follow the trend in their predictions.</p> <p> \u26a0 Warning </p> <p>Derivation is a useful strategy for overcoming the problems of modeling trend when using a tree-based model, but it is not a magic bullet. By transforming the original time series values into the rate of change (first derivative), the model can extrapolate the original data once the transformation is reversed. Note, however, that the model is still limited to predicting rates of change within the range observed in the training data.</p> <p>For example, if the rate of change increases over time and the changes become larger and larger, the model will underestimate the increases and decreases, and the resulting predictions will deviate from the actual trend.</p> In\u00a0[8]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\ndiferenciator = TimeSeriesDifferentiator(order=1)\ndata_diff = diferenciator.fit_transform(data.to_numpy())\ndata_diff = pd.Series(data_diff, index=data.index).dropna()\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=963),\n                 lags      = 15\n             )\nforecaster.fit(y=data_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\npredictions_1.head(5)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== diferenciator = TimeSeriesDifferentiator(order=1) data_diff = diferenciator.fit_transform(data.to_numpy()) data_diff = pd.Series(data_diff, index=data.index).dropna()  forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=963),                  lags      = 15              ) forecaster.fit(y=data_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' predictions_1.head(5) Out[8]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[9]: Copied! <pre># Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\npredictions_2.head(5)\n</pre> # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1              )  forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) predictions_2.head(5) Out[9]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[10]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>The predictions are the same for both approaches.</p> <p>The results are also equivalent when a transformation, such as standardization, is applied to the time series. In this case, the order of the steps is: transformation -&gt; differentiation -&gt; model fitting -&gt; prediction -&gt; inverse differentiation -&gt; inverse transformation.</p> In\u00a0[11]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nscaler = StandardScaler()\nscaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1))\ndata_scaled = scaler.transform(data.to_numpy().reshape(-1, 1))\ndata_scaled = pd.Series(data_scaled.flatten(), index=data.index)\ndata_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled.to_numpy())\ndata_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()\n\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=963),\n                 lags          = 15,\n             )\nforecaster.fit(y=data_scaled_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data_scaled.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\n# Revert the scaling\npredictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1))\npredictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy())\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\ndisplay(predictions_1.head(5))\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1,\n                 transformer_y = StandardScaler()\n             )\n\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\ndisplay(predictions_2.head(5))\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== scaler = StandardScaler() scaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1)) data_scaled = scaler.transform(data.to_numpy().reshape(-1, 1)) data_scaled = pd.Series(data_scaled.flatten(), index=data.index) data_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled.to_numpy()) data_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()  forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=963),                  lags          = 15,              ) forecaster.fit(y=data_scaled_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data_scaled.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] # Revert the scaling predictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1)) predictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy()) predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' display(predictions_1.head(5))   # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1,                  transformer_y = StandardScaler()              )  forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) display(predictions_2.head(5)) <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>Next, the outcomes of the backtesting process are subjected to a comparative analysis. This comparison is more complex than the previous one, as the process of undoing the differentiation must be performed separately for each backtesting fold.</p> In\u00a0[13]: Copied! <pre># Backtesting with the time series differentiated by preprocessing before training\n# ==============================================================================\nsteps = 5\nforecaster_1 = ForecasterAutoreg(\n                   regressor = RandomForestRegressor(random_state=963),\n                   lags      = 15\n               )\n\n_, predictions_1 = backtesting_forecaster(\n                       forecaster            = forecaster_1,\n                       y                     = data_diff,\n                       steps                 = steps,\n                       metric                = 'mean_squared_error',\n                       initial_train_size    = len(data_diff.loc[:end_train]),\n                       fixed_train_size      = False,\n                       gap                   = 0,\n                       allow_incomplete_fold = True,\n                       refit                 = True,\n                       n_jobs                = 'auto',\n                       verbose               = False,\n                       show_progress         = True  \n                   )\n\n# Revert differentiation of predictions. Predictions of each fold must be reverted\n# individually. An id is added to each prediction to identify the fold to which it belongs.\npredictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'})\nfolds = len(predictions_1) / steps\nfolds = int(np.ceil(folds))\npredictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]\n\n# Add the previously observed value of the time series (only to the first prediction of each fold)\nprevious_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps]\nprevious_overved_values.name = 'previous_overved_value'\npredictions_1 = predictions_1.merge(\n                    previous_overved_values,\n                    left_index  = True,\n                    right_index = True,\n                    how         = 'left'\n                )\npredictions_1 = predictions_1.fillna(0)\npredictions_1['summed_value'] = (\n    predictions_1['pred_diff'] + predictions_1['previous_overved_value']\n)\n\n# Revert differentiation using the cumulative sum by fold\npredictions_1['pred'] = (\n    predictions_1\n    .groupby('backtesting_fold_id')\n    .apply(lambda x: x['summed_value'].cumsum())\n    .to_numpy()\n)\n\npredictions_1.head(5)\n</pre> # Backtesting with the time series differentiated by preprocessing before training # ============================================================================== steps = 5 forecaster_1 = ForecasterAutoreg(                    regressor = RandomForestRegressor(random_state=963),                    lags      = 15                )  _, predictions_1 = backtesting_forecaster(                        forecaster            = forecaster_1,                        y                     = data_diff,                        steps                 = steps,                        metric                = 'mean_squared_error',                        initial_train_size    = len(data_diff.loc[:end_train]),                        fixed_train_size      = False,                        gap                   = 0,                        allow_incomplete_fold = True,                        refit                 = True,                        n_jobs                = 'auto',                        verbose               = False,                        show_progress         = True                      )  # Revert differentiation of predictions. Predictions of each fold must be reverted # individually. An id is added to each prediction to identify the fold to which it belongs. predictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'}) folds = len(predictions_1) / steps folds = int(np.ceil(folds)) predictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]  # Add the previously observed value of the time series (only to the first prediction of each fold) previous_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps] previous_overved_values.name = 'previous_overved_value' predictions_1 = predictions_1.merge(                     previous_overved_values,                     left_index  = True,                     right_index = True,                     how         = 'left'                 ) predictions_1 = predictions_1.fillna(0) predictions_1['summed_value'] = (     predictions_1['pred_diff'] + predictions_1['previous_overved_value'] )  # Revert differentiation using the cumulative sum by fold predictions_1['pred'] = (     predictions_1     .groupby('backtesting_fold_id')     .apply(lambda x: x['summed_value'].cumsum())     .to_numpy() )  predictions_1.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>C:\\Users\\jaesc2\\AppData\\Local\\Temp\\ipykernel_28172\\227738721.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x['summed_value'].cumsum())\n</pre> Out[13]: pred_diff backtesting_fold_id previous_overved_value summed_value pred 1957-01-01 6.00 0 306.0 312.00 312.00 1957-02-01 -9.07 0 0.0 -9.07 302.93 1957-03-01 38.68 0 0.0 38.68 341.61 1957-04-01 -3.58 0 0.0 -3.58 338.03 1957-05-01 3.94 0 0.0 3.94 341.97 In\u00a0[14]: Copied! <pre># Backtesting with the time series differentiated internally\n# ==============================================================================\nforecaster_2 = ForecasterAutoreg(\n                   regressor       = RandomForestRegressor(random_state=963),\n                   lags            = 15,\n                   differentiation = 1\n               )\n\n_, predictions_2 = backtesting_forecaster(\n                       forecaster            = forecaster_2,\n                       y                     = data,\n                       steps                 = steps,\n                       metric                = 'mean_squared_error',\n                       initial_train_size    = len(data.loc[:end_train]),\n                       fixed_train_size      = False,\n                       gap                   = 0,\n                       allow_incomplete_fold = True,\n                       refit                 = True,\n                       n_jobs                = 'auto',\n                       verbose               = False,\n                       show_progress         = True  \n                   )\n\npredictions_2.head(5)\n</pre> # Backtesting with the time series differentiated internally # ============================================================================== forecaster_2 = ForecasterAutoreg(                    regressor       = RandomForestRegressor(random_state=963),                    lags            = 15,                    differentiation = 1                )  _, predictions_2 = backtesting_forecaster(                        forecaster            = forecaster_2,                        y                     = data,                        steps                 = steps,                        metric                = 'mean_squared_error',                        initial_train_size    = len(data.loc[:end_train]),                        fixed_train_size      = False,                        gap                   = 0,                        allow_incomplete_fold = True,                        refit                 = True,                        n_jobs                = 'auto',                        verbose               = False,                        show_progress         = True                      )  predictions_2.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[14]: pred 1957-01-01 312.00 1957-02-01 302.93 1957-03-01 341.61 1957-04-01 338.03 1957-05-01 341.97 In\u00a0[15]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred'])\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred']) <p>Finally, the validation is also performed for the predictions obtained with <code>predict_boostrapping</code>.</p> In\u00a0[16]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nforecaster_1 = ForecasterAutoreg(\n                   regressor = RandomForestRegressor(random_state=963),\n                   lags      = 15\n               )\n\nforecaster_1.fit(y=data_diff.loc[:end_train])\nboot_predictions_diff = forecaster_1.predict_bootstrapping(\n                            steps  = steps,\n                            n_boot = 10\n                        )\n# Revert differentiation of predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\nboot_predictions_1 = boot_predictions_diff.copy()\nboot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0]\nboot_predictions_1 = boot_predictions_1.sort_index()\nboot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,]\nboot_predictions_1 = boot_predictions_1.asfreq('MS')\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster_2 = ForecasterAutoreg(\n                   regressor       = RandomForestRegressor(random_state=963),\n                   lags            = 15,\n                   differentiation = 1\n               )\n\nforecaster_2.fit(y=data.loc[:end_train])\nboot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== forecaster_1 = ForecasterAutoreg(                    regressor = RandomForestRegressor(random_state=963),                    lags      = 15                )  forecaster_1.fit(y=data_diff.loc[:end_train]) boot_predictions_diff = forecaster_1.predict_bootstrapping(                             steps  = steps,                             n_boot = 10                         ) # Revert differentiation of predictions last_value_train = data.loc[:end_train].iloc[[-1]] boot_predictions_1 = boot_predictions_diff.copy() boot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0] boot_predictions_1 = boot_predictions_1.sort_index() boot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,] boot_predictions_1 = boot_predictions_1.asfreq('MS')   # Time series differentiated internally by the forecaster # ============================================================================== forecaster_2 = ForecasterAutoreg(                    regressor       = RandomForestRegressor(random_state=963),                    lags            = 15,                    differentiation = 1                )  forecaster_2.fit(y=data.loc[:end_train]) boot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10) In\u00a0[17]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2)"},{"location":"user_guides/time-series-differentiation.html#time-series-differentiation","title":"Time series differentiation\u00b6","text":""},{"location":"user_guides/time-series-differentiation.html#timeseriesdifferentiator","title":"TimeSeriesDifferentiator\u00b6","text":"<p>TimeSeriesDifferentiator is a custom transformer that follows the preprocessing sklearn API. This means it has the method <code>fit</code>, <code>transform</code>, <code>fit_transform</code> and <code>inverse_transform</code>.</p>"},{"location":"user_guides/time-series-differentiation.html#forecasting-with-differentiation","title":"Forecasting with differentiation\u00b6","text":""},{"location":"user_guides/time-series-differentiation.html#internal-differentiation-vs-pre-processing","title":"Internal differentiation vs pre-processing\u00b6","text":"<p>Forecasters manage the differentiation process internally, so there is no need for additional pre-processing of the time series and post-processing of the predictions. This has several advantages, but before diving in, the results of both approaches are compared.</p>"},{"location":"user_guides/time-series-differentiation.html#conslusions","title":"Conslusions\u00b6","text":"<p>If, as demonstrated, the values are equivalent when differentiating the time series in a preprocessing step or when allowing the Forecaster to manage the differentiation internally, why the second alternative is better?</p> <ul> <li><p>Allowing the forecaster to manage all transformations internally guarantees that the same transformations are applied when the model is run on new data.</p> </li> <li><p>When the model is applied to new data that does not follow immediately after the training data (for example, if a model is not retrained for each prediction phase), the forecaster automatically increases the size of the last window needed to generate the predictors, as well as applying the differentiation to the incoming data and undoing it in the final predictions.</p> </li> </ul> <p>These transformations are non-trivial and very error-prone, so skforecast tries to avoid overcomplicating the already challenging task of forecasting time series.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html","title":"Weighted time series forecasting","text":"<p> \u270e Note </p> <p>The examples that follow demonstrate how a portion of the time series can be excluded from model training by assigning it a weight of zero. However, the use of weights extends beyond the inclusion or exclusion of observations and can also balance the degree of influence that each observation has on the forecasting model. For instance, an observation assigned a weight of 10 will have ten times more impact on the model training than an observation assigned a weight of 1.</p> <p> \u26a0 Warning </p> <p>In most gradient boosting implementations, such as LightGBM, XGBoost, and CatBoost, samples with zero weight are typically excluded when calculating gradients and Hessians. However, these samples are still taken into account when constructing the feature histograms, which can result in a model that differs from one trained without zero-weighted samples. For more information on this issue, please refer to this GitHub issue.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/energy_production_shutdown.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/energy_production_shutdown.csv') data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: production date 2012-01-01 375.1 2012-01-02 474.5 2012-01-03 573.9 2012-01-04 539.5 2012-01-05 445.4 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Dates train : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00   (n=731)\nDates test  : 2014-01-01 00:00:00 --- 2014-12-30 00:00:00   (n=364)\n</pre> In\u00a0[4]: Copied! <pre># Time series plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['production'].plot(ax=ax, label='train', linewidth=1)\ndata_test['production'].plot(ax=ax, label='test', linewidth=1)\nax.axvspan(\n    pd.to_datetime('2012-06'),\n    pd.to_datetime('2012-10'), \n    label=\"Shutdown\",\n    color=\"red\",\n    alpha=0.1\n)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Time series plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['production'].plot(ax=ax, label='train', linewidth=1) data_test['production'].plot(ax=ax, label='test', linewidth=1) ax.axvspan(     pd.to_datetime('2012-06'),     pd.to_datetime('2012-10'),      label=\"Shutdown\",     color=\"red\",     alpha=0.1 ) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[5]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between 2012-06-01 and 2012-10-21.     \"\"\"     weights = np.where(                   (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),                    0,                    1               )      return weights <p>A <code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[6]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = Ridge(random_state=123),\n                 lags        = 21,\n                 weight_func = custom_weights\n             )\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = Ridge(random_state=123),                  lags        = 21,                  weight_func = custom_weights              ) <p> \u26a0 Warning </p> <p>If the regressor used in the model's fitting method does not support <code>sample_weight</code> within its <code>fit</code> method, the <code>weight_func</code> argument will be ignored.</p> <p>The <code>source_code_weight_func</code> argument stores the source code of the <code>weight_func</code> added to the forecaster.</p> In\u00a0[7]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p>After creating the forecaster, a backtesting process is performed to simulate its behavior if the test set had been predicted in batches of 12 days.</p> In\u00a0[8]: Copied! <pre># Backtesting: predict batches of 12 days\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data['production'],\n                                   initial_train_size = len(data.loc[:end_train]),\n                                   fixed_train_size   = False,\n                                   steps              = 12,\n                                   metric             = 'mean_absolute_error',\n                                   refit              = False,\n                                   verbose            = False,\n                                   show_progress      = True\n                               )\n\nmetric\n</pre> # Backtesting: predict batches of 12 days # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data['production'],                                    initial_train_size = len(data.loc[:end_train]),                                    fixed_train_size   = False,                                    steps              = 12,                                    metric             = 'mean_absolute_error',                                    refit              = False,                                    verbose            = False,                                    show_progress      = True                                )  metric <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[8]: mean_absolute_error 0 26.821189 In\u00a0[9]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions_backtest.head()\n</pre> # Backtest predictions # ============================================================================== predictions_backtest.head() Out[9]: pred 2014-01-01 406.122211 2014-01-02 444.103631 2014-01-03 469.424876 2014-01-04 449.407001 2014-01-05 414.945674 In\u00a0[10]: Copied! <pre># Predictions plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['production'].plot(ax=ax, label='test', linewidth=1)\npredictions_backtest.plot(ax=ax, label='predictions', linewidth=1)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Predictions plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['production'].plot(ax=ax, label='test', linewidth=1) predictions_backtest.plot(ax=ax, label='predictions', linewidth=1) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend();"},{"location":"user_guides/weighted-time-series-forecasting.html#weighted-time-series-forecasting","title":"Weighted time series forecasting\u00b6","text":"<p>In many real-world scenarios, historical data is available for forecasting, but not all of it is reliable. For example, IoT sensors capture raw data from the physical world, but they are often prone to failure, malfunction, and attrition due to harsh deployment environments, leading to unusual or erroneous readings. Similarly, factories may shut down for maintenance, repair, or overhaul, resulting in gaps in the data. The Covid-19 pandemic has also affected population behavior, impacting many time series such as production, sales, and transportation.</p> <p>The presence of unreliable or unrepresentative values in the data history poses a significant challenge, as it hinders model learning. However, most forecasting algorithms require complete time series data, making it impossible to remove these observations. An alternative solution is to reduce the weight of the affected observations during model training. This document demonstrates how skforecast makes it easy to implement this strategy with two examples.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html#data","title":"Data\u00b6","text":"<p>Power plants used for energy generation are complex installations that require a high level of maintenance. It is typical for power plants to require periodic shutdowns for maintenance, repair, or overhaul activities. The following data set simulates these events, where energy production experiences a decrease.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#exclude-part-of-the-time-series","title":"Exclude part of the time series\u00b6","text":"<p>Between 2012-06-01 and 2012-09-30, the factory underwent a shutdown. To reduce the impact of these dates on the model, a custom function is created. This function assigns a value of 0 to any index date that falls within the shutdown period or up to 21 days later (lags used by the model), and a value of 1 to all other dates. Observations assigned a weight of 0 have no influence on the model training.</p>"},{"location":"user_guides/window-features-and-custom-features.html","title":"Window and custom features","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.datasets import fetch_dataset from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> <p>The following functions calculate some of the most commonly used window and custom features in time series forecasting:</p> <ul> <li><p>Calculate the first 10 lags of the time series.</p> </li> <li><p>Calculate the moving average with a window of 20.</p> </li> <li><p>Calculate the moving standard deviation with a window of 20.</p> </li> <li><p>Calculate the moving minimum and maximum with a window of 20.</p> </li> <li><p>Exponential Moving Average (EMA) with a window of 20 for smoothing.</p> </li> <li><p>The difference between the last value and the moving average to capture recent trends compared to the longer term average.</p> </li> </ul> In\u00a0[3]: Copied! <pre># Custom function to create features\n# ==============================================================================\ndef create_predictors(y):\n    \"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    Calculate the moving standard deviation with window 20.\n    Calculate moving minimum and maximum with window 20.\n    Calculate exponential moving average with window 20.\n    Calculate the difference between the last value and the moving average.\n    \"\"\"\n\n    # Calculate lags\n    lags = y[-1:-11:-1]  # window size needed = 10\n\n    # Calculate moving average\n    mean = np.mean(y[-20:])  # window size needed = 20\n\n    # Calculate moving standard deviation\n    std_dev = np.std(y[-20:])  # window size needed = 20\n\n    # Calculate moving minimum and maximum\n    min_val = np.min(y[-20:])\n    max_val = np.max(y[-20:])\n\n    # Calculate exponential moving average\n    ema = np.mean(y[-20:] * np.exp(-np.linspace(1, 20, 20) / 20))\n\n    # Calculate the difference between the last value and the moving average\n    diff_mean = y[-1] - mean\n\n    # Combine all predictors\n    predictors = np.hstack([lags, mean, std_dev, min_val, max_val, ema, diff_mean])\n\n    return predictors\n\n\nfeature_names = [f\"lag {i}\" for i in range(1, 11)] + [\n    \"moving_avg_20\",\n    \"moving_std_20\",\n    \"moving_min_20\",\n    \"moving_max_20\",\n    \"ema_20\",\n    \"diff_mean\",\n]\n</pre> # Custom function to create features # ============================================================================== def create_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     Calculate the moving standard deviation with window 20.     Calculate moving minimum and maximum with window 20.     Calculate exponential moving average with window 20.     Calculate the difference between the last value and the moving average.     \"\"\"      # Calculate lags     lags = y[-1:-11:-1]  # window size needed = 10      # Calculate moving average     mean = np.mean(y[-20:])  # window size needed = 20      # Calculate moving standard deviation     std_dev = np.std(y[-20:])  # window size needed = 20      # Calculate moving minimum and maximum     min_val = np.min(y[-20:])     max_val = np.max(y[-20:])      # Calculate exponential moving average     ema = np.mean(y[-20:] * np.exp(-np.linspace(1, 20, 20) / 20))      # Calculate the difference between the last value and the moving average     diff_mean = y[-1] - mean      # Combine all predictors     predictors = np.hstack([lags, mean, std_dev, min_val, max_val, ema, diff_mean])      return predictors   feature_names = [f\"lag {i}\" for i in range(1, 11)] + [     \"moving_avg_20\",     \"moving_std_20\",     \"moving_min_20\",     \"moving_max_20\",     \"ema_20\",     \"diff_mean\", ] <p> \u26a0 Warning </p> <p>The <code>window_size</code> parameter specifies the size of the data window that <code>fun_predictors</code> uses to generate each row of predictors. Choosing the appropriate value for this parameter is crucial to avoid losing data when constructing the training matrices.  In this case, the <code>window_size</code> required by the mean is the largest (most restrictive), so <code>window_size = 20</code>.</p> In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),\n                 fun_predictors  = create_predictors,\n                 name_predictors = feature_names,\n                 window_size     = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),                  fun_predictors  = create_predictors,                  name_predictors = feature_names,                  window_size     = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(y=data_train) forecaster Out[4]: <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: LGBMRegressor(n_jobs=-1, random_state=123, verbose=-1) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 20 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 18:17:19 \nLast fit date: 2024-07-29 18:17:19 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps) predictions.head(3) Out[5]: <pre>2005-07-01    0.951229\n2005-08-01    0.969577\n2005-09-01    1.127967\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n\nprint(f\"Test error (MSE): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )  print(f\"Test error (MSE): {error_mse}\") <pre>Test error (MSE): 0.03392508498594471\n</pre> In\u00a0[8]: Copied! <pre># Predictors importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Predictors importances # ============================================================================== forecaster.get_feature_importances() Out[8]: feature importance 0 lag 1 62 1 lag 2 60 11 moving_std_20 56 4 lag 5 55 14 ema_20 36 9 lag 10 35 2 lag 3 32 6 lag 7 30 15 diff_mean 30 3 lag 4 19 8 lag 9 17 10 moving_avg_20 17 5 lag 6 15 12 moving_min_20 14 7 lag 8 9 13 moving_max_20 0 In\u00a0[9]: Copied! <pre># Training matrix (X)\n# ==============================================================================\nX, y = forecaster.create_train_X_y(data_train)\nX.head()\n</pre> # Training matrix (X) # ============================================================================== X, y = forecaster.create_train_X_y(data_train) X.head() Out[9]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 moving_std_20 moving_min_20 moving_max_20 ema_20 diff_mean datetime 1993-03-01 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.496401 0.124727 0.33622 0.771258 0.298062 -0.108846 1993-04-01 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.496275 0.124796 0.33622 0.771258 0.299714 -0.068992 1993-05-01 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.496924 0.124331 0.33622 0.771258 0.302649 -0.083034 1993-06-01 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.496759 0.124419 0.33622 0.771258 0.304446 -0.067900 1993-07-01 0.470126 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.495638 0.124552 0.33622 0.771258 0.304076 -0.025512 In\u00a0[10]: Copied! <pre># Target variable (y)\n# ==============================================================================\ny.head()\n</pre> # Target variable (y) # ============================================================================== y.head() Out[10]: <pre>datetime\n1993-03-01    0.427283\n1993-04-01    0.413890\n1993-05-01    0.428859\n1993-06-01    0.470126\n1993-07-01    0.509210\nFreq: MS, Name: y, dtype: float64</pre> <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[11]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),\n                 fun_predictors  = create_predictors,\n                 name_predictors = feature_names,\n                 window_size     = 20, # window_size needed by the mean is the most restrictive one\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),                  fun_predictors  = create_predictors,                  name_predictors = feature_names,                  window_size     = 20, # window_size needed by the mean is the most restrictive one                  differentiation = 1              )  forecaster.fit(y=data_train) forecaster Out[11]: <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: LGBMRegressor(n_jobs=-1, random_state=123, verbose=-1) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 20 \nWeight function included: False \nDifferentiation order: 1 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1} \nfit_kwargs: {} \nCreation date: 2024-07-29 18:17:20 \nLast fit date: 2024-07-29 18:17:20 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[12]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[12]: <pre>2005-07-01    0.891992\n2005-08-01    0.899966\n2005-09-01    1.031671\nFreq: MS, Name: pred, dtype: float64</pre> <p> \ud83d\udca1 Tip </p> <p><code>ForecasterAutoregMultiSeriesCustom</code> API Reference.</p> <p> \u270e Note </p> <p>Skforecast offers additional approaches to create Global Forecasting Models:</p> <ul> <li> Global Forecasting Models: Independent multi-series forecasting </li> <li> Global Forecasting Models: Time series with different lengths and different exogenous variables </li> <li> Global Forecasting Models: Dependent multi-series forecasting (Multivariate forecasting) </li> </ul> In\u00a0[13]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[13]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[14]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00  (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00  (n=170)\n</pre> In\u00a0[15]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); In\u00a0[16]: Copied! <pre># Custom function to create features\n# ==============================================================================\ndef create_predictors(y):\n    \"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    Calculate the moving standard deviation with window 20.\n    Calculate moving minimum and maximum with window 20.\n    Calculate exponential moving average with window 20.\n    Calculate the difference between the last value and the moving average.\n    \"\"\"\n\n    # Calculate lags\n    lags = y[-1:-11:-1]  # window size needed = 10\n\n    # Calculate moving average\n    mean = np.mean(y[-20:])  # window size needed = 20\n\n    # Calculate moving standard deviation\n    std_dev = np.std(y[-20:])  # window size needed = 20\n\n    # Calculate moving minimum and maximum\n    min_val = np.min(y[-20:])\n    max_val = np.max(y[-20:])\n\n    # Calculate exponential moving average\n    ema = np.mean(y[-20:] * np.exp(-np.linspace(1, 20, 20) / 20))\n\n    # Calculate the difference between the last value and the moving average\n    diff_mean = y[-1] - mean\n\n    # Combine all predictors\n    predictors = np.hstack([lags, mean, std_dev, min_val, max_val, ema, diff_mean])\n\n    return predictors\n\n\nfeature_names = [f\"lag {i}\" for i in range(1, 11)] + [\n    \"moving_avg_20\",\n    \"moving_std_20\",\n    \"moving_min_20\",\n    \"moving_max_20\",\n    \"ema_20\",\n    \"diff_mean\",\n]\n</pre> # Custom function to create features # ============================================================================== def create_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     Calculate the moving standard deviation with window 20.     Calculate moving minimum and maximum with window 20.     Calculate exponential moving average with window 20.     Calculate the difference between the last value and the moving average.     \"\"\"      # Calculate lags     lags = y[-1:-11:-1]  # window size needed = 10      # Calculate moving average     mean = np.mean(y[-20:])  # window size needed = 20      # Calculate moving standard deviation     std_dev = np.std(y[-20:])  # window size needed = 20      # Calculate moving minimum and maximum     min_val = np.min(y[-20:])     max_val = np.max(y[-20:])      # Calculate exponential moving average     ema = np.mean(y[-20:] * np.exp(-np.linspace(1, 20, 20) / 20))      # Calculate the difference between the last value and the moving average     diff_mean = y[-1] - mean      # Combine all predictors     predictors = np.hstack([lags, mean, std_dev, min_val, max_val, ema, diff_mean])      return predictors   feature_names = [f\"lag {i}\" for i in range(1, 11)] + [     \"moving_avg_20\",     \"moving_std_20\",     \"moving_min_20\",     \"moving_max_20\",     \"ema_20\",     \"diff_mean\", ] <p> \u26a0 Warning </p> <p>The <code>window_size</code> parameter specifies the size of the data window that <code>fun_predictors</code> uses to generate each row of predictors. Choosing the appropriate value for this parameter is crucial to avoid losing data when constructing the training matrices.  In this case, the <code>window_size</code> required by the mean is the largest (most restrictive), so <code>window_size = 20</code>.</p> In\u00a0[17]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeriesCustom(\n                 regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),\n                 fun_predictors  = create_predictors,\n                 name_predictors = feature_names,\n                 window_size     = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeriesCustom(                  regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),                  fun_predictors  = create_predictors,                  name_predictors = feature_names,                  window_size     = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(series=data_train) forecaster Out[17]: <pre>================================== \nForecasterAutoregMultiSeriesCustom \n================================== \nRegressor: LGBMRegressor(n_jobs=-1, random_state=123, verbose=-1) \nPredictors created with function: create_predictors \nTransformer for series: None \nTransformer for exog: None \nSeries encoding: ordinal_category \nWindow size: 20 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [\"'item_1': ['2012-01-01', '2014-07-15']\", \"'item_2': ['2012-01-01', '2014-07-15']\", \"'item_3': ['2012-01-01', '2014-07-15']\"] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: boosting_type: gbdt, class_weight: None, colsample_bytree: 1.0, importance_type: split, learning_rate: 0.1, ... \nfit_kwargs: {} \nCreation date: 2024-07-29 18:17:23 \nLast fit date: 2024-07-29 18:17:24 \nSkforecast version: 0.13.0 \nPython version: 3.11.5 \nForecaster id: None </pre> In\u00a0[18]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps, levels=None)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps, levels=None) predictions.head(3) Out[18]: item_1 item_2 item_3 2014-07-16 25.691486 11.033865 13.535166 2014-07-17 25.404832 10.536406 12.908039 2014-07-18 25.234903 12.085611 13.228491 In\u00a0[19]: Copied! <pre># Training matrix (X)\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> # Training matrix (X) # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) X_train.head() Out[19]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 moving_std_20 moving_min_20 moving_max_20 ema_20 diff_mean _level_skforecast date 2012-01-21 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 21.717691 21.954065 4.161762 8.253175 29.245869 13.267277 2.064703 0 2012-01-22 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 22.666583 2.727996 18.976196 29.245869 13.948776 -0.163050 0 2012-01-23 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 22.567441 2.758017 18.976196 29.245869 13.907556 -1.772455 0 2012-01-24 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 22.389038 2.536512 18.976196 29.245869 13.684263 1.592000 0 2012-01-25 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 22.495203 2.718929 18.976196 29.245869 13.606471 5.523627 0 In\u00a0[20]: Copied! <pre># Target variable (y)\n# ==============================================================================\ny_train.head()\n</pre> # Target variable (y) # ============================================================================== y_train.head() Out[20]: <pre>date\n2012-01-21    22.503533\n2012-01-22    20.794986\n2012-01-23    23.981037\n2012-01-24    28.018830\n2012-01-25    28.747482\nName: y, dtype: float64</pre>"},{"location":"user_guides/window-features-and-custom-features.html#window-and-custom-features","title":"Window and custom features\u00b6","text":"<p>In forecasting time series data, it can be useful to consider additional characteristics beyond just the lagged values. For instance, the moving average of the previous n values can help capture the trend in the series.</p> <p>The <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes are similar to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregMultiSeries</code>, respectively, but they allow the user to define their own function for generating features.</p> <p>To create a custom features, the user needs to write a function that takes a time series as input (a NumPy ndarray) and outputs another NumPy ndarray containing the predictors. The Forecaster parameters used to specify this function are <code>fun_predictors</code> and <code>window_size</code> and <code>name_predictors</code> (optional).</p>"},{"location":"user_guides/window-features-and-custom-features.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#forecasterautoregcustom","title":"ForecasterAutoregCustom\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#window-and-custom-features","title":"Window and custom features\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/window-features-and-custom-features.html#forecasterautoregmultiseriescustom","title":"ForecasterAutoregMultiSeriesCustom\u00b6","text":"<p>This Forecaster aims to model multiple time series simultaneously. It is based on the <code>ForecasterAutoregMultiSeries</code> class, but it allows the user to define their own function for generating predictors.</p> <p>Please refer to the Global Forecasting Models: Independent multi-series forecasting user guide if you want to learn more about this type of forecasting.</p>"},{"location":"user_guides/window-features-and-custom-features.html#train","title":"Train\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#predict","title":"Predict\u00b6","text":"<p>If no value is specified for the <code>levels</code> argument, predictions will be computed for all available levels.</p>"},{"location":"user_guides/window-features-and-custom-features.html#training-matrices","title":"Training matrices\u00b6","text":""}]}