{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"skforecast \u00b6 Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (pipelines, CatBoost, LightGBM, XGBoost, Ranger...). Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit). All notable changes are listed in Releases . Installation \u00b6 1 pip install skforecast Specific version: 1 pip install skforecast==0.3 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master Dependencies \u00b6 1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=1.0.1 statsmodels>=0.12.2 Features \u00b6 Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance Tutorials \u00b6 English Skforecast: time series forecasting with Python and Scikit-learn Forecasting electricity demand with Python Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost Forecasting web traffic with machine learning and Python Espa\u00f1ol Skforecast: forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web Forecasting series temporales con gradient boosting: Skforecast, XGBoost, LightGBM y CatBoost References \u00b6 Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance Licence \u00b6 joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Readme"},{"location":"index.html#skforecast","text":"Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (pipelines, CatBoost, LightGBM, XGBoost, Ranger...). Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit). All notable changes are listed in Releases .","title":"skforecast"},{"location":"index.html#installation","text":"1 pip install skforecast Specific version: 1 pip install skforecast==0.3 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master","title":"Installation"},{"location":"index.html#dependencies","text":"1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=1.0.1 statsmodels>=0.12.2","title":"Dependencies"},{"location":"index.html#features","text":"Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance","title":"Features"},{"location":"index.html#tutorials","text":"English Skforecast: time series forecasting with Python and Scikit-learn Forecasting electricity demand with Python Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost Forecasting web traffic with machine learning and Python Espa\u00f1ol Skforecast: forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web Forecasting series temporales con gradient boosting: Skforecast, XGBoost, LightGBM y CatBoost","title":"Tutorials"},{"location":"index.html#references","text":"Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance","title":"References"},{"location":"index.html#licence","text":"joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Licence"},{"location":"api/ForecasterAutoreg.html","text":"ForecasterAutoreg \u00b6 class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range : include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . Returns (feature_importance : pandas DataFrame) Feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#forecasterautoreg","text":"class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range : include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . Returns (feature_importance : pandas DataFrame) Feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoregCustom.html","text":"ForecasterAutoregCustom \u00b6 class skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. fun_predictors (Callable) \u2014 Function that takes a numpy ndarray as a window of values as input and returns a numpy ndarray with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. create_predictors (Callable) \u2014 Function that takes a numpy ndarray as a window of values as input and returns a numpy ndarray with the predictors associated with that window. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor compatible with the scikit-learn API) \u2014 An instance of a regressor compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. source_code_create_predictors (str) \u2014 Source code of the custom function used to create the predictors. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame) \u2014 Create training matrices from univariate time series. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. summary ( ) \u2014 Show forecaster information. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame) Pandas DataFrame with the training values (predictors). ain : pandas Series Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . Returns (feature_importance : pandas DataFrame) Feature importance associated with each predictor.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#forecasterautoregcustom","text":"class skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. fun_predictors (Callable) \u2014 Function that takes a numpy ndarray as a window of values as input and returns a numpy ndarray with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. create_predictors (Callable) \u2014 Function that takes a numpy ndarray as a window of values as input and returns a numpy ndarray with the predictors associated with that window. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor compatible with the scikit-learn API) \u2014 An instance of a regressor compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. source_code_create_predictors (str) \u2014 Source code of the custom function used to create the predictors. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame) \u2014 Create training matrices from univariate time series. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. summary ( ) \u2014 Show forecaster information. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame) Pandas DataFrame with the training values (predictors). ain : pandas Series Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the regressor stored in the forecaster. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return feature importance of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importance in the attribute feature_importances_ . Returns (feature_importance : pandas DataFrame) Feature importance associated with each predictor.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregMultiOutput.html","text":"ForecasterAutoregMultiOutput \u00b6 class skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. steps (int) \u2014 Maximum number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (tuple) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (tuple) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. One instance of this regressor is trainned for each step. All them are stored in self.regressors_ . regressors_ (dict) \u2014 Dictionary with regressors trained for each step. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. training_range (pandas Index) \u2014 First and last index of samples used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hyperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregMultiOutput object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : pandas DataFrame) \u2014 Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( step ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve the information. get_feature_importance ( step ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregMultiOutput object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) Pandas DataFrame with the training values (predictors) for each step. ain : pd.DataFrame, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train for each step. method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (pandas DataFrame) \u2014 Pandas DataFrame with the training values (predictors). y_train (pandas Series) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : pandas DataFrame) Pandas DataFrame with the training values (predictors) for step. ain_step : pandas Series, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps=None , last_window=None , exog=None ) Predict n steps ahead. Parameters steps (int, None, default `None`) \u2014 Predict n steps ahead. steps must lower or equal to the value of steps defined when initializing the forecaster. If None , as many steps as defined in the initialization are predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method get_coef ( step ) Return estimated coefficients for the regressor stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve the information. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). First step is 1. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( step ) Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). First step is 1. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#forecasterautoregmultioutput","text":"class skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. steps (int) \u2014 Maximum number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (tuple) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (tuple) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. One instance of this regressor is trainned for each step. All them are stored in self.regressors_ . regressors_ (dict) \u2014 Dictionary with regressors trained for each step. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. training_range (pandas Index) \u2014 First and last index of samples used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hyperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregMultiOutput object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : pandas DataFrame) \u2014 Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( step ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the regressor stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve the information. get_feature_importance ( step ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregMultiOutput object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) Pandas DataFrame with the training values (predictors) for each step. ain : pd.DataFrame, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train for each step. method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (pandas DataFrame) \u2014 Pandas DataFrame with the training values (predictors). y_train (pandas Series) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : pandas DataFrame) Pandas DataFrame with the training values (predictors) for step. ain_step : pandas Series, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps=None , last_window=None , exog=None ) Predict n steps ahead. Parameters steps (int, None, default `None`) \u2014 Predict n steps ahead. steps must lower or equal to the value of steps defined when initializing the forecaster. If None , as many steps as defined in the initialization are predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method get_coef ( step ) Return estimated coefficients for the regressor stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve the information. Only valid when regressor stores internally the feature coefficients in the attribute coef_ . Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). First step is 1. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( step ) Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). First step is 1. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregMultiOutput"},{"location":"api/model_selection.html","text":"model_selection \u00b6 function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , random_state=123 , in_sample_residuals=True , verbose=False , set_out_sample_residuals='deprecated' ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so it is not modified during the process. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors, so no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out_sample_residuals are used if they are already stored inside the forecaster. verbose (bool, default `False`) \u2014 Print number of folds and index of training and validation sets used for backtesting. set_out_sample_residuals ('deprecated') \u2014 Deprecated since version 0.4.2, will be removed on version 0.5.0. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_splitter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection.html#model_selection","text":"function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , random_state=123 , in_sample_residuals=True , verbose=False , set_out_sample_residuals='deprecated' ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so it is not modified during the process. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors, so no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int, default 123) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out_sample_residuals are used if they are already stored inside the forecaster. verbose (bool, default `False`) \u2014 Print number of folds and index of training and validation sets used for backtesting. set_out_sample_residuals ('deprecated') \u2014 Deprecated since version 0.4.2, will be removed on version 0.5.0. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_splitter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection_statsmodels.html","text":"model_selection_statsmodels \u00b6 function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {'n', 'c', 't', 'ct'}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where 'c' indicates a constant (i.e. a degree zero component of the trend polynomial), 't' indicates a linear trend with time, and 'ct' is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {'n', 'c', 't', 'ct'}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where 'c' indicates a constant (i.e. a degree zero component of the trend polynomial), 't' indicates a linear trend with time, and 'ct' is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"api/model_selection_statsmodels.html#model_selection_statsmodels","text":"function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {'n', 'c', 't', 'ct'}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where 'c' indicates a constant (i.e. a degree zero component of the trend polynomial), 't' indicates a linear trend with time, and 'ct' is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {'n', 'c', 't', 'ct'}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where 'c' indicates a constant (i.e. a degree zero component of the trend polynomial), 't' indicates a linear trend with time, and 'ct' is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"examples/examples.html","text":"Examples \u00b6 Examples and tutorials where skforecast library is used for forecasting. English \u00b6 Skforecast: time series forecasting with Python and Scikit-learn Forecasting electricity demand with Python Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost Forecasting web traffic with machine learning and Python Espa\u00f1ol \u00b6 Skforecast: forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web Forecasting series temporales con gradient boosting: Skforecast, XGBoost, LightGBM y CatBoost","title":"Forecasting web page traffic"},{"location":"examples/examples.html#examples","text":"Examples and tutorials where skforecast library is used for forecasting.","title":"Examples"},{"location":"examples/examples.html#english","text":"Skforecast: time series forecasting with Python and Scikit-learn Forecasting electricity demand with Python Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost Forecasting web traffic with machine learning and Python","title":"English"},{"location":"examples/examples.html#espanol","text":"Skforecast: forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web Forecasting series temporales con gradient boosting: Skforecast, XGBoost, LightGBM y CatBoost","title":"Espa\u00f1ol"},{"location":"guides/autoregresive-forecaster-exogenous.html","text":"Recursive multi-step forecasting with exogenous variables \u00b6 ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i]. Libraries \u00b6 1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'datetime' , 'y' , 'exog_1' , 'exog_2' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :] Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:24:18 Last fit date: 2022-01-02 16:24:18 Skforecast version: 0.4.2 Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they should be provided when predictions. 1 2 3 4 5 6 7 8 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] ) predictions . head ( 3 ) datetime 2005-07-01 0.908832 2005-08-01 0.953925 2005-09-01 1.100887 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.004022228812838391 Feature importance \u00b6 1 forecaster . get_feature_importance () feature importance lag_1 0.0133541 lag_2 0.0611202 lag_3 0.00908617 lag_4 0.00272094 lag_5 0.00247847 lag_6 0.00315493 lag_7 0.00217887 lag_8 0.00815443 lag_9 0.0103189 lag_10 0.0205869 lag_11 0.00703555 lag_12 0.773389 lag_13 0.00458297 lag_14 0.0181272 lag_15 0.00873237 exog_1 0.0103638 exog_2 0.0446156","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#recursive-multi-step-forecasting-with-exogenous-variables","text":"ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i].","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#libraries","text":"1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster-exogenous.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'datetime' , 'y' , 'exog_1' , 'exog_2' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :]","title":"Data"},{"location":"guides/autoregresive-forecaster-exogenous.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:24:18 Last fit date: 2022-01-02 16:24:18 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster-exogenous.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they should be provided when predictions. 1 2 3 4 5 6 7 8 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] ) predictions . head ( 3 ) datetime 2005-07-01 0.908832 2005-08-01 0.953925 2005-09-01 1.100887 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.004022228812838391","title":"Prediction"},{"location":"guides/autoregresive-forecaster-exogenous.html#feature-importance","text":"1 forecaster . get_feature_importance () feature importance lag_1 0.0133541 lag_2 0.0611202 lag_3 0.00908617 lag_4 0.00272094 lag_5 0.00247847 lag_6 0.00315493 lag_7 0.00217887 lag_8 0.00815443 lag_9 0.0103189 lag_10 0.0205869 lag_11 0.00703555 lag_12 0.773389 lag_13 0.00458297 lag_14 0.0181272 lag_15 0.00873237 exog_1 0.0103638 exog_2 0.0446156","title":"Feature importance"},{"location":"guides/autoregresive-forecaster.html","text":"Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using machine learning models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Libraries \u00b6 1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend () Create and train forecaster \u00b6 1 2 3 4 5 6 7 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:14:39 Last fit date: 2022-01-02 16:14:39 Skforecast version: 0.4.2 Prediction \u00b6 1 2 predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.921840 2005-08-01 0.954921 2005-09-01 1.101716 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.00429855684785846 Feature importance \u00b6 1 forecaster . get_feature_importance () feature importance lag_1 0.0123397 lag_2 0.0851603 lag_3 0.0134071 lag_4 0.00437446 lag_5 0.00318805 lag_6 0.00343593 lag_7 0.00313612 lag_8 0.00714094 lag_9 0.00783053 lag_10 0.0127507 lag_11 0.00901919 lag_12 0.807098 lag_13 0.00481128 lag_14 0.0163282 lag_15 0.0099792","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using machine learning models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom .","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#libraries","text":"1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ()","title":"Data"},{"location":"guides/autoregresive-forecaster.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:14:39 Last fit date: 2022-01-02 16:14:39 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster.html#prediction","text":"1 2 predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.921840 2005-08-01 0.954921 2005-09-01 1.101716 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.00429855684785846","title":"Prediction"},{"location":"guides/autoregresive-forecaster.html#feature-importance","text":"1 forecaster . get_feature_importance () feature importance lag_1 0.0123397 lag_2 0.0851603 lag_3 0.0134071 lag_4 0.00437446 lag_5 0.00318805 lag_6 0.00343593 lag_7 0.00313612 lag_8 0.00714094 lag_9 0.00783053 lag_10 0.0127507 lag_11 0.00901919 lag_12 0.807098 lag_13 0.00481128 lag_14 0.0163282 lag_15 0.0099792","title":"Feature importance"},{"location":"guides/backtesting-sarimax-arima.html","text":"Backtesting SARIMAX and ARIMA models \u00b6 Libraries \u00b6 1 2 3 4 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.model_selection_statsmodels import backtesting_sarimax Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split data in train snd backtest # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_backtest = data [ - n_backtest :] Backtest \u00b6 1 2 3 4 5 6 7 8 9 10 11 metric , predictions_backtest = backtesting_sarimax ( y = data , order = ( 12 , 1 , 1 ), seasonal_order = ( 0 , 0 , 0 , 0 ), initial_train_size = len ( data_train ), steps = 7 , metric = 'mean_absolute_error' , refit = False , verbose = True , fit_kwargs = { 'maxiter' : 250 , 'disp' : 0 }, ) Number of observations used for training: 96 Number of observations used for backtesting: 108 Number of folds: 16 Number of steps per fold: 7 Last fold only includes 3 observations. 1 print ( f \"Error backtest: { metric } \" ) Error backtest: [0.05544072] 1 predictions_backtest . head ( 4 ) predicted_mean lower y upper y 1999-07-01 00:00:00 0.734616 0.650009 0.819222 1999-08-01 00:00:00 0.751851 0.660717 0.842986 1999-09-01 00:00:00 0.86531 0.768134 0.962486 1999-10-01 00:00:00 0.832383 0.730362 0.934404","title":"Backtesting SARIMAX and ARIMA models"},{"location":"guides/backtesting-sarimax-arima.html#backtesting-sarimax-and-arima-models","text":"","title":"Backtesting SARIMAX and ARIMA models"},{"location":"guides/backtesting-sarimax-arima.html#libraries","text":"1 2 3 4 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.model_selection_statsmodels import backtesting_sarimax","title":"Libraries"},{"location":"guides/backtesting-sarimax-arima.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split data in train snd backtest # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_backtest = data [ - n_backtest :]","title":"Data"},{"location":"guides/backtesting-sarimax-arima.html#backtest","text":"1 2 3 4 5 6 7 8 9 10 11 metric , predictions_backtest = backtesting_sarimax ( y = data , order = ( 12 , 1 , 1 ), seasonal_order = ( 0 , 0 , 0 , 0 ), initial_train_size = len ( data_train ), steps = 7 , metric = 'mean_absolute_error' , refit = False , verbose = True , fit_kwargs = { 'maxiter' : 250 , 'disp' : 0 }, ) Number of observations used for training: 96 Number of observations used for backtesting: 108 Number of folds: 16 Number of steps per fold: 7 Last fold only includes 3 observations. 1 print ( f \"Error backtest: { metric } \" ) Error backtest: [0.05544072] 1 predictions_backtest . head ( 4 ) predicted_mean lower y upper y 1999-07-01 00:00:00 0.734616 0.650009 0.819222 1999-08-01 00:00:00 0.751851 0.660717 0.842986 1999-09-01 00:00:00 0.86531 0.768134 0.962486 1999-10-01 00:00:00 0.832383 0.730362 0.934404","title":"Backtest"},{"location":"guides/backtesting.html","text":"Backtesting \u00b6 Backtesting with refit The model is trained each time before making the predictions, in this way, the model use all the information available so far. It is a variation of the standard cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting without refit After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time. Libraries \u00b6 1 2 3 4 5 6 7 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split data in train snd backtest # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_backtest = data [ - n_backtest :] Backtest \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , verbose = True ) Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 print ( f \"Error de backtest: { metric } \" ) Error de backtest: [0.00726621] 1 predictions_backtest . head ( 4 ) pred 1999-07-01 00:00:00 0.712336 1999-08-01 00:00:00 0.750542 1999-09-01 00:00:00 0.802371 1999-10-01 00:00:00 0.806941 1 2 3 4 5 6 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_backtest . plot ( ax = ax ) predictions_backtest . plot ( ax = ax ) ax . legend (); Backtest with prediction intervals \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_backtest . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Backtesting forecaster"},{"location":"guides/backtesting.html#backtesting","text":"Backtesting with refit The model is trained each time before making the predictions, in this way, the model use all the information available so far. It is a variation of the standard cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting without refit After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time.","title":"Backtesting"},{"location":"guides/backtesting.html#libraries","text":"1 2 3 4 5 6 7 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/backtesting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split data in train snd backtest # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_backtest = data [ - n_backtest :]","title":"Data"},{"location":"guides/backtesting.html#backtest","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , verbose = True ) Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 print ( f \"Error de backtest: { metric } \" ) Error de backtest: [0.00726621] 1 predictions_backtest . head ( 4 ) pred 1999-07-01 00:00:00 0.712336 1999-08-01 00:00:00 0.750542 1999-09-01 00:00:00 0.802371 1999-10-01 00:00:00 0.806941 1 2 3 4 5 6 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_backtest . plot ( ax = ax ) predictions_backtest . plot ( ax = ax ) ax . legend ();","title":"Backtest"},{"location":"guides/backtesting.html#backtest-with-prediction-intervals","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_backtest . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Backtest with prediction intervals"},{"location":"guides/custom-predictors.html","text":"Custom predictors for recursive multi-step forecasting \u00b6 It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors. Libraries \u00b6 1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # Custom function to create predictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' lags = y [ - 1 : - 11 : - 1 ] mean = np . mean ( y [ - 20 :]) predictors = np . hstack ([ lags , mean ]) return predictors 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ======================= ForecasterAutoregCustom ======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with function: create_predictors Window size: 20 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:20:24 Last fit date: 2022-01-02 16:20:24 Skforecast version: 0.4.2 Prediction \u00b6 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.04487765885818191 Feature importance \u00b6 1 forecaster . get_feature_importance () feature importance custom_predictor_0 0.53972 custom_predictor_1 0.119097 custom_predictor_2 0.0464036 custom_predictor_3 0.0241653 custom_predictor_4 0.0305667 custom_predictor_5 0.0151391 custom_predictor_6 0.0428832 custom_predictor_7 0.012742 custom_predictor_8 0.018938 custom_predictor_9 0.108639 custom_predictor_10 0.0417066","title":"Custom predictors"},{"location":"guides/custom-predictors.html#custom-predictors-for-recursive-multi-step-forecasting","text":"It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors.","title":"Custom predictors for  recursive multi-step forecasting"},{"location":"guides/custom-predictors.html#libraries","text":"1 2 3 4 5 6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/custom-predictors.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/custom-predictors.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # Custom function to create predictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' lags = y [ - 1 : - 11 : - 1 ] mean = np . mean ( y [ - 20 :]) predictors = np . hstack ([ lags , mean ]) return predictors 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ======================= ForecasterAutoregCustom ======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with function: create_predictors Window size: 20 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:20:24 Last fit date: 2022-01-02 16:20:24 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/custom-predictors.html#prediction","text":"1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.04487765885818191","title":"Prediction"},{"location":"guides/custom-predictors.html#feature-importance","text":"1 forecaster . get_feature_importance () feature importance custom_predictor_0 0.53972 custom_predictor_1 0.119097 custom_predictor_2 0.0464036 custom_predictor_3 0.0241653 custom_predictor_4 0.0305667 custom_predictor_5 0.0151391 custom_predictor_6 0.0428832 custom_predictor_7 0.012742 custom_predictor_8 0.018938 custom_predictor_9 0.108639 custom_predictor_10 0.0417066","title":"Feature importance"},{"location":"guides/direct-multi-step-forecasting.html","text":"Direct multi-step forecaster \u00b6 ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model. Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = Ridge (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ============================ ForecasterAutoregMultiOutput ============================ Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Maximum steps predicted: 36 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Creation date: 2022-01-02 16:26:58 Last fit date: 2022-01-02 16:26:58 Skforecast version: 0.4.2 Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they should be provided when prediction. 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.952051 2005-08-01 1.004145 2005-09-01 1.114590 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.008419597278831953 Feature importance \u00b6 Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 forecaster . get_coef ( step = 1 ) feature coef lag_1 0.139299 lag_2 0.0510889 lag_3 0.0441923 lag_4 -0.0198684 lag_5 -0.0179349 lag_6 -0.0132333 lag_7 -0.0210635 lag_8 -0.0125908 lag_9 0.0119178 lag_10 0.0205112 lag_11 0.15403 lag_12 0.551652 lag_13 0.0575131 lag_14 -0.0710707 lag_15 -0.0352375 Extract training matrix \u00b6 Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 8 9 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , ) print ( X_1 . head ( 4 )) print ( y_1 . head ( 4 )) lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 y_step_1 0.595223 0.771258 0.751503 0.387554","title":"Direct multi-step forecasting"},{"location":"guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","text":"ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model.","title":"Direct multi-step forecaster"},{"location":"guides/direct-multi-step-forecasting.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/direct-multi-step-forecasting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/direct-multi-step-forecasting.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = Ridge (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ============================ ForecasterAutoregMultiOutput ============================ Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Maximum steps predicted: 36 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Creation date: 2022-01-02 16:26:58 Last fit date: 2022-01-02 16:26:58 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/direct-multi-step-forecasting.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they should be provided when prediction. 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 2005-07-01 0.952051 2005-08-01 1.004145 2005-09-01 1.114590 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) Test error (mse): 0.008419597278831953","title":"Prediction"},{"location":"guides/direct-multi-step-forecasting.html#feature-importance","text":"Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 forecaster . get_coef ( step = 1 ) feature coef lag_1 0.139299 lag_2 0.0510889 lag_3 0.0441923 lag_4 -0.0198684 lag_5 -0.0179349 lag_6 -0.0132333 lag_7 -0.0210635 lag_8 -0.0125908 lag_9 0.0119178 lag_10 0.0205112 lag_11 0.15403 lag_12 0.551652 lag_13 0.0575131 lag_14 -0.0710707 lag_15 -0.0352375","title":"Feature importance"},{"location":"guides/direct-multi-step-forecasting.html#extract-training-matrix","text":"Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 8 9 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , ) print ( X_1 . head ( 4 )) print ( y_1 . head ( 4 )) lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 0.432159 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.33622 0.660119 0.602652 0.502369 0.492543 y_step_1 0.595223 0.771258 0.751503 0.387554","title":"Extract training matrix"},{"location":"guides/feature-importance.html","text":"Feature importance \u00b6 The importance of predictors can be obtained using the methods get_coef and get_feature_importance . These methods access the attributes coef_ and feature_importances_ of the internal regressor, respectively. \u26a0 WARNING: This methods only return values if the regressor used inside the forecaster has the attribute coef_ or feature_importances_ . Libraries \u00b6 1 2 3 4 5 6 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput Data \u00b6 1 2 3 4 5 6 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) Extract feature importance from trained forecaster \u00b6 1 2 3 4 5 6 7 8 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_feature_importance () feature importance 0 lag_1 0.522544 1 lag_2 0.0998894 2 lag_3 0.0203564 3 lag_4 0.078274 4 lag_5 0.067372 5 exog_1 0.0483118 6 exog_2 0.163252 1 2 3 4 5 6 7 8 orecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_coef () feature coef 0 lag_1 0.327688 1 lag_2 -0.0735932 2 lag_3 -0.152202 3 lag_4 -0.217106 4 lag_5 -0.1458 5 exog_1 0.379798 6 exog_2 0.668162 When using a ForecasterAutoregMultiOutput , since a different model is fit for each step, it is needed to indicate from which model extract the information. 1 2 3 4 5 6 7 8 9 forecaster = ForecasterAutoregMultiOutput ( regressor = RandomForestRegressor (), steps = 10 , lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_feature_importance ( step = 1 ) feature importance 0 lag_1 0.529107 1 lag_2 0.109431 2 lag_3 0.0197598 3 lag_4 0.0813373 4 lag_5 0.0480799 5 exog_1 0.0371122 6 exog_2 0.175173","title":"Feature importance"},{"location":"guides/feature-importance.html#feature-importance","text":"The importance of predictors can be obtained using the methods get_coef and get_feature_importance . These methods access the attributes coef_ and feature_importances_ of the internal regressor, respectively. \u26a0 WARNING: This methods only return values if the regressor used inside the forecaster has the attribute coef_ or feature_importances_ .","title":"Feature importance"},{"location":"guides/feature-importance.html#libraries","text":"1 2 3 4 5 6 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput","title":"Libraries"},{"location":"guides/feature-importance.html#data","text":"1 2 3 4 5 6 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' )","title":"Data"},{"location":"guides/feature-importance.html#extract-feature-importance-from-trained-forecaster","text":"1 2 3 4 5 6 7 8 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_feature_importance () feature importance 0 lag_1 0.522544 1 lag_2 0.0998894 2 lag_3 0.0203564 3 lag_4 0.078274 4 lag_5 0.067372 5 exog_1 0.0483118 6 exog_2 0.163252 1 2 3 4 5 6 7 8 orecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_coef () feature coef 0 lag_1 0.327688 1 lag_2 -0.0735932 2 lag_3 -0.152202 3 lag_4 -0.217106 4 lag_5 -0.1458 5 exog_1 0.379798 6 exog_2 0.668162 When using a ForecasterAutoregMultiOutput , since a different model is fit for each step, it is needed to indicate from which model extract the information. 1 2 3 4 5 6 7 8 9 forecaster = ForecasterAutoregMultiOutput ( regressor = RandomForestRegressor (), steps = 10 , lags = 5 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster . get_feature_importance ( step = 1 ) feature importance 0 lag_1 0.529107 1 lag_2 0.109431 2 lag_3 0.0197598 3 lag_4 0.0813373 4 lag_5 0.0480799 5 exog_1 0.0371122 6 exog_2 0.175173","title":"Extract feature importance from trained forecaster"},{"location":"guides/forecast-time-ahead_training.html","text":"Use forecaster after training \u00b6 By default, when using predict method on a trained forecaster object, predictions starts right after the last training observation. Libraries \u00b6 1 2 3 4 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg Data \u00b6 1 2 3 4 5 6 7 8 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) data_train = data . loc [: '2005-01-01' ] data_train . tail () date y 2004-09-01 00:00:00 1.13443 2004-10-01 00:00:00 1.18101 2004-11-01 00:00:00 1.21604 2004-12-01 00:00:00 1.25724 2005-01-01 00:00:00 1.17069 Predict \u00b6 By default, when using predict method on a trained forecaster object, predictions starts right after the last training observation. 1 2 3 4 5 6 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 5 ) forecaster . fit ( y = data_train [ 'y' ]) 2005-02-01 0.927480 2005-03-01 0.756215 2005-04-01 0.692595 Freq: MS, Name: pred, dtype: float64 As expected, predictions follow directly from the end of training data. If the training sample is relatively small or if it is desired compute the best possible forecasts, the forecaster should be retrained using all the available data before making predictions. However, if that strategy is infeasible (for example, because the training set is very large), it is useful to generate predictions without retraining the model each time. With skforecast, it is possible to generate predictions starting time ahead of training date. When last_window is provided, the forecaster use this data to generate the lads needed as predictors. 1 forecaster . predict ( steps = 3 , last_window = data [ 'y' ] . tail ( 5 )) 2008-07-01 0.803853 2008-08-01 0.870858 2008-09-01 0.905003 Freq: MS, Name: pred, dtype: float64 Since the provided last_window contains values from 2008-02-01 to 2008-06-01, the forecaster is able to create the needed lags and predict the next 5 steps. \u26a0 WARNING: It is important to note that the length of last windows must be enough to include the maximum lag used by the forecaster. Fore example, if the forecaster uses lags 1, 24, 48 and 72 last_window must include the last 72 values of the series.","title":"Forecasting time ahead training"},{"location":"guides/forecast-time-ahead_training.html#use-forecaster-after-training","text":"By default, when using predict method on a trained forecaster object, predictions starts right after the last training observation.","title":"Use forecaster after training"},{"location":"guides/forecast-time-ahead_training.html#libraries","text":"1 2 3 4 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg","title":"Libraries"},{"location":"guides/forecast-time-ahead_training.html#data","text":"1 2 3 4 5 6 7 8 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) data_train = data . loc [: '2005-01-01' ] data_train . tail () date y 2004-09-01 00:00:00 1.13443 2004-10-01 00:00:00 1.18101 2004-11-01 00:00:00 1.21604 2004-12-01 00:00:00 1.25724 2005-01-01 00:00:00 1.17069","title":"Data"},{"location":"guides/forecast-time-ahead_training.html#predict","text":"By default, when using predict method on a trained forecaster object, predictions starts right after the last training observation. 1 2 3 4 5 6 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 5 ) forecaster . fit ( y = data_train [ 'y' ]) 2005-02-01 0.927480 2005-03-01 0.756215 2005-04-01 0.692595 Freq: MS, Name: pred, dtype: float64 As expected, predictions follow directly from the end of training data. If the training sample is relatively small or if it is desired compute the best possible forecasts, the forecaster should be retrained using all the available data before making predictions. However, if that strategy is infeasible (for example, because the training set is very large), it is useful to generate predictions without retraining the model each time. With skforecast, it is possible to generate predictions starting time ahead of training date. When last_window is provided, the forecaster use this data to generate the lads needed as predictors. 1 forecaster . predict ( steps = 3 , last_window = data [ 'y' ] . tail ( 5 )) 2008-07-01 0.803853 2008-08-01 0.870858 2008-09-01 0.905003 Freq: MS, Name: pred, dtype: float64 Since the provided last_window contains values from 2008-02-01 to 2008-06-01, the forecaster is able to create the needed lags and predict the next 5 steps. \u26a0 WARNING: It is important to note that the length of last windows must be enough to include the maximum lag used by the forecaster. Fore example, if the forecaster uses lags 1, 24, 48 and 72 last_window must include the last 72 values of the series.","title":"Predict"},{"location":"guides/grid-search-sarimax-arima.html","text":"Grid search SARIMAX and ARIMA models \u00b6 SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with eXogenous factors) is a generalization of the ARIMA model that allows incorporating seasonality and exogenous variables. This model have a total of 6 hyperparameters that must specified when training the model: p: Trend autoregression order. d: Trend difference order. q: Trend moving average order. P: Seasonal autoregressive order. D: Seasonal difference order. Q: Seasonal moving average order. m: The number of time steps for a single seasonal period. One way to find the best values is by using grid search. The grid_search_sarimax function of the skforecast.model_selection_statsmodels module is a wrapper that automates this process using the SARIMAX implementation available in the statsmodels library. Libraries \u00b6 1 2 3 4 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.model_selection_statsmodels import grid_search_sarimax Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :] Grid search \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 param_grid = { 'order' : [( 12 , 0 , 0 ), ( 12 , 2 , 0 ), ( 12 , 1 , 0 ), ( 12 , 1 , 1 ), ( 14 , 1 , 4 )], 'seasonal_order' : [( 0 , 0 , 0 , 0 )], 'trend' : [ None , 'n' , 'c' ]} results_grid = grid_search_sarimax ( y = data . loc [: '2006-01-01' ], param_grid = param_grid , initial_train_size = len ( data_train ), steps = 7 , metric = 'mean_absolute_error' , refit = False , verbose = False , fit_kwargs = { 'maxiter' : 200 , 'disp' : 0 } ) print ( results_grid . to_markdown ( tablefmt = \"github\" , index = False )) 1 2 root INFO Number of models compared: 15 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [01:08<00:00, 4.60s/it] 1 results_grid params metric order seasonal_order trend {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0504759 (12, 1, 1) (0, 0, 0, 0) {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0504759 (12, 1, 1) (0, 0, 0, 0) n {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0505509 (14, 1, 4) (0, 0, 0, 0) {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0505509 (14, 1, 4) (0, 0, 0, 0) n {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0518921 (12, 1, 1) (0, 0, 0, 0) c {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0528397 (12, 1, 0) (0, 0, 0, 0) {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0528397 (12, 1, 0) (0, 0, 0, 0) n {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0534996 (12, 1, 0) (0, 0, 0, 0) c {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0537079 (14, 1, 4) (0, 0, 0, 0) c {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.054425 (12, 2, 0) (0, 0, 0, 0) {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.054425 (12, 2, 0) (0, 0, 0, 0) n {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0544985 (12, 2, 0) (0, 0, 0, 0) c {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0636531 (12, 0, 0) (0, 0, 0, 0) {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0636531 (12, 0, 0) (0, 0, 0, 0) n {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0640216 (12, 0, 0) (0, 0, 0, 0) c","title":"Grid search SARIMAX and ARIMA models"},{"location":"guides/grid-search-sarimax-arima.html#grid-search-sarimax-and-arima-models","text":"SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with eXogenous factors) is a generalization of the ARIMA model that allows incorporating seasonality and exogenous variables. This model have a total of 6 hyperparameters that must specified when training the model: p: Trend autoregression order. d: Trend difference order. q: Trend moving average order. P: Seasonal autoregressive order. D: Seasonal difference order. Q: Seasonal moving average order. m: The number of time steps for a single seasonal period. One way to find the best values is by using grid search. The grid_search_sarimax function of the skforecast.model_selection_statsmodels module is a wrapper that automates this process using the SARIMAX implementation available in the statsmodels library.","title":"Grid search SARIMAX and ARIMA models"},{"location":"guides/grid-search-sarimax-arima.html#libraries","text":"1 2 3 4 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.model_selection_statsmodels import grid_search_sarimax","title":"Libraries"},{"location":"guides/grid-search-sarimax-arima.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :]","title":"Data"},{"location":"guides/grid-search-sarimax-arima.html#grid-search","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 param_grid = { 'order' : [( 12 , 0 , 0 ), ( 12 , 2 , 0 ), ( 12 , 1 , 0 ), ( 12 , 1 , 1 ), ( 14 , 1 , 4 )], 'seasonal_order' : [( 0 , 0 , 0 , 0 )], 'trend' : [ None , 'n' , 'c' ]} results_grid = grid_search_sarimax ( y = data . loc [: '2006-01-01' ], param_grid = param_grid , initial_train_size = len ( data_train ), steps = 7 , metric = 'mean_absolute_error' , refit = False , verbose = False , fit_kwargs = { 'maxiter' : 200 , 'disp' : 0 } ) print ( results_grid . to_markdown ( tablefmt = \"github\" , index = False )) 1 2 root INFO Number of models compared: 15 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [01:08<00:00, 4.60s/it] 1 results_grid params metric order seasonal_order trend {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0504759 (12, 1, 1) (0, 0, 0, 0) {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0504759 (12, 1, 1) (0, 0, 0, 0) n {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0505509 (14, 1, 4) (0, 0, 0, 0) {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0505509 (14, 1, 4) (0, 0, 0, 0) n {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0518921 (12, 1, 1) (0, 0, 0, 0) c {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0528397 (12, 1, 0) (0, 0, 0, 0) {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0528397 (12, 1, 0) (0, 0, 0, 0) n {'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0534996 (12, 1, 0) (0, 0, 0, 0) c {'order': (14, 1, 4), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0537079 (14, 1, 4) (0, 0, 0, 0) c {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.054425 (12, 2, 0) (0, 0, 0, 0) {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.054425 (12, 2, 0) (0, 0, 0, 0) n {'order': (12, 2, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0544985 (12, 2, 0) (0, 0, 0, 0) c {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': None} 0.0636531 (12, 0, 0) (0, 0, 0, 0) {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'} 0.0636531 (12, 0, 0) (0, 0, 0, 0) n {'order': (12, 0, 0), 'seasonal_order': (0, 0, 0, 0), 'trend': 'c'} 0.0640216 (12, 0, 0) (0, 0, 0, 0) c","title":"Grid search"},{"location":"guides/input-data.html","text":"Input data \u00b6 Since Version 0.4.0 only pandas series and dataframes are allowed (although internally numpy arrays are used for performance). Base on the type of pandas index, the following rules are applied: If index is not of type DatetimeIndex, a RangeIndex is created. If index is of type DatetimeIndex but has no frequency, a RangeIndex is created. If index is of type DatetimeIndex and has frequency, nothing is changed. Note: There is nothing wrong with using data that does not have an associated DatetimeIndex with frequency. However, results will have less informative index. Libraries \u00b6 1 2 3 4 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg Data \u00b6 1 2 3 4 5 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) Train and predict using input with datetime and frequency index \u00b6 1 2 3 4 5 6 7 8 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 5 ) forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 5 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 2008-10-01 0.845027 2008-11-01 0.914621 Freq: MS, Name: pred, dtype: float64 Train and predict using input without datetime index \u00b6 1 2 data = data . reset_index ( drop = True ) data y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 1 2 forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 5 ) 204 0.714526 205 0.789144 206 0.818433 207 0.845027 208 0.914621 Name: pred, dtype: float64","title":"Input data"},{"location":"guides/input-data.html#input-data","text":"Since Version 0.4.0 only pandas series and dataframes are allowed (although internally numpy arrays are used for performance). Base on the type of pandas index, the following rules are applied: If index is not of type DatetimeIndex, a RangeIndex is created. If index is of type DatetimeIndex but has no frequency, a RangeIndex is created. If index is of type DatetimeIndex and has frequency, nothing is changed. Note: There is nothing wrong with using data that does not have an associated DatetimeIndex with frequency. However, results will have less informative index.","title":"Input data"},{"location":"guides/input-data.html#libraries","text":"1 2 3 4 import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg","title":"Libraries"},{"location":"guides/input-data.html#data","text":"1 2 3 4 5 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' )","title":"Data"},{"location":"guides/input-data.html#train-and-predict-using-input-with-datetime-and-frequency-index","text":"1 2 3 4 5 6 7 8 forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 5 ) forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 5 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 2008-10-01 0.845027 2008-11-01 0.914621 Freq: MS, Name: pred, dtype: float64","title":"Train and predict using input with datetime and frequency index"},{"location":"guides/input-data.html#train-and-predict-using-input-without-datetime-index","text":"1 2 data = data . reset_index ( drop = True ) data y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 1 2 forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 5 ) 204 0.714526 205 0.789144 206 0.818433 207 0.845027 208 0.914621 Name: pred, dtype: float64","title":"Train and predict using input without datetime index"},{"location":"guides/introduction-forecasting.html","text":"Introduction to forcasting \u00b6 Time Series and forecasting \u00b6 A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable. Direct multi-step forecasting \u00b6 This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#introduction-to-forcasting","text":"","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#time-series-and-forecasting","text":"A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions.","title":"Time Series and forecasting"},{"location":"guides/introduction-forecasting.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable.","title":"Recursive multi-step forecasting"},{"location":"guides/introduction-forecasting.html#direct-multi-step-forecasting","text":"This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Direct multi-step forecasting"},{"location":"guides/prediction-intervals.html","text":"Prediction intervals \u00b6 Libraries \u00b6 1 2 3 4 5 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend () Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Creation date: 2022-01-02 16:46:14 Last fit date: 2022-01-02 16:46:14 Skforecast version: 0.4.2 Prediction intervals \u00b6 1 2 3 4 5 6 7 8 9 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = 36 , interval = [ 5 , 95 ], n_boot = 500 ) predictions . head ( 4 ) pred lower_bound upper_bound 2005-07-01 00:00:00 0.973131 0.876141 1.06889 2005-08-01 00:00:00 1.02215 0.926207 1.13022 2005-09-01 00:00:00 1.15133 1.05097 1.26154 2005-10-01 00:00:00 1.2064 1.10905 1.30338 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions [ 'pred' ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions [ 'lower_bound' ], predictions [ 'upper_bound' ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#prediction-intervals","text":"","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#libraries","text":"1 2 3 4 5 import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge","title":"Libraries"},{"location":"guides/prediction-intervals.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ()","title":"Data"},{"location":"guides/prediction-intervals.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': 'deprecated', 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Creation date: 2022-01-02 16:46:14 Last fit date: 2022-01-02 16:46:14 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/prediction-intervals.html#prediction-intervals_1","text":"1 2 3 4 5 6 7 8 9 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = 36 , interval = [ 5 , 95 ], n_boot = 500 ) predictions . head ( 4 ) pred lower_bound upper_bound 2005-07-01 00:00:00 0.973131 0.876141 1.06889 2005-08-01 00:00:00 1.02215 0.926207 1.13022 2005-09-01 00:00:00 1.15133 1.05097 1.26154 2005-10-01 00:00:00 1.2064 1.10905 1.30338 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions [ 'pred' ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions [ 'lower_bound' ], predictions [ 'upper_bound' ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction intervals"},{"location":"guides/predictions-on-training-data.html","text":"Predictions on training data \u00b6 Predictions on training data can be obtained either by using the backtesting_forecaster() function or by accessing the predict() method of the regressor stored inside the forecaster object. Libraries \u00b6 1 2 3 4 5 6 7 8 9 10 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) print ( data . head ( 4 )) datetime y 1991-07-01 00:00:00 0.429795 1991-08-01 00:00:00 0.400906 1991-09-01 00:00:00 0.432159 1991-10-01 00:00:00 0.492543 1 2 3 4 5 # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax , label = 'data' ) ax . legend (); Backtesting forecaster on training data \u00b6 First, the forecaster is trained. 1 2 3 4 5 6 7 8 # Fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data [ 'y' ]) It is possible to perform backtesting using an already trained forecaster without modifying it if arguments initial_train_size = None and refit = False . 1 2 3 4 5 6 7 8 9 10 11 12 13 # Backtest train data # ============================================================================== metric , predictions_train = backtesting_forecaster ( forecaster = forecaster , y = data [ 'y' ], initial_train_size = None , steps = 1 , metric = 'mean_squared_error' , refit = False , verbose = False ) print ( f \"Backtest training error: { metric } \" ) Backtest training error: [0.00045087] 1 predictions_train . head ( 4 ) pred 1992-10-01 00:00:00 0.553134 1992-11-01 00:00:00 0.567766 1992-12-01 00:00:00 0.721389 1993-01-01 00:00:00 0.750997 The first 15 observations are not predicted since they are needed to create the lags used as predictors. 1 2 3 4 5 6 # Plot training predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) predictions_train . plot ( ax = ax ) ax . legend (); Predict using the internal regressor \u00b6 1 2 3 4 5 6 7 8 # Fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data [ 'y' ) 1 2 3 4 5 6 # Create training matrix # ============================================================================== X , y = forecaster . create_train_X_y ( y = data [ 'y' ], exog = None ) Using the internal regressor only allows predicting one step. 1 2 3 # Predict using the internal regressor # ============================================================================== forecaster . regressor . predict ( X )[: 4 ] array([0.55313393, 0.56776596, 0.72138941, 0.75099737])","title":"Predictions on training data"},{"location":"guides/predictions-on-training-data.html#predictions-on-training-data","text":"Predictions on training data can be obtained either by using the backtesting_forecaster() function or by accessing the predict() method of the regressor stored inside the forecaster object.","title":"Predictions on training data"},{"location":"guides/predictions-on-training-data.html#libraries","text":"1 2 3 4 5 6 7 8 9 10 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/predictions-on-training-data.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) print ( data . head ( 4 )) datetime y 1991-07-01 00:00:00 0.429795 1991-08-01 00:00:00 0.400906 1991-09-01 00:00:00 0.432159 1991-10-01 00:00:00 0.492543 1 2 3 4 5 # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax , label = 'data' ) ax . legend ();","title":"Data"},{"location":"guides/predictions-on-training-data.html#backtesting-forecaster-on-training-data","text":"First, the forecaster is trained. 1 2 3 4 5 6 7 8 # Fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data [ 'y' ]) It is possible to perform backtesting using an already trained forecaster without modifying it if arguments initial_train_size = None and refit = False . 1 2 3 4 5 6 7 8 9 10 11 12 13 # Backtest train data # ============================================================================== metric , predictions_train = backtesting_forecaster ( forecaster = forecaster , y = data [ 'y' ], initial_train_size = None , steps = 1 , metric = 'mean_squared_error' , refit = False , verbose = False ) print ( f \"Backtest training error: { metric } \" ) Backtest training error: [0.00045087] 1 predictions_train . head ( 4 ) pred 1992-10-01 00:00:00 0.553134 1992-11-01 00:00:00 0.567766 1992-12-01 00:00:00 0.721389 1993-01-01 00:00:00 0.750997 The first 15 observations are not predicted since they are needed to create the lags used as predictors. 1 2 3 4 5 6 # Plot training predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) predictions_train . plot ( ax = ax ) ax . legend ();","title":"Backtesting forecaster on training data"},{"location":"guides/predictions-on-training-data.html#predict-using-the-internal-regressor","text":"1 2 3 4 5 6 7 8 # Fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data [ 'y' ) 1 2 3 4 5 6 # Create training matrix # ============================================================================== X , y = forecaster . create_train_X_y ( y = data [ 'y' ], exog = None ) Using the internal regressor only allows predicting one step. 1 2 3 # Predict using the internal regressor # ============================================================================== forecaster . regressor . predict ( X )[: 4 ] array([0.55313393, 0.56776596, 0.72138941, 0.75099737])","title":"Predict using the internal regressor"},{"location":"guides/save-load-forecaster.html","text":"Save and load forecaster \u00b6 Skforecast models can be loaded and stored using pickle or joblib library. A simple example using joblib is shown below. Libraries \u00b6 1 2 3 4 5 import numpy as np import pandas as pd from joblib import dump , load from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg Data \u00b6 1 2 3 4 5 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) Save and load model \u00b6 1 2 3 4 5 6 7 8 # Create and train forecaster forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 5 ) forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 3 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 8 # Save model dump ( forecaster , filename = 'forecaster.py' ) # Load model forecaster_loaded = load ( 'forecaster.py' ) # Predict forecaster_loaded . predict ( steps = 3 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 Freq: MS, Name: pred, dtype: float64","title":"Save and load forecaster"},{"location":"guides/save-load-forecaster.html#save-and-load-forecaster","text":"Skforecast models can be loaded and stored using pickle or joblib library. A simple example using joblib is shown below.","title":"Save and load forecaster"},{"location":"guides/save-load-forecaster.html#libraries","text":"1 2 3 4 5 import numpy as np import pandas as pd from joblib import dump , load from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg","title":"Libraries"},{"location":"guides/save-load-forecaster.html#data","text":"1 2 3 4 5 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'date' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' )","title":"Data"},{"location":"guides/save-load-forecaster.html#save-and-load-model","text":"1 2 3 4 5 6 7 8 # Create and train forecaster forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 5 ) forecaster . fit ( y = data [ 'y' ]) forecaster . predict ( steps = 3 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 8 # Save model dump ( forecaster , filename = 'forecaster.py' ) # Load model forecaster_loaded = load ( 'forecaster.py' ) # Predict forecaster_loaded . predict ( steps = 3 ) 2008-07-01 0.714526 2008-08-01 0.789144 2008-09-01 0.818433 Freq: MS, Name: pred, dtype: float64","title":"Save and load model"},{"location":"guides/sklearn-pipeline.html","text":"Forecasting with scikit-learn pipelines \u00b6 Since version 0.4.0, skforecast allows using scikit-learn pipelines as regressors. This is useful since, many machine learning models, need specific data preprocessing transformations. For example, linear models with Ridge or Lasso regularization benefits from features been scaled. \u26a0 WARNING: Version 0.4 does not allow including ColumnTransformer in the pipeline used as regressor, so if the preprocessing transformations only apply to some specific columns, they have to be applied on the data set before training the model. A more detailed example can be found here . Libraries \u00b6 1 2 3 4 5 6 7 import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster Data \u00b6 1 2 3 4 5 6 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) Create pipeline \u00b6 1 2 pipe = make_pipeline ( StandardScaler (), Ridge ()) pipe Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 pipe = make_pipeline ( StandardScaler (), Ridge ()) forecaster = ForecasterAutoreg ( regressor = pipe , lags = 10 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__normalize': 'deprecated', 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.001} Creation date: 2022-01-02 16:47:46 Last fit date: 2022-01-02 16:47:46 Skforecast version: 0.4.2 Grid Search \u00b6 When performing grid search over a sklearn pipeline, the name of the parameters is preceded by the name of the model. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 pipe = make_pipeline ( StandardScaler (), Ridge ()) forecaster = ForecasterAutoreg ( regressor = pipe , lags = 10 # This value will be replaced in the grid search ) # Regressor's hyperparameters param_grid = { 'ridge__alpha' : np . logspace ( - 3 , 5 , 10 )} # Lags used as predictors lags_grid = [ 5 , 24 , [ 1 , 2 , 3 , 23 , 24 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]], param_grid = param_grid , lags_grid = lags_grid , steps = 5 , metric = 'mean_absolute_error' , refit = False , initial_train_size = len ( data . loc [: '2000-04-01' ]), return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Number of models compared: 30 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 10%|\u2588\u2588\u2588\u258b | 1/10 [00:00<00:01, 7.71it/s] loop param_grid: 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/10 [00:00<00:00, 10.74it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 5/10 [00:00<00:00, 12.45it/s] loop param_grid: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 7/10 [00:00<00:00, 13.30it/s] loop param_grid: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 9/10 [00:00<00:00, 12.91it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1/3 [00:00<00:01, 1.17it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 2/10 [00:00<00:00, 11.57it/s] loop param_grid: 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 4/10 [00:00<00:00, 10.51it/s] loop param_grid: 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 6/10 [00:00<00:00, 11.47it/s] loop param_grid: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 8/10 [00:00<00:00, 12.24it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 11.99it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2/3 [00:01<00:00, 1.17it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 2/10 [00:00<00:00, 13.18it/s] loop param_grid: 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 4/10 [00:00<00:00, 11.01it/s] loop param_grid: 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 6/10 [00:00<00:00, 11.97it/s] loop param_grid: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 8/10 [00:00<00:00, 12.65it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 11.66it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00, 1.17it/s] Refitting `forecaster` using the best found lags and parameters and the whole data set: Lags: [1 2 3 4 5] Parameters: {'ridge__alpha': 0.001} Backtesting metric: 6.845311709573567e-05 lags params metric ridge__alpha 0 [1 2 3 4 5] {'ridge__alpha': 0.001} 6.84531e-05 0.001 10 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.001} 0.000187797 0.001 1 [1 2 3 4 5] {'ridge__alpha': 0.007742636826811269} 0.000526168 0.00774264 11 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.007742636826811269} 0.00141293 0.00774264 2 [1 2 3 4 5] {'ridge__alpha': 0.05994842503189409} 0.00385988 0.0599484 12 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.05994842503189409} 0.00896885 0.0599484 3 [1 2 3 4 5] {'ridge__alpha': 0.46415888336127775} 0.0217507 0.464159 13 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.46415888336127775} 0.0295054 0.464159 14 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 3.593813663804626} 0.046323 3.59381 23 [ 1 2 3 23 24] {'ridge__alpha': 0.46415888336127775} 0.0606231 0.464159 22 [ 1 2 3 23 24] {'ridge__alpha': 0.05994842503189409} 0.0615665 0.0599484 21 [ 1 2 3 23 24] {'ridge__alpha': 0.007742636826811269} 0.0617473 0.00774264 20 [ 1 2 3 23 24] {'ridge__alpha': 0.001} 0.0617715 0.001 24 [ 1 2 3 23 24] {'ridge__alpha': 3.593813663804626} 0.0635121 3.59381 15 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 27.825594022071257} 0.0645505 27.8256 4 [1 2 3 4 5] {'ridge__alpha': 3.593813663804626} 0.0692201 3.59381 25 [ 1 2 3 23 24] {'ridge__alpha': 27.825594022071257} 0.077934 27.8256 16 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 215.44346900318823} 0.130016 215.443 5 [1 2 3 4 5] {'ridge__alpha': 27.825594022071257} 0.143189 27.8256 26 [ 1 2 3 23 24] {'ridge__alpha': 215.44346900318823} 0.146446 215.443 17 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 1668.1005372000557} 0.204469 1668.1 6 [1 2 3 4 5] {'ridge__alpha': 215.44346900318823} 0.205496 215.443 27 [ 1 2 3 23 24] {'ridge__alpha': 1668.1005372000557} 0.212896 1668.1 18 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 12915.496650148827} 0.227536 12915.5 28 [ 1 2 3 23 24] {'ridge__alpha': 12915.496650148827} 0.228974 12915.5 19 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 100000.0} 0.231157 100000 29 [ 1 2 3 23 24] {'ridge__alpha': 100000.0} 0.231356 100000 7 [1 2 3 4 5] {'ridge__alpha': 1668.1005372000557} 0.236227 1668.1 8 [1 2 3 4 5] {'ridge__alpha': 12915.496650148827} 0.244788 12915.5 9 [1 2 3 4 5] {'ridge__alpha': 100000.0} 0.246091 100000","title":"Scikit-learn Pipeline"},{"location":"guides/sklearn-pipeline.html#forecasting-with-scikit-learn-pipelines","text":"Since version 0.4.0, skforecast allows using scikit-learn pipelines as regressors. This is useful since, many machine learning models, need specific data preprocessing transformations. For example, linear models with Ridge or Lasso regularization benefits from features been scaled. \u26a0 WARNING: Version 0.4 does not allow including ColumnTransformer in the pipeline used as regressor, so if the preprocessing transformations only apply to some specific columns, they have to be applied on the data set before training the model. A more detailed example can be found here .","title":"Forecasting with scikit-learn pipelines"},{"location":"guides/sklearn-pipeline.html#libraries","text":"1 2 3 4 5 6 7 import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster","title":"Libraries"},{"location":"guides/sklearn-pipeline.html#data","text":"1 2 3 4 5 6 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' )","title":"Data"},{"location":"guides/sklearn-pipeline.html#create-pipeline","text":"1 2 pipe = make_pipeline ( StandardScaler (), Ridge ()) pipe Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])","title":"Create pipeline"},{"location":"guides/sklearn-pipeline.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 pipe = make_pipeline ( StandardScaler (), Ridge ()) forecaster = ForecasterAutoreg ( regressor = pipe , lags = 10 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__normalize': 'deprecated', 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.001} Creation date: 2022-01-02 16:47:46 Last fit date: 2022-01-02 16:47:46 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/sklearn-pipeline.html#grid-search","text":"When performing grid search over a sklearn pipeline, the name of the parameters is preceded by the name of the model. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 pipe = make_pipeline ( StandardScaler (), Ridge ()) forecaster = ForecasterAutoreg ( regressor = pipe , lags = 10 # This value will be replaced in the grid search ) # Regressor's hyperparameters param_grid = { 'ridge__alpha' : np . logspace ( - 3 , 5 , 10 )} # Lags used as predictors lags_grid = [ 5 , 24 , [ 1 , 2 , 3 , 23 , 24 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]], param_grid = param_grid , lags_grid = lags_grid , steps = 5 , metric = 'mean_absolute_error' , refit = False , initial_train_size = len ( data . loc [: '2000-04-01' ]), return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Number of models compared: 30 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 10%|\u2588\u2588\u2588\u258b | 1/10 [00:00<00:01, 7.71it/s] loop param_grid: 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/10 [00:00<00:00, 10.74it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 5/10 [00:00<00:00, 12.45it/s] loop param_grid: 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 7/10 [00:00<00:00, 13.30it/s] loop param_grid: 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 9/10 [00:00<00:00, 12.91it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1/3 [00:00<00:01, 1.17it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 2/10 [00:00<00:00, 11.57it/s] loop param_grid: 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 4/10 [00:00<00:00, 10.51it/s] loop param_grid: 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 6/10 [00:00<00:00, 11.47it/s] loop param_grid: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 8/10 [00:00<00:00, 12.24it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 11.99it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2/3 [00:01<00:00, 1.17it/s] loop param_grid: 0%| | 0/10 [00:00<?, ?it/s] loop param_grid: 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 2/10 [00:00<00:00, 13.18it/s] loop param_grid: 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 4/10 [00:00<00:00, 11.01it/s] loop param_grid: 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 6/10 [00:00<00:00, 11.97it/s] loop param_grid: 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 8/10 [00:00<00:00, 12.65it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 11.66it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00, 1.17it/s] Refitting `forecaster` using the best found lags and parameters and the whole data set: Lags: [1 2 3 4 5] Parameters: {'ridge__alpha': 0.001} Backtesting metric: 6.845311709573567e-05 lags params metric ridge__alpha 0 [1 2 3 4 5] {'ridge__alpha': 0.001} 6.84531e-05 0.001 10 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.001} 0.000187797 0.001 1 [1 2 3 4 5] {'ridge__alpha': 0.007742636826811269} 0.000526168 0.00774264 11 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.007742636826811269} 0.00141293 0.00774264 2 [1 2 3 4 5] {'ridge__alpha': 0.05994842503189409} 0.00385988 0.0599484 12 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.05994842503189409} 0.00896885 0.0599484 3 [1 2 3 4 5] {'ridge__alpha': 0.46415888336127775} 0.0217507 0.464159 13 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 0.46415888336127775} 0.0295054 0.464159 14 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 3.593813663804626} 0.046323 3.59381 23 [ 1 2 3 23 24] {'ridge__alpha': 0.46415888336127775} 0.0606231 0.464159 22 [ 1 2 3 23 24] {'ridge__alpha': 0.05994842503189409} 0.0615665 0.0599484 21 [ 1 2 3 23 24] {'ridge__alpha': 0.007742636826811269} 0.0617473 0.00774264 20 [ 1 2 3 23 24] {'ridge__alpha': 0.001} 0.0617715 0.001 24 [ 1 2 3 23 24] {'ridge__alpha': 3.593813663804626} 0.0635121 3.59381 15 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 27.825594022071257} 0.0645505 27.8256 4 [1 2 3 4 5] {'ridge__alpha': 3.593813663804626} 0.0692201 3.59381 25 [ 1 2 3 23 24] {'ridge__alpha': 27.825594022071257} 0.077934 27.8256 16 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 215.44346900318823} 0.130016 215.443 5 [1 2 3 4 5] {'ridge__alpha': 27.825594022071257} 0.143189 27.8256 26 [ 1 2 3 23 24] {'ridge__alpha': 215.44346900318823} 0.146446 215.443 17 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 1668.1005372000557} 0.204469 1668.1 6 [1 2 3 4 5] {'ridge__alpha': 215.44346900318823} 0.205496 215.443 27 [ 1 2 3 23 24] {'ridge__alpha': 1668.1005372000557} 0.212896 1668.1 18 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 12915.496650148827} 0.227536 12915.5 28 [ 1 2 3 23 24] {'ridge__alpha': 12915.496650148827} 0.228974 12915.5 19 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] {'ridge__alpha': 100000.0} 0.231157 100000 29 [ 1 2 3 23 24] {'ridge__alpha': 100000.0} 0.231356 100000 7 [1 2 3 4 5] {'ridge__alpha': 1668.1005372000557} 0.236227 1668.1 8 [1 2 3 4 5] {'ridge__alpha': 12915.496650148827} 0.244788 12915.5 9 [1 2 3 4 5] {'ridge__alpha': 100000.0} 0.246091 100000","title":"Grid Search"},{"location":"guides/tuning-forecaster.html","text":"Tuning forecaster \u00b6 Skforecast library allows to combine grid search strategy with backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediction performance. Libraries \u00b6 1 2 3 4 import numpy as np import pandas as pd from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :] Grid search \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Grid search hyperparameter and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hyperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 , 15 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data . loc [: '2006-01-01' ], param_grid = param_grid , lags_grid = lags_grid , steps = 12 , refit = True , metric = 'mean_squared_error' , initial_train_size = len ( data_train ), return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Number of models compared: 18 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:02, 2.01it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:03, 1.13it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.40it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.20it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:03<00:00, 1.42it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:04<00:00, 1.12it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1/3 [00:04<00:09, 4.93s/it] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:03, 1.43it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:04, 1.02s/it] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.24it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.14it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:04<00:00, 1.33it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:05<00:00, 1.20it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2/3 [00:09<00:04, 4.98s/it] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:02, 2.08it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:03, 1.33it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.25it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.01it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:04<00:00, 1.22it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:04<00:00, 1.17it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.98s/it] Refitting `forecaster` using the best found lags and parameters and the whole data set: Lags: [ 1 2 3 4 5 6 7 8 9 10] Parameters: {'max_depth': 5, 'n_estimators': 50} Backtesting metric: 0.03344857370906804 1 results_grid lags params metric max_depth n_estimators [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0334486 5 50 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0392212 10 50 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 15, 'n_estimators': 100} 0.0392658 15 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0395258 5 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0402408 10 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 15, 'n_estimators': 50} 0.0407645 15 50 [ 1 2 3 20] {'max_depth': 15, 'n_estimators': 100} 0.0439092 15 100 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0449923 5 100 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0462237 5 50 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0486662 5 50 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0489914 10 100 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0501932 10 50 [1 2 3] {'max_depth': 15, 'n_estimators': 100} 0.0505563 15 100 [ 1 2 3 20] {'max_depth': 15, 'n_estimators': 50} 0.0512172 15 50 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0531229 5 100 [1 2 3] {'max_depth': 15, 'n_estimators': 50} 0.0602604 15 50 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.0609513 10 50 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.0673343 10 100 If argument return_best = True , the forecaster is retrained using all data available and the best combination of lags and hyperparameters. 1 skforecast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(max_depth=5, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:40:40 Last fit date: 2022-01-02 16:40:55 Skforecast version: 0.4.2","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#tuning-forecaster","text":"Skforecast library allows to combine grid search strategy with backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediction performance.","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#libraries","text":"1 2 3 4 import numpy as np import pandas as pd from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge","title":"Libraries"},{"location":"guides/tuning-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :]","title":"Data"},{"location":"guides/tuning-forecaster.html#grid-search","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Grid search hyperparameter and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hyperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 , 15 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data . loc [: '2006-01-01' ], param_grid = param_grid , lags_grid = lags_grid , steps = 12 , refit = True , metric = 'mean_squared_error' , initial_train_size = len ( data_train ), return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Number of models compared: 18 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:02, 2.01it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:03, 1.13it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.40it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.20it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:03<00:00, 1.42it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:04<00:00, 1.12it/s] loop lags_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1/3 [00:04<00:09, 4.93s/it] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:03, 1.43it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:04, 1.02s/it] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.24it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.14it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:04<00:00, 1.33it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:05<00:00, 1.20it/s] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2/3 [00:09<00:04, 4.98s/it] loop param_grid: 0%| | 0/6 [00:00<?, ?it/s] loop param_grid: 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 1/6 [00:00<00:02, 2.08it/s] loop param_grid: 33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/6 [00:01<00:03, 1.33it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3/6 [00:02<00:02, 1.25it/s] loop param_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 4/6 [00:03<00:01, 1.01it/s] loop param_grid: 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 5/6 [00:04<00:00, 1.22it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:04<00:00, 1.17it/s] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.98s/it] Refitting `forecaster` using the best found lags and parameters and the whole data set: Lags: [ 1 2 3 4 5 6 7 8 9 10] Parameters: {'max_depth': 5, 'n_estimators': 50} Backtesting metric: 0.03344857370906804 1 results_grid lags params metric max_depth n_estimators [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0334486 5 50 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0392212 10 50 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 15, 'n_estimators': 100} 0.0392658 15 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0395258 5 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0402408 10 100 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 15, 'n_estimators': 50} 0.0407645 15 50 [ 1 2 3 20] {'max_depth': 15, 'n_estimators': 100} 0.0439092 15 100 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0449923 5 100 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0462237 5 50 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0486662 5 50 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0489914 10 100 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0501932 10 50 [1 2 3] {'max_depth': 15, 'n_estimators': 100} 0.0505563 15 100 [ 1 2 3 20] {'max_depth': 15, 'n_estimators': 50} 0.0512172 15 50 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0531229 5 100 [1 2 3] {'max_depth': 15, 'n_estimators': 50} 0.0602604 15 50 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.0609513 10 50 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.0673343 10 100 If argument return_best = True , the forecaster is retrained using all data available and the best combination of lags and hyperparameters. 1 skforecast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(max_depth=5, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2022-01-02 16:40:40 Last fit date: 2022-01-02 16:40:55 Skforecast version: 0.4.2","title":"Grid search"},{"location":"guides/xgboost.html","text":"Forecasting with XGBoost \u00b6 XGBoost, acronym for Extreme Gradient Boosting, is a very efficient implementation of the stochastic gradient boosting algorithm that has become a benchmark in the field of machine learning. In addition to its own API, XGBoost library includes the XGBRegressor class which follows the scikit learn API and therefore it is compatible with skforecast. NOTE: Since the success of XGBoost as a machine learning algorithm, new implementations have been developed that also achieve excellent results, two of them are: LightGBM and CatBoost . A more detailed example can be found here . Libraries \u00b6 1 2 3 4 5 import numpy as np import pandas as pd import matplotlib.pyplot as plt from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg Data \u00b6 1 2 3 4 5 6 7 8 9 10 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :] Create and train forecaster \u00b6 1 2 3 4 5 6 7 forecaster = ForecasterAutoreg ( regressor = XGBRegressor (), lags = 8 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ================= ForecasterAutoreg ================= Regressor: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Lags: [1 2 3 4 5 6 7 8] Window size: 8 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 8, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None} Creation date: 2022-01-02 16:49:14 Last fit date: 2022-01-02 16:49:14 Skforecast version: 0.4.2 Prediction \u00b6 1 forecaster . predict ( steps = 10 , exog = data_test [[ 'exog_1' , 'exog_2' ]]) 2008-07-01 0.700701 2008-08-01 0.829139 2008-09-01 0.983677 2008-10-01 1.098782 2008-11-01 1.078021 2008-12-01 1.206761 2009-01-01 1.149827 2009-02-01 1.049927 2009-03-01 0.947129 2009-04-01 0.700440 Freq: MS, Name: pred, dtype: float64 Feature importance \u00b6 1 forecaster . get_feature_importance () feature importance 0 lag_1 0.358967 1 lag_2 0.0935667 2 lag_3 0.0167286 3 lag_4 0.0446115 4 lag_5 0.054733 5 lag_6 0.0095096 6 lag_7 0.0971787 7 lag_8 0.0279641 8 exog_1 0.186294 9 exog_2 0.110447","title":"XGBoost"},{"location":"guides/xgboost.html#forecasting-with-xgboost","text":"XGBoost, acronym for Extreme Gradient Boosting, is a very efficient implementation of the stochastic gradient boosting algorithm that has become a benchmark in the field of machine learning. In addition to its own API, XGBoost library includes the XGBRegressor class which follows the scikit learn API and therefore it is compatible with skforecast. NOTE: Since the success of XGBoost as a machine learning algorithm, new implementations have been developed that also achieve excellent results, two of them are: LightGBM and CatBoost . A more detailed example can be found here .","title":"Forecasting with XGBoost"},{"location":"guides/xgboost.html#libraries","text":"1 2 3 4 5 import numpy as np import pandas as pd import matplotlib.pyplot as plt from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg","title":"Libraries"},{"location":"guides/xgboost.html#data","text":"1 2 3 4 5 6 7 8 9 10 url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'date' , 'y' , 'exog_1' , 'exog_2' ]) data [ 'date' ] = pd . to_datetime ( data [ 'date' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'date' ) data = data . asfreq ( 'MS' ) steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :]","title":"Data"},{"location":"guides/xgboost.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 forecaster = ForecasterAutoreg ( regressor = XGBRegressor (), lags = 8 ) forecaster . fit ( y = data [ 'y' ], exog = data [[ 'exog_1' , 'exog_2' ]]) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ================= ForecasterAutoreg ================= Regressor: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Lags: [1 2 3 4 5 6 7 8] Window size: 8 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 8, 'num_parallel_tree': 1, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None} Creation date: 2022-01-02 16:49:14 Last fit date: 2022-01-02 16:49:14 Skforecast version: 0.4.2","title":"Create and train forecaster"},{"location":"guides/xgboost.html#prediction","text":"1 forecaster . predict ( steps = 10 , exog = data_test [[ 'exog_1' , 'exog_2' ]]) 2008-07-01 0.700701 2008-08-01 0.829139 2008-09-01 0.983677 2008-10-01 1.098782 2008-11-01 1.078021 2008-12-01 1.206761 2009-01-01 1.149827 2009-02-01 1.049927 2009-03-01 0.947129 2009-04-01 0.700440 Freq: MS, Name: pred, dtype: float64","title":"Prediction"},{"location":"guides/xgboost.html#feature-importance","text":"1 forecaster . get_feature_importance () feature importance 0 lag_1 0.358967 1 lag_2 0.0935667 2 lag_3 0.0167286 3 lag_4 0.0446115 4 lag_5 0.054733 5 lag_6 0.0095096 6 lag_7 0.0971787 7 lag_8 0.0279641 8 exog_1 0.186294 9 exog_2 0.110447","title":"Feature importance"},{"location":"releases/releases.html","text":"Releases \u00b6 All notable changes to this project will be documented in this file. [0.4.2] - [2022-01-08] \u00b6 Added \u00b6 Increased verbosity of function backtesting_forecaster() . Random state argument in backtesting_forecaster() . Changed \u00b6 Function backtesting_forecaster() do not modify the original forecaster. Deprecated argument set_out_sample_residuals in function backtesting_forecaster() . Function model_selection.time_series_spliter renamed to model_selection.time_series_splitter Fixed \u00b6 Methods get_coef and get_feature_importance of ForecasterAutoregMultiOutput class return proper feature names. [0.4.1] - [2021-12-13] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 fit and predict transform pandas series and dataframes to numpy arrays if regressor is XGBoost. [0.4.0] - [2021-12-10] \u00b6 Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit). Added \u00b6 ForecasterBase as parent class Changed \u00b6 Argument y must be pandas Series. Numpy ndarrays are not allowed anymore. Argument exog must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore. Output of predict is a pandas Series with index according to the steps predicted. Scikitlearn pipelines are allowed as regressors. backtesting_forecaster and backtesting_forecaster_intervals have been combined in a single function. It is possible to backtest forecasters already trained. ForecasterAutoregMultiOutput allows incomplete folds. It is possible to update out_sample_residuals with backtesting residuals. cv_forecaster has the option to update out_sample_residuals with backtesting residuals. backtesting_sarimax_statsmodels and cv_sarimax_statsmodels have been combined in a single function. gridsearch_forecaster use backtesting as validation strategy with the option of refit. Extended information when printing Forecaster object. All static methods for checking and preprocessing inputs moved to module utils. Remove deprecated class ForecasterCustom . [0.3.0] - [2021-09-01] \u00b6 Added \u00b6 New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels Changed \u00b6 cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements Fixed \u00b6 [0.2.0] - [2021-08-26] \u00b6 Added \u00b6 Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Fixed \u00b6 [0.1.9] - 2121-07-27 \u00b6 Added \u00b6 Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog . Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric. Fixed \u00b6 Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster . [0.1.8.1] - 2021-05-17 \u00b6 Added \u00b6 set_out_sample_residuals method to store or update out of sample residuals used by predict_interval . Changed \u00b6 backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals . Fixed \u00b6 Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput . [0.1.8] - 2021-04-02 \u00b6 Added \u00b6 Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals. Changed \u00b6 Fixed \u00b6 [0.1.7] - 2021-03-19 \u00b6 Added \u00b6 Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors. Changed \u00b6 grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg . Fixed \u00b6 [0.1.6] - 2021-03-14 \u00b6 Added \u00b6 Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection . Changed \u00b6 Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster . Fixed \u00b6 [0.1.4] - 2021-02-15 \u00b6 Added \u00b6 Method get_coef to skforecast.ForecasterAutoreg . Changed \u00b6 Fixed \u00b6","title":"Releases"},{"location":"releases/releases.html#releases","text":"All notable changes to this project will be documented in this file.","title":"Releases"},{"location":"releases/releases.html#042-2022-01-08","text":"","title":"[0.4.2] - [2022-01-08]"},{"location":"releases/releases.html#added","text":"Increased verbosity of function backtesting_forecaster() . Random state argument in backtesting_forecaster() .","title":"Added"},{"location":"releases/releases.html#changed","text":"Function backtesting_forecaster() do not modify the original forecaster. Deprecated argument set_out_sample_residuals in function backtesting_forecaster() . Function model_selection.time_series_spliter renamed to model_selection.time_series_splitter","title":"Changed"},{"location":"releases/releases.html#fixed","text":"Methods get_coef and get_feature_importance of ForecasterAutoregMultiOutput class return proper feature names.","title":"Fixed"},{"location":"releases/releases.html#041-2021-12-13","text":"","title":"[0.4.1] - [2021-12-13]"},{"location":"releases/releases.html#added_1","text":"","title":"Added"},{"location":"releases/releases.html#changed_1","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_1","text":"fit and predict transform pandas series and dataframes to numpy arrays if regressor is XGBoost.","title":"Fixed"},{"location":"releases/releases.html#040-2021-12-10","text":"Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit).","title":"[0.4.0] - [2021-12-10]"},{"location":"releases/releases.html#added_2","text":"ForecasterBase as parent class","title":"Added"},{"location":"releases/releases.html#changed_2","text":"Argument y must be pandas Series. Numpy ndarrays are not allowed anymore. Argument exog must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore. Output of predict is a pandas Series with index according to the steps predicted. Scikitlearn pipelines are allowed as regressors. backtesting_forecaster and backtesting_forecaster_intervals have been combined in a single function. It is possible to backtest forecasters already trained. ForecasterAutoregMultiOutput allows incomplete folds. It is possible to update out_sample_residuals with backtesting residuals. cv_forecaster has the option to update out_sample_residuals with backtesting residuals. backtesting_sarimax_statsmodels and cv_sarimax_statsmodels have been combined in a single function. gridsearch_forecaster use backtesting as validation strategy with the option of refit. Extended information when printing Forecaster object. All static methods for checking and preprocessing inputs moved to module utils. Remove deprecated class ForecasterCustom .","title":"Changed"},{"location":"releases/releases.html#030-2021-09-01","text":"","title":"[0.3.0] - [2021-09-01]"},{"location":"releases/releases.html#added_3","text":"New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels","title":"Added"},{"location":"releases/releases.html#changed_3","text":"cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements","title":"Changed"},{"location":"releases/releases.html#fixed_2","text":"","title":"Fixed"},{"location":"releases/releases.html#020-2021-08-26","text":"","title":"[0.2.0] - [2021-08-26]"},{"location":"releases/releases.html#added_4","text":"Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing","title":"Added"},{"location":"releases/releases.html#changed_4","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput .","title":"Changed"},{"location":"releases/releases.html#fixed_3","text":"","title":"Fixed"},{"location":"releases/releases.html#019-2121-07-27","text":"","title":"[0.1.9] - 2121-07-27"},{"location":"releases/releases.html#added_5","text":"Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog .","title":"Added"},{"location":"releases/releases.html#changed_5","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.","title":"Changed"},{"location":"releases/releases.html#fixed_4","text":"Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster .","title":"Fixed"},{"location":"releases/releases.html#0181-2021-05-17","text":"","title":"[0.1.8.1] - 2021-05-17"},{"location":"releases/releases.html#added_6","text":"set_out_sample_residuals method to store or update out of sample residuals used by predict_interval .","title":"Added"},{"location":"releases/releases.html#changed_6","text":"backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals .","title":"Changed"},{"location":"releases/releases.html#fixed_5","text":"Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput .","title":"Fixed"},{"location":"releases/releases.html#018-2021-04-02","text":"","title":"[0.1.8] - 2021-04-02"},{"location":"releases/releases.html#added_7","text":"Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals.","title":"Added"},{"location":"releases/releases.html#changed_7","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_6","text":"","title":"Fixed"},{"location":"releases/releases.html#017-2021-03-19","text":"","title":"[0.1.7] - 2021-03-19"},{"location":"releases/releases.html#added_8","text":"Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors.","title":"Added"},{"location":"releases/releases.html#changed_8","text":"grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg .","title":"Changed"},{"location":"releases/releases.html#fixed_7","text":"","title":"Fixed"},{"location":"releases/releases.html#016-2021-03-14","text":"","title":"[0.1.6] - 2021-03-14"},{"location":"releases/releases.html#added_9","text":"Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection .","title":"Added"},{"location":"releases/releases.html#changed_9","text":"Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster .","title":"Changed"},{"location":"releases/releases.html#fixed_8","text":"","title":"Fixed"},{"location":"releases/releases.html#014-2021-02-15","text":"","title":"[0.1.4] - 2021-02-15"},{"location":"releases/releases.html#added_10","text":"Method get_coef to skforecast.ForecasterAutoreg .","title":"Added"},{"location":"releases/releases.html#changed_10","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_9","text":"","title":"Fixed"}]}