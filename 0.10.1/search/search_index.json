{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to skforecast","text":"Package Testing Meta"},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>Skforecast is a Python library that eases using scikit-learn regressors as single and multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (LightGBM, XGBoost, CatBoost, ...).</p> <p>Why use skforecast?</p> <p>The fields of statistics and machine learning have developed many excellent regression algorithms that can be useful for forecasting, but applying them effectively to time series analysis can still be a challenge. To address this issue, the skforecast library provides a comprehensive set of tools for training, validation and prediction in a variety of scenarios commonly encountered when working with time series. The library is built using the widely used scikit-learn API, making it easy to integrate into existing workflows. With skforecast, users have access to a wide range of functionalities such as feature engineering, model selection, hyperparameter tuning and many others. This allows users to focus on the essential aspects of their projects and leave the intricacies of time series analysis to skforecast. In addition, skforecast is developed according to the following priorities:</p> <ul> <li>Fast and robust prototyping. </li> <li>Validation and backtesting methods to have a realistic assessment of model performance. </li> <li>Models must be deployed in production. </li> <li>Models must be interpretable. </li> </ul> <p>Share Your Thoughts with Us</p> <p>Thank you for choosing skforecast! We value your suggestions, bug reports and recommendations as they help us identify areas for improvement and ensure that skforecast meets the needs of the community. Please consider sharing your experiences, reporting bugs, making suggestions or even contributing to the codebase on GitHub. Together, let's make time series forecasting more accessible and accurate for everyone.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>The default installation of skforecast only installs hard dependencies.</p> <pre><code>pip install skforecast\n</code></pre> <p>Specific version:</p> <pre><code>pip install skforecast==0.10.1\n</code></pre> <p>Latest (unstable):</p> <pre><code>pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master\n</code></pre> <p>Install the full version (all dependencies):</p> <pre><code>pip install skforecast[full]\n</code></pre> <p>Install optional dependencies:</p> <pre><code>pip install skforecast[sarimax]\n</code></pre> <pre><code>pip install skforecast[plotting]\n</code></pre>"},{"location":"index.html#dependencies","title":"Dependencies","text":"<ul> <li>Python &gt;= 3.8</li> </ul>"},{"location":"index.html#hard-dependencies","title":"Hard dependencies","text":"<ul> <li>numpy&gt;=1.20, &lt;1.26</li> <li>pandas&gt;=1.2, &lt;2.1</li> <li>tqdm&gt;=4.57.0, &lt;4.66</li> <li>scikit-learn&gt;=1.0, &lt;1.4</li> <li>optuna&gt;=2.10.0, &lt;3.3</li> <li>joblib&gt;=1.1.0, &lt;1.4</li> </ul>"},{"location":"index.html#optional-dependencies","title":"Optional dependencies","text":"<ul> <li>matplotlib&gt;=3.3, &lt;3.8</li> <li>seaborn&gt;=0.11, &lt;0.13</li> <li>statsmodels&gt;=0.12, &lt;0.15</li> <li>pmdarima&gt;=2.0, &lt;2.1</li> </ul>"},{"location":"index.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"index.html#features","title":"Features","text":"<ul> <li>Create Forecasters from any regressor that follows the scikit-learn API</li> <li>Include exogenous variables as predictors</li> <li>Include custom predictors (rolling mean, rolling variance ...)</li> <li>Multiple backtesting methods for model validation</li> <li>Grid search, random search and Bayesian search to find optimal lags (predictors) and best hyperparameters</li> <li>Prediction interval estimated by bootstrapping and quantile regression</li> <li>Include custom metrics for model validation and grid search</li> <li>Get predictor importance</li> <li>Forecaster in production</li> </ul>"},{"location":"index.html#examples-and-tutorials","title":"Examples and tutorials","text":""},{"location":"index.html#english","title":"English","text":"<p> Skforecast: time series forecasting with Python and Scikit-learn </p> <p> ARIMA and SARIMAX models</p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost </p> <p> Modelling time series trend with tree based models</p> <p> Forecasting electricity demand with Python </p> <p> Forecasting web traffic with machine learning and Python </p> <p> Bitcoin price prediction with Python</p> <p> Prediction intervals in forecasting models</p> <p> Multi-series forecasting</p> <p> Reducing the influence of Covid-19 on time series forecasting models</p> <p> Forecasting time series with missing values</p> <p> Intermittent demand forecasting</p>"},{"location":"index.html#espanol","title":"Espa\u00f1ol","text":"<p> Skforecast: forecasting series temporales con Python y Scikit-learn </p> <p> Modelos ARIMA y SARIMAX</p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost </p> <p> Modelar series temporales con tendencia utilizando modelos de \u00e1rboles</p> <p> Forecasting de la demanda el\u00e9ctrica </p> <p> Forecasting de las visitas a una p\u00e1gina web </p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p> <p> Intervalos de predicci\u00f3n en modelos de forecasting</p> <p> Multi-series forecasting</p> <p> Predicci\u00f3n demanda intermitente</p>"},{"location":"index.html#how-to-contribute","title":"How to contribute","text":"<p>Primarily, skforecast development consists of adding and creating new Forecasters, new validation strategies, or improving the performance of the current code. However, there are many other ways to contribute:</p> <ul> <li>Submit a bug report or feature request on GitHub Issues.</li> <li>Contribute a Jupyter notebook to our examples.</li> <li>Write unit or integration tests for our project.</li> <li>Answer questions on our issues, Stack Overflow, and elsewhere.</li> <li>Translate our documentation into another language.</li> <li>Write a blog post, tweet, or share our project with others.</li> </ul> <p>For more information on how to contribute to skforecast, see our Contribution Guide.</p> <p>Visit our authors section to meet all the contributors to skforecast.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use skforecast for a scientific publication, we would appreciate citations to the published software.</p> <p>Zenodo</p> <pre><code>Amat Rodrigo, Joaquin, &amp; Escobar Ortiz, Javier. (2023). skforecast (v0.10.1). Zenodo. https://doi.org/10.5281/zenodo.8382788\n</code></pre> <p>APA: <pre><code>Amat Rodrigo, J., &amp; Escobar Ortiz, J. (2023). skforecast (Version 0.10.1) [Computer software]. https://doi.org/10.5281/zenodo.8382788\n</code></pre></p> <p>BibTeX: <pre><code>@software{skforecast,\nauthor = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},\ntitle = {skforecast},\nversion = {0.10.1},\nmonth = {9},\nyear = {2023},\nlicense = {BSD-3-Clause},\nurl = {https://skforecast.org/},\ndoi = {10.5281/zenodo.8382788}\n}\n</code></pre></p>"},{"location":"index.html#donating","title":"Donating","text":"<p>If you found skforecast useful, you can support us with a donation. Your contribution will help to continue developing and improving this project. Many thanks!  </p> <p> </p> <p></p>"},{"location":"index.html#license","title":"License","text":"<p>BSD-3-Clause License</p>"},{"location":"api/ForecasterAutoreg.html","title":"<code>ForecasterAutoreg</code>","text":""},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg","title":"<code>ForecasterAutoreg(regressor, lags, transformer_y=None, transformer_exog=None, weight_func=None, differentiation=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.10.0</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>differentiation</code> <code>int, default `None`</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.10.0</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>. If <code>differentiation</code> is not <code>None</code>, the size of the window is <code>max_lag</code> + <code>differentiation</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window</code> will have 8 values.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Union[int, np.ndarray, list],\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    differentiation: Optional[int]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.differentiation         = differentiation\n    self.differentiator          = None\n    self.source_code_weight_func = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.X_train_col_names       = None\n    self.in_sample_residuals     = None\n    self.out_sample_residuals    = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    if self.differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                (f\"Argument `differentiation` must be an integer equal to or \"\n                 f\"greater than 1. Got {differentiation}.\")\n            )\n        self.window_size += self.differentiation\n        self.differentiator = TimeSeriesDifferentiator(order=self.differentiation)\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series ({len(y)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag: -lag]\n\n    y_data = y[self.max_lag:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors). Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag, )</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if self.differentiation is not None:\n        y_values = self.differentiator.fit_transform(y_values)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f'`exog` must have same number of samples as `y`. '\n                 f'length `exog`: ({len(exog)}), length `y`: ({len(y)})')\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_train, y_train = self._create_lags(y=y_values)\n    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag: ]\n              )\n\n    if exog is not None:\n        # The first `self.max_lag` positions have to be removed from exog\n        # since they are not in X_train.\n        exog_to_train = exog.iloc[self.max_lag:, ]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = y_index[self.max_lag: ],\n                  name  = 'y'\n              )\n\n    if self.differentiation is not None:\n        y_train = y_train.iloc[self.differentiation: ]\n        X_train = X_train.iloc[self.differentiation: ]\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight,\n                           **self.fit_kwargs)\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = (y_train - self.regressor.predict(X_train)).to_numpy()\n\n        if len(residuals) &gt; 1000:\n            # Only up to 1000 residuals are stored\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(\n                            a       = residuals, \n                            size    = 1000, \n                            replace = False\n                        )\n\n        self.in_sample_residuals = residuals\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated. It\n    # also includes the values need to calculate the diferenctiation.\n    self.last_window = y.iloc[-self.window_size:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._recursive_predict","title":"<code>_recursive_predict(steps, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = last_window[-self.lags].reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    predictions = self._recursive_predict(\n                      steps       = steps,\n                      last_window = copy(last_window_values),\n                      exog        = copy(exog_values)\n                  )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = expand_index(\n                                  index = last_window_index,\n                                  steps = steps\n                              ),\n                      name = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if not in_sample_residuals and self.out_sample_residuals is None:\n        raise ValueError(\n            (\"`forecaster.out_sample_residuals` is `None`. Use \"\n             \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n             \"before `predict_interval()`, `predict_bootstrapping()` or \"\n             \"`predict_dist()`.\")\n        )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           dtype      = float\n                       )\n    rng = np.random.default_rng(seed=random_state)\n    seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n    if in_sample_residuals:\n        residuals = self.in_sample_residuals\n    else:\n        residuals = self.out_sample_residuals\n\n    for i in range(n_boot):\n        # In each bootstraping iteration the initial last_window and exog \n        # need to be restored.\n        last_window_boot = last_window_values.copy()\n        exog_boot = exog_values.copy() if exog is not None else None\n\n        rng = np.random.default_rng(seed=seeds[i])\n        sample_residuals = rng.choice(\n                               a       = residuals,\n                               size    = steps,\n                               replace = True\n                           )\n\n        for step in range(steps):\n\n            prediction = self._recursive_predict(\n                             steps       = 1,\n                             last_window = last_window_boot,\n                             exog        = exog_boot \n                         )\n\n            prediction_with_residual  = prediction + sample_residuals[step]\n            boot_predictions[step, i] = prediction_with_residual[0]\n\n            last_window_boot = np.append(\n                                   last_window_boot[1:],\n                                   prediction_with_residual\n                               )\n\n            if exog is not None:\n                exog_boot = exog_boot[1:]\n\n        if self.differentiation is not None:\n            boot_predictions[:, i] = (\n                self.differentiator.inverse_transform_next_window(boot_predictions[:, i])\n            )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = expand_index(last_window_index, steps=steps),\n                           columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                       )\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which each prediction is used as a predictor for the next step, and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which each prediction is used as a predictor\n    for the next step, and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_dist","title":"<code>predict_dist(steps, distribution, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).  \n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"\n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = max(self.lags)\n    if self.differentiation is not None:\n        self.window_size += self.differentiation        \n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>numpy ndarray</code> <p>Values of residuals. If len(residuals) &gt; 1000, only a random sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: np.ndarray, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : numpy ndarray\n        Values of residuals. If len(residuals) &gt; 1000, only a random sample\n        of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, np.ndarray):\n        raise TypeError(\n            f\"`residuals` argument must be `numpy ndarray`. Got {type(residuals)}.\"\n        )\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new residuals \"\n             f\"are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure that the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n\n        residuals = transform_series(\n                        series            = pd.Series(residuals, name='residuals'),\n                        transformer       = self.transformer_y,\n                        fit               = False,\n                        inverse_transform = False\n                    ).to_numpy()\n\n    if len(residuals) &gt; 1000:\n        rng = np.random.default_rng(seed=random_state)\n        residuals = rng.choice(a=residuals, size=1000, replace=False)\n\n    if append and self.out_sample_residuals is not None:\n        free_space = max(0, 1000 - len(self.out_sample_residuals))\n        if len(residuals) &lt; free_space:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals\n                        ))\n        else:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals[:free_space]\n                        ))\n\n    self.out_sample_residuals = residuals\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoreg\\ForecasterAutoreg.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html","title":"<code>ForecasterAutoregCustom</code>","text":""},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom","title":"<code>ForecasterAutoregCustom(regressor, fun_predictors, window_size, name_predictors=None, transformer_y=None, transformer_exog=None, weight_func=None, differentiation=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns  another numpy ndarray with the predictors.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> required <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the position the predictor has in the returned array of <code>fun_predictors</code>. <code>`None`</code> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Diferentiarion is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.10.0</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor compatible with the scikit-learn API</code> <p>An instance of a regressor compatible with the scikit-learn API.</p> <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. New in version 0.7.0</p> <code>source_code_fun_predictors</code> <code>str</code> <p>Source code of the custom function used to create the predictors. New in version 0.7.0</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors. If <code>differentiation</code> is not <code>None</code>, <code>window_size</code> is increased by the order of differentiation.</p> <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the position the predictor has in the returned array of <code>fun_predictors</code>. <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>differentiation</code> <code>int, default `None`</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Differentiation is reversed in the output of <code>predict()</code> and <code>predict_interval()</code>. WARNING: This argument is newly introduced and requires special attention. It is still experimental and may undergo changes. New in version 0.10.0</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>last_window</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window</code> are expanded as many values as the order of differentiation. For example, if <code>fun_predictors()</code> requires 7 lagged values and <code>differentiation=1</code>, <code>last_window</code> will contain 8 values.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def __init__(\n    self, \n    regressor: object, \n    fun_predictors: Callable, \n    window_size: int,\n    name_predictors: Optional[list]=None,\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    differentiation: Optional[int]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor                  = regressor\n    self.fun_predictors             = fun_predictors\n    self.source_code_fun_predictors = None\n    self.window_size                = window_size\n    self.name_predictors            = name_predictors\n    self.transformer_y              = transformer_y\n    self.transformer_exog           = transformer_exog\n    self.weight_func                = weight_func\n    self.differentiation            = differentiation\n    self.differentiator             = None\n    self.source_code_weight_func    = None\n    self.last_window                = None\n    self.index_type                 = None\n    self.index_freq                 = None\n    self.training_range             = None\n    self.included_exog              = False\n    self.exog_type                  = None\n    self.exog_dtypes                = None\n    self.exog_col_names             = None\n    self.X_train_col_names          = None\n    self.in_sample_residuals        = None\n    self.out_sample_residuals       = None\n    self.fitted                     = False\n    self.creation_date              = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                   = None\n    self.skforcast_version          = skforecast.__version__\n    self.python_version             = sys.version.split(\" \")[0]\n    self.forecaster_id              = forecaster_id\n\n    if not isinstance(window_size, int) or window_size &lt; 1:\n        raise ValueError(\n            (f\"Argument `window_size` must be an integer equal to or \"\n             f\"greater than 1. Got {window_size}.\")\n        )\n\n    if self.differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                (f\"Argument `differentiation` must be an integer equal to or \"\n                 f\"greater than 1. Got {differentiation}.\")\n            )\n        self.window_size += self.differentiation\n        self.differentiator = TimeSeriesDifferentiator(order=self.differentiation)\n\n    if not isinstance(fun_predictors, Callable):\n        raise TypeError(\n            f\"Argument `fun_predictors` must be a Callable. Got {type(fun_predictors)}.\"\n        )\n\n    self.source_code_fun_predictors = inspect.getsource(fun_predictors)\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Pandas DataFrame with the training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n\n    \"\"\"\n\n    if len(y) &lt; self.window_size + 1:\n        raise ValueError(\n            (f\"`y` must have as many values as the windows_size needed by \"\n             f\"{self.fun_predictors.__name__}. For this Forecaster the \"\n             f\"minimum length is {self.window_size + 1}\")\n        )\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if self.differentiation is not None:\n        y_values = self.differentiator.fit_transform(y_values)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                f'`exog` must have same number of samples as `y`. '\n                f'length `exog`: ({len(exog)}), length `y`: ({len(y)})'\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                        series            = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                        df                = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_train  = []\n    y_train  = []\n\n    for i in range(len(y) - self.window_size):\n\n        train_index = np.arange(i, self.window_size + i)\n        test_index  = self.window_size + i\n\n        X_train.append(self.fun_predictors(y=y_values[train_index]))\n        y_train.append(y_values[test_index])\n\n    X_train = np.vstack(X_train)\n    y_train = np.array(y_train)\n\n    if self.name_predictors is None:\n        X_train_col_names = [f\"custom_predictor_{i}\" for i in range(X_train.shape[1])]\n    else:\n        if len(self.name_predictors) != X_train.shape[1]:\n            raise ValueError(\n                (\"The length of provided predictors names (`name_predictors`) do not \"\n                 \"match the number of columns created by `fun_predictors()`.\")\n            )\n        X_train_col_names = self.name_predictors.copy()\n\n    if np.isnan(X_train).any():\n        raise ValueError(\n            \"`fun_predictors()` is returning `NaN` values.\"\n        )\n\n    expected = self.fun_predictors(y_values[:-1])\n    observed = X_train[-1, :]\n\n    if expected.shape != observed.shape or not (expected == observed).all():\n        raise ValueError(\n            (f\"The `window_size` argument ({self.window_size}), declared when \"\n             f\"initializing the forecaster, does not correspond to the window \"\n             f\"used by `fun_predictors()`.\")\n        )\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.window_size: ]\n              )\n\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train.\n        exog_to_train = exog.iloc[self.window_size:, ]\n        check_exog_dtypes(exog_to_train)\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = y_index[self.window_size: ],\n                  name  = 'y'\n              )\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight,\n                           **self.fit_kwargs)\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = (y_train - self.regressor.predict(X_train)).to_numpy()\n\n        if len(residuals) &gt; 1000:\n            # Only up to 1000 residuals are stored\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(\n                            a       = residuals, \n                            size    = 1000, \n                            replace = False\n                        )\n\n        self.in_sample_residuals = residuals\n\n    # The last time window of training data is stored so that predictors in\n    # the first iteration of `predict()` can be calculated. It also includes\n    # the values need to calculate the diferenctiation.\n    self.last_window = y.iloc[-self.window_size:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom._recursive_predict","title":"<code>_recursive_predict(steps, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = self.fun_predictors(y=last_window).reshape(1, -1)\n        if np.isnan(X).any():\n            raise ValueError(\n                \"`fun_predictors()` is returning `NaN` values.\"\n            )\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    predictions = self._recursive_predict(\n                      steps       = steps,\n                      last_window = copy(last_window_values),\n                      exog        = copy(exog_values)\n                  )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = expand_index(\n                                  index = last_window_index,\n                                  steps = steps\n                              ),\n                      name = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if not in_sample_residuals and self.out_sample_residuals is None:\n        raise ValueError(\n            ('`forecaster.out_sample_residuals` is `None`. Use '\n             '`in_sample_residuals=True` or method `set_out_sample_residuals()` '\n             'before `predict_interval()`, `predict_bootstrapping()` or '\n             '`predict_dist()`.')\n        )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           dtype      = float\n                       )\n    rng = np.random.default_rng(seed=random_state)\n    seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n    if in_sample_residuals:\n        residuals = self.in_sample_residuals\n    else:\n        residuals = self.out_sample_residuals\n\n    for i in range(n_boot):\n        # In each bootstraping iteration the initial last_window and exog \n        # need to be restored.\n        last_window_boot = last_window_values.copy()\n        exog_boot = exog_values.copy() if exog is not None else None\n\n        rng = np.random.default_rng(seed=seeds[i])\n        sample_residuals = rng.choice(\n                               a       = residuals,\n                               size    = steps,\n                               replace = True\n                           )\n\n        for step in range(steps):\n\n            prediction = self._recursive_predict(\n                             steps       = 1,\n                             last_window = last_window_boot,\n                             exog        = exog_boot \n                         )\n\n            prediction_with_residual  = prediction + sample_residuals[step]\n            boot_predictions[step, i] = prediction_with_residual[0]\n\n            last_window_boot = np.append(\n                                   last_window_boot[1:],\n                                   prediction_with_residual\n                               )\n\n            if exog is not None:\n                exog_boot = exog_boot[1:]\n\n        if self.differentiation is not None:\n            boot_predictions[:, i] = (\n                self.differentiator.inverse_transform_next_window(boot_predictions[:, i])\n            )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = expand_index(last_window_index, steps=steps),\n                           columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                       )\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which each prediction is used as a predictor for the next step, and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which each prediction is used as a predictor\n    for the next step, and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_dist","title":"<code>predict_dist(steps, distribution, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>numpy ndarray</code> <p>Values of residuals. If len(residuals) &gt; 1000, only a random sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: np.ndarray, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : numpy ndarray\n        Values of residuals. If len(residuals) &gt; 1000, only a random sample\n        of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, np.ndarray):\n        raise TypeError(\n            f\"`residuals` argument must be `numpy ndarray`. Got {type(residuals)}.\"\n        )\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new residuals \"\n             f\"are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure that the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n\n        residuals = transform_series(\n                        series            = pd.Series(residuals, name='residuals'),\n                        transformer       = self.transformer_y,\n                        fit               = False,\n                        inverse_transform = False\n                    ).to_numpy()\n\n    if len(residuals) &gt; 1000:\n        rng = np.random.default_rng(seed=random_state)\n        residuals = rng.choice(a=residuals, size=1000, replace=False)\n\n    if append and self.out_sample_residuals is not None:\n        free_space = max(0, 1000 - len(self.out_sample_residuals))\n        if len(residuals) &lt; free_space:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals\n                        ))\n        else:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals[:free_space]\n                        ))\n\n    self.out_sample_residuals = residuals\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoregCustom\\ForecasterAutoregCustom.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html","title":"<code>ForecasterAutoregDirect</code>","text":""},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect","title":"<code>ForecasterAutoregDirect(regressor, steps, lags, transformer_y=None, transformer_exog=None, weight_func=None, fit_kwargs=None, n_jobs='auto', forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive direct multi-step forecaster. A separate model is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>int, 'auto', default `'auto'`</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect--notes","title":"Notes","text":"<p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def __init__(\n    self, \n    regressor: object,\n    steps: int,\n    lags: Union[int, np.ndarray, list],\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    n_jobs: Optional[Union[int, str]]='auto',\n    forecaster_id: Optional[Union[str, int]]=None,\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.steps                   = steps\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.source_code_weight_func = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.X_train_col_names       = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(\n            f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals = None\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor_name  = type(self.regressor).__name__,\n                      )\n    else:\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the values of the time series related to each  row of <code>X_data</code> for each step.  Shape: (len(self.steps), samples - max(self.lags))</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        2d numpy ndarray with the values of the time series related to each \n        row of `X_data` for each step. \n        Shape: (len(self.steps), samples - max(self.lags))\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - (self.steps - 1) # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series minus the number of steps ({len(y)-(self.steps-1)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n\n    y_data = np.full(shape=(self.steps, n_splits), fill_value=np.nan, dtype=float)\n    for step in range(self.steps):\n        y_data[step, ] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method. Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>y_train</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step of the form {step: y_step_[i]}. Shape of each series: (len(y) - self.max_lag, )</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, dict]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    y_train : dict\n        Values (target) of the time series related to each row of `X_train` \n        for each step of the form {step: y_step_[i]}.\n        Shape of each series: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    if len(y) &lt; self.max_lag + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `y` for training this forecaster is \"\n             f\"{self.max_lag + self.steps}. Got {len(y)}. Reduce the \"\n             f\"number of predicted steps, {self.steps}, or the maximum \"\n             f\"lag, {self.max_lag}, if no more data is available.\")\n        )\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.included_exog = True\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")      \n            )\n\n    X_train, y_train = self._create_lags(y=y_values)\n    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag + (self.steps -1): ]\n              )\n\n    if exog is not None:\n        # Transform exog to match direct format\n        # The first `self.max_lag` positions have to be removed from X_exog\n        # since they are not in X_lags.\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        ).iloc[-X_train.shape[0]:, :]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = {step: pd.Series(\n                         data  = y_train[step-1], \n                         index = y_index[self.max_lag + step-1:][:len(y_train[0])],\n                         name  = f\"y_step_{step}\"\n                     )\n               for step in range(1, self.steps + 1)}\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict,\n    remove_suffix: bool=False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        Step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `create_train_X_y` method, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.included_exog:\n        X_train_step = X_train\n    else:\n        idx_columns_lags = np.arange(len(self.lags))\n        n_exog = (len(self.X_train_col_names) - len(self.lags)) / self.steps\n        idx_columns_exog = (\n            np.arange((step-1)*n_exog, (step)*n_exog) + idx_columns_lags[-1] + 1\n        )\n        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n                                for col_name in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return  X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>create_train_X_y</code> and filter_train_X_y_for_step` methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `create_train_X_y` and filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = {step: None for step in range(1, self.steps + 1)}\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n\"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n        store_in_sample_residuals : bool\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting.\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor and in-sample residuals.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            residuals = (\n                (y_train_step - regressor.predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n        else:\n            residuals = None\n\n        return step, regressor, residuals\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor=copy(self.regressor),\n            X_train=X_train,\n            y_train=y_train,\n            step=step,\n            store_in_sample_residuals=store_in_sample_residuals\n        )\n        for step in range(1, self.steps + 1))\n    )\n\n    self.regressors_ = {step: regressor \n                        for step, regressor, _ in results_fit}\n\n    if store_in_sample_residuals:\n        self.in_sample_residuals = {step: residuals \n                                    for step, _, residuals in results_fit}\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    self.last_window = y.iloc[-self.max_lag:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict","title":"<code>predict(steps=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = self.steps,\n        levels           = None,\n        series_col_names = None\n    ) \n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    X_lags = last_window_values[-self.lags].reshape(1, -1)\n\n    if exog is None:\n        Xs = [X_lags] * len(steps)\n    else:\n        n_exog = exog.shape[1] if isinstance(exog, pd.DataFrame) else 1\n        Xs = [\n            np.hstack([X_lags, exog_values[(step-1)*n_exog:(step)*n_exog].reshape(1, -1)])\n            for step in steps\n        ]\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.simplefilter(\"ignore\")\n        predictions = [\n            regressor.predict(X)[0] for regressor, X in zip(regressors, Xs)\n        ]\n\n    idx = expand_index(index=last_window_index, steps=max(steps))\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = idx[np.array(steps)-1],\n                      name  = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    if in_sample_residuals:\n        if not set(steps).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for steps: \"\n                 f\"{set(steps) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n                 \"before `predict_interval()`, `predict_bootstrapping()` or \"\n                 \"`predict_dist()`.\")\n            )\n        else:\n            if not set(steps).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for steps: \"\n                     f\"{set(steps) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for step in steps:\n        if residuals[step] is None:\n            raise ValueError(\n                (f\"forecaster residuals for step {step} are `None`. \"\n                 f\"Check {check_residuals}.\")\n            )\n        elif (residuals[step] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for step {step} contains `None` values. \"\n                 f\"Check {check_residuals}.\")\n            )\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog \n                  )\n\n    # Predictions must be in the transformed scale before adding residuals\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    boot_predictions = pd.concat([predictions] * n_boot, axis=1)\n    boot_predictions.columns= [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    for i, step in enumerate(steps):\n        rng = np.random.default_rng(seed=random_state)\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions.iloc[i, :] = boot_predictions.iloc[i, :] + sample_residuals\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Bootstrapping based prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Bootstrapping based prediction intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = max(self.lags)\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {step: None \n                                     for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            f\"\"\"\n            Only residuals of models (steps) \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new \"\n             f\"residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_series(\n                                 series            = pd.Series(value, name='residuals'),\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             ).to_numpy()\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[key]))\n            if len(value) &lt; free_space:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value\n                        ))\n            else:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals[key] = value\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.get_feature_importances","title":"<code>get_feature_importances(step)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py</code> <pre><code>def get_feature_importances(\n    self, \n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f'`step` must be an integer. Got {type(step)}.'\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    idx_columns_lags = np.arange(len(self.lags))\n    if self.included_exog:\n        idx_columns_exog = np.flatnonzero(\n                            [name.endswith(f\"step_{step}\") for name in self.X_train_col_names]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_col_names[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html","title":"<code>ForecasterAutoregMultiSeries</code>","text":""},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries","title":"<code>ForecasterAutoregMultiSeries(regressor, lags, transformer_series=None, transformer_exog=None, weight_func=None, series_weights=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>`None`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>`None`</code> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict, default `None`</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series.It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>index_values</code> <code>pandas Index</code> <p>Values of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series (levels) used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries--notes","title":"Notes","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <ul> <li><code>series_weights</code> : controls the relative importance of each series. If a  series has twice as much weight as the others, the observations of that series  influence the training twice as much. The higher the weight of a series  relative to the others, the more the model will focus on trying to learn  that series.</li> <li><code>weight_func</code> : controls the relative importance of each observation  according to its index value. For example, a function that assigns a lower  weight to certain dates.</li> </ul> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Union[int, np.ndarray, list],\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Union[Callable, dict]]=None,\n    series_weights: Optional[dict]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.transformer_series      = transformer_series\n    self.transformer_series_     = None\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.weight_func_            = None\n    self.source_code_weight_func = None\n    self.series_weights          = series_weights\n    self.series_weights_         = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.index_values            = None\n    self.training_range          = None\n    self.last_window             = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.series_col_names        = None\n    self.X_train_col_names       = None\n    self.in_sample_residuals     = None\n    self.out_sample_residuals    = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = series_weights\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_lags","title":"<code>_create_lags(y, series_name)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <code>series_name</code> <code>str</code> <p>Name of the series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray, \n    series_name: str\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n    series_name : str\n        Name of the series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series '{series_name}', ({len(y)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag: -lag]\n\n    y_data = y[self.max_lag:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(series) - self.max_lag, )</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>series</code>.</p> <code>y_train_index</code> <code>pandas Index</code> <p>Index of <code>y_train</code>.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(series) - self.max_lag, )\n    y_index : pandas Index\n        Index of `series`.\n    y_train_index: pandas Index\n        Index of `y_train`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    series_col_names = list(series.columns)\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items() \n            if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                 f\" No transformation is applied to these series.\"),\n                 IgnoredArgumentWarning\n            )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_levels = []\n    len_series = []\n    X_train_col_names = [f\"lag_{lag}\" for lag in self.lags]\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        y_values = y.to_numpy()\n\n        if np.isnan(y_values).all():\n            raise ValueError(f\"All values of series '{serie}' are NaN.\")\n\n        first_no_nan_idx = np.argmax(~np.isnan(y_values))\n        y_values = y_values[first_no_nan_idx:]\n\n        if np.isnan(y_values).any():\n            raise ValueError(\n                (f\"'{serie}' Time series has missing values in between or \"\n                 f\"at the end of the time series. When working with series \"\n                 f\"of different lengths, all series must be complete after \"\n                 f\"the first non-null value.\")\n            )\n\n        y = transform_series(\n                series            = y.iloc[first_no_nan_idx:],\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values = y.to_numpy()\n        X_train_values, y_train_values = self._create_lags(y=y_values, series_name=serie)\n\n        if i == 0:\n            X_train = X_train_values\n            y_train = y_train_values\n        else:\n            X_train = np.concatenate((X_train, X_train_values), axis=0)\n            y_train = np.concatenate((y_train, y_train_values), axis=0)\n\n        X_level = [serie]*len(X_train_values)\n        X_levels.extend(X_level)\n        len_series.append(len(y_train_values))\n\n    X_levels = pd.Series(X_levels)\n    X_levels = pd.get_dummies(X_levels, dtype=float)\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names\n              )\n\n    if exog is not None:\n        # The first `self.max_lag` positions have to be removed from exog\n        # since they are not in X_train. Then Exog is cloned as many times \n        # as there are series, taking into account the length of the series.\n        exog_to_train = [exog.iloc[-length:, ] for length in len_series]\n        exog_to_train = pd.concat(exog_to_train).reset_index(drop=True)\n    else:\n        exog_to_train = None\n\n    X_train = pd.concat([X_train, exog_to_train, X_levels], axis=1)\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = pd.Series(\n                  data = y_train,\n                  name = 'y'\n              )\n\n    _, y_index = preprocess_y(y=series, return_values=False)\n\n    y_index_numpy = y_index.to_numpy()\n    y_train_index = pd.Index(\n                        np.concatenate(\n                            [y_index_numpy[-length:, ] for length in len_series]\n                        )\n                    )\n\n    return X_train, y_train, y_index, y_train_index\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_sample_weights","title":"<code>create_sample_weights(series, X_train, y_train_index)</code>","text":"<p>Crate weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Time series used to create <code>X_train</code> with the method <code>create_train_X_y</code>.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train_index</code> <code>pandas Index</code> <p>Index created with the <code>create_train_X_y</code> method, fourth return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def create_sample_weights(\n    self,\n    series: pd.DataFrame,\n    X_train: pd.DataFrame,\n    y_train_index: pd.Index,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Time series used to create `X_train` with the method `create_train_X_y`.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train_index : pandas Index\n        Index created with the `create_train_X_y` method, fourth return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    weights_series = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = set(series.columns) - set(self.series_weights.keys())\n        if series_not_in_series_weights:\n            warnings.warn(\n                (f\"{series_not_in_series_weights} not present in `series_weights`. \"\n                 f\"A weight of 1 is given to all their samples.\"),\n                 IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series.columns}\n        self.series_weights_.update((k, v) for k, v in self.series_weights.items() \n                                    if k in self.series_weights_)\n        weights_series = [np.repeat(self.series_weights_[serie], sum(X_train[serie])) \n                          for serie in series.columns]\n        weights_series = np.concatenate(weights_series)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {col: copy(self.weight_func) \n                                 for col in series.columns}\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = set(series.columns) - set(self.weight_func.keys())\n            if series_not_in_weight_func:\n                warnings.warn(\n                    (f\"{series_not_in_weight_func} not present in `weight_func`. \"\n                     f\"A weight of 1 is given to all their samples.\"),\n                     IgnoredArgumentWarning\n                )\n            self.weight_func_ = {col: lambda x: np.ones_like(x, dtype=float) \n                                 for col in series.columns}\n            self.weight_func_.update((k, v) for k, v in self.weight_func.items() \n                                     if k in self.weight_func_)\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            idx = y_train_index[X_train[X_train[key] == 1.0].index]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if weights_series is not None:\n        weights = weights_series\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                (\"The resulting `weights` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.index_values        = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)        \n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train, y_index, y_train_index = self.create_train_X_y(series=series, exog=exog)\n    sample_weight = self.create_sample_weights(\n                        series        = series,\n                        X_train       = X_train,\n                        y_train_index = y_train_index,\n                    )\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y_index[[0, -1]]\n    self.index_type = type(y_index)\n    if isinstance(y_index, pd.DatetimeIndex):\n        self.index_freq = y_index.freqstr\n    else: \n        self.index_freq = y_index.step\n    self.index_values = y_index\n\n    in_sample_residuals = {}\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = y_train - self.regressor.predict(X_train)\n\n        for serie in series.columns:\n            in_sample_residuals[serie] = residuals.loc[X_train[serie] == 1.].to_numpy()\n            if len(in_sample_residuals[serie]) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                in_sample_residuals[serie] = rng.choice(\n                                                 a       = in_sample_residuals[serie], \n                                                 size    = 1000, \n                                                 replace = False\n                                             )\n    else:\n        for serie in series.columns:\n            in_sample_residuals[serie] = None\n\n    self.in_sample_residuals = in_sample_residuals\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated.\n    self.last_window = series.iloc[-self.max_lag:, ].copy()\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._recursive_predict","title":"<code>_recursive_predict(steps, level, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>level</code> <code>str</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    level: str,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    level : str\n        Time series to be predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = last_window[-self.lags].reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        levels_dummies = np.zeros(shape=(1, len(self.series_col_names)), dtype=float)\n        levels_dummies[0][self.series_col_names.index(level)] = 1.\n\n        X = np.column_stack((X, levels_dummies.reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict","title":"<code>predict(steps, levels=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values, one column for each level.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values, one column for each level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    predictions = []\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        preds_level = self._recursive_predict(\n                          steps       = steps,\n                          level       = level,\n                          last_window = copy(last_window_values),\n                          exog        = copy(exog_values)\n                      )\n\n        preds_level = pd.Series(\n                          data  = preds_level,\n                          index = expand_index(\n                                      index = last_window_index,\n                                      steps = steps\n                                  ),\n                          name = level\n                      )\n\n        preds_level = transform_series(\n                          series            = preds_level,\n                          transformer       = self.transformer_series_[level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions.append(preds_level)    \n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>dict</code> <p>Predictions generated by bootstrapping for each level. {level: pandas DataFrame, shape (steps, n_boot)}</p>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; dict:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.      \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : dict\n        Predictions generated by bootstrapping for each level.\n        {level: pandas DataFrame, shape (steps, n_boot)}\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if in_sample_residuals:\n        if not set(levels).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for levels: \"\n                 f\"{set(levels) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals_levels = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method \"\n                 \"`set_out_sample_residuals()` before `predict_interval()`, \"\n                 \"`predict_bootstrapping()` or `predict_dist()`.\")\n            )\n        else:\n            if not set(levels).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for levels: \"\n                     f\"{set(levels) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals_levels = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for level in levels:\n        if residuals_levels[level] is None:\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' are `None`. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (residuals_levels[level] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"values. Check `{check_residuals}`.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    boot_predictions = {}\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        level_boot_predictions = np.full(\n                                     shape      = (steps, n_boot),\n                                     fill_value = np.nan,\n                                     dtype      = float\n                                 )\n        rng = np.random.default_rng(seed=random_state)\n        seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n        residuals = residuals_levels[level]\n\n        for i in range(n_boot):\n            # In each bootstraping iteration the initial last_window and exog \n            # need to be restored.\n            last_window_boot = last_window_values.copy()\n            exog_boot = exog_values.copy() if exog is not None else None\n\n            rng = np.random.default_rng(seed=seeds[i])\n            sample_residuals = rng.choice(\n                                   a       = residuals,\n                                   size    = steps,\n                                   replace = True\n                               )\n\n            for step in range(steps):\n\n                prediction = self._recursive_predict(\n                                 steps       = 1,\n                                 level       = level,\n                                 last_window = last_window_boot,\n                                 exog        = exog_boot \n                             )\n\n                prediction_with_residual = prediction + sample_residuals[step]\n                level_boot_predictions[step, i] = prediction_with_residual[0]\n\n                last_window_boot = np.append(\n                                       last_window_boot[1:],\n                                       prediction_with_residual\n                                   )\n                if exog is not None:\n                    exog_boot = exog_boot[1:]\n\n        level_boot_predictions = pd.DataFrame(\n                                     data    = level_boot_predictions,\n                                     index   = expand_index(last_window_index, steps=steps),\n                                     columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                                 )\n\n        if self.transformer_series_[level]:\n            for col in level_boot_predictions.columns:\n                level_boot_predictions[col] = transform_series(\n                                                  series            = level_boot_predictions[col],\n                                                  transformer       = self.transformer_series_[level],\n                                                  fit               = False,\n                                                  inverse_transform = True\n                                              )\n\n        boot_predictions[level] = level_boot_predictions\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_interval","title":"<code>predict_interval(steps, levels=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction  intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>level: predictions.</li> <li>level_lower_bound: lower bound of the interval.</li> <li>level_upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which, each prediction, is used as a predictor\n    for the next step and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.  \n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction \n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - level: predictions.\n            - level_lower_bound: lower bound of the interval.\n            - level_upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    check_interval(interval=interval)\n\n    preds = self.predict(\n                steps       = steps,\n                levels      = levels,\n                last_window = last_window,\n                exog        = exog\n            )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           levels              = levels,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions = []\n\n    for level in levels:\n        preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n        preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n        predictions.append(preds[level])\n        predictions.append(preds_interval)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_dist","title":"<code>predict_dist(steps, distribution, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step and level.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step and level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       levels              = levels,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p=='x'] + [\"loc\",\"scale\"]\n    predictions = []\n\n    for level in levels:\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples[level])\n        level_param_names = [f'{level}_{p}' for p in param_names]\n\n        pred_level = pd.DataFrame(\n                         data    = param_values,\n                         columns = level_param_names,\n                         index   = boot_samples[level].index\n                     )\n\n        predictions.append(pred_level)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)            \n    self.max_lag  = max(self.lags)\n    self.window_size = max(self.lags)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each level in the form {level: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored. Keys must be the same as <code>levels</code>.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_series.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict,\n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each level in the\n        form {level: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored. Keys must be the same as `levels`.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_series.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{level: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {level: None for level in self.series_col_names}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of levels \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    for level, value in residuals.items():\n\n        residuals_level = value\n\n        if not transform and self.transformer_series_[level] is not None:\n            warnings.warn(\n                (\"Argument `transform` is set to `False` but forecaster was \"\n                f\"trained using a transformer {self.transformer_series_[level]} \"\n                f\"for level {level}. Ensure that the new residuals are \"\n                 \"already transformed or set `transform=True`.\")\n            )\n\n        if transform and self.transformer_series_ and self.transformer_series_[level]:\n            warnings.warn(\n                (\"Residuals will be transformed using the same transformer used \"\n                f\"when training the forecaster for level {level} : \"\n                f\"({self.transformer_series_[level]}). Ensure that the new \"\n                 \"residuals are on the same scale as the original time series.\")\n            )\n            residuals_level = transform_series(\n                                  series            = pd.Series(residuals_level, name='residuals'),\n                                  transformer       = self.transformer_series_[level],\n                                  fit               = False,\n                                  inverse_transform = False\n                              ).to_numpy()\n\n        if len(residuals_level) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[level] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[level]))\n            if len(residuals_level) &lt; free_space:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level\n                                    ))\n            else:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level[:free_space]\n                                    ))\n\n        self.out_sample_residuals[level] = residuals_level\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html","title":"<code>ForecasterAutoregMultiSeriesCustom</code>","text":""},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom","title":"<code>ForecasterAutoregMultiSeriesCustom(regressor, fun_predictors, window_size, name_predictors=None, transformer_series=None, transformer_exog=None, weight_func=None, series_weights=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series with a custom function to create predictors. New in version 0.7.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. The same function is applied  to all series.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> required <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the  position the predictor has in the returned array of <code>fun_predictors</code>. <code>`None`</code> <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>`None`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>`None`</code> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. The same function is applied  to all series.</p> <code>source_code_fun_predictors</code> <code>str</code> <p>Source code of the custom function used to create the predictors.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the  position the predictor has in the returned array of <code>fun_predictors</code>. <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series.It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>index_values</code> <code>pandas Index</code> <p>Values of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series (levels) used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom--notes","title":"Notes","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <ul> <li><code>series_weights</code> : controls the relative importance of each series. If a  series has twice as much weight as the others, the observations of that series  influence the training twice as much. The higher the weight of a series  relative to the others, the more the model will focus on trying to learn  that series.</li> <li><code>weight_func</code> : controls the relative importance of each observation  according to its index value. For example, a function that assigns a lower  weight to certain dates.</li> </ul> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    fun_predictors: Callable, \n    window_size: int,\n    name_predictors: Optional[list]=None,\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Union[Callable, dict]]=None,\n    series_weights: Optional[dict]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor                  = regressor\n    self.fun_predictors             = fun_predictors\n    self.source_code_fun_predictors = None\n    self.window_size                = window_size\n    self.name_predictors            = name_predictors\n    self.transformer_series         = transformer_series\n    self.transformer_series_        = None\n    self.transformer_exog           = transformer_exog\n    self.weight_func                = weight_func\n    self.weight_func_               = None\n    self.source_code_weight_func    = None\n    self.series_weights             = series_weights\n    self.series_weights_            = None\n    self.index_type                 = None\n    self.index_freq                 = None\n    self.index_values               = None\n    self.training_range             = None\n    self.last_window                = None\n    self.included_exog              = False\n    self.exog_type                  = None\n    self.exog_dtypes                = None\n    self.exog_col_names             = None\n    self.series_col_names           = None\n    self.X_train_col_names          = None\n    self.in_sample_residuals        = None\n    self.out_sample_residuals       = None\n    self.fitted                     = False\n    self.creation_date              = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                   = None\n    self.skforcast_version          = skforecast.__version__\n    self.python_version             = sys.version.split(\" \")[0]\n    self.forecaster_id              = forecaster_id\n\n    if not isinstance(window_size, int):\n        raise TypeError(\n            f\"Argument `window_size` must be an int. Got {type(window_size)}.\"\n        )\n\n    if not isinstance(fun_predictors, Callable):\n        raise TypeError(\n            f\"Argument `fun_predictors` must be a Callable. Got {type(fun_predictors)}.\"\n        )\n\n    self.source_code_fun_predictors = inspect.getsource(fun_predictors)\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = series_weights\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(series) - self.max_lag, )</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>series</code>.</p> <code>y_train_index</code> <code>pandas Index</code> <p>Index of <code>y_train</code>.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(series) - self.max_lag, )\n    y_index : pandas Index\n        Index of `series`.\n    y_train_index: pandas Index\n        Index of `y_train`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    if len(series) &lt; self.window_size + 1:\n        raise ValueError(\n            (f\"`series` must have as many values as the windows_size needed by \"\n             f\"{self.fun_predictors.__name__}. For this Forecaster the \"\n             f\"minimum length is {self.window_size + 1}\")\n        )\n\n    series_col_names = list(series.columns)\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items() \n            if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n        if series_not_in_transformer_series:\n                warnings.warn(\n                    (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                     f\" No transformation is applied to these series.\"),\n                     IgnoredArgumentWarning\n                )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_levels = []\n    len_series = []\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        y_values = y.to_numpy()\n\n        if np.isnan(y_values).all():\n            raise ValueError(f\"All values of series '{serie}' are NaN.\")\n\n        first_no_nan_idx = np.argmax(~np.isnan(y_values))\n        y_values = y_values[first_no_nan_idx:]\n\n        if np.isnan(y_values).any():\n            raise ValueError(\n                (f\"'{serie}' Time series has missing values in between or \"\n                 f\"at the end of the time series. When working with series \"\n                 f\"of different lengths, all series must be complete after \"\n                 f\"the first non-null value.\")\n            )\n\n        y = transform_series(\n                series            = y.iloc[first_no_nan_idx:],\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values = y.to_numpy()\n\n        X_train_values  = []\n        y_train_values  = []\n\n        for j in range(len(y) - self.window_size):\n\n            temp_X_index = np.arange(j, self.window_size + j)\n            temp_y_index  = self.window_size + j\n\n            X_train_values.append(self.fun_predictors(y=y_values[temp_X_index]))\n            y_train_values.append(y_values[temp_y_index])\n\n        X_train_values = np.vstack(X_train_values)\n        y_train_values = np.array(y_train_values)\n\n        if np.isnan(X_train_values).any():\n            raise ValueError(\n                f\"`fun_predictors()` is returning `NaN` values for series '{serie}'.\"\n            )\n\n        if i == 0:\n            X_train = X_train_values\n            y_train = y_train_values\n        else:\n            X_train = np.concatenate((X_train, X_train_values), axis=0)\n            y_train = np.concatenate((y_train, y_train_values), axis=0)\n\n        X_level = [serie]*len(X_train_values)\n        X_levels.extend(X_level)\n        len_series.append(len(y_train_values))\n\n    if self.name_predictors is None:\n        X_train_col_names = [f\"custom_predictor_{i}\" \n                             for i in range(X_train.shape[1])]\n    else:\n        if len(self.name_predictors) != X_train.shape[1]:\n            raise ValueError(\n                (\"The length of provided predictors names (`name_predictors`) do \"\n                 \"not match the number of columns created by `fun_predictors()`.\")\n            )\n        X_train_col_names = self.name_predictors.copy()\n\n    # y_values correspond only to the last series of `series`. Since the columns\n    # of X_train are the same for all series, the check is the same.\n    expected = self.fun_predictors(y_values[:-1])\n    observed = X_train[-1, :]\n\n    if expected.shape != observed.shape or not (expected == observed).all():\n        raise ValueError(\n            (f\"The `window_size` argument ({self.window_size}), declared when \"\n             f\"initializing the forecaster, does not correspond to the window \"\n             f\"used by `fun_predictors()`.\")\n        )\n\n    X_levels = pd.Series(X_levels)\n    X_levels = pd.get_dummies(X_levels, dtype=float)\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names\n              )\n\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train. Then Exog is cloned as many times \n        # as there are series, taking into account the length of the series.\n        exog_to_train = [exog.iloc[-length:, ] for length in len_series]\n        exog_to_train = pd.concat(exog_to_train).reset_index(drop=True)\n    else:\n        exog_to_train = None\n\n    X_train = pd.concat([X_train, exog_to_train, X_levels], axis=1)\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = pd.Series(\n                  data = y_train,\n                  name = 'y'\n              )\n\n    _, y_index = preprocess_y(y=series, return_values=False)\n\n    y_index_numpy = y_index.to_numpy()\n    y_train_index = pd.Index(\n                        np.concatenate(\n                            [y_index_numpy[-length:, ] for length in len_series]\n                        )\n                    )\n\n    return X_train, y_train, y_index, y_train_index\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.create_sample_weights","title":"<code>create_sample_weights(series, X_train, y_train_index)</code>","text":"<p>Crate weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Time series used to create <code>X_train</code> with the method <code>create_train_X_y</code>.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train_index</code> <code>pandas Index</code> <p>Index created with the <code>create_train_X_y</code> method, fourth return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def create_sample_weights(\n    self,\n    series: pd.DataFrame,\n    X_train: pd.DataFrame,\n    y_train_index: pd.Index,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Time series used to create `X_train` with the method `create_train_X_y`.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train_index : pandas Index\n        Index created with the `create_train_X_y` method, fourth return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    weights_series = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = set(series.columns) - set(self.series_weights.keys())\n        if series_not_in_series_weights:\n            warnings.warn(\n                (f\"{series_not_in_series_weights} not present in `series_weights`.\"\n                 f\" A weight of 1 is given to all their samples.\"),\n                IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series.columns}\n        self.series_weights_.update((k, v) for k, v in self.series_weights.items() \n                                    if k in self.series_weights_)\n        weights_series = [np.repeat(self.series_weights_[serie], sum(X_train[serie])) \n                          for serie in series.columns]\n        weights_series = np.concatenate(weights_series)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {col: copy(self.weight_func) \n                                 for col in series.columns}\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = set(series.columns) - set(self.weight_func.keys())\n            if series_not_in_weight_func:\n                warnings.warn(\n                    (f\"{series_not_in_weight_func} not present in `weight_func`.\"\n                     f\" A weight of 1 is given to all their samples.\"),\n                    IgnoredArgumentWarning\n                )\n            self.weight_func_ = {col: lambda x: np.ones_like(x, dtype=float) \n                                 for col in series.columns}\n            self.weight_func_.update((k, v) for k, v in self.weight_func.items() \n                                     if k in self.weight_func_)\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            idx = y_train_index[X_train[X_train[key] == 1.0].index]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if weights_series is not None:\n        weights = weights_series\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                (\"The resulting `weights` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.index_values        = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train, y_index, y_train_index = self.create_train_X_y(series=series, exog=exog)\n    sample_weight = self.create_sample_weights(\n                        series        = series,\n                        X_train       = X_train,\n                        y_train_index = y_train_index,\n                    )\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y_index[[0, -1]]\n    self.index_type = type(y_index)\n    if isinstance(y_index, pd.DatetimeIndex):\n        self.index_freq = y_index.freqstr\n    else: \n        self.index_freq = y_index.step\n    self.index_values = y_index\n\n    in_sample_residuals = {}\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = y_train - self.regressor.predict(X_train)\n\n        for serie in series.columns:\n            in_sample_residuals[serie] = residuals.loc[X_train[serie] == 1.].to_numpy()\n            if len(in_sample_residuals[serie]) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                in_sample_residuals[serie] = rng.choice(\n                                                 a       = in_sample_residuals[serie], \n                                                 size    = 1000, \n                                                 replace = False\n                                             )\n    else:\n        for serie in series.columns:\n            in_sample_residuals[serie] = np.array([None])\n\n    self.in_sample_residuals = in_sample_residuals\n\n    # The last time window of training data is stored so that predictors in\n    # the first iteration of `predict()` can be calculated.\n    self.last_window = series.iloc[-self.window_size:].copy()\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom._recursive_predict","title":"<code>_recursive_predict(steps, level, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>level</code> <code>str</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    level: str,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    level : str\n        Time series to be predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = self.fun_predictors(y=last_window).reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        levels_dummies = np.zeros(shape=(1, len(self.series_col_names)), dtype=float)\n        levels_dummies[0][self.series_col_names.index(level)] = 1.\n\n        X = np.column_stack((X, levels_dummies.reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict","title":"<code>predict(steps, levels=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values, one column for each level.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values, one column for each level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:, ]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    predictions = []\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        preds_level = self._recursive_predict(\n                          steps       = steps,\n                          level       = level,\n                          last_window = copy(last_window_values),\n                          exog        = copy(exog_values)\n                      )\n\n        preds_level = pd.Series(\n                          data  = preds_level,\n                          index = expand_index(\n                                      index = last_window_index,\n                                      steps = steps\n                                  ),\n                          name = level\n                      )\n\n        preds_level = transform_series(\n                          series            = preds_level,\n                          transformer       = self.transformer_series_[level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions.append(preds_level)    \n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>dict</code> <p>Predictions generated by bootstrapping for each level.  {level: pandas DataFrame, shape (steps, n_boot)}</p>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; dict:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.        \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : dict\n        Predictions generated by bootstrapping for each level. \n        {level: pandas DataFrame, shape (steps, n_boot)}\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if in_sample_residuals:\n        if not set(levels).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for levels: \"\n                 f\"{set(levels) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals_levels = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method \"\n                 \"`set_out_sample_residuals()` before `predict_interval()`, \"\n                 \"`predict_bootstrapping()` or `predict_dist()`.\")\n            )\n        else:\n            if not set(levels).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for levels: \"\n                     f\"{set(levels) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals_levels = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for level in levels:\n        if residuals_levels[level] is None:\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' are `None`. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (residuals_levels[level] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"values. Check `{check_residuals}`.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:, ]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    boot_predictions = {}\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        level_boot_predictions = np.full(\n                                     shape      = (steps, n_boot),\n                                     fill_value = np.nan,\n                                     dtype      = float\n                                 )\n        rng = np.random.default_rng(seed=random_state)\n        seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n        residuals = residuals_levels[level]\n\n        for i in range(n_boot):\n            # In each bootstraping iteration the initial last_window and exog \n            # need to be restored.\n            last_window_boot = last_window_values.copy()\n            exog_boot = exog_values.copy() if exog is not None else None\n\n            rng = np.random.default_rng(seed=seeds[i])\n            sample_residuals = rng.choice(\n                                   a       = residuals,\n                                   size    = steps,\n                                   replace = True\n                               )\n\n            for step in range(steps):\n\n                prediction = self._recursive_predict(\n                                 steps       = 1,\n                                 level       = level,\n                                 last_window = last_window_boot,\n                                 exog        = exog_boot \n                             )\n\n                prediction_with_residual = prediction + sample_residuals[step]\n                level_boot_predictions[step, i] = prediction_with_residual[0]\n\n                last_window_boot = np.append(\n                                       last_window_boot[1:],\n                                       prediction_with_residual\n                                   )\n                if exog is not None:\n                    exog_boot = exog_boot[1:]\n\n        level_boot_predictions = pd.DataFrame(\n                                     data    = level_boot_predictions,\n                                     index   = expand_index(last_window_index, steps=steps),\n                                     columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                                 )\n\n        if self.transformer_series_[level]:\n            for col in level_boot_predictions.columns:\n                level_boot_predictions[col] = transform_series(\n                                                  series            = level_boot_predictions[col],\n                                                  transformer       = self.transformer_series_[level],\n                                                  fit               = False,\n                                                  inverse_transform = True\n                                              )\n\n        boot_predictions[level] = level_boot_predictions\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_interval","title":"<code>predict_interval(steps, levels=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>level: predictions.</li> <li>level_lower_bound: lower bound of the interval.</li> <li>level_upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which, each prediction, is used as a predictor\n    for the next step and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.  \n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - level: predictions.\n            - level_lower_bound: lower bound of the interval.\n            - level_upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    check_interval(interval=interval)\n\n    preds = self.predict(\n                steps       = steps,\n                levels      = levels,\n                last_window = last_window,\n                exog        = exog\n            )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           levels              = levels,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions = []\n\n    for level in levels:\n        preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n        preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n        predictions.append(preds[level])\n        predictions.append(preds_interval)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_dist","title":"<code>predict_dist(steps, distribution, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats. For example scipy.stats.norm.</p> required <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Values of the series used to create the predictors needed in the first re of prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step and level.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats. For example scipy.stats.norm.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.  \n    last_window : pandas DataFrame, default `None`\n        Values of the series used to create the predictors needed in the first\n        re of prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step and level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       levels              = levels,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p=='x'] + [\"loc\",\"scale\"]\n    predictions = []\n\n    for level in levels:\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples[level])\n        level_param_names = [f'{level}_{p}' for p in param_names]\n\n        pred_level = pd.DataFrame(\n                         data    = param_values,\n                         columns = level_param_names,\n                         index   = boot_samples[level].index\n                     )\n\n        predictions.append(pred_level)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each level in the form {level: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored. Keys must be the same as <code>levels</code>.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_series.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict,\n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each level in the\n        form {level: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored. Keys must be the same as `levels`.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_series.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{level: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {level: None for level in self.series_col_names}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of levels \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    for level, value in residuals.items():\n\n        residuals_level = value\n\n        if not transform and self.transformer_series_[level] is not None:\n            warnings.warn(\n                (\"Argument `transform` is set to `False` but forecaster was \"\n                f\"trained using a transformer {self.transformer_series_[level]} \"\n                f\"for level {level}. Ensure that the new residuals are \"\n                 \"already transformed or set `transform=True`.\")\n            )\n\n        if transform and self.transformer_series_ and self.transformer_series_[level]:\n            warnings.warn(\n                (\"Residuals will be transformed using the same transformer used \"\n                f\"when training the forecaster for level {level} : \"\n                f\"({self.transformer_series_[level]}). Ensure that the new \"\n                 \"residuals are on the same scale as the original time series.\")\n            )\n            residuals_level = transform_series(\n                                  series            = pd.Series(residuals_level, name='residuals'),\n                                  transformer       = self.transformer_series_[level],\n                                  fit               = False,\n                                  inverse_transform = False\n                              ).to_numpy()\n\n        if len(residuals_level) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[level] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[level]))\n            if len(residuals_level) &lt; free_space:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level\n                                    ))\n            else:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level[:free_space]\n                                    ))\n\n        self.out_sample_residuals[level] = residuals_level\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoregMultiSeriesCustom\\ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html","title":"<code>ForecasterAutoregMultiVariate</code>","text":""},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate","title":"<code>ForecasterAutoregMultiVariate(regressor, level, steps, lags, transformer_series=None, transformer_exog=None, weight_func=None, fit_kwargs=None, n_jobs='auto', forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive multivariate direct multi-step forecaster. A separate model  is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>level</code> <code>str</code> <p>Name of the time series to be predicted.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value must be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series.  {'series_column_name': lags}.</li> </ul> required <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>`None`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>`None`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray, dict</code> <p>Lags used as predictors.</p> <code>lags_</code> <code>dict</code> <p>Dictionary containing the lags of each series. Created from <code>lags</code> and  used internally.</p> <code>transformer_series</code> <code>transformer (preprocessor), dict, default `None`</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.   </p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>int, 'auto', default `'auto'`</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the fuction skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate--notes","title":"Notes","text":"<p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    level: str,\n    steps: int,\n    lags: Union[int, np.ndarray, list, dict],\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    n_jobs: Optional[Union[int, str]]='auto',\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.level                   = level\n    self.steps                   = steps\n    self.transformer_series      = transformer_series\n    self.transformer_series_     = None\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.source_code_weight_func = None\n    self.max_lag                 = None\n    self.window_size             = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.series_col_names        = None\n    self.X_train_col_names       = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(level, str):\n        raise TypeError(\n            f\"`level` argument must be a str. Got {type(level)}.\"\n        )\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(\n            f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        for key in lags:\n            self.lags[key] = initialize_lags(\n                                 forecaster_name = type(self).__name__,\n                                 lags            = lags[key]\n                             )\n    else:\n        self.lags = initialize_lags(\n                        forecaster_name = type(self).__name__, \n                        lags            = lags\n                    )\n\n    self.lags_ = self.lags\n    self.max_lag = (\n        max(list(chain(*self.lags.values())))\n        if isinstance(self.lags, dict)\n        else max(self.lags)\n    )\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals = None\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor_name  = type(self.regressor).__name__,\n                      )\n    else:\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._create_lags","title":"<code>_create_lags(y, lags)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <code>lags</code> <code>numpy ndarray</code> <p>lags to create.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the values of the time series related to each  row of <code>X_data</code> for each step.  Shape: (len(self.steps), samples - max(self.lags))</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    lags: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Transforms a 1d array into a 2d array (X) and a 2d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n    lags : numpy ndarray\n        lags to create.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        2d numpy ndarray with the values of the time series related to each \n        row of `X_data` for each step. \n        Shape: (len(self.steps), samples - max(self.lags))\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - (self.steps - 1) # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series minus the number of steps ({len(y)-(self.steps-1)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(lags)), fill_value=np.nan, dtype=float)\n    for i, lag in enumerate(lags):\n        X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n\n    y_data = np.full(shape=(self.steps, n_splits), fill_value=np.nan, dtype=float)\n    for step in range(self.steps):\n        y_data[step, ] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method. Shape: (len(series) - self.max_lag, len(self.lags)len(series.columns) + exog.shape[1]steps)</p> <code>y_train</code> <code>dict</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step of the form {step: y_step_[i]}. Shape of each series: (len(y) - self.max_lag, )</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, dict]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n        Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)\n    y_train : dict\n        Values (target) of the time series related to each row of `X_train` \n        for each step of the form {step: y_step_[i]}.\n        Shape of each series: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    series_col_names = list(series.columns)\n\n    if self.level not in series_col_names:\n        raise ValueError(\n            (f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n             f\"    forecaster `level` : {self.level}.\\n\"\n             f\"    `series` columns   : {series_col_names}.\")\n        )\n\n    self.lags_ = self.lags\n    if isinstance(self.lags_, dict):\n        if list(self.lags_.keys()) != series_col_names:\n            raise ValueError(\n                (f\"When `lags` parameter is a `dict`, its keys must be the \"\n                 f\"same as `series` column names.\\n\"\n                 f\"    Lags keys        : {list(self.lags_.keys())}.\\n\"\n                 f\"    `series` columns : {series_col_names}.\")\n            )\n    else:\n        self.lags_ = {serie: self.lags_ for serie in series_col_names}\n\n    if len(series) &lt; self.max_lag + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `series` for training this forecaster is \"\n             f\"{self.max_lag + self.steps}. Got {len(series)}. Reduce the \"\n             f\"number of predicted steps, {self.steps}, or the maximum \"\n             f\"lag, {self.max_lag}, if no more data is available.\")\n        )\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items()\n            if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = (\n            set(series.columns) - set(self.transformer_series.keys())\n        )\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                 f\" No transformation is applied to these series.\"),\n                 IgnoredArgumentWarning\n            )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.included_exog = True \n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                        series            = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                        df                = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\") \n            )\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        check_y(y=y)\n        y = transform_series(\n                series            = y,\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values, y_index = preprocess_y(y=y)\n        X_train_values, y_train_values = self._create_lags(\n                                             y    = y_values,\n                                             lags = self.lags_[serie]\n                                         )\n        if i == 0:\n            X_train = X_train_values\n        else:\n            X_train = np.hstack((X_train, X_train_values))\n\n        if serie == self.level:\n            y_train = y_train_values\n\n    X_train_col_names = [f\"{key}_lag_{lag}\" for key in self.lags_\n                         for lag in self.lags_[key]]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag + (self.steps -1): ]\n              )\n\n    if exog is not None:\n        # Transform exog to match direct format\n        # The first `self.max_lag` positions have to be removed from X_exog\n        # since they are not in X_lags.\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        ).iloc[-X_train.shape[0]:, :]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = {step: pd.Series(\n                         data  = y_train[step-1], \n                         index = y_index[self.max_lag + step-1:][:len(y_train[0])],\n                         name  = f\"{self.level}_step_{step}\"\n                     )\n               for step in range(1, self.steps + 1)}\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict,\n    remove_suffix: bool=False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `create_train_X_y` method, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.included_exog:\n        X_train_step = X_train\n    else:\n        len_columns_lags = len(list(chain(*self.lags_.values())))\n        idx_columns_lags = np.arange(len_columns_lags)\n        n_exog = (len(self.X_train_col_names) - len_columns_lags) / self.steps\n        idx_columns_exog = (\n            np.arange((step-1)*n_exog, (step)*n_exog) + idx_columns_lags[-1] + 1 \n        )\n        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n                                for col_name in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return  X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>. </p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>create_train_X_y</code> and filter_train_X_y_for_step` methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`. \n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `create_train_X_y` and filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>`None`</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = {step: None for step in range(1, self.steps + 1)}\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n            exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train = self.create_train_X_y(series=series, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n\"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n        store_in_sample_residuals : bool\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting.\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor and in-sample residuals.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            residuals = (\n                (y_train_step - regressor.predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n        else:\n            residuals = None\n\n        return step, regressor, residuals\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor=copy(self.regressor),\n            X_train=X_train,\n            y_train=y_train,\n            step=step,\n            store_in_sample_residuals=store_in_sample_residuals\n        )\n        for step in range(1, self.steps + 1))\n    )\n\n    self.regressors_ = {step: regressor \n                        for step, regressor, _ in results_fit}\n\n    if store_in_sample_residuals:\n        self.in_sample_residuals = {step: residuals \n                                    for step, _, residuals in results_fit}\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(\n                            y = series[self.level],\n                            return_values = False\n                          )[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    self.last_window = series.iloc[-self.max_lag:].copy()\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict","title":"<code>predict(steps=None, last_window=None, exog=None, levels=None)</code>","text":"<p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = self.steps,\n        levels           = None,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n    else:\n        exog_values = None\n\n    X_lags = np.array([[]], dtype=float)\n\n    for serie in self.series_col_names:\n\n        last_window_serie = transform_series(\n                                series            = last_window[serie],\n                                transformer       = self.transformer_series_[serie],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_serie\n                                                )\n\n        X_lags = np.hstack(\n                    [X_lags, last_window_values[-self.lags_[serie]].reshape(1, -1)]\n                 )\n\n    predictions = np.full(shape=len(steps), fill_value=np.nan)\n\n    if exog is None:\n        Xs = [X_lags] * len(steps)\n    else:\n        n_exog = exog.shape[1] if isinstance(exog, pd.DataFrame) else 1\n        Xs = [\n            np.hstack(\n                [X_lags, exog_values[(step-1)*n_exog:(step)*n_exog].reshape(1, -1)]\n            )\n            for step in steps\n        ]\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.simplefilter(\"ignore\")\n        predictions = [\n            regressor.predict(X)[0] for regressor, X in zip(regressors, Xs)\n        ]\n\n    idx = expand_index(index=last_window_index, steps=max(steps))\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      columns = [self.level],\n                      index   = idx[np.array(steps)-1]\n                  )\n\n    predictions = transform_dataframe(\n                      df                = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.     \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.               \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    if in_sample_residuals:\n        if not set(steps).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for steps: \"\n                 f\"{set(steps) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n                 \"before `predict_interval()`, `predict_bootstrapping()` or \"\n                 \"`predict_dist()`.\")\n            )\n        else:\n            if not set(steps).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for steps: \"\n                     f\"{set(steps) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals = self.out_sample_residuals\n\n    check_residuals = (\n        'forecaster.in_sample_residuals' if in_sample_residuals\n        else 'forecaster.out_sample_residuals'\n    )\n    for step in steps:\n        if residuals[step] is None:\n            raise ValueError(\n                (f\"forecaster residuals for step {step} are `None`. \"\n                 f\"Check {check_residuals}.\")\n            )\n        elif (residuals[step] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for step {step} contains `None` values. \"\n                 f\"Check {check_residuals}.\")\n            )\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog \n                  )\n\n    # Predictions must be in the transformed scale before adding residuals\n    predictions = transform_dataframe(\n                      df                = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    boot_predictions = pd.concat([predictions] * n_boot, axis=1)\n    boot_predictions.columns= [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    for i, step in enumerate(steps):\n        rng = np.random.default_rng(seed=random_state)\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions.iloc[i, :] = boot_predictions.iloc[i, :] + sample_residuals\n\n    if self.transformer_series_[self.level]:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_series_[self.level],\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Bootstrapping based prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`[5, 95]`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Bootstrapping based prediction intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>int, list, None</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>`None`</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>`True`</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters \n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series.  {'series_column_name': lags}.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, np.ndarray, list, dict]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, dict\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. \n            {'series_column_name': lags}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        for key in lags:\n            self.lags[key] = initialize_lags(\n                                 forecaster_name = type(self).__name__,\n                                 lags            = lags[key]\n                             )\n    else:\n        self.lags = initialize_lags(\n                        forecaster_name = type(self).__name__, \n                        lags            = lags\n                    )\n\n    self.lags_ = self.lags\n    self.max_lag = (\n        max(list(chain(*self.lags.values()))) if isinstance(self.lags, dict)\n        else max(self.lags)\n    )\n    self.window_size = self.max_lag\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>`True`</code> <code>transform</code> <code>bool</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>`True`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {step: None \n                                     for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of models (steps) \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value for key, value in residuals.items()\n                 if key in self.out_sample_residuals.keys()}\n\n    if not transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_series_[self.level]}. Ensure \"\n             f\"that the new residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used when \"\n             f\"training the forecaster ({self.transformer_series_[self.level]}). Ensure \"\n             f\"the new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_series(\n                                 series            = pd.Series(value, name='residuals'),\n                                 transformer       = self.transformer_series_[self.level],\n                                 fit               = False,\n                                 inverse_transform = False\n                             ).to_numpy()\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[key]))\n            if len(value) &lt; free_space:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value\n                        ))\n            else:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals[key] = value\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.get_feature_importances","title":"<code>get_feature_importances(step)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py</code> <pre><code>def get_feature_importances(\n    self,\n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f'`step` must be an integer. Got {type(step)}.'\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    len_columns_lags = len(list(chain(*self.lags_.values())))\n    idx_columns_lags = np.arange(len_columns_lags)\n    if self.included_exog:\n        idx_columns_exog = np.flatnonzero(\n                            [name.endswith(f\"step_{step}\")\n                             for name in self.X_train_col_names]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_col_names[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterSarimax.html","title":"<code>ForecasterSarimax</code>","text":""},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax","title":"<code>ForecasterSarimax(regressor, transformer_y=None, transformer_exog=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>This class turns ARIMA model from either the skforecast or pmdarima library  into a Forecaster compatible with the skforecast API. New in version 0.10.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>skforecast.Sarimax.Sarimax, pmdarima.arima.ARIMA</code> <p>An ARIMA model instance from either the skforecast or pmdarima library.</p> required <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>`None`</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>`None`</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.  When using the skforecast Sarimax model, the fit kwargs should be passed  using the model parameter <code>sm_fit_kwargs</code> and not this one.</p> <code>`None`</code> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>skforecast.Sarimax.Sarimax, pmdarima.arima.ARIMA</code> <p>An ARIMA model instance from either the skforecast or pmdarima library.</p> <code>params</code> <code>dict</code> <p>Parameters of the sarimax model.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>window_size</code> <code>int</code> <p>Not used, present here for API consistency by convention.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>extended_index</code> <code>pandas Index</code> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index. Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int</code> <p>Name used as an identifier of the forecaster.</p> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def __init__(\n    self,\n    regressor: ARIMA,\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor         = regressor\n    self.transformer_y     = transformer_y\n    self.transformer_exog  = transformer_exog\n    self.window_size       = 1\n    self.last_window       = None\n    self.extended_index    = None\n    self.fitted            = False\n    self.index_type        = None\n    self.index_freq        = None\n    self.training_range    = None\n    self.included_exog     = False\n    self.exog_type         = None\n    self.exog_col_names    = None\n    self.creation_date     = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date          = None\n    self.skforcast_version = skforecast.__version__\n    self.python_version    = sys.version.split(\" \")[0]\n    self.forecaster_id     = forecaster_id\n\n    if isinstance(self.regressor, pmdarima.arima.ARIMA):\n        self.engine = 'pmdarima'\n    elif isinstance(self.regressor, skforecast.Sarimax.Sarimax):\n        self.engine = 'skforecast'\n    else:\n        raise TypeError(\n            (f\"`regressor` must be an instance of type pmdarima.arima.ARIMA \"\n             f\"or skforecast.Sarimax.Sarimax. Got {type(regressor)}.\")\n        )\n\n    self.params = self.regressor.get_params(deep=True)\n\n    if self.engine == 'pmdarima':\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n    else:\n        if fit_kwargs:\n            warnings.warn(\n                (\"When using the skforecast Sarimax model, the fit kwargs should \"\n                 \"be passed using the model parameter `sm_fit_kwargs`.\"),\n                 IgnoredArgumentWarning\n            )\n        self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.fit","title":"<code>fit(y, exog=None, suppress_warnings=False)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>`False`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    suppress_warnings: bool=False\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    suppress_warnings : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_y(y=y)\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        check_exog(exog=exog)\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.extended_index      = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog   \n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = True,\n                   inverse_transform = False\n               )\n\n    if suppress_warnings:\n        warnings.filterwarnings(\"ignore\")\n\n    if self.engine == 'pmdarima':\n        self.regressor.fit(y=y, X=exog, **self.fit_kwargs)\n    else:\n        self.regressor.fit(y=y, exog=exog)\n\n    if suppress_warnings:\n        warnings.filterwarnings(\"default\")\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y.index[[0, -1]]\n    self.index_type = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq = y.index.freqstr\n    else: \n        self.index_freq = y.index.step\n\n    self.last_window = y.copy()\n\n    if self.engine == 'pmdarima':\n        self.extended_index = self.regressor.arima_res_.fittedvalues.index.copy()\n    else:\n        self.extended_index = self.regressor.sarimax_res.fittedvalues.index.copy()\n\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict","title":"<code>predict(steps, last_window=None, last_window_exog=None, exog=None)</code>","text":"<p>Forecast future values.</p> <p>Generate predictions (forecasts) n steps in the future. Note that if  exogenous variables were used in the model fit, they will be expected  for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Value of the exogenous variable/s for the next steps.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Forecast future values.\n\n    Generate predictions (forecasts) n steps in the future. Note that if \n    exogenous variables were used in the model fit, they will be expected \n    for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Value of the exogenous variable/s for the next steps.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append if not needed\n    last_window_check = last_window.copy() if last_window is not None else self.last_window.copy()\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    # If last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            (\"To make predictions unrelated to the original data, both \"\n             \"`last_window` and `last_window_exog` must be provided.\")\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. To make predictions \"\n             \"unrelated to the original data, same variable/s must be provided \"\n             \"using `last_window_exog`.\")\n        )\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # check index append values\n        expected_index = expand_index(index=self.extended_index, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                (f\"To make predictions unrelated to the original data, `last_window` \"\n                 f\"has to start at the end of the index seen by the forecaster.\\n\"\n                 f\"    Series last index         : {self.extended_index[-1]}.\\n\"\n                 f\"    Expected index            : {expected_index}.\\n\"\n                 f\"    `last_window` index start : {last_window.index[0]}.\")\n            )\n\n        last_window = transform_series(\n                          series            = last_window.copy(),\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # TODO -----------------------------------------------------------------------------------------------------\n        # This is done because pmdarima deletes the series name\n        # Check issue: https://github.com/alkaline-ml/pmdarima/issues/535\n        if self.engine == 'pmdarima':\n            last_window.name = None\n        # ----------------------------------------------------------------------------------------------------------\n\n        # last_window_exog\n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    (f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                     f\"has to start at the end of the index seen by the forecaster.\\n\"\n                     f\"    Series last index              : {self.extended_index[-1]}.\\n\"\n                     f\"    Expected index                 : {expected_index}.\\n\"\n                     f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\")\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog \n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n        if self.engine == 'pmdarima':\n            self.regressor.arima_res_ = self.regressor.arima_res_.append(\n                                            endog = last_window,\n                                            exog  = last_window_exog,\n                                            refit = False\n                                        )\n            self.extended_index = self.regressor.arima_res_.fittedvalues.index\n        else:\n            self.regressor.append(\n                y     = last_window,\n                exog  = last_window_exog,\n                refit = False\n            )\n            self.extended_index = self.regressor.sarimax_res.fittedvalues.index\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    # Get following n steps predictions\n    if self.engine == 'pmdarima':\n        predictions = self.regressor.predict(\n                          n_periods = steps,\n                          X         = exog\n                      )\n    else:\n        predictions = self.regressor.predict(\n                          steps = steps,\n                          exog  = exog\n                      )\n        predictions = predictions.iloc[:, 0]\n\n    # Reverse the transformation if needed\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n    predictions.name = 'pred'\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict_interval","title":"<code>predict_interval(steps, last_window=None, last_window_exog=None, exog=None, alpha=0.05, interval=None)</code>","text":"<p>Forecast future values and their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only need when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>`0.05`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    alpha: float=0.05,\n    interval: list=None,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Forecast future values and their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        need when `last_window` is not None and the forecaster has been\n        trained including exogenous variables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append if not needed\n    last_window_check = last_window.copy() if last_window is not None else self.last_window.copy()\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = interval,\n        alpha            = alpha,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    # If last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            (\"To make predictions unrelated to the original data, both \"\n             \"`last_window` and `last_window_exog` must be provided.\")\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. To make predictions \"\n             \"unrelated to the original data, same variable/s must be provided \"\n             \"using `last_window_exog`.\")\n        )  \n\n    # If interval and alpha take alpha, if interval transform to alpha\n    if alpha is None:\n        if 100 - interval[1] != interval[0]:\n            raise ValueError(\n                (f\"When using `interval` in ForecasterSarimax, it must be symmetrical. \"\n                 f\"For example, interval of 95% should be as `interval = [2.5, 97.5]`. \"\n                 f\"Got {interval}.\")\n            )\n        alpha = 2*(100 - interval[1])/100\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # check index append values\n        expected_index = expand_index(index=self.extended_index, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                (f\"To make predictions unrelated to the original data, `last_window` \"\n                 f\"has to start at the end of the index seen by the forecaster.\\n\"\n                 f\"    Series last index         : {self.extended_index[-1]}.\\n\"\n                 f\"    Expected index            : {expected_index}.\\n\"\n                 f\"    `last_window` index start : {last_window.index[0]}.\")\n            )\n\n        last_window = transform_series(\n                          series            = last_window,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # TODO -----------------------------------------------------------------------------------------------------\n        # This is done because pmdarima deletes the series name\n        # Check issue: https://github.com/alkaline-ml/pmdarima/issues/535\n        if self.engine == 'pmdarima':\n            last_window.name = None\n        # ----------------------------------------------------------------------------------------------------------\n\n        # Transform last_window_exog    \n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    (f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                     f\"has to start at the end of the index seen by the forecaster.\\n\"\n                     f\"    Series last index              : {self.extended_index[-1]}.\\n\"\n                     f\"    Expected index                 : {expected_index}.\\n\"\n                     f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\")\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog \n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n        if self.engine == 'pmdarima':\n            self.regressor.arima_res_ = self.regressor.arima_res_.append(\n                                            endog = last_window,\n                                            exog  = last_window_exog,\n                                            refit = False\n                                        )\n            self.extended_index = self.regressor.arima_res_.fittedvalues.index\n        else:\n            self.regressor.append(\n                y     = last_window,\n                exog  = last_window_exog,\n                refit = False\n            )\n            self.extended_index = self.regressor.sarimax_res.fittedvalues.index\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    # Get following n steps predictions with intervals\n    if self.engine == 'pmdarima':\n        predicted_mean, conf_int = self.regressor.predict(\n                                       n_periods       = steps,\n                                       X               = exog,\n                                       return_conf_int = True,\n                                       alpha           = alpha\n                                   )\n\n        predictions = predicted_mean.to_frame(name=\"pred\")\n        predictions['lower_bound'] = conf_int[:, 0]\n        predictions['upper_bound'] = conf_int[:, 1]\n    else:\n        predictions = self.regressor.predict(\n                          steps = steps,\n                          exog  = exog,\n                          return_conf_int = True,\n                          alpha = alpha\n                      )\n\n\n    # Reverse the transformation if needed\n    if self.transformer_y:\n        for col in predictions.columns:\n            predictions[col] = transform_series(\n                                series            = predictions[col],\n                                transformer       = self.transformer_y,\n                                fit               = False,\n                                inverse_transform = True\n                           )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the model stored in the forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.engine == 'pmdarima':\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n    else:\n        warnings.warn(\n            (\"When using the skforecast Sarimax model, the fit kwargs should \"\n             \"be passed using the model parameter `sm_fit_kwargs`.\"),\n             IgnoredArgumentWarning\n        )\n        self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    feature_importances = self.regressor.params().to_frame().reset_index()\n    feature_importances.columns = ['feature', 'importance']\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_info_criteria","title":"<code>get_info_criteria(criteria='aic', method='standard')</code>","text":"<p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>`'aic'`</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>`'standard'`</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast\\ForecasterSarimax\\ForecasterSarimax.py</code> <pre><code>def get_info_criteria(\n    self, \n    criteria: str='aic', \n    method: str='standard'\n) -&gt; float:\n\"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default `'aic'`\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default `'standard'`\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            (f\"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n             f\"and 'hqic'.\")\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            (f\"Invalid value for `method`. Valid options are 'standard' and \"\n             f\"'lutkepohl'.\")\n        )\n\n    if self.engine == 'pmdarima':\n        metric = self.regressor.arima_res_.info_criteria(criteria=criteria, method=method)\n    else:\n        metric = self.regressor.get_info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/Sarimax.html","title":"<code>Sarimax</code>","text":""},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax","title":"<code>Sarimax(order=(1, 0, 0), seasonal_order=(0, 0, 0, 0), trend=None, measurement_error=False, time_varying_regression=False, mle_regression=True, simple_differencing=False, enforce_stationarity=True, enforce_invertibility=True, hamilton_representation=False, concentrate_scale=False, trend_offset=1, use_exact_diffuse=False, dates=None, freq=None, missing='none', validate_specification=True, method='lbfgs', maxiter=50, start_params=None, disp=False, sm_init_kwargs={}, sm_fit_kwargs={}, sm_predict_kwargs={})</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A universal scikit-learn style wrapper for statsmodels SARIMAX.</p> <p>This class wraps the statsmodels.tsa.statespace.sarimax.SARIMAX model to  follow the scikit-learn style. The following docstring is based on the  statmodels documentation and it is highly recommended to visit their site  for the best level of detail.</p> <p>https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.html</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters. </p> <ul> <li><code>d</code> must be an integer indicating the integration order of the process.</li> <li><code>p</code> and <code>q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include.</li> </ul> <code>`(1, 0, 0)`</code> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity. </p> <ul> <li><code>D</code> must be an integer indicating the integration order of the process.</li> <li><code>P</code> and <code>Q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include. </li> <li><code>s</code> is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data.</li> </ul> <code>`(0, 0, 0, 0)`</code> <code>trend</code> <code>str</code> <p>Parameter controlling the deterministic trend polynomial <code>A(t)</code>.</p> <ul> <li><code>'c'</code> indicates a constant (i.e. a degree zero component of the  trend polynomial).</li> <li><code>'t'</code> indicates a linear trend with time.</li> <li><code>'ct'</code> indicates both, <code>'c'</code> and <code>'t'</code>. </li> <li>Can also be specified as an iterable defining the non-zero polynomial  exponents to include, in increasing order. For example, <code>[1,1,0,1]</code>  denotes <code>a + b*t + ct^3</code>.</li> </ul> <code>`None`</code> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>`False`</code> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>`False`</code> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>`True`</code> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation. </p> <ul> <li>If <code>True</code>, differencing is performed prior to estimation, which  discards the first <code>s*D + d</code> initial rows but results in a smaller  state-space formulation. </li> <li>If <code>False</code>, the full SARIMAX model is put in state-space form so  that all datapoints can be used in estimation.</li> </ul> <code>`False`</code> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>`True`</code> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>`True`</code> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>`False`</code> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>`False`</code> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values. Default is 1, so that if <code>trend='t'</code> the trend is equal to 1, 2, ..., nobs. Typically is only set when the model created by extending a previous dataset.</p> <code>`1`</code> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states. Default is <code>False</code> (in which case approximate diffuse initialization is used).</p> <code>`False`</code> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used, and it  can be chosen from among the following strings:</p> <ul> <li><code>'newton'</code> for Newton-Raphson</li> <li><code>'nm'</code> for Nelder-Mead</li> <li><code>'bfgs'</code> for Broyden-Fletcher-Goldfarb-Shanno (BFGS)</li> <li><code>'lbfgs'</code> for limited-memory BFGS with optional box constraints</li> <li><code>'powell'</code> for modified Powell`s method</li> <li><code>'cg'</code> for conjugate gradient</li> <li><code>'ncg'</code> for Newton-conjugate gradient</li> <li><code>'basinhopping'</code> for global basin-hopping solver</li> </ul> <code>`'lbfgs'`</code> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>`50`</code> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.  If <code>None</code>, the default is given by regressor.start_params.</p> <code>`None`</code> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>`False`</code> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>`{}`</code> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model. The statsmodels SARIMAX.fit parameters  <code>method</code>, <code>max_iter</code>, <code>start_params</code> and <code>disp</code> have been moved to the  initialization of this model and will have priority over those provided  by the user using via <code>sm_fit_kwargs</code>.</p> <code>`{}` </code> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>`{}`</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters.</p> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity.</p> <code>trend</code> <code>str</code> <p>Deterministic trend polynomial <code>A(t)</code>.</p> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation.</p> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values.</p> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states.</p> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used.</p> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.</p> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model.</p> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>_sarimax_params</code> <code>dict</code> <p>Parameters of this model that can be set with the <code>set_params</code> method.</p> <code>output_type</code> <code>str</code> <p>Format of the object returned by the predict method. This is set  automatically according to the type of <code>y</code> used in the fit method to  train the model, <code>'numpy'</code> or <code>'pandas'</code>.</p> <code>sarimax</code> <code>object</code> <p>The statsmodels.tsa.statespace.sarimax.SARIMAX object created.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>sarimax_res</code> <code>object</code> <p>The resulting statsmodels.tsa.statespace.sarimax.SARIMAXResults object  created by statsmodels after fitting the SARIMAX model.</p> <code>training_index</code> <code>pandas Index</code> <p>Index of the training series as long as it is a pandas Series or Dataframe.</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>def __init__(\n    self,\n    order: tuple = (1, 0, 0),\n    seasonal_order: tuple = (0, 0, 0, 0),\n    trend: str = None,\n    measurement_error: bool = False,\n    time_varying_regression: bool = False,\n    mle_regression: bool = True,\n    simple_differencing: bool = False,\n    enforce_stationarity: bool = True,\n    enforce_invertibility: bool = True,\n    hamilton_representation: bool = False,\n    concentrate_scale: bool = False,\n    trend_offset: int = 1,\n    use_exact_diffuse: bool = False,\n    dates = None,\n    freq = None,\n    missing = 'none',\n    validate_specification: bool = True,\n    method: str = 'lbfgs',\n    maxiter: int = 50,\n    start_params: np.ndarray = None,\n    disp: bool = False,\n    sm_init_kwargs: dict = {},\n    sm_fit_kwargs: dict = {},\n    sm_predict_kwargs: dict = {}\n) -&gt; None:\n\n    self.order                   = order\n    self.seasonal_order          = seasonal_order\n    self.trend                   = trend\n    self.measurement_error       = measurement_error\n    self.time_varying_regression = time_varying_regression\n    self.mle_regression          = mle_regression\n    self.simple_differencing     = simple_differencing\n    self.enforce_stationarity    = enforce_stationarity\n    self.enforce_invertibility   = enforce_invertibility\n    self.hamilton_representation = hamilton_representation\n    self.concentrate_scale       = concentrate_scale\n    self.trend_offset            = trend_offset\n    self.use_exact_diffuse       = use_exact_diffuse\n    self.dates                   = dates\n    self.freq                    = freq\n    self.missing                 = missing\n    self.validate_specification  = validate_specification\n    self.method                  = method\n    self.maxiter                 = maxiter\n    self.start_params            = start_params\n    self.disp                    = disp\n\n    # Create the dictionaries with the additional statsmodels parameters to be  \n    # used during the init, fit and predict methods. Note that the statsmodels \n    # SARIMAX.fit parameters `method`, `max_iter`, `start_params` and `disp` \n    # have been moved to the initialization of this model and will have \n    # priority over those provided by the user using via `sm_fit_kwargs`.\n    self.sm_init_kwargs    = sm_init_kwargs\n    self.sm_fit_kwargs     = sm_fit_kwargs\n    self.sm_predict_kwargs = sm_predict_kwargs\n\n    # Params that can be set with the `set_params` method\n    _, _, _, _sarimax_params = inspect.getargvalues(inspect.currentframe())\n    _sarimax_params.pop(\"self\")\n    self._sarimax_params = _sarimax_params\n\n    self._consolidate_kwargs()\n\n    # Create Results Attributes \n    self.output_type    = None\n    self.sarimax        = None\n    self.fitted         = False\n    self.sarimax_res    = None\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax._consolidate_kwargs","title":"<code>_consolidate_kwargs()</code>","text":"<p>Create the dictionaries to be used during the init, fit, and predict methods. Note that the parameters in this model's initialization take precedence  over those provided by the user using via the statsmodels kwargs dicts.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>def _consolidate_kwargs(\n    self\n) -&gt; None:\n\"\"\"\n    Create the dictionaries to be used during the init, fit, and predict methods.\n    Note that the parameters in this model's initialization take precedence \n    over those provided by the user using via the statsmodels kwargs dicts.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # statsmodels.tsa.statespace.SARIMAX parameters\n    _init_kwargs = self.sm_init_kwargs.copy()\n    _init_kwargs.update({\n       'order': self.order,\n       'seasonal_order': self.seasonal_order,\n       'trend': self.trend,\n       'measurement_error': self.measurement_error,\n       'time_varying_regression': self.time_varying_regression,\n       'mle_regression': self.mle_regression,\n       'simple_differencing': self.simple_differencing,\n       'enforce_stationarity': self.enforce_stationarity,\n       'enforce_invertibility': self.enforce_invertibility,\n       'hamilton_representation': self.hamilton_representation,\n       'concentrate_scale': self.concentrate_scale,\n       'trend_offset': self.trend_offset,\n       'use_exact_diffuse': self.use_exact_diffuse,\n       'dates': self.dates,\n       'freq': self.freq,\n       'missing': self.missing,\n       'validate_specification': self.validate_specification\n    })\n    self._init_kwargs = _init_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAX.fit parameters\n    _fit_kwargs = self.sm_fit_kwargs.copy()\n    _fit_kwargs.update({\n       'method': self.method,\n       'maxiter': self.maxiter,\n       'start_params': self.start_params,\n       'disp': self.disp,\n    })        \n    self._fit_kwargs = _fit_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAXResults.get_forecast parameters\n    self._predict_kwargs = self.sm_predict_kwargs.copy()\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax._create_sarimax","title":"<code>_create_sarimax(endog, exog=None)</code>","text":"<p>A helper method to create a new statsmodel SARIMAX model.</p> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized can be added with the <code>init_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>endog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The endogenous variable.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The exogenous variables.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>def _create_sarimax(\n    self,\n    endog: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None\n) -&gt; None:\n\"\"\"\n    A helper method to create a new statsmodel SARIMAX model.\n\n    Additional keyword arguments to pass to the statsmodels SARIMAX model \n    when it is initialized can be added with the `init_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    endog : numpy ndarray, pandas Series, pandas DataFrame\n        The endogenous variable.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        The exogenous variables.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.sarimax = SARIMAX(endog=endog, exog=exog, **self._init_kwargs)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.fit","title":"<code>fit(y, exog=None)</code>","text":"<p>Fit the model to the data.</p> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model can be added with the <code>fit_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>def fit(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None\n) -&gt; None:\n\"\"\"\n    Fit the model to the data.\n\n    Additional keyword arguments to pass to the `fit` method of the\n    statsmodels SARIMAX model can be added with the `fit_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        Training time series.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n\n    self.output_type = 'numpy' if isinstance(y, np.ndarray) else 'pandas'\n\n    self._create_sarimax(endog=y, exog=exog)\n    self.sarimax_res = self.sarimax.fit(**self._fit_kwargs)\n    self.fitted = True\n\n    if self.output_type == 'pandas':\n        self.training_index = y.index\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.predict","title":"<code>predict(steps, exog=None, return_conf_int=False, alpha=0.05)</code>","text":"<p>Forecast future values and, if desired, their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAX model can be added with the <code>predict_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Value of the exogenous variable/s for the next steps. The number of  observations needed is the number of steps to predict.</p> <code>`None`</code> <code>return_conf_int</code> <code>bool</code> <p>Whether to get the confidence intervals of the forecasts.</p> <code>`False`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>`0.05`</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray, pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval. The  output type is the same as the type of <code>y</code> used in the fit method.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval. (if <code>return_conf_int</code>)</li> <li>upper_bound: upper bound of the interval. (if <code>return_conf_int</code>)</li> </ul> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef predict(\n    self,\n    steps: int,\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None, \n    return_conf_int: bool = False,\n    alpha: float = 0.05\n) -&gt; Union[np.ndarray, pd.DataFrame]:\n\"\"\"\n    Forecast future values and, if desired, their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    Additional keyword arguments to pass to the `get_forecast` method of the\n    statsmodels SARIMAX model can be added with the `predict_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        Value of the exogenous variable/s for the next steps. The number of \n        observations needed is the number of steps to predict. \n    return_conf_int : bool, default `False`\n        Whether to get the confidence intervals of the forecasts.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n\n    Returns\n    -------\n    predictions : numpy ndarray, pandas DataFrame\n        Values predicted by the forecaster and their estimated interval. The \n        output type is the same as the type of `y` used in the fit method.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval. (if `return_conf_int`)\n            - upper_bound: upper bound of the interval. (if `return_conf_int`)\n\n    \"\"\"\n\n    # This is done because statsmodels doesn't allow `exog` length greater than\n    # the number of steps\n    if exog is not None and len(exog) &gt; steps:\n        warnings.warn(\n            (f\"when predicting using exogenous variables, the `exog` parameter \"\n             f\"must have the same length as the number of predicted steps. Since \"\n             f\"len(exog) &gt; steps, only the first {steps} observations are used.\")\n        )\n        exog = exog[:steps]\n\n    predictions = self.sarimax_res.get_forecast(\n                      steps = steps,\n                      exog  = exog,\n                      **self._predict_kwargs\n                  )\n\n    if not return_conf_int:\n        predictions = predictions.predicted_mean\n        if self.output_type == 'pandas':\n            predictions = predictions.rename(\"pred\").to_frame()\n    else:\n        if self.output_type == 'numpy':\n            predictions = np.column_stack(\n                              [predictions.predicted_mean,\n                               predictions.conf_int(alpha=alpha)]\n                          )\n        else:\n            predictions = pd.concat((\n                              predictions.predicted_mean,\n                              predictions.conf_int(alpha=alpha)),\n                              axis = 1\n                          )\n            predictions.columns = ['pred', 'lower_bound', 'upper_bound']\n\n    return predictions\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.append","title":"<code>append(y, exog=None, refit=False, copy_initialization=False, **kwargs)</code>","text":"<p>Recreate the results object with new data appended to the original data.</p> <p>Creates a new result object applied to a dataset that is created by  appending new data to the end of the model's original data. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, based on the combined dataset.</p> <code>`False`</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>`False`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.append--notes","title":"Notes","text":"<p>The <code>y</code> and <code>exog</code> arguments to this method must be formatted in the same  way (e.g. Pandas Series versus Numpy array) as were the <code>y</code> and <code>exog</code>  arrays passed to the original model.</p> <p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of <code>y</code>. For any other kind of  dataset, see the apply method.</p> <p>This method will apply filtering to all of the original data as well as  to the new data. To apply filtering only to the new data (which can be  much faster if the original dataset is large), see the extend method.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef append(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n\"\"\"\n    Recreate the results object with new data appended to the original data.\n\n    Creates a new result object applied to a dataset that is created by \n    appending new data to the end of the model's original data. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the parameters, based on the combined dataset.\n    copy_initialization : bool, default `False`\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` and `exog` arguments to this method must be formatted in the same \n    way (e.g. Pandas Series versus Numpy array) as were the `y` and `exog` \n    arrays passed to the original model.\n\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of `y`. For any other kind of \n    dataset, see the apply method.\n\n    This method will apply filtering to all of the original data as well as \n    to the new data. To apply filtering only to the new data (which can be \n    much faster if the original dataset is large), see the extend method.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.append(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.apply","title":"<code>apply(y, exog=None, refit=False, copy_initialization=False, **kwargs)</code>","text":"<p>Apply the fitted parameters to new data unrelated to the original data.</p> <p>Creates a new result object using the current fitted parameters, applied  to a completely new dataset that is assumed to be unrelated to the model's original data. The new results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, using the new dataset.</p> <code>`False`</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>`False`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.apply--notes","title":"Notes","text":"<p>The <code>y</code> argument to this method should consist of new observations that  are not necessarily related to the original model's <code>y</code> dataset. For  observations that continue that original dataset by follow directly after  its last element, see the append and extend methods.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef apply(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n\"\"\"\n    Apply the fitted parameters to new data unrelated to the original data.\n\n    Creates a new result object using the current fitted parameters, applied \n    to a completely new dataset that is assumed to be unrelated to the model's\n    original data. The new results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the parameters, using the new dataset.\n    copy_initialization : bool, default `False`\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    are not necessarily related to the original model's `y` dataset. For \n    observations that continue that original dataset by follow directly after \n    its last element, see the append and extend methods.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.apply(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.extend","title":"<code>extend(y, exog=None, **kwargs)</code>","text":"<p>Recreate the results object for new data that extends the original data.</p> <p>Creates a new result object applied to a new dataset that is assumed to  follow directly from the end of the model's original data. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.extend--notes","title":"Notes","text":"<p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of the model's original <code>y</code>  array. For any other kind of dataset, see the apply method.</p> <p>This method will apply filtering only to the new data provided by the <code>y</code>  argument, which can be much faster than re-filtering the entire dataset.  However, the returned results object will only have results for the new  data. To retrieve results for both the new data and the original data,  see the append method.</p> <p>https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef extend(\n    self,\n    y: Union[np.ndarray, pd.Series, pd.DataFrame],\n    exog: Optional[Union[np.ndarray, pd.Series, pd.DataFrame]] = None,\n    **kwargs\n) -&gt; None:\n\"\"\"\n    Recreate the results object for new data that extends the original data.\n\n    Creates a new result object applied to a new dataset that is assumed to \n    follow directly from the end of the model's original data. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default `None`\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of the model's original `y` \n    array. For any other kind of dataset, see the apply method.\n\n    This method will apply filtering only to the new data provided by the `y` \n    argument, which can be much faster than re-filtering the entire dataset. \n    However, the returned results object will only have results for the new \n    data. To retrieve results for both the new data and the original data, \n    see the append method.\n\n    https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend\n\n    \"\"\"\n\n    self.sarimax_res = self.sarimax_res.extend(\n                           endog = y,\n                           exog  = exog,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set new values to the parameters of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>def set_params(\n    self, \n    **params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the regressor.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    params = {k:v for k,v in params.items() if k in self._sarimax_params}\n    for key, value in params.items():\n        setattr(self, key, value)\n\n    self._consolidate_kwargs()\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.params","title":"<code>params()</code>","text":"<p>Get the parameters of the model. The order of variables is the trend coefficients, the <code>k_exog</code> exogenous coefficients, the <code>k_ar</code> AR  coefficients, and finally the <code>k_ma</code> MA coefficients.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>numpy ndarray, pandas Series</code> <p>The parameters of the model.</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef params(\n    self\n) -&gt; Union[np.ndarray, pd.Series]:\n\"\"\"\n    Get the parameters of the model. The order of variables is the trend\n    coefficients, the `k_exog` exogenous coefficients, the `k_ar` AR \n    coefficients, and finally the `k_ma` MA coefficients.\n\n    Returns\n    -------\n    params : numpy ndarray, pandas Series\n        The parameters of the model.\n\n    \"\"\"\n\n    return self.sarimax_res.params\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.summary","title":"<code>summary(alpha=0.05, start=None)</code>","text":"<p>Get a summary of the SARIMAXResults object.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>`0.05`</code> <code>start</code> <code>int</code> <p>Integer of the start observation.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>Summary instance</code> <p>This holds the summary table and text, which can be printed or  converted to various output formats.</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef summary(\n    self,\n    alpha: float = 0.05,\n    start: int = None\n) -&gt; object:\n\"\"\"\n    Get a summary of the SARIMAXResults object.\n\n    Parameters\n    ----------\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n    start : int, default `None`\n        Integer of the start observation.\n\n    Returns\n    -------\n    summary : Summary instance\n        This holds the summary table and text, which can be printed or \n        converted to various output formats.\n\n    \"\"\"\n\n    return self.sarimax_res.summary(alpha=alpha, start=start)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.Sarimax.Sarimax.Sarimax.get_info_criteria","title":"<code>get_info_criteria(criteria='aic', method='standard')</code>","text":"<p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>`'aic'`</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>`'standard'`</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast\\Sarimax\\Sarimax.py</code> <pre><code>@_check_fitted\ndef get_info_criteria(\n    self,\n    criteria: str = 'aic',\n    method: str = 'standard'\n) -&gt; float:\n\"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default `'aic'`\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default `'standard'`\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            (f\"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n             f\"and 'hqic'.\")\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            (f\"Invalid value for `method`. Valid options are 'standard' and \"\n             f\"'lutkepohl'.\")\n        )\n\n    metric = self.sarimax_res.info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/exceptions.html","title":"<code>exceptions</code>","text":""},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingValuesExogWarning","title":"<code>MissingValuesExogWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify there are missing values in the exogenous data. This warning occurs when the input data passed to the <code>exog</code> arguments contains missing values. Most machine learning models do not accept missing values, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTypeWarning","title":"<code>DataTypeWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify that a large number of models will be trained and the the process may take a while to run.</p> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify that an argument is ignored when using a method  or a function.</p> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/model_selection.html","title":"<code>model_selection</code>","text":""},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.backtesting_forecaster","title":"<code>backtesting_forecaster(forecaster, y, steps, metric, initial_train_size=None, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, n_jobs='auto', verbose=False, show_progress=True)</code>","text":"<p>Backtesting of forecaster model.</p> <ul> <li>If <code>refit</code> is <code>False</code>, the model will be trained only once using the  <code>initial_train_size</code> first observations. </li> <li>If <code>refit</code> is <code>True</code>, the model is trained on each iteration, increasing the training set. </li> <li>If <code>refit</code> is an <code>integer</code>, the model will be trained every that number  of iterations.</li> <li>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is <code>None</code>, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</li> </ul> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> <code>`None`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>. If <code>None</code>, no intervals are estimated.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_value</code> <code>float, list</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\model_selection\\model_selection.py</code> <pre><code>def backtesting_forecaster(\n    forecaster,\n    y: pd.Series,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int]=None,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[Union[float, list], pd.DataFrame]:\n\"\"\"\n    Backtesting of forecaster model.\n\n    - If `refit` is `False`, the model will be trained only once using the \n    `initial_train_size` first observations. \n    - If `refit` is `True`, the model is trained on each iteration, increasing\n    the training set. \n    - If `refit` is an `integer`, the model will be trained every that number \n    of iterations.\n    - If `forecaster` is already trained and `initial_train_size` is `None`,\n    no initial train will be done and all data will be used to evaluate the model.\n    However, the first `len(forecaster.last_window)` observations are needed\n    to create the initial predictors, so no predictions are calculated for them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metrics_value : float, list\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterAutoreg', \n                                         'ForecasterAutoregCustom', \n                                         'ForecasterAutoregDirect']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterAutoreg`, `ForecasterAutoregCustom` \"\n             \"or `ForecasterAutoregDirect`, for all other types of forecasters \"\n             \"use the functions available in the other `model_selection` modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        y                     = y,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    if type(forecaster).__name__ == 'ForecasterAutoregDirect' and \\\n       forecaster.steps &lt; steps + gap:\n        raise ValueError(\n            (\"When using a ForecasterAutoregDirect, the combination of steps \"\n             f\"+ gap ({steps+gap}) cannot be greater than the `steps` parameter \"\n             f\"declared when the forecaster is initialized ({forecaster.steps}).\")\n        )\n\n    metrics_values, backtest_predictions = _backtesting_forecaster(\n        forecaster            = forecaster,\n        y                     = y,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    return metrics_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.grid_search_forecaster","title":"<code>grid_search_forecaster(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\model_selection.py</code> <pre><code>def grid_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.random_search_forecaster","title":"<code>random_search_forecaster(forecaster, y, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\model_selection.py</code> <pre><code>def random_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.bayesian_search_forecaster","title":"<code>bayesian_search_forecaster(forecaster, y, search_space, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, n_trials=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True, engine='optuna', kwargs_create_study={}, kwargs_study_optimize={})</code>","text":"<p>Bayesian optimization for a Forecaster object using time series backtesting and  optuna library.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>search_space</code> <code>Callable(optuna)</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>engine</code> <code>str</code> <p>Bayesian optimization runs through the optuna library.</p> <code>`'optuna'`</code> <code>kwargs_create_study</code> <code>dict</code> <p>Only applies to engine='optuna'. Keyword arguments (key, value mappings)  to pass to optuna.create_study.</p> <code>`{'direction':'minimize', 'sampler':TPESampler(seed=123)}`</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Only applies to engine='optuna'. Other keyword arguments (key, value mappings)  to pass to study.optimize().</p> <code>`{}`</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> <code>results_opt_best</code> <code>optuna object (optuna)  </code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast\\model_selection\\model_selection.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    search_space: Callable,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    n_trials: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True,\n    engine: str='optuna',\n    kwargs_create_study: dict={},\n    kwargs_study_optimize: dict={}\n) -&gt; Tuple[pd.DataFrame, object]:\n\"\"\"\n    Bayesian optimization for a Forecaster object using time series backtesting and \n    optuna library.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    search_space : Callable (optuna)\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    n_trials : int, default `10`\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default `123`\n        Sets a seed to the sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n    engine : str, default `'optuna'`\n        Bayesian optimization runs through the optuna library.\n    kwargs_create_study : dict, default `{'direction':'minimize', 'sampler':TPESampler(seed=123)}`\n        Only applies to engine='optuna'. Keyword arguments (key, value mappings) \n        to pass to optuna.create_study.\n    kwargs_study_optimize : dict, default `{}`\n        Only applies to engine='optuna'. Other keyword arguments (key, value mappings) \n        to pass to study.optimize().\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n    results_opt_best : optuna object (optuna)  \n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f'`exog` must have same number of samples as `y`. '\n            f'length `exog`: ({len(exog)}), length `y`: ({len(y)})'\n        )\n\n    if engine == 'skopt':\n        warnings.warn(\n            (\"The engine 'skopt' for `bayesian_search_forecaster` is deprecated \"\n             \"in favor of 'optuna' engine. To continue using it, use skforecast \"\n             \"0.6.0. The optimization will be performed using the 'optuna' engine.\")\n        )\n        engine = 'optuna'\n\n    if engine not in ['optuna']:\n        raise ValueError(\n            f\"\"\"`engine` only allows 'optuna', got {engine}.\"\"\"\n        )\n\n    results, results_opt_best = _bayesian_search_optuna(\n                                    forecaster            = forecaster,\n                                    y                     = y,\n                                    exog                  = exog,\n                                    lags_grid             = lags_grid,\n                                    search_space          = search_space,\n                                    steps                 = steps,\n                                    metric                = metric,\n                                    refit                 = refit,\n                                    initial_train_size    = initial_train_size,\n                                    fixed_train_size      = fixed_train_size,\n                                    gap                   = gap,\n                                    allow_incomplete_fold = allow_incomplete_fold,\n                                    n_trials              = n_trials,\n                                    random_state          = random_state,\n                                    return_best           = return_best,\n                                    n_jobs                = n_jobs,\n                                    verbose               = verbose,\n                                    show_progress         = show_progress,\n                                    kwargs_create_study   = kwargs_create_study,\n                                    kwargs_study_optimize = kwargs_study_optimize\n                                )\n\n    return results, results_opt_best\n</code></pre>"},{"location":"api/model_selection_multiseries.html","title":"<code>model_selection_multiseries</code>","text":""},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.backtesting_forecaster_multiseries","title":"<code>backtesting_forecaster_multiseries(forecaster, series, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, n_jobs='auto', verbose=False, show_progress=True)</code>","text":"<p>Backtesting for multi-series and multivariate forecasters.</p> <ul> <li>If <code>refit</code> is <code>False</code>, the model will be trained only once using the  <code>initial_train_size</code> first observations. </li> <li>If <code>refit</code> is <code>True</code>, the model is trained on each iteration, increasing the training set. </li> <li>If <code>refit</code> is an <code>integer</code>, the model will be trained every that number  of iterations.</li> <li>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is <code>None</code>, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</li> </ul> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> <code>`None`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If <code>None</code>, no intervals are estimated.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>. If there is more than one level, this structure will be repeated for each of them.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def backtesting_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int],\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Backtesting for multi-series and multivariate forecasters.\n\n    - If `refit` is `False`, the model will be trained only once using the \n    `initial_train_size` first observations. \n    - If `refit` is `True`, the model is trained on each iteration, increasing\n    the training set. \n    - If `refit` is an `integer`, the model will be trained every that number \n    of iterations.\n    - If `forecaster` is already trained and `initial_train_size` is `None`,\n    no initial train will be done and all data will be used to evaluate the model.\n    However, the first `len(forecaster.last_window)` observations are needed\n    to create the initial predictors, so no predictions are calculated for them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterAutoregMultiSeries', \n                                         'ForecasterAutoregMultiSeriesCustom', \n                                         'ForecasterAutoregMultiVariate']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterAutoregMultiSeries`, \"\n             \"`ForecasterAutoregMultiSeriesCustom` or `ForecasterAutoregMultiVariate`, \"\n             \"for all other types of forecasters use the functions available in \"\n             f\"the `model_selection` module. Got {type(forecaster).__name__}\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        series                = series,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    if type(forecaster).__name__ in ['ForecasterAutoregMultiSeries', \n                                     'ForecasterAutoregMultiSeriesCustom'] \\\n        and levels is not None and not isinstance(levels, (str, list)):\n        raise TypeError(\n            (\"`levels` must be a `list` of column names, a `str` of a column name \"\n             \"or `None` when using a `ForecasterAutoregMultiSeries` or \"\n             \"`ForecasterAutoregMultiSeriesCustom`. If the forecaster is of type \"\n             \"`ForecasterAutoregMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterAutoregMultiVariate' \\\n        and levels and levels != forecaster.level and levels != [forecaster.level]:\n        warnings.warn(\n            (f\"`levels` argument have no use when the forecaster is of type \"\n             f\"`ForecasterAutoregMultiVariate`. The level of this forecaster is \"\n             f\"'{forecaster.level}', to predict another level, change the `level` \"\n             f\"argument when initializing the forecaster.\"),\n             IgnoredArgumentWarning\n        )\n\n    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        steps                 = steps,\n        levels                = levels,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.grid_search_forecaster_multiseries","title":"<code>grid_search_forecaster_multiseries(forecaster, series, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def grid_search_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster            = forecaster,\n                  series                = series,\n                  param_grid            = param_grid,\n                  steps                 = steps,\n                  metric                = metric,\n                  initial_train_size    = initial_train_size,\n                  fixed_train_size      = fixed_train_size,\n                  gap                   = gap,\n                  allow_incomplete_fold = allow_incomplete_fold,\n                  levels                = levels,\n                  exog                  = exog,\n                  lags_grid             = lags_grid,\n                  refit                 = refit,\n                  n_jobs                = n_jobs,\n                  return_best           = return_best,\n                  verbose               = verbose,\n                  show_progress         = show_progress\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.random_search_forecaster_multiseries","title":"<code>random_search_forecaster_multiseries(forecaster, series, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def random_search_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, \n                                       random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster            = forecaster,\n                  series                = series,\n                  param_grid            = param_grid,\n                  steps                 = steps,\n                  metric                = metric,\n                  initial_train_size    = initial_train_size,\n                  fixed_train_size      = fixed_train_size,\n                  gap                   = gap,\n                  allow_incomplete_fold = allow_incomplete_fold,\n                  levels                = levels,\n                  exog                  = exog,\n                  lags_grid             = lags_grid,\n                  refit                 = refit,\n                  return_best           = return_best,\n                  n_jobs                = n_jobs,\n                  verbose               = verbose,\n                  show_progress         = show_progress\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.backtesting_forecaster_multivariate","title":"<code>backtesting_forecaster_multivariate(forecaster, series, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, n_jobs='auto', verbose=False, show_progress=True)</code>","text":"<p>This function is an alias of backtesting_forecaster_multiseries.</p> <p>Backtesting for multi-series and multivariate forecasters.</p> <p>If <code>refit</code> is False, the model is trained only once using the <code>initial_train_size</code> first observations. If <code>refit</code> is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so  it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> <code>`None`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If <code>None</code>, no intervals are estimated.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>. If there is more than one level, this structure will be repeated for each of them.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def backtesting_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int],\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    This function is an alias of backtesting_forecaster_multiseries.\n\n    Backtesting for multi-series and multivariate forecasters.\n\n    If `refit` is False, the model is trained only once using the `initial_train_size`\n    first observations. If `refit` is True, the model is trained in each iteration\n    increasing the training set. A copy of the original forecaster is created so \n    it is not modified during the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0** \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n\n    )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.grid_search_forecaster_multivariate","title":"<code>grid_search_forecaster_multivariate(forecaster, series, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>This function is an alias of grid_search_forecaster_multiseries.</p> <p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def grid_search_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    This function is an alias of grid_search_forecaster_multiseries.\n\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    results = grid_search_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.random_search_forecaster_multivariate","title":"<code>random_search_forecaster_multivariate(forecaster, series, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, show_progress=True)</code>","text":"<p>This function is an alias of random_search_forecaster_multiseries.</p> <p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>levels</code> <code>str, list</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range</code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_multiseries\\model_selection_multiseries.py</code> <pre><code>def random_search_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: Optional[Union[bool, int]]=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    This function is an alias of random_search_forecaster_multiseries.\n\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    results = random_search_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        param_distributions   = param_distributions,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        n_iter                = n_iter,\n        random_state          = random_state,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    ) \n\n    return results\n</code></pre>"},{"location":"api/model_selection_sarimax.html","title":"<code>model_selection_sarimax</code>","text":""},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.backtesting_sarimax","title":"<code>backtesting_sarimax(forecaster, y, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, alpha=None, interval=None, n_jobs='auto', verbose=False, suppress_warnings_fit=False, show_progress=True)</code>","text":"<p>Backtesting of ForecasterSarimax.</p> <ul> <li>If <code>refit</code> is <code>False</code>, the model will be trained only once using the  <code>initial_train_size</code> first observations. </li> <li>If <code>refit</code> is <code>True</code>, the model is trained on each iteration, increasing the training set. </li> <li>If <code>refit</code> is an <code>integer</code>, the model will be trained every that number  of iterations.</li> </ul> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>`0.05`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>`None`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_value</code> <code>float, list</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\model_selection_sarimax\\model_selection_sarimax.py</code> <pre><code>def backtesting_sarimax(\n    forecaster,\n    y: pd.Series,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    alpha: Optional[float]=None,\n    interval: Optional[list]=None,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=False,\n    suppress_warnings_fit: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[Union[float, list], pd.DataFrame]:\n\"\"\"\n    Backtesting of ForecasterSarimax.\n\n    - If `refit` is `False`, the model will be trained only once using the \n    `initial_train_size` first observations. \n    - If `refit` is `True`, the model is trained on each iteration, increasing\n    the training set. \n    - If `refit` is an `integer`, the model will be trained every that number \n    of iterations.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int\n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**     \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metrics_value : float, list\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterSarimax`, for all other \"\n             \"types of forecasters use the functions available in the other \"\n             \"`model_selection` modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        y                     = y,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        alpha                 = alpha,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    metrics_values, backtest_predictions = _backtesting_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        alpha                 = alpha,\n        interval              = interval,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return metrics_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.grid_search_sarimax","title":"<code>grid_search_sarimax(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, return_best=True, n_jobs='auto', verbose=True, suppress_warnings_fit=False, show_progress=True)</code>","text":"<p>Exhaustive search over specified parameter values for a ForecasterSarimax object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_sarimax\\model_selection_sarimax.py</code> <pre><code>def grid_search_sarimax(\n    forecaster,\n    y: pd.Series,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    suppress_warnings_fit: bool=False,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a ForecasterSarimax object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.random_search_sarimax","title":"<code>random_search_sarimax(forecaster, y, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, n_iter=10, random_state=123, return_best=True, n_jobs='auto', verbose=True, suppress_warnings_fit=False, show_progress=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>`None`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled.  n_iter trades off runtime vs quality of the solution.</p> <code>`10`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>`123`</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>`True`</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. New in version 0.10.0</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection_sarimax\\model_selection_sarimax.py</code> <pre><code>def random_search_sarimax(\n    forecaster,\n    y: pd.Series,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: Optional[Union[bool, int]]=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=True,\n    suppress_warnings_fit: bool=False,\n    show_progress: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n        **New in version 0.9.0**\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n        **New in version 0.10.0**\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return results\n</code></pre>"},{"location":"api/plot.html","title":"<code>plot</code>","text":""},{"location":"api/plot.html#skforecast.plot.plot.plot_residuals","title":"<code>plot_residuals(residuals=None, y_true=None, y_pred=None, fig=None, **fig_kw)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>pandas Series, numpy ndarray</code> <p>Values of residuals. If <code>None</code>, residuals are calculated internally using <code>y_true</code> and <code>y_true</code>.</p> <code>`None`.</code> <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>Ground truth (correct) values. Ignored if residuals is not <code>None</code>.</p> <code>`None`.</code> <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Values of predictions. Ignored if residuals is not <code>None</code>.</p> <code>`None`. </code> <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure() internally.</p> <code>`None`. </code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.figure()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_residuals(\n    residuals: Union[np.ndarray, pd.Series]=None,\n    y_true: Union[np.ndarray, pd.Series]=None,\n    y_pred: Union[np.ndarray, pd.Series]=None,\n    fig: matplotlib.figure.Figure=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Parameters\n    ----------\n    residuals : pandas Series, numpy ndarray, default `None`.\n        Values of residuals. If `None`, residuals are calculated internally using\n        `y_true` and `y_true`.\n    y_true : pandas Series, numpy ndarray, default `None`.\n        Ground truth (correct) values. Ignored if residuals is not `None`.\n    y_pred : pandas Series, numpy ndarray, default `None`. \n        Values of predictions. Ignored if residuals is not `None`.\n    fig : matplotlib.figure.Figure, default `None`. \n        Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure()\n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.figure()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if residuals is None and (y_true is None or y_pred is None):\n        raise ValueError(\n            \"If `residuals` argument is None then, `y_true` and `y_pred` must be provided.\"\n        )\n\n    if residuals is None:\n        residuals = y_pred - y_true\n\n    if fig is None:\n        fig = plt.figure(constrained_layout=True, **fig_kw)\n\n    gs  = matplotlib.gridspec.GridSpec(2, 2, figure=fig)\n    ax1 = plt.subplot(gs[0, :])\n    ax2 = plt.subplot(gs[1, 0])\n    ax3 = plt.subplot(gs[1, 1])\n\n    ax1.plot(residuals)\n    sns.histplot(residuals, kde=True, bins=30, ax=ax2)\n    plot_acf(residuals, ax=ax3, lags=60)\n\n    ax1.set_title(\"Residuals\")\n    ax2.set_title(\"Distribution\")\n    ax3.set_title(\"Autocorrelation\")\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_multivariate_time_series_corr","title":"<code>plot_multivariate_time_series_corr(corr, ax=None, **fig_kw)</code>","text":"<p>Heatmap plot of a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>pandas DataFrame</code> <p>correlation matrix</p> required <code>ax</code> <code>matplotlib.axes.Axes</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()  internally.</p> <code>`None`. </code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_multivariate_time_series_corr(\n    corr: pd.DataFrame,\n    ax: matplotlib.axes.Axes=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Heatmap plot of a correlation matrix.\n\n    Parameters\n    ----------\n    corr : pandas DataFrame\n        correlation matrix\n    ax : matplotlib.axes.Axes, default `None`. \n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() \n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n\n    sns.heatmap(\n        corr,\n        annot=True,\n        linewidths=.5,\n        ax=ax,\n        cmap=sns.color_palette(\"viridis\", as_cmap=True)\n    )\n\n    ax.set_xlabel('Time series')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_distribution","title":"<code>plot_prediction_distribution(bootstrapping_predictions, bw_method=None, **fig_kw)</code>","text":"<p>Ridge plot of bootstrapping predictions. This plot is very useful to understand  the uncertainty of forecasting predictions.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrapping_predictions</code> <code>pandas DataFrame</code> <p>Bootstrapping predictions created with <code>Forecaster.predict_bootstrapping</code>.</p> required <code>bw_method</code> <code>str, scalar, Callable</code> <p>The method used to calculate the estimator bandwidth. This can be 'scott',  'silverman', a scalar constant or a Callable. If None (default), 'scott'  is used. See scipy.stats.gaussian_kde for more information.</p> <code>`None`</code> <code>fig_kw</code> <code>dict</code> <p>All additional keyword arguments are passed to the <code>pyplot.figure</code> call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_prediction_distribution(\n    bootstrapping_predictions: pd.DataFrame,\n    bw_method: Optional[Any]=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Ridge plot of bootstrapping predictions. This plot is very useful to understand \n    the uncertainty of forecasting predictions.\n\n    Parameters\n    ----------\n    bootstrapping_predictions : pandas DataFrame\n        Bootstrapping predictions created with `Forecaster.predict_bootstrapping`.\n    bw_method : str, scalar, Callable, default `None`\n        The method used to calculate the estimator bandwidth. This can be 'scott', \n        'silverman', a scalar constant or a Callable. If None (default), 'scott' \n        is used. See scipy.stats.gaussian_kde for more information.\n    fig_kw : dict\n        All additional keyword arguments are passed to the `pyplot.figure` call.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    index = bootstrapping_predictions.index.astype(str).to_list()[::-1]\n    palette = sns.cubehelix_palette(len(index), rot=-.25, light=.7, reverse=False)\n    fig, axs = plt.subplots(len(index), 1, sharex=True, **fig_kw)\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    for i, step in enumerate(index):\n        plot = (\n            bootstrapping_predictions.loc[step, :]\n            .plot.kde(ax=axs[i], bw_method=bw_method, lw=0.5)\n        )\n\n        # Fill density area\n        x = plot.get_children()[0]._x\n        y = plot.get_children()[0]._y\n        axs[i].fill_between(x, y, color=palette[i])\n        prediction_mean = bootstrapping_predictions.loc[step, :].mean()\n\n        # Closest point on x to the prediction mean\n        idx = np.abs(x - prediction_mean).argmin()\n        axs[i].vlines(x[idx], ymin=0, ymax=y[idx], linestyle=\"dashed\", color='w')\n\n        axs[i].spines['top'].set_visible(False)\n        axs[i].spines['right'].set_visible(False)\n        axs[i].spines['bottom'].set_visible(False)\n        axs[i].spines['left'].set_visible(False)\n        axs[i].set_yticklabels([])\n        axs[i].set_yticks([])\n        axs[i].set_ylabel(step, rotation='horizontal')\n        axs[i].set_xlabel('prediction')\n\n    fig.subplots_adjust(hspace=-0)\n    fig.suptitle('Forecasting distribution per step')\n\n    return fig\n</code></pre>"},{"location":"api/preprocessing.html","title":"<code>preprocessing</code>","text":""},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator","title":"<code>TimeSeriesDifferentiator(order=1)</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differentiated time series of order n. It also reverts the differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>Order of differentiation.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>int</code> <p>Order of differentiation.</p> <code>initial_values</code> <code>list</code> <p>List with the initial value of the time series after each differentiation. This is used to revert the differentiation.</p> <code>last_values</code> <code>list</code> <p>List with the last value of the time series after each differentiation. This is used to revert the differentiation of a new window of data. A new window of data is a time series that starts right after the time series used to fit the transformer.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def __init__(\n    self, \n    order: int=1\n) -&gt; None:\n\n    if not isinstance(order, int):\n        raise TypeError(f\"Parameter 'order' must be an integer. Found {type(order)}.\")\n    if order &lt; 1:\n        raise ValueError(f\"Parameter 'order' must be an integer greater than 0. Found {order}.\")\n\n    self.order = order\n    self.initial_values = []\n    self.last_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the transformer. This method only removes the values stored in <code>self.initial_values</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>TimeSeriesDifferentiator</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def fit(\n    self, \n    X: np.ndarray, \n    y: Any=None\n) -&gt; Self:\n\"\"\"\n    Fits the transformer. This method only removes the values stored in\n    `self.initial_values`.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    self : TimeSeriesDifferentiator\n\n    \"\"\"\n\n    self.initial_values = []\n    self.last_values = []\n\n    for i in range(self.order):\n        if i == 0:\n            self.initial_values.append(X[0])\n            self.last_values.append(X[-1])\n            X_diff = np.diff(X, n=1)\n        else:\n            self.initial_values.append(X_diff[0])\n            self.last_values.append(X_diff[-1])\n            X_diff = np.diff(X_diff, n=1)\n\n    return self\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transforms a time series into a differentiated time series of order n and stores the values needed to revert the differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Differentiated time series. The length of the array is the same as the original time series but the first n=<code>order</code> values are nan.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def transform(\n    self, \n    X: np.ndarray, \n    y: Any=None\n) -&gt; np.ndarray:\n\"\"\"\n    Transforms a time series into a differentiated time series of order n and\n    stores the values needed to revert the differentiation.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Differentiated time series. The length of the array is the same as\n        the original time series but the first n=`order` values are nan.\n\n    \"\"\"\n\n    X_diff = np.diff(X, n=self.order)\n\n    X_diff = np.append((np.full(shape=self.order, fill_value=np.nan)), X_diff)\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Reverts the differentiation. To do so, the input array is assumed to be a differentiated time series of order n that starts right after the the time series used to fit the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def inverse_transform(\n    self, \n    X: np.ndarray, \n    y: Any=None\n) -&gt; np.ndarray:\n\"\"\"\n    Reverts the differentiation. To do so, the input array is assumed to be\n    a differentiated time series of order n that starts right after the\n    the time series used to fit the transformer.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    # Remove initial nan values if present\n    X = X[np.argmax(~np.isnan(X)):]\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.insert(X, 0, self.initial_values[-1])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n        else:\n            X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i+1)])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"<code>inverse_transform_next_window(X, y=None)</code>","text":"<p>Reverts the differentiation. The input array <code>x</code> is assumed to be a  differentiated time series of order n that starts right after the the time series used to fit the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series. It is assumed o start right after the time series used to fit the transformer.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_undiff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def inverse_transform_next_window(\n    self,\n    X: np.ndarray,\n    y: Any=None\n)  -&gt; np.ndarray:\n\"\"\"\n    Reverts the differentiation. The input array `x` is assumed to be a \n    differentiated time series of order n that starts right after the\n    the time series used to fit the transformer.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series. It is assumed o start right after\n        the time series used to fit the transformer.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_undiff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    # Remove initial nan values if present\n    X = X[np.argmax(~np.isnan(X)):]\n\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.cumsum(X, dtype=float) + self.last_values[-1]\n        else:\n            X_undiff = np.cumsum(X_undiff, dtype=float) + self.last_values[-(i+1)]\n\n    return X_undiff\n</code></pre>"},{"location":"api/utils.html","title":"<code>utils</code>","text":""},{"location":"api/utils.html#skforecast.utils.utils.save_forecaster","title":"<code>save_forecaster(forecaster, file_name, verbose=True)</code>","text":"<p>Save forecaster model using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <p>Forecaster created with skforecast library.</p> required <code>file_name</code> <code>str</code> <p>File name given to the object.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster saved.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def save_forecaster(\n    forecaster, \n    file_name: str, \n    verbose: bool=True\n) -&gt; None:\n\"\"\"\n    Save forecaster model using joblib.\n\n    Parameters\n    ----------\n    forecaster: forecaster\n        Forecaster created with skforecast library.\n    file_name: str\n        File name given to the object.\n    verbose: bool, default `True`\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    joblib.dump(forecaster, filename=file_name)\n\n    if verbose:\n        forecaster.summary()\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.load_forecaster","title":"<code>load_forecaster(file_name, verbose=True)</code>","text":"<p>Load forecaster model using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Object file name.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster loaded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>forecaster</code> <code>forecaster</code> <p>Forecaster created with skforecast library.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def load_forecaster(\n    file_name: str,\n    verbose: bool=True\n) -&gt; object:\n\"\"\"\n    Load forecaster model using joblib.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: forecaster\n        Forecaster created with skforecast library.\n\n    \"\"\"\n\n    forecaster = joblib.load(filename=file_name)\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Check lags argument input and generate the corresponding numpy ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>lags</code> <code>Any</code> <p>Lags used as predictors.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -&gt; np.ndarray:\n\"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray\n        Lags used as predictors.\n\n    \"\"\"\n\n    if isinstance(lags, int) and lags &lt; 1:\n        raise ValueError(\"Minimum value of lags allowed is 1.\")\n\n    if isinstance(lags, (list, np.ndarray)):\n        for lag in lags:\n            if not isinstance(lag, (int, np.int64, np.int32)):\n                raise TypeError(\"All values in `lags` must be int.\")\n\n    if isinstance(lags, (list, range, np.ndarray)) and min(lags) &lt; 1:\n        raise ValueError(\"Minimum value of lags allowed is 1.\")\n\n    if isinstance(lags, int):\n        lags = np.arange(lags) + 1\n    elif isinstance(lags, (list, range)):\n        lags = np.array(lags)\n    elif isinstance(lags, np.ndarray):\n        lags = lags\n    else:\n        if not forecaster_name == 'ForecasterAutoregMultiVariate':\n            raise TypeError(\n                (\"`lags` argument must be an int, 1d numpy ndarray, range or list. \"\n                 f\"Got {type(lags)}.\")\n            )\n        else:\n            raise TypeError(\n                (\"`lags` argument must be a dict, int, 1d numpy ndarray, range or list. \"\n                 f\"Got {type(lags)}.\")\n            )\n\n    return lags\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, regressor, weight_func, series_weights)</code>","text":"<p>Check weights arguments, <code>weight_func</code> and <code>series_weights</code> for the different  forecasters. Create <code>source_code_weight_func</code>, source code of the custom  function(s) used to create weights.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>Regressor of the forecaster.</p> required <code>weight_func</code> <code>Callable, dict</code> <p>Argument <code>weight_func</code> of the forecaster.</p> required <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> required <p>Returns:</p> Name Type Description <code>weight_func</code> <code>Callable, dict</code> <p>Argument <code>weight_func</code> of the forecaster.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Argument <code>source_code_weight_func</code> of the forecaster.</p> <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -&gt; Tuple[Union[Callable, dict], Union[str, dict], dict]:\n\"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries', 'ForecasterAutoregMultiSeriesCustom']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(regressor, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only the keys that are used by the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>object</code> <p>Regressor object.</p> required <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to pass to the `fit' method of the forecaster.</p> <code>`None`</code> <p>Returns:</p> Name Type Description <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to be passed to the <code>fit</code> method of the  regressor after removing the unused keys.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict]=None\n) -&gt; dict:\n\"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n\n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k:v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_y","title":"<code>check_y(y)</code>","text":"<p>Raise Exception if <code>y</code> is not pandas Series or if it has missing values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_y(\n    y: Any\n) -&gt; None:\n\"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n\n    Parameters\n    ----------\n    y : Any\n        Time series values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\"`y` must be a pandas Series.\")\n\n    if y.isnull().any():\n        raise ValueError(\"`y` has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True)</code>","text":"<p>Raise Exception if <code>exog</code> is not pandas Series or pandas DataFrame. If <code>allow_nan = True</code>, issue a warning if <code>exog</code> contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Any</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows the presence of NaN values in <code>exog</code>. If False (default), issue a warning if <code>exog</code> contains NaN values.</p> <code>`True`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_exog(\n    exog: Any,\n    allow_nan: bool=True\n) -&gt; None:\n\"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n\n    Parameters\n    ----------\n    exog : Any\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\"`exog` must be a pandas Series or DataFrame.\")\n\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(\n                (\"`exog` has missing values. Most machine learning models do not allow \"\n                 \"missing values. Fitting the forecaster may fail.\"), \n                 MissingValuesExogWarning\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Store dtypes of <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Name Type Description <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with the dtypes in <code>exog</code>.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def get_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -&gt; dict:\n\"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n\n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog)</code>","text":"<p>Raise Exception if <code>exog</code> has categorical columns with non integer values. This is needed when using machine learning regressors that allow categorical features. Issue a Warning if <code>exog</code> has columns that are not <code>init</code>, <code>float</code>, or <code>category</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -&gt; None:\n\"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_exog(exog=exog, allow_nan=False)\n\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(\n                (\"`exog` may contain only `int`, `float` or `category` dtypes. Most \"\n                 \"machine learning models do not allow other types of values. \"\n                 \"Fitting the forecaster may fail.\"), DataTypeWarning\n            )\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError(\n                    (\"Categorical columns in exog must contain only integer values. \"\n                     \"See skforecast docs for more info about how to include \"\n                     \"categorical features https://skforecast.org/\"\n                     \"latest/user_guides/categorical-features.html\")\n                )\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', \n        'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(\n                (\"`exog` may contain only `int`, `float` or `category` dtypes. Most \"\n                 \"machine learning models do not allow other types of values. \"\n                 \"Fitting the forecaster may fail.\"), DataTypeWarning\n            )\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int,\n        np.int32, np.int64]:\n            raise TypeError(\n                (\"If exog is of type category, it must contain only integer values. \"\n                 \"See skforecast docs for more info about how to include \"\n                 \"categorical features https://skforecast.org/\"\n                 \"latest/user_guides/categorical-features.html\")\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_interval","title":"<code>check_interval(interval=None, alpha=None)</code>","text":"<p>Check provided confidence interval sequence is valid.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_interval(\n    interval: list=None,\n    alpha: float=None\n) -&gt; None:\n\"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError(\n                (\"`interval` must be a `list`. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                (\"`interval` must contain exactly 2 values, respectively the \"\n                 \"lower and upper interval bounds. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if (interval[0] &lt; 0.) or (interval[0] &gt;= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.) or (interval[1] &gt; 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                (\"`alpha` must be a `float`. For example, interval of 95% \"\n                 \"should be as `alpha = 0.05`.\")\n            )\n\n        if (alpha &lt;= 0.) or (alpha &gt;= 1):\n            raise ValueError(\n                f\"`alpha` must have a value between 0 and 1. Got {alpha}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, fitted, included_exog, index_type, index_freq, window_size, last_window=None, last_window_exog=None, exog=None, exog_type=None, exog_col_names=None, interval=None, alpha=None, max_steps=None, levels=None, series_col_names=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>steps</code> <code>int, list</code> <p>Number of future steps predicted.</p> required <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> required <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> required <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to  <code>max_lag</code>.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the series used to create the predictors (lags) need in the  first iteration of prediction (t + 1).</p> <code>`None`</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code> in  ForecasterSarimax predictions.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>`None`</code> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>`None`</code> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>`None`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <code>max_steps</code> <code>Optional[int]</code> <p>Maximum number of steps allowed (<code>ForecasterAutoregDirect</code> and  <code>ForecasterAutoregMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>str, list</code> <p>Time series to be predicted (<code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>).</p> <code>`None`</code> <code>series_col_names</code> <code>list</code> <p>Names of the columns used during fit (<code>ForecasterAutoregMultiSeries</code>,  <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> <code>`None`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, list],\n    fitted: bool,\n    included_exog: bool,\n    index_type: type,\n    index_freq: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog_type: Optional[Union[type, None]]=None,\n    exog_col_names: Optional[Union[list, None]]=None,\n    interval: Optional[list]=None,\n    alpha: Optional[float]=None,\n    max_steps: Optional[int]=None,\n    levels: Optional[Union[str, list]]=None,\n    series_col_names: Optional[list]=None\n) -&gt; None:\n\"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    steps : int, list\n        Number of future steps predicted.\n    fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    included_exog : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type : type\n        Type of index of the input used in training.\n    index_freq : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_col_names : list, default `None`\n        Names of columns of `exog` if `exog` used in training was a pandas\n        DataFrame.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterAutoregDirect` and \n        `ForecasterAutoregMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterAutoregMultiSeries` and\n        `ForecasterAutoregMultiSeriesCustom`).\n    series_col_names : list, default `None`\n        Names of the columns used during fit (`ForecasterAutoregMultiSeries`, \n        `ForecasterAutoregMultiSeriesCustom` and `ForecasterAutoregMultiVariate`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This Forecaster instance is not fitted yet. Call `fit` with \"\n             \"appropriate arguments before using predict.\")\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n           (f\"The minimum value of `steps` must be equal to or greater than 1. \"\n            f\"Got {min(steps)}.\")\n        )\n\n    if max_steps is not None:\n        if max(steps) &gt; max_steps:\n            raise ValueError(\n                (f\"The maximum value of `steps` must be less than or equal to \"\n                 f\"the value of steps defined when initializing the forecaster. \"\n                 f\"Got {max(steps)}, but the maximum is {max_steps}.\")\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterAutoregMultiSeriesCustom']:\n        if levels is not None and not isinstance(levels, (str, list)):\n            raise TypeError(\n                (\"`levels` must be a `list` of column names, a `str` of a \"\n                 \"column name or `None`.\")\n            )\n        if len(set(levels) - set(series_col_names)) != 0:\n            raise ValueError(\n                f\"`levels` must be in `series_col_names` : {series_col_names}.\"\n            )\n\n    if exog is None and included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. \"\n             \"Same variable/s must be provided when predicting.\")\n        )\n\n    if exog is not None and not included_exog:\n        raise ValueError(\n            (\"Forecaster trained without exogenous variable/s. \"\n             \"`exog` must be `None` when predicting.\")\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterAutoregMultiSeriesCustom',\n                           'ForecasterAutoregMultiVariate']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries', \n                               'ForecasterAutoregMultiSeriesCustom'] and \\\n            len(set(levels) - set(last_window.columns)) != 0:\n            raise ValueError(\n                (f\"`last_window` must contain a column(s) named as the level(s) \"\n                 f\"to be predicted.\\n\"\n                 f\"    `levels` : {levels}.\\n\"\n                 f\"    `last_window` columns : {list(last_window.columns)}.\")\n            )\n\n        if forecaster_name == 'ForecasterAutoregMultiVariate' and \\\n            (series_col_names != list(last_window.columns)):\n            raise ValueError(\n                (f\"`last_window` columns must be the same as `series` column names.\\n\"\n                 f\"    `last_window` columns : {list(last_window.columns)}.\\n\"\n                 f\"    `series` columns      : {series_col_names}.\")\n            )    \n    else:    \n        if not isinstance(last_window, pd.Series):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series. Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) &lt; window_size:\n        raise ValueError(\n            (f\"`last_window` must have as many values as needed to \"\n             f\"generate the predictors. For this forecaster it is {window_size}.\")\n        )\n    if last_window.isnull().any().all():\n        raise ValueError(\n            (\"`last_window` has missing values.\")\n        )\n    _, last_window_index = preprocess_last_window(\n                               last_window  = last_window.iloc[:0],\n                               return_values = False\n                           ) \n    if not isinstance(last_window_index, index_type):\n        raise TypeError(\n            (f\"Expected index of type {index_type} for `last_window`. \"\n             f\"Got {type(last_window_index)}.\")\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq:\n            raise TypeError(\n                (f\"Expected frequency of type {index_freq} for `last_window`. \"\n                 f\"Got {last_window_index.freqstr}.\")\n            )\n\n    # Checks exog\n    if exog is not None:\n        # Check type, nulls and expected type\n        if not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(\"`exog` must be a pandas Series or DataFrame.\")\n        if exog.isnull().any().any():\n            warnings.warn(\n                (\"`exog` has missing values. Most of machine learning models do \"\n                 \"not allow missing values. `predict` method may fail.\"), \n                 MissingValuesExogWarning\n            )\n        if not isinstance(exog, exog_type):\n            raise TypeError(\n                f\"Expected type for `exog`: {exog_type}. Got {type(exog)}.\"    \n            )\n\n        # Check exog has many values as distance to max step predicted\n        last_step = max(steps) if isinstance(steps, list) else steps\n        if len(exog) &lt; last_step:\n            raise ValueError(\n                (f\"`exog` must have at least as many values as the distance to \"\n                 f\"the maximum step predicted, {last_step}.\")\n            )\n\n        # Check all columns are in the pandas DataFrame\n        if isinstance(exog, pd.DataFrame):\n            col_missing = set(exog_col_names).difference(set(exog.columns))\n            if col_missing:\n                raise ValueError(\n                    (f\"Missing columns in `exog`. Expected {exog_col_names}. \"\n                     f\"Got {exog.columns.to_list()}.\") \n                )\n\n        # Check index dtype and freq\n        _, exog_index = preprocess_exog(\n                            exog          = exog.iloc[:0, ],\n                            return_values = False\n                        )\n        if not isinstance(exog_index, index_type):\n            raise TypeError(\n                (f\"Expected index of type {index_type} for `exog`. \"\n                 f\"Got {type(exog_index)}.\")\n            )   \n        if isinstance(exog_index, pd.DatetimeIndex):\n            if not exog_index.freqstr == index_freq:\n                raise TypeError(\n                    (f\"Expected frequency of type {index_freq} for `exog`. \"\n                     f\"Got {exog_index.freqstr}.\")\n                )\n\n        # Check exog starts one step ahead of last_window end.\n        expected_index = expand_index(last_window.index, 1)[0]\n        if expected_index != exog.index[0]:\n            raise ValueError(\n                (f\"To make predictions `exog` must start one step ahead of `last_window`.\\n\"\n                 f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                 f\"    `exog` starts at      : {exog.index[0]}.\\n\"\n                 f\"     Expected index       : {expected_index}.\")\n            )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not included_exog:\n                raise ValueError(\n                    (\"Forecaster trained without exogenous variable/s. \"\n                     \"`last_window_exog` must be `None` when predicting.\")\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`last_window_exog` must be a pandas Series or a \"\n                     f\"pandas DataFrame. Got {type(last_window_exog)}.\")\n                )\n            if len(last_window_exog) &lt; window_size:\n                raise ValueError(\n                    (f\"`last_window_exog` must have as many values as needed to \"\n                     f\"generate the predictors. For this forecaster it is {window_size}.\")\n                )\n            if last_window_exog.isnull().any().all():\n                warnings.warn(\n                (\"`last_window_exog` has missing values. Most of machine learning \"\n                 \"models do not allow missing values. `predict` method may fail.\"),\n                MissingValuesExogWarning\n            )\n            _, last_window_exog_index = preprocess_last_window(\n                                            last_window   = last_window_exog.iloc[:0],\n                                            return_values = False\n                                        ) \n            if not isinstance(last_window_exog_index, index_type):\n                raise TypeError(\n                    (f\"Expected index of type {index_type} for `last_window_exog`. \"\n                     f\"Got {type(last_window_exog_index)}.\")\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq:\n                    raise TypeError(\n                        (f\"Expected frequency of type {index_freq} for \"\n                         f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\")\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_col_names).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        (f\"Missing columns in `exog`. Expected {exog_col_names}. \"\n                         f\"Got {last_window_exog.columns.to_list()}.\") \n                    )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_y","title":"<code>preprocess_y(y, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series, pandas DataFrame</code> <p>Time series.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>y</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>y_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>y</code>.</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>y</code> modified according to the rules.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool=True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n\"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"`y` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"`y` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy() if return_values else None\n\n    return y_values, y_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_last_window","title":"<code>preprocess_last_window(last_window, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Time series values.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>last_window</code> as numpy ndarray. This option  is intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>last_window_values</code> <code>numpy ndarray</code> <p>Numpy array with values of <code>last_window</code>.</p> <code>last_window_index</code> <code>pandas Index</code> <p>Index of <code>last_window</code> modified according to the rules.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def preprocess_last_window(\n    last_window: Union[pd.Series, pd.DataFrame],\n    return_values: bool=True\n ) -&gt; Tuple[np.ndarray, pd.Index]:\n\"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn(\n            (\"`last_window` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n    else:\n        warnings.warn(\n            (\"`last_window` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n\n    last_window_values = last_window.to_numpy() if return_values else None\n\n    return last_window_values, last_window_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_exog","title":"<code>preprocess_exog(exog, return_values=True)</code>","text":"<p>Return values and index of series or data frame separately. Index is overwritten  according to the next rules:</p> <ul> <li>If index is of type <code>DatetimeIndex</code> and has frequency, nothing is  changed.</li> <li>If index is of type <code>RangeIndex</code>, nothing is changed.</li> <li>If index is of type <code>DatetimeIndex</code> but has no frequency, a  <code>RangeIndex</code> is created.</li> <li>If index is not of type <code>DatetimeIndex</code>, a <code>RangeIndex</code> is created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>exog</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>`True`</code> <p>Returns:</p> Name Type Description <code>exog_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>exog</code>.</p> <code>exog_index</code> <code>pandas Index</code> <p>Index of <code>exog</code> modified according to the rules.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def preprocess_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    return_values: bool=True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n\"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn(\n            (\"`exog` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    else:\n        warnings.warn(\n            (\"`exog` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    exog_values = exog.to_numpy() if return_values else None\n\n    return exog_values, exog_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.cast_exog_dtypes","title":"<code>cast_exog_dtypes(exog, exog_dtypes)</code>","text":"<p>Cast <code>exog</code> to a specified types. This is done because, for a forecaster to  accept a categorical exog, it must contain only integer values. Due to the  internal modifications of numpy, the values may be casted to <code>float</code>, so  they have to be re-converted to <code>int</code>.</p> <ul> <li>If <code>exog</code> is a pandas Series, <code>exog_dtypes</code> must be a dict with a  single value.</li> <li>If <code>exog_dtypes</code> is <code>category</code> but the current type of <code>exog</code> is <code>float</code>,  then the type is cast to <code>int</code> and then to <code>category</code>. </li> </ul> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with name and type of the series or data frame columns.</p> required <p>Returns:</p> Name Type Description <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables casted to the indicated dtypes.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def cast_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    exog_dtypes: dict,\n) -&gt; Union[pd.Series, pd.DataFrame]: # pragma: no cover\n\"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k:v for k, v in exog_dtypes.items() if k in exog.columns}\n\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes==float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>pandas DataFrame</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def exog_to_direct(\n    exog: Union[pd.Series, pd.DataFrame],\n    steps: int\n)-&gt; pd.DataFrame:\n\"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : pandas DataFrame\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_column_transformed.index = pd.RangeIndex(len(exog_column_transformed))\n        exog_column_transformed.columns = [f\"{col}_step_{i+1}\" \n                                           for col in exog_column_transformed.columns]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = pd.concat(exog_transformed, axis=1, copy=False)\n    else:\n        exog_transformed = exog_column_transformed\n\n    exog_transformed.index = exog_idx[-len(exog_transformed):]\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>numpy ndarray, shape(samples,)</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>numpy ndarray</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray,\n    steps: int\n)-&gt; np.ndarray:\n\"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : numpy ndarray, shape(samples,)\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : numpy ndarray\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, np.ndarray):\n        raise TypeError(f\"`exog` must be a numpy ndarray. Got {type(exog)}.\")\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog[i : n_rows - (steps - 1 - i)]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = np.concatenate(exog_transformed, axis=1)\n    else:\n        exog_transformed = exog_column_transformed\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index of length <code>steps</code> starting at the end of the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index, None</code> <p>Original index.</p> required <code>steps</code> <code>int</code> <p>Number of steps to expand.</p> required <p>Returns:</p> Name Type Description <code>new_index</code> <code>pandas Index</code> <p>New index.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def expand_index(\n    index: Union[pd.Index, None], \n    steps: int\n) -&gt; pd.Index:\n\"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n\n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if isinstance(index, pd.Index):\n\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n    else: \n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n\n    return new_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_series","title":"<code>transform_series(series, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas Series with a scikit-learn alike transformer (preprocessor). The transformer used must have the following methods: fit,  transform, fit_transform and inverse_transform. ColumnTransformers are not  allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series</code> <p>Series to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer (preprocessor).</code> <p>scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed  since they do not have inverse_transform method.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>`False`</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>series_transformed</code> <code>pandas Series, pandas DataFrame</code> <p>Transformed Series. Depending on the transformer used, the output may  be a Series or a DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def transform_series(\n    series: pd.Series,\n    transformer,\n    fit: bool=False,\n    inverse_transform: bool=False\n) -&gt; Union[pd.Series, pd.DataFrame]:\n\"\"\"      \n    Transform raw values of pandas Series with a scikit-learn alike transformer\n    (preprocessor). The transformer used must have the following methods: fit, \n    transform, fit_transform and inverse_transform. ColumnTransformers are not \n    allowed since they do not have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer (preprocessor).\n        scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform. ColumnTransformers are not allowed \n        since they do not have inverse_transform method.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n\n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            (f\"`series` argument must be a pandas Series. Got {type(series)}.\")\n        )\n\n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n\n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    if inverse_transform:\n        values_transformed = transformer.inverse_transform(data)\n    else:\n        values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.flatten(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer. <code>inverse_transform</code> is not  available when using ColumnTransformers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor or ColumnTransformer.</code> <p>scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>`False`</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>`False`</code> <p>Returns:</p> Name Type Description <code>df_transformed</code> <code>pandas DataFrame</code> <p>Transformed DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer,\n    fit: bool=False,\n    inverse_transform: bool=False\n) -&gt; pd.DataFrame:\n\"\"\"      \n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. `inverse_transform` is not \n    available when using ColumnTransformers.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor or ColumnTransformer.\n        scikit-learn alike transformer, preprocessor or ColumnTransformer.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise Exception(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_optional_dependency(\n    package_name: str\n) -&gt; None:\n\"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.multivariate_time_series_corr","title":"<code>multivariate_time_series_corr(time_series, other, lags, method='pearson')</code>","text":"<p>Compute correlation between a time_series and the lagged values of other  time series. </p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>pandas Series</code> <p>Target time series.</p> required <code>other</code> <code>pandas DataFrame</code> <p>Time series whose lagged values are correlated to <code>time_series</code>.</p> required <code>lags</code> <code>int, list, numpy ndarray</code> <p>Lags to be included in the correlation analysis.</p> required <code>method</code> <code>str</code> <ul> <li>'pearson': standard correlation coefficient.</li> <li>'kendall': Kendall Tau correlation coefficient.</li> <li>'spearman': Spearman rank correlation.</li> </ul> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>corr</code> <code>pandas DataFrame</code> <p>Correlation values.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: Union[int, list, np.array],\n    method: str='pearson'\n)-&gt; pd.DataFrame:\n\"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n\n    return corr\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_backtesting_input","title":"<code>check_backtesting_input(forecaster, steps, metric, y=None, series=None, initial_train_size=None, fixed_train_size=True, gap=0, allow_incomplete_fold=True, refit=False, interval=None, alpha=None, n_boot=500, random_state=123, in_sample_residuals=True, n_jobs='auto', verbose=False, show_progress=True)</code>","text":"<p>This is a helper function to check most inputs of backtesting functions in  modules <code>model_selection</code>, <code>model_selection_multiseries</code> and  <code>model_selection_sarimax</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model.</p> required <code>steps</code> <code>int, list</code> <p>Number of future steps predicted.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>pandas DataFrame</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code>  is already trained, no initial train is done and all data is used to  evaluate the model.</p> <code>`None`</code> <code>fixed_train_size</code> <code>bool</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>`True`</code> <code>gap</code> <code>int</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>`0`</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>`True`</code> <code>refit</code> <code>bool, int</code> <p>Whether to re-fit the forecaster in each iteration. If <code>refit</code> is an integer,  the Forecaster will be trained every that number of iterations.</p> <code>`False`</code> <code>interval</code> <code>list</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive.</p> <code>`None`</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>`None`</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>`500`</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>`123`</code> <code>in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>`True`</code> <code>n_jobs</code> <code>int, auto</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the fuction skforecast.utils.select_n_jobs_fit_forecaster. New in version 0.9.0</p> <code>`'auto'`</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>`False`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_backtesting_input(\n    forecaster: object,\n    steps: int,\n    metric: Union[str, Callable, list],\n    y: Optional[pd.Series]=None,\n    series: Optional[pd.DataFrame]=None,\n    initial_train_size: Optional[int]=None,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    refit: Optional[Union[bool, int]]=False,\n    interval: Optional[list]=None,\n    alpha: Optional[float]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    n_jobs: Optional[Union[int, str]]='auto',\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; None:\n\"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`, `model_selection_multiseries` and \n    `model_selection_sarimax`.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    steps : int, list\n        Number of future steps predicted.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    y : pandas Series\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame\n        Training time series for multi-series forecasters.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` \n        is already trained, no initial train is done and all data is used to \n        evaluate the model.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    refit : bool, int, default `False`\n        Whether to re-fit the forecaster in each iteration. If `refit` is an integer, \n        the Forecaster will be trained every that number of iterations.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n            The number of jobs to run in parallel. If `-1`, then the number of jobs is \n            set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n            skforecast.utils.select_n_jobs_fit_forecaster.\n            **New in version 0.9.0**\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecasters_uni = ['ForecasterAutoreg', 'ForecasterAutoregCustom', \n                       'ForecasterAutoregDirect', 'ForecasterSarimax']\n    forecasters_multi = ['ForecasterAutoregMultiSeries', \n                         'ForecasterAutoregMultiSeriesCustom', \n                         'ForecasterAutoregMultiVariate']\n\n    if type(forecaster).__name__ in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    if type(forecaster).__name__ in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n\n    if not isinstance(steps, (int, np.integer)) or steps &lt; 1:\n        raise TypeError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n    if not isinstance(gap, (int, np.integer)) or gap &lt; 0:\n        raise TypeError(\n            f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n        )\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            (f\"`metric` must be a string, a callable function, or a list containing \"\n             f\"multiple strings and/or callables. Got {type(metric)}.\")\n        )\n\n    if initial_train_size is not None:\n        if not isinstance(initial_train_size, (int, np.integer)):\n            raise TypeError(\n                (f\"If used, `initial_train_size` must be an integer greater than the \"\n                 f\"window_size of the forecaster. Got type {type(initial_train_size)}.\")\n            )\n        if initial_train_size &gt;= data_length:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer smaller \"\n                 f\"than the length of `{data_name}` ({data_length}).\")\n            )    \n        if initial_train_size &lt; forecaster.window_size:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer greater than \"\n                 f\"the window_size of the forecaster ({forecaster.window_size}).\")\n            )\n        if initial_train_size + gap &gt;= data_length:\n            raise ValueError(\n                (f\"The combination of initial_train_size {initial_train_size} and \"\n                 f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                 f\"({data_length}).\")\n            )\n        if data_name == 'series':\n            for serie in series:\n                if np.isnan(series[serie].to_numpy()[:initial_train_size]).all():\n                    raise ValueError(\n                        (f\"All values of series '{serie}' are NaN. When working \"\n                         f\"with series of different lengths, make sure that \"\n                         f\"`initial_train_size` has an appropriate value so that \"\n                         f\"all series reach the first non-null value.\")\n                    )\n    else:\n        if type(forecaster).__name__ == 'ForecasterSarimax':\n            raise ValueError(\n                (f\"`initial_train_size` must be an integer smaller than the \"\n                 f\"length of `{data_name}` ({data_length}).\")\n            )    \n        else:\n            if not forecaster.fitted:\n                raise NotFittedError(\n                    (\"`forecaster` must be already trained if no `initial_train_size` \"\n                     \"is provided.\")\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if not isinstance(fixed_train_size, bool):\n        raise TypeError(\"`fixed_train_size` must be a boolean: `True`, `False`.\")\n    if not isinstance(allow_incomplete_fold, bool):\n        raise TypeError(\"`allow_incomplete_fold` must be a boolean: `True`, `False`.\")\n    if not isinstance(refit, (bool, int, np.integer)) or refit &lt; 0:\n        raise TypeError(f\"`refit` must be a boolean or an integer greater than 0. Got {refit}.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot &lt; 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state &lt; 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(in_sample_residuals, bool):\n        raise TypeError(\"`in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(verbose, bool):\n        raise TypeError(\"`verbose` must be a boolean: `True`, `False`.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) &lt; steps:\n        raise ValueError(\n            (f\"There is not enough data to evaluate {steps} steps in a single \"\n             f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n             f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n             f\"    Steps                   : {steps}\")\n        )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.select_n_jobs_backtesting","title":"<code>select_n_jobs_backtesting(forecaster_name, regressor_name, refit)</code>","text":"<p>Select the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.</p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If <code>refit</code> is an integer, then n_jobs=1. This is because parallelization doesn't  work with intermittent refit.</li> <li>If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom' and regressor_name is a linear regressor, then n_jobs=1.</li> <li>If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom', regressor_name is not a linear regressor and refit=<code>True</code>, then n_jobs=cpu_count().</li> <li>If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom', regressor_name is not a linear regressor and refit=<code>False</code>, then n_jobs=1.</li> <li>If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and refit=<code>True</code>, then n_jobs=cpu_count().</li> <li>If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and refit=<code>False</code>, then n_jobs=1.</li> <li>If forecaster_name is 'ForecasterAutoregMultiseries', then n_jobs=cpu_count().</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>The type of Forecaster.</p> required <code>regressor_name</code> <code>str</code> <p>The type of regressor.</p> required <code>refit</code> <code>bool, int</code> <p>If the forecaster is refitted during the backtesting process.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def select_n_jobs_backtesting(\n    forecaster_name: str,\n    regressor_name: str,\n    refit: Union[bool, int]\n) -&gt; int:\n\"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then n_jobs=1. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom' and\n    regressor_name is a linear regressor, then n_jobs=1.\n    - If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom',\n    regressor_name is not a linear regressor and refit=`True`, then\n    n_jobs=cpu_count().\n    - If forecaster_name is 'ForecasterAutoreg' or 'ForecasterAutoregCustom',\n    regressor_name is not a linear regressor and refit=`False`, then\n    n_jobs=1.\n    - If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and refit=`True`, then n_jobs=cpu_count().\n    - If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and refit=`False`, then n_jobs=1.\n    - If forecaster_name is 'ForecasterAutoregMultiseries', then n_jobs=cpu_count().\n\n    Parameters\n    ----------\n    forecaster_name : str\n        The type of Forecaster.\n    regressor_name : str\n        The type of regressor.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterAutoreg', 'ForecasterAutoregCustom']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            else:\n                n_jobs = joblib.cpu_count() if refit else 1\n        elif forecaster_name in ['ForecasterAutoregDirect', 'ForecasterAutoregMultiVariate']:\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterAutoregMultiseries', 'ForecasterAutoregMultiSeriesCustom']:\n            n_jobs = joblib.cpu_count()\n        elif forecaster_name in ['ForecasterSarimax']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.select_n_jobs_fit_forecaster","title":"<code>select_n_jobs_fit_forecaster(forecaster_name, regressor_name)</code>","text":"<p>Select the optimal number of jobs to use in the fitting process. This selection is based on heuristics and is not guaranteed to be optimal. </p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate' and regressor_name is a linear regressor, then n_jobs=1, otherwise n_jobs=cpu_count().</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>The type of Forecaster.</p> required <code>regressor_name</code> <code>str</code> <p>The type of regressor.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor_name: str,\n) -&gt; int:\n\"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n\n    The number of jobs is chosen as follows:\n\n    - If forecaster_name is 'ForecasterAutoregDirect' or 'ForecasterAutoregMultiVariate'\n    and regressor_name is a linear regressor, then n_jobs=1, otherwise n_jobs=cpu_count().\n\n    Parameters\n    ----------\n    forecaster_name : str\n        The type of Forecaster.\n    regressor_name : str\n        The type of regressor.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterAutoregDirect', 'ForecasterAutoregMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        else:\n            n_jobs = joblib.cpu_count()\n    else:\n        n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"authors/authors.html","title":"Authors","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.</p> <p>A detailed overview on how to contribute can be found in the contributing guide.</p>"},{"location":"authors/authors.html#core-development-team","title":"Core Development Team","text":"<ul> <li> <p>Joaqu\u00edn Amat Rodrigo (j.amatrodrigo@gmail.com)</p> </li> <li> <p>Javier Escobar Ortiz (javier.escobar.ortiz@gmail.com)</p> </li> </ul>"},{"location":"authors/authors.html#contributions","title":"Contributions","text":"<ul> <li> <p>Josh Wong, josh-wong</p> </li> <li> <p>Edgar Bahilo Rodr\u00edguez, edgBR</p> </li> <li> <p>Mwainwright, syndct</p> </li> </ul>"},{"location":"examples/examples.html","title":"Examples and tutorials","text":""},{"location":"examples/examples.html#english","title":"English","text":"<p> Skforecast: time series forecasting with Python and Scikit-learn </p> <p> ARIMA and SARIMAX models</p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost </p> <p> Modelling time series trend with tree based models</p> <p> Forecasting electricity demand with Python </p> <p> Forecasting web traffic with machine learning and Python </p> <p> Bitcoin price prediction with Python</p> <p> Prediction intervals in forecasting models</p> <p> Multi-series forecasting</p> <p> Reducing the influence of Covid-19 on time series forecasting models</p> <p> Forecasting time series with missing values</p> <p> Intermittent demand forecasting</p> <p></p>"},{"location":"examples/examples.html#espanol","title":"Espa\u00f1ol","text":"<p> Skforecast: forecasting series temporales con Python y Scikit-learn </p> <p> Modelos ARIMA y SARIMAX</p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost </p> <p> Modelar series temporales con tendencia utilizando modelos de \u00e1rboles</p> <p> Forecasting de la demanda el\u00e9ctrica</p> <p> Forecasting de las visitas a una p\u00e1gina web </p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p> <p> Intervalos de predicci\u00f3n en modelos de forecasting</p> <p> Multi-series forecasting</p> <p> Predicci\u00f3n demanda intermitente</p>"},{"location":"faq/cyclical-features-time-series.html","title":"Cyclical features in time series","text":"<p>Cyclical features play an important role in time series prediction because they capture recurring patterns or oscillations within a data set. These patterns repeat at fixed intervals, and the effective incorporation of cyclical features into a machine learning model requires careful preprocessing and feature engineering.</p> <p>Due to the circular nature of cyclical features, it is not recommended to use them directly as numerical inputs in a machine learning model. Instead, they should be encoded in a format that captures their cyclical behavior. There are several common encoding techniques:</p> <ul> <li><p>One-hot encoding: If the cyclical feature consists of distinct categories, such as seasons or months, one-hot encoding can be used. This approach creates binary variables for each category, allowing the model to understand the presence or absence of specific categories.</p> </li> <li><p>Trigonometric coding: For periodic features such as time of day or day of the week, trigonometric functions such as sine and cosine can be used for coding. By mapping the cyclic feature onto a unit circle, these functions preserve the cyclic relationships. In addition, this method introduces only two additional features, making it an efficient coding technique.</p> </li> <li><p>Basis functions: Basis functions are mathematical functions that span a vector space and can be used to represent other functions within that space. When using basis functions, the cyclic feature is transformed into a new set of features based on the chosen basis functions. Some commonly used basis functions for encoding cyclic features include Fourier basis functions, B-spline basis functions, and Gaussian basis functions. B-splines are a way to approximate nonlinear functions using a piecewise combination of polynomials.</p> </li> </ul> <p>By applying these encoding techniques, cyclic features can be effectively incorporated into a machine learning model, allowing it to capture and exploit the valuable recurring patterns present in time series data.</p> <p>  \u00a0 Note </p> <p>The following examples are is inspired by Time-related feature engineering, scikit-lego\u2019s documentation and Three Approaches to Encoding Time Information as Features for ML Models By Eryk Lewinson.</p> In\u00a0[6]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklego.preprocessing import RepeatingBasisFunction\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n\n# Warnings configuration\n# ==============================================================================\nimport warnings\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import SplineTransformer from sklearn.compose import make_column_transformer from sklego.preprocessing import RepeatingBasisFunction from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster  # Warnings configuration # ============================================================================== import warnings In\u00a0[7]: Copied! <pre># Data simulation\n# ==============================================================================\nnp.random.seed(123)\ndates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\")\ndata = pd.DataFrame(index=dates)\ndata.index.name = \"date\"\ndata[\"day_idx\"] = range(len(data))\ndata['month'] = data.index.month\n\n# Create the components that will be combined to get the target series\nsignal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi)\nsignal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2)\nnoise = np.random.normal(0, 0.85, len(data))\ny = signal_1 + signal_2 + noise\n\ndata[\"y\"] = y\ndata = data[[\"y\", \"month\"]]\ndata.head(3)\n</pre> # Data simulation # ============================================================================== np.random.seed(123) dates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\") data = pd.DataFrame(index=dates) data.index.name = \"date\" data[\"day_idx\"] = range(len(data)) data['month'] = data.index.month  # Create the components that will be combined to get the target series signal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi) signal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2) noise = np.random.normal(0, 0.85, len(data)) y = signal_1 + signal_2 + noise  data[\"y\"] = y data = data[[\"y\", \"month\"]] data.head(3) Out[7]: y month date 2020-01-01 2.928244 1 2020-01-02 4.866145 1 2020-01-03 4.425159 1 In\u00a0[8]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2022-06-30 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2022-06-30 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-01-01 00:00:00 --- 2022-06-30 00:00:00  (n=912)\nDates test  : 2022-07-01 00:00:00 --- 2023-12-31 00:00:00  (n=549)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(9, 3))\ndata_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax)\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax=plt.subplots(figsize=(9, 3)) data_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax) ax.legend(); In\u00a0[10]: Copied! <pre># One hot encoding of week_day and hour_day\n# ==============================================================================\none_hot_encoder = make_column_transformer(\n                        (\n                            OneHotEncoder(sparse_output=False, drop='if_binary'),\n                            ['month'],\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n\ndata_encoded_oh = one_hot_encoder.fit_transform(data)\ndata_encoded_oh.head(3)\n</pre> # One hot encoding of week_day and hour_day # ============================================================================== one_hot_encoder = make_column_transformer(                         (                             OneHotEncoder(sparse_output=False, drop='if_binary'),                             ['month'],                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\")  data_encoded_oh = one_hot_encoder.fit_transform(data) data_encoded_oh.head(3) Out[10]: month_1 month_2 month_3 month_4 month_5 month_6 month_7 month_8 month_9 month_10 month_11 month_12 y date 2020-01-01 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.928244 2020-01-02 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.866145 2020-01-03 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.425159 In\u00a0[11]: Copied! <pre># Cyclical encoding with sine/cosine transformation\n# ==============================================================================\ndef sin_transformer(period):\n\"\"\"\n\tReturns a transformer that applies sine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\ndef cos_transformer(period):\n\"\"\"\n\tReturns a transformer that applies cosine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n\ndata_encoded_sin_cos = data.copy()\ndata_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos.head()\n</pre> # Cyclical encoding with sine/cosine transformation # ============================================================================== def sin_transformer(period): \t\"\"\" \tReturns a transformer that applies sine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))  def cos_transformer(period): \t\"\"\" \tReturns a transformer that applies cosine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))  data_encoded_sin_cos = data.copy() data_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos.head() Out[11]: y month month_sin month_cos date 2020-01-01 2.928244 1 0.5 0.866025 2020-01-02 4.866145 1 0.5 0.866025 2020-01-03 4.425159 1 0.5 0.866025 2020-01-04 3.069222 1 0.5 0.866025 2020-01-05 4.021290 1 0.5 0.866025 In\u00a0[12]: Copied! <pre># Plot of the transformation\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(4., 3.5))\nsp = ax.scatter(\n        data_encoded_sin_cos[\"month_sin\"],\n        data_encoded_sin_cos[\"month_cos\"],\n        c=data_encoded_sin_cos[\"month\"],\n        cmap='viridis'\n     )\nax.set(\n    xlabel=\"sin(month)\",\n    ylabel=\"cos(month)\",\n)\n_ = fig.colorbar(sp)\ndata_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month')\n</pre> # Plot of the transformation # ============================================================================== fig, ax = plt.subplots(figsize=(4., 3.5)) sp = ax.scatter(         data_encoded_sin_cos[\"month_sin\"],         data_encoded_sin_cos[\"month_cos\"],         c=data_encoded_sin_cos[\"month\"],         cmap='viridis'      ) ax.set(     xlabel=\"sin(month)\",     ylabel=\"cos(month)\", ) _ = fig.colorbar(sp) data_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month') In\u00a0[13]: Copied! <pre># Create feature day of year\n# ==============================================================================\ndata['day_of_year'] = data.index.day_of_year\ndata.head(3)\n</pre> # Create feature day of year # ============================================================================== data['day_of_year'] = data.index.day_of_year data.head(3) Out[13]: y month day_of_year date 2020-01-01 2.928244 1 1 2020-01-02 4.866145 1 2 2020-01-03 4.425159 1 3 In\u00a0[14]: Copied! <pre># B-spline functions\n# ==============================================================================\ndef spline_transformer(period, degree=3, extrapolation=\"periodic\"):\n\"\"\"\n    Returns a transformer that applies B-spline transformation.\n    \"\"\"\n    return SplineTransformer(\n                degree=degree,\n                n_knots=period+1,\n                knots='uniform',\n                extrapolation=extrapolation,\n                include_bias=True\n            ).set_output(transform=\"pandas\")\n\nsplines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']])\nsplines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))]\n</pre> # B-spline functions # ============================================================================== def spline_transformer(period, degree=3, extrapolation=\"periodic\"):     \"\"\"     Returns a transformer that applies B-spline transformation.     \"\"\"     return SplineTransformer(                 degree=degree,                 n_knots=period+1,                 knots='uniform',                 extrapolation=extrapolation,                 include_bias=True             ).set_output(transform=\"pandas\")  splines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']]) splines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))] <p>The graph below shows the 12 spline functions generated using the day of the year as input. Since 12 splines are created with knots evenly distributed along the range 1 to 365 (day_of_year), each curve represents the proximity to the beginning of a particular month.</p> In\u00a0[15]: Copied! <pre># Location of the maximum value of each spline\n# ==============================================================================\nsplines_month.idxmax()\n</pre> # Location of the maximum value of each spline # ============================================================================== splines_month.idxmax() Out[15]: <pre>spline0    2020-12-01\nspline1    2020-01-01\nspline2    2020-01-31\nspline3    2020-03-02\nspline4    2020-04-01\nspline5    2020-05-02\nspline6    2020-06-01\nspline7    2020-07-02\nspline8    2020-08-01\nspline9    2020-08-31\nspline10   2020-10-01\nspline11   2020-10-31\ndtype: datetime64[ns]</pre> In\u00a0[16]: Copied! <pre># Plot of the B-splines functions for the first 365 days\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 5))\nsplines_month.head(365).plot(\n    ax       = ax,\n    subplots = True,\n    sharex   = True,\n    legend   = False,\n    yticks   = [],\n    title    = 'Splines functions for the first 365 days'\n);\n</pre> # Plot of the B-splines functions for the first 365 days # ============================================================================== fig, ax = plt.subplots(figsize=(8, 5)) splines_month.head(365).plot(     ax       = ax,     subplots = True,     sharex   = True,     legend   = False,     yticks   = [],     title    = 'Splines functions for the first 365 days' ); <pre>/tmp/ipykernel_5424/2664573984.py:4: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n  splines_month.head(365).plot(\n</pre> In\u00a0[17]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_splines = pd.concat([data, splines_month], axis=1)\ndata_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month'])\ndata_encoded_splines.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_splines = pd.concat([data, splines_month], axis=1) data_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month']) data_encoded_splines.head(3) Out[17]: y spline0 spline1 spline2 spline3 spline4 spline5 spline6 spline7 spline8 spline9 spline10 spline11 date 2020-01-01 2.928244 0.166667 0.666667 0.166667 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 4.866145 0.150763 0.665604 0.183628 0.000006 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 4.425159 0.135904 0.662485 0.201563 0.000047 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[18]: Copied! <pre># Radial basis functions\n# ==============================================================================\nrbf = RepeatingBasisFunction(\n            n_periods   = 12,\n            remainder   = 'drop',\n            column      = 'day_of_year',\n            input_range = (1, 365)\n        )\nrbf.fit(data)\nrbf_month = rbf.transform(data)\nrbf_month = pd.DataFrame(\n                data    = rbf_month,\n                index   = data.index,\n                columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]\n            )\nrbf_month.head(3)\n</pre> # Radial basis functions # ============================================================================== rbf = RepeatingBasisFunction(             n_periods   = 12,             remainder   = 'drop',             column      = 'day_of_year',             input_range = (1, 365)         ) rbf.fit(data) rbf_month = rbf.transform(data) rbf_month = pd.DataFrame(                 data    = rbf_month,                 index   = data.index,                 columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]             ) rbf_month.head(3) Out[18]: rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[19]: Copied! <pre># Location of the maximum value of each rbf\n# ==============================================================================\nrbf_month.idxmax()\n</pre> # Location of the maximum value of each rbf # ============================================================================== rbf_month.idxmax() Out[19]: <pre>rbf_0    2020-01-01\nrbf_1    2020-01-31\nrbf_2    2020-03-02\nrbf_3    2020-04-01\nrbf_4    2020-05-01\nrbf_5    2020-06-01\nrbf_6    2020-07-01\nrbf_7    2020-07-31\nrbf_8    2020-08-31\nrbf_9    2020-09-30\nrbf_10   2020-10-30\nrbf_11   2020-11-30\ndtype: datetime64[ns]</pre> In\u00a0[20]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_rbf = pd.concat([data, rbf_month], axis=1)\ndata_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month'])\ndata_encoded_rbf.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_rbf = pd.concat([data, rbf_month], axis=1) data_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month']) data_encoded_rbf.head(3) Out[20]: y rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 2.928244 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 4.866145 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 4.425159 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[21]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags = [70]\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags = [70]              ) In\u00a0[22]: Copied! <pre># Train and validate a forecaster using each encoding method\n# ==============================================================================\ndatasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,\n            data_encoded_rbf]\nencoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',\n                    'rbf encoding']\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\n\nfor i, data_encoded in enumerate(datasets):\n\n    metric, predictions = backtesting_forecaster(\n                              forecaster         = forecaster,\n                              y                  = data_encoded['y'],\n                              exog               = data_encoded.drop(columns='y'),\n                              initial_train_size = len(data_encoded.loc[:end_train]),\n                              fixed_train_size   = False,\n                              steps              = 365,\n                              refit              = False,\n                              metric             = 'mean_squared_error',\n                              verbose            = False, # Change to True to see detailed information\n                              show_progress      = False\n                          )\n\n    print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")\n    predictions.plot(label=encoding_methods[i], ax=ax)\n    ax.legend(labels=['test'] + encoding_methods);\n</pre> # Train and validate a forecaster using each encoding method # ============================================================================== datasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,             data_encoded_rbf] encoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',                     'rbf encoding']  fig, ax = plt.subplots(figsize=(8, 4)) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)  for i, data_encoded in enumerate(datasets):      metric, predictions = backtesting_forecaster(                               forecaster         = forecaster,                               y                  = data_encoded['y'],                               exog               = data_encoded.drop(columns='y'),                               initial_train_size = len(data_encoded.loc[:end_train]),                               fixed_train_size   = False,                               steps              = 365,                               refit              = False,                               metric             = 'mean_squared_error',                               verbose            = False, # Change to True to see detailed information                               show_progress      = False                           )      print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")     predictions.plot(label=encoding_methods[i], ax=ax)     ax.legend(labels=['test'] + encoding_methods); <pre>Backtest error using one hot encoding: 1.10\nBacktest error using sine/cosine encoding: 1.12\nBacktest error using spline encoding: 0.75\nBacktest error using rbf encoding: 0.74\n</pre> <p>%%html</p>"},{"location":"faq/cyclical-features-time-series.html#cyclical-features-in-time-series-forecasting","title":"Cyclical features in time series forecasting\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#data","title":"Data\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#one-hot-encoding","title":"One hot encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#sine-cosine-encoding","title":"Sine-Cosine encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#b-splines-functions","title":"B-splines functions\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#radial-basis-functions-rbf","title":"Radial basis functions (RBF)\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#compare-forecasting-results","title":"Compare forecasting results\u00b6","text":"<p>A non-informative lag is included so that the impact of cyclical features can be assessed without being obscured by the autoregressive component.</p>"},{"location":"faq/faq.html","title":"Frequently Asked Questions and forecasting tips","text":"<p>Thank you for choosing skforecast and visiting our Frequently Asked Questions (FAQ) page. Here, we aim to provide solutions to commonly encountered issues on our Github repository. If your question is not answered on this page, we encourage you to create a new issue on our Github so that it can be addressed, and other users can also benefit from it. Additionally, we have included some forecasting tips to help you get the most out of skforecast.</p> <ul> <li> <p>Avoid negative predictions when forecasting</p> </li> <li> <p>Forecasting time series with missing values</p> </li> <li> <p>Cyclical features in time series</p> </li> <li> <p>Time series differentiation (modelling trend)</p> </li> <li> <p>Parallelization in skforecast</p> </li> <li> <p>Profiling skforecast</p> </li> </ul>"},{"location":"faq/forecasting-time-series-with-missing-values.html","title":"Forecasting time series with missing values","text":"<p>In many real use cases of forecasting, although historical data are available, it is common for the time series to be incomplete. The presence of missing values in the data is a major problem since most forecasting algorithms require the time series to be complete in order to train a model.</p> <p>A commonly employed strategy to overcome this problem is to impute missing values before training the model, for example, using a moving average. However, the quality of the imputations may not be good, impairing the training of the model. One way to improve the imputation strategy is to combine it with weighted time series forecasting. The latter consists of reducing the weight of the imputed observations and thus their influence during model training.</p> <p>This document shows two examples of how skforecast makes it easy to apply this strategy.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12,\n    'lines.linewidth': 1.5\n}\nplt.rcParams.update(dark_style)\n\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt  plt.style.use('fivethirtyeight') dark_style = {     'figure.facecolor': '#212946',     'axes.facecolor': '#212946',     'savefig.facecolor':'#212946',     'axes.grid': True,     'axes.grid.which': 'both',     'axes.spines.left': False,     'axes.spines.right': False,     'axes.spines.top': False,     'axes.spines.bottom': False,     'grid.color': '#2A3459',     'grid.linewidth': '1',     'text.color': '0.9',     'axes.labelcolor': '0.9',     'xtick.color': '0.9',     'ytick.color': '0.9',     'font.size': 12,     'lines.linewidth': 1.5 } plt.rcParams.update(dark_style)  from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster <p>  \u00a0 Note </p> <p>In this document, a forecaster of type <code>ForecasterAutoreg</code> is used. The same strategy can be applied with any forecaster from skforecast.</p> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n    'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d')\ndata = data[['fecha', 'Usos bicis total d\u00eda']]\ndata.columns = ['date', 'users']\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Data download # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'     'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d') data = data[['fecha', 'Usos bicis total d\u00eda']] data.columns = ['date', 'users'] data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head(3) Out[2]: users date 2014-06-23 99 2014-06-24 72 2014-06-25 119 In\u00a0[3]: Copied! <pre># Generating gaps with missing values\n# ==============================================================================\ngaps = [\n    ['2020-09-01', '2020-10-10'],\n    ['2020-11-08', '2020-12-15'],\n]\n\nfor gap in gaps:\n    data.loc[gap[0]:gap[1]] = np.nan\n</pre> # Generating gaps with missing values # ============================================================================== gaps = [     ['2020-09-01', '2020-10-10'],     ['2020-11-08', '2020-12-15'], ]  for gap in gaps:     data.loc[gap[0]:gap[1]] = np.nan In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2020-06-01': '2021-06-01']\nend_train = '2021-03-01'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2020-06-01': '2021-06-01'] end_train = '2021-03-01' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-06-01 00:00:00 --- 2021-03-01 00:00:00  (n=274)\nDates test  : 2021-03-01 00:00:00 --- 2021-06-01 00:00:00  (n=93)\n</pre> In\u00a0[5]: Copied! <pre># Time series plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(12, 4))\ndata_train.users.plot(ax=ax, label='train', linewidth=1)\ndata_test.users.plot(ax=ax, label='test', linewidth=1)\n\nfor gap in gaps:\n    ax.plot(\n        [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],\n        [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],\n         data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],\n        color = 'red',\n        linestyle = '--',\n        label = 'gap'\n        )\n\nax.set_title('Number of users BiciMAD')\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax.legend(by_label.values(), by_label.keys(), loc='lower right');\n</pre> # Time series plot # ============================================================================== fig, ax = plt.subplots(figsize=(12, 4)) data_train.users.plot(ax=ax, label='train', linewidth=1) data_test.users.plot(ax=ax, label='test', linewidth=1)  for gap in gaps:     ax.plot(         [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],         [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],          data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],         color = 'red',         linestyle = '--',         label = 'gap'         )  ax.set_title('Number of users BiciMAD') handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) ax.legend(by_label.values(), by_label.keys(), loc='lower right'); In\u00a0[6]: Copied! <pre># Value imputation using linear interpolation\n# ======================================================================================\ndata['users_imputed'] = data['users'].interpolate(method='linear')\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n</pre> # Value imputation using linear interpolation # ====================================================================================== data['users_imputed'] = data['users'].interpolate(method='linear') data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :] <p>To minimize the influence on the model of imputed values, a custom function is defined to create weights following the rules:</p> <ul> <li><p>Weight of 0 if the index date has been imputed or is within 14 days ahead of an imputed day.</p> </li> <li><p>Weight of 1 otherwise.</p> </li> </ul> <p>If an observation has a weight of 0, it has no influence at all during model training.</p> <p>  \u00a0 Note </p> <p>Imputed values should neither participate in the training process as a target nor as a predictor (lag). Therefore, values within a window size as large as the lags used should also be excluded.</p> In\u00a0[7]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is in any gap.\n    \"\"\"\n    gaps = [\n        ['2020-09-01', '2020-10-10'],\n        ['2020-11-08', '2020-12-15'],\n    ]\n    \n    missing_dates = [pd.date_range(\n                        start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),\n                        end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),\n                        freq  = 'D'\n                    ) for gap in gaps]\n    missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))   \n    weights = np.where(index.isin(missing_dates), 0, 1)\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is in any gap.     \"\"\"     gaps = [         ['2020-09-01', '2020-10-10'],         ['2020-11-08', '2020-12-15'],     ]          missing_dates = [pd.date_range(                         start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),                         end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),                         freq  = 'D'                     ) for gap in gaps]     missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))        weights = np.where(index.isin(missing_dates), 0, 1)      return weights <p><code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[8]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = LGBMRegressor(random_state=123),\n                 lags        = 14,\n                 weight_func = custom_weights\n             )\n\n# Backtesting: predict the next 7 days at a time.\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                            forecaster         = forecaster,\n                            y                  = data.users_imputed,\n                            initial_train_size = len(data.loc[:end_train]),\n                            fixed_train_size   = False,\n                            steps              = 7,\n                            metric             = 'mean_absolute_error',\n                            refit              = True,\n                            verbose            = False,\n                            show_progress      = True\n                        )\nprint(f\"Backtesting metric (mean_absolute_error): {metric:.2f}\")\npredictions.head(4)\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = LGBMRegressor(random_state=123),                  lags        = 14,                  weight_func = custom_weights              )  # Backtesting: predict the next 7 days at a time. # ============================================================================== metric, predictions = backtesting_forecaster(                             forecaster         = forecaster,                             y                  = data.users_imputed,                             initial_train_size = len(data.loc[:end_train]),                             fixed_train_size   = False,                             steps              = 7,                             metric             = 'mean_absolute_error',                             refit              = True,                             verbose            = False,                             show_progress      = True                         ) print(f\"Backtesting metric (mean_absolute_error): {metric:.2f}\") predictions.head(4) <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric (mean_absolute_error): 1904.83\n</pre> Out[8]: pred 2021-03-02 10524.159747 2021-03-03 10087.283682 2021-03-04 8882.926166 2021-03-05 9474.810215 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/forecasting-time-series-with-missing-values.html#forecasting-time-series-with-missing-values","title":"Forecasting time series with missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#data","title":"Data\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#impute-missing-values","title":"Impute missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#give-weight-of-zero-to-imputed-values","title":"Give weight of zero to imputed values\u00b6","text":""},{"location":"faq/non-negative-predictions.html","title":"Avoid negative predictions when forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import GammaRegressor\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.linear_model import Ridge from sklearn.linear_model import GammaRegressor from sklearn.linear_model import TweedieRegressor from sklearn.linear_model import PoissonRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.metrics import mean_squared_error from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'\n       'learning-python/master/data/bike_sharing_dataset_clean.csv')\ndata = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000)\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('H')\ndata = data.sort_index()\n</pre> # Downloading data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'        'learning-python/master/data/bike_sharing_dataset_clean.csv') data = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000) data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('H') data = data.sort_index() In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2011-01-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2011-01-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train: 2011-01-01 00:00:00 --- 2011-01-31 23:00:00  (n=744)\nDates test : 2011-02-01 00:00:00 --- 2011-02-11 15:00:00  (n=256)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['users'].plot(ax=ax, label='train')\ndata_test['users'].plot(ax=ax, label='test')\nax.set_title('Number of users')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['users'].plot(ax=ax, label='train') data_test['users'].plot(ax=ax, label='test') ax.set_title('Number of users') ax.set_xlabel('') ax.legend(); In\u00a0[5]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24\n             )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 24              ) In\u00a0[6]: Copied! <pre># Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.set_xlabel('') ax.legend(); <p>The graph above shows that some predictions are negative.</p> In\u00a0[8]: Copied! <pre># Negative predictions\n# ==============================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ============================================================================== predictions[predictions.pred &lt; 0] Out[8]: pred 2011-02-01 00:00:00 -23.337236 2011-02-01 01:00:00 -17.691405 2011-02-02 00:00:00 -5.243456 2011-02-02 01:00:00 -19.363139 2011-02-03 01:00:00 -6.441943 2011-02-04 01:00:00 -10.579940 2011-02-05 00:00:00 -6.026119 2011-02-05 01:00:00 -21.396841 2011-02-07 04:00:00 -3.412043 2011-02-07 05:00:00 -3.701964 2011-02-08 00:00:00 -17.045913 2011-02-08 01:00:00 -13.233004 2011-02-09 00:00:00 -25.315228 2011-02-09 01:00:00 -24.743686 2011-02-10 01:00:00 -5.704407 2011-02-11 01:00:00 -11.758940 In\u00a0[9]: Copied! <pre># Transform data into a logarithmic scale\n# =============================================================================\ndata_log = np.log1p(data)\ndata_train_log = np.log1p(data_train)\ndata_test_log  = np.log1p(data_test)\n</pre> # Transform data into a logarithmic scale # ============================================================================= data_log = np.log1p(data) data_train_log = np.log1p(data_train) data_test_log  = np.log1p(data_test) In\u00a0[10]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\n# Since the data has been transformed outside the forecaster, the predictions\n# and metric are not in the original scale. They need to be transformed back.\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data_log['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train_log),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== # Since the data has been transformed outside the forecaster, the predictions # and metric are not in the original scale. They need to be transformed back. metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data_log['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train_log),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Revert the transformation\n# ==============================================================================\npredictions = np.expm1(predictions)\npredictions.head(4)\n</pre> # Revert the transformation # ============================================================================== predictions = np.expm1(predictions) predictions.head(4) Out[11]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 In\u00a0[12]: Copied! <pre># Backtesting metric (test data)\n# ==============================================================================\n# The error metric is calculated once the transformation is reversed.\nmetric = mean_squared_error(y_true=data_test['users'], y_pred=predictions)\nprint(f\"Backtesting metric (test data): {metric}\")\n</pre> # Backtesting metric (test data) # ============================================================================== # The error metric is calculated once the transformation is reversed. metric = mean_squared_error(y_true=data_test['users'], y_pred=predictions) print(f\"Backtesting metric (test data): {metric}\") <pre>Backtesting metric (test data): 1991.9332571759892\n</pre> In\u00a0[13]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation. If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[14]: Copied! <pre># Create a custom transformer\n# =============================================================================\ntransformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n\n# Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 24,\n                 transformer_y    = transformer_y,\n                 transformer_exog = None\n             )\nforecaster.fit(data['users'])\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\n# Since the transformation is included in the forecaster, predictions are\n# automatically transformed back into the original scale. And the metric is\n# calculated in the original scale.\nprint(f\"Backtesting metric: {metric}\")\npredictions.head()\n</pre> # Create a custom transformer # ============================================================================= transformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)  # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 24,                  transformer_y    = transformer_y,                  transformer_exog = None              ) forecaster.fit(data['users'])  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  # Since the transformation is included in the forecaster, predictions are # automatically transformed back into the original scale. And the metric is # calculated in the original scale. print(f\"Backtesting metric: {metric}\") predictions.head() <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric: 1991.9332571759892\n</pre> Out[14]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 2011-02-01 04:00:00 3.865169 <p>The same results are obtained as if the data were log-transformed outside the model. However, by including the log transformation in the model, all inverse transformations are handled automatically.</p> <p>A forecaster with a linear model (scikit learn GammaRegressor) that uses a different link function is evaluated.</p> In\u00a0[15]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = GammaRegressor(alpha=1, max_iter=100000),\n                 lags      = 20,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\nprint(f\"Backtesting metric: {metric}\")\npredictions.head()\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = GammaRegressor(alpha=1, max_iter=100000),                  lags      = 20,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  print(f\"Backtesting metric: {metric}\") predictions.head() <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric: 3509.69378608639\n</pre> Out[15]: pred 2011-02-01 00:00:00 6.685830 2011-02-01 01:00:00 10.238422 2011-02-01 02:00:00 11.950507 2011-02-01 03:00:00 9.194079 2011-02-01 04:00:00 6.372489 <p>In this case the gamma regressor model performed poorly, but if we look to the negative results:</p> In\u00a0[16]: Copied! <pre># Negative predictions\n# ======================================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ====================================================================================== predictions[predictions.pred &lt; 0] Out[16]: pred In\u00a0[17]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Now a XGBoost model is trained, but with the objective function set to <code>reg:gamma</code>, so that the output is a mean of the gamma distribution, which is defined for positive values only.</p> In\u00a0[18]: Copied! <pre># Create forecaster and train\n# ==============================================================================\nparams = {\n    'tree_method': 'hist',\n    'objective': 'reg:gamma'\n}\n\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(**params),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\nprint(f\"Backtesting metric: {metric}\")\n</pre> # Create forecaster and train # ============================================================================== params = {     'tree_method': 'hist',     'objective': 'reg:gamma' }  forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(**params),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  print(f\"Backtesting metric: {metric}\") <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric: 1665.3412504662654\n</pre> In\u00a0[20]: Copied! <pre># Negative predictions\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions predictions[predictions.pred &lt; 0] Out[20]: pred In\u00a0[21]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>The forecaster's performance is better than using a Ridge regressor with a log-transformer, and no negative predictions are made.</p> In\u00a0[22]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/non-negative-predictions.html#avoid-negative-predictions-in-forecasting","title":"Avoid Negative Predictions in Forecasting\u00b6","text":"<p>When training a forecasting model, even if none of the training observations are negative, some predictions may turn out to be negative. This is quite common when trying to predict attendance, sales, or rainfall, among other things. To avoid this, it is possible to use the following approaches:</p> <ul> <li><p>Use a $log+K$ transformation to always have positive values in the transformed time series. $K$ is a positive integer to avoid the undefinability of the $log(x)$ function at 0. Despite the simplicity of the approach, one must be aware that some caveats may arise depending on the metric we use to optimize our models.</p> </li> <li><p>Use a different link function in the models. The link function provides the relationship between the linear predictor and the expected value of the response variable. When using machine learning models, several algorithms support different objective functions to account for this situation. For example, when predicting the number of visitors, one can use <code>count:poisson</code> in XGBoost or LightGBM as well as gamma regression if the target is always strictly positive.</p> </li> </ul> <p>The following tutorial will explore both possibilities.</p>"},{"location":"faq/non-negative-predictions.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/non-negative-predictions.html#data","title":"Data\u00b6","text":""},{"location":"faq/non-negative-predictions.html#forecaster","title":"Forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#modeling-time-series-in-logarithmic-scale","title":"Modeling time series in logarithmic scale\u00b6","text":""},{"location":"faq/non-negative-predictions.html#include-a-logarithmic-transformer-as-part-of-the-forecaster","title":"Include a logarithmic transformer as part of the forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-link-functions","title":"Usage of link functions\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-xgboost-objective-functions","title":"Usage of XGBoost objective functions\u00b6","text":""},{"location":"faq/parallelization-skforecast.html","title":"Parallelization in skforecast","text":"<p>  \u00a0 Warning </p> <p>The automatic selection of the parallelization level relies on heuristics and is therefore not guaranteed to be optimal. In addition, it is important to keep in mind that many regressors already parallelize their fitting procedures inherently. As a result, introducing additional parallelization may not necessarily improve overall performance. For a more detailed look at parallelization, visit select_n_jobs_backtesting and select_n_jobs_fit_forecaster.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport platform\nimport psutil\nimport skforecast\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport sklearn\nimport time\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multivariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multivariate\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n</pre> # Libraries # ============================================================================== import platform import psutil import skforecast import pandas as pd import numpy as np import scipy import sklearn import time import warnings import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from lightgbm import LGBMRegressor from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multivariate from skforecast.model_selection_multiseries import backtesting_forecaster_multivariate from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate In\u00a0[7]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"scipy version       : {scipy.__version__}\")\nprint(f\"psutil version      : {psutil.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\n#Computer network name\nprint(f\"Computer network name: {platform.node()}\")\n#Machine type\nprint(f\"Machine type: {platform.machine()}\")\n#Processor type\nprint(f\"Processor type: {platform.processor()}\")\n#Platform type\nprint(f\"Platform type: {platform.platform()}\")\n#Operating system\nprint(f\"Operating system: {platform.system()}\")\n#Operating system release\nprint(f\"Operating system release: {platform.release()}\")\n#Operating system version\nprint(f\"Operating system version: {platform.version()}\")\n#Physical cores\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\n#Logical cores\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"scipy version       : {scipy.__version__}\") print(f\"psutil version      : {psutil.__version__}\") print(\"\")  # System information # ============================================================================== #Computer network name print(f\"Computer network name: {platform.node()}\") #Machine type print(f\"Machine type: {platform.machine()}\") #Processor type print(f\"Processor type: {platform.processor()}\") #Platform type print(f\"Platform type: {platform.platform()}\") #Operating system print(f\"Operating system: {platform.system()}\") #Operating system release print(f\"Operating system release: {platform.release()}\") #Operating system version print(f\"Operating system version: {platform.version()}\") #Physical cores print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") #Logical cores print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.11.4\nscikit-learn version: 1.3.0\nskforecast version  : 0.9.1\npandas version      : 2.0.3\nnumpy version       : 1.25.2\nscipy version       : 1.11.1\npsutil version      : 5.9.5\n\nComputer network name: EU-HYYV0J3\nMachine type: AMD64\nProcessor type: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\nPlatform type: Windows-10-10.0.19045-SP0\nOperating system: Windows\nOperating system release: 10\nOperating system version: 10.0.19045\nNumber of physical cores: 4\nNumber of logical cores: 8\n</pre> In\u00a0[8]: Copied! <pre># Data\n# ==============================================================================\nn = 5_000\nrgn = np.random.default_rng(seed=123)\ny = pd.Series(rgn.random(size=(n)), name=\"y\")\nexog = pd.DataFrame(rgn.random(size=(n, 10)))\nexog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])]\nmulti_series = pd.DataFrame(rgn.random(size=(n, 10)))\nmulti_series.columns = [f\"series_{i+1}\" for i in range(multi_series.shape[1])]\ny_train = y[:-int(n/2)]\ndisplay(y.head())\ndisplay(exog.head())   \ndisplay(multi_series.head())\n</pre> # Data # ============================================================================== n = 5_000 rgn = np.random.default_rng(seed=123) y = pd.Series(rgn.random(size=(n)), name=\"y\") exog = pd.DataFrame(rgn.random(size=(n, 10))) exog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])] multi_series = pd.DataFrame(rgn.random(size=(n, 10))) multi_series.columns = [f\"series_{i+1}\" for i in range(multi_series.shape[1])] y_train = y[:-int(n/2)] display(y.head()) display(exog.head())    display(multi_series.head()) <pre>0    0.682352\n1    0.053821\n2    0.220360\n3    0.184372\n4    0.175906\nName: y, dtype: float64</pre> exog_0 exog_1 exog_2 exog_3 exog_4 exog_5 exog_6 exog_7 exog_8 exog_9 0 0.593121 0.353471 0.336277 0.399734 0.915459 0.822278 0.480418 0.929802 0.950948 0.863556 1 0.764104 0.638191 0.956624 0.178105 0.434077 0.137480 0.837667 0.768947 0.244235 0.815336 2 0.475312 0.312415 0.353596 0.272162 0.772064 0.110216 0.596551 0.688549 0.651380 0.191837 3 0.039253 0.962713 0.189194 0.910629 0.169796 0.697751 0.830913 0.484824 0.634634 0.862865 4 0.872447 0.861421 0.394829 0.877763 0.286779 0.131008 0.450185 0.898167 0.590147 0.045838 series_1 series_2 series_3 series_4 series_5 series_6 series_7 series_8 series_9 series_10 0 0.967448 0.580646 0.643348 0.461737 0.450859 0.894496 0.037967 0.097698 0.094356 0.893528 1 0.207450 0.194904 0.377063 0.975065 0.351034 0.812253 0.265956 0.262733 0.784995 0.674256 2 0.520431 0.985069 0.039559 0.541797 0.612761 0.640336 0.823467 0.768387 0.561777 0.600835 3 0.866694 0.165510 0.819767 0.691179 0.717778 0.392694 0.094067 0.271990 0.467866 0.041054 4 0.406310 0.657688 0.630730 0.694424 0.943934 0.888538 0.470363 0.518283 0.719674 0.010789 In\u00a0[9]: Copied! <pre>warnings.filterwarnings(\"ignore\")\n\nprint(\"-----------------\")\nprint(\"ForecasterAutoreg\")\nprint(\"-----------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoreg(\n                        regressor=regressor,\n                        lags=lags,\n                        transformer_exog=StandardScaler()\n                )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoreg, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\")\n</pre> warnings.filterwarnings(\"ignore\")  print(\"-----------------\") print(\"ForecasterAutoreg\") print(\"-----------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoreg(                         regressor=regressor,                         lags=lags,                         transformer_exog=StandardScaler()                 )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                 forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoreg, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\") <pre>-----------------\nForecasterAutoreg\n-----------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 9.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 0.571370 0.416455 27.112945 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 6.569071 2.720030 58.593380 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.007874 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.277259 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 6.699500 6.123097 8.603664 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 0.751951 0.546975 27.259220 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 7.001708 2.424567 65.371771 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.008184 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 0.275215 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 6.601839 7.825191 -18.530468 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.744464 0.378917 49.102068 11 Ridge(alpha=0.1, random_state=77) backtest_refit 1.571386 12.706505 -708.617632 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.011935 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.024534 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 4.548398 2.756721 39.391399 Out[9]: <pre>Text(0.5, 0, 'Method')</pre> In\u00a0[10]: Copied! <pre>print(\"-----------------------\")\nprint(\"ForecasterAutoregDirect\")\nprint(\"-----------------------\")\nsteps = 25\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregDirect(\n                    regressor=regressor,\n                    lags=lags,\n                    steps=steps,\n                    transformer_exog=StandardScaler()\n                )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                    forecaster          = forecaster,\n                                    y                   = y,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n\n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                    forecaster          = forecaster,\n                    y                   = y,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregDirect, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"-----------------------\") print(\"ForecasterAutoregDirect\") print(\"-----------------------\") steps = 25 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregDirect(                     regressor=regressor,                     lags=lags,                     steps=steps,                     transformer_exog=StandardScaler()                 )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                 forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                     forecaster          = forecaster,                                     y                   = y,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)           print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                     forecaster          = forecaster,                     y                   = y,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregDirect, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\"); <pre>-----------------------\nForecasterAutoregDirect\n-----------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 9.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nNumber of models compared: 12.\nProfiling GridSearch no refit no parallel\nNumber of models compared: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 5.992304 10.283535 -71.612377 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 141.952965 75.919333 46.517966 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.072996 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 4.144056 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 43.838513 76.157306 -73.722375 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 5.886309 10.738465 -82.431196 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 143.764385 64.597212 55.067305 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.060571 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 4.122978 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 42.607671 75.015008 -76.059866 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.323168 0.319153 1.242526 11 Ridge(alpha=0.1, random_state=77) backtest_refit 5.785780 2.094785 63.794244 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.033940 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.430777 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 4.431143 3.941903 11.040943 In\u00a0[11]: Copied! <pre>print(\"----------------------------\")\nprint(\"ForecasterAutoregMultiseries\")\nprint(\"----------------------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregMultiSeries(\n        regressor=regressor,\n        lags=lags,\n        transformer_exog=StandardScaler()\n    )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit and parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = len(y_train),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = len(y_train),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultiseries, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"----------------------------\") print(\"ForecasterAutoregMultiseries\") print(\"----------------------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregMultiSeries(         regressor=regressor,         lags=lags,         transformer_exog=StandardScaler()     )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit and parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = len(y_train),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = len(y_train),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultiseries, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\"); <pre>----------------------------\nForecasterAutoregMultiseries\n----------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n9 models compared for 10 level(s). Number of iterations: 9.\nProfiling GridSearch no refit no parallel\n9 models compared for 10 level(s). Number of iterations: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 10 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 10 level(s). Number of iterations: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 10 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 10 level(s). Number of iterations: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 3.856087 1.785516 53.696183 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 26.799302 14.820522 44.698105 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.082853 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 1.439756 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 42.083872 29.374772 30.199456 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 5.492170 2.099951 61.764638 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 30.017390 14.576546 51.439661 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.079670 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 1.465376 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 72.783301 36.314506 50.105992 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 3.122103 1.041682 66.635227 11 Ridge(alpha=0.1, random_state=77) backtest_refit 6.525677 2.931308 55.080398 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.119956 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 0.277841 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 26.023329 13.008654 50.011569 In\u00a0[12]: Copied! <pre>print(\"-----------------------------\")\nprint(\"ForecasterAutoregMultivariate\")\nprint(\"-----------------------------\")\nsteps = 25\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterAutoregMultiVariate(\n        regressor=regressor,\n        lags=lags,\n        steps=steps,\n        level=\"series_1\",\n        transformer_exog=StandardScaler()\n    )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = True,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = -1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                    forecaster          = forecaster,\n                                    series              = multi_series,\n                                    exog                = exog,\n                                    initial_train_size  = int(len(y)*0.9),\n                                    fixed_train_size    = False,\n                                    steps               = steps,\n                                    metric              = 'mean_squared_error',\n                                    refit               = False,\n                                    interval            = None,\n                                    n_boot              = 500,\n                                    random_state        = 123,\n                                    in_sample_residuals = True,\n                                    verbose             = False,\n                                    show_progress       = False,\n                                    n_jobs              = 1\n                            )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = -1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                    forecaster          = forecaster,\n                    series              = multi_series,\n                    exog                = exog,\n                    initial_train_size  = int(len(y)*0.9),\n                    steps               = steps,\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    refit              = False,\n                    metric             = 'mean_squared_error',\n                    fixed_train_size   = False,\n                    return_best        = False,\n                    verbose            = False,\n                    show_progress      = False,\n                    n_jobs             = 1\n            )\n    end = time.time()\n    elapsed_times.append(end - start)\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n     \"method\": np.tile(methods, len(regressors)),\n     \"elapsed_time\": elapsed_times\n})\n\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultivariate, n={n})\")\nax.set_ylabel(\"Percent difference\")\nax.set_xlabel(\"Method\");\n</pre> print(\"-----------------------------\") print(\"ForecasterAutoregMultivariate\") print(\"-----------------------------\") steps = 25 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]}, ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterAutoregMultiVariate(         regressor=regressor,         lags=lags,         steps=steps,         level=\"series_1\",         transformer_exog=StandardScaler()     )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = True,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = -1                             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                     forecaster          = forecaster,                                     series              = multi_series,                                     exog                = exog,                                     initial_train_size  = int(len(y)*0.9),                                     fixed_train_size    = False,                                     steps               = steps,                                     metric              = 'mean_squared_error',                                     refit               = False,                                     interval            = None,                                     n_boot              = 500,                                     random_state        = 123,                                     in_sample_residuals = True,                                     verbose             = False,                                     show_progress       = False,                                     n_jobs              = 1                             )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = -1             )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                     forecaster          = forecaster,                     series              = multi_series,                     exog                = exog,                     initial_train_size  = int(len(y)*0.9),                     steps               = steps,                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     refit              = False,                     metric             = 'mean_squared_error',                     fixed_train_size   = False,                     return_best        = False,                     verbose            = False,                     show_progress      = False,                     n_jobs             = 1             )     end = time.time()     elapsed_times.append(end - start)  methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({      \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),      \"method\": np.tile(methods, len(regressors)),      \"elapsed_time\": elapsed_times })  results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot) fig, ax = plt.subplots(figsize=(10, 5)) sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) ax.set_title(f\"Parallel vs Sequential (ForecasterAutoregMultivariate, n={n})\") ax.set_ylabel(\"Percent difference\") ax.set_xlabel(\"Method\");  <pre>-----------------------------\nForecasterAutoregMultivariate\n-----------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n9 models compared for 1 level(s). Number of iterations: 9.\nProfiling GridSearch no refit no parallel\n9 models compared for 1 level(s). Number of iterations: 9.\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 1 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 1 level(s). Number of iterations: 12.\n\nLGBMRegressor(max_depth=5, n_estimators=50, random_state=77) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\n12 models compared for 1 level(s). Number of iterations: 12.\nProfiling GridSearch no refit no parallel\n12 models compared for 1 level(s). Number of iterations: 12.\n</pre> regressor method False True pct_improvement 0 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 34.915484 41.451426 -18.719322 1 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 773.195707 615.900236 20.343552 2 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.163573 NaN NaN 3 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 34.095271 NaN NaN 4 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 315.079898 407.191276 -29.234292 5 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_no_refit 30.237485 29.182785 3.488057 6 LGBMRegressor(max_depth=5, n_estimators=50, ra... backtest_refit 536.445092 462.887896 13.711971 7 LGBMRegressor(max_depth=5, n_estimators=50, ra... create_train_X_y 0.110270 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, ra... fit 24.590466 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, ra... gridSearch_no_refit 313.850108 366.675623 -16.831447 10 Ridge(alpha=0.1, random_state=77) backtest_no_refit 2.171713 2.316724 -6.677227 11 Ridge(alpha=0.1, random_state=77) backtest_refit 80.197614 44.966058 43.930928 12 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.228073 NaN NaN 13 Ridge(alpha=0.1, random_state=77) fit 5.533158 NaN NaN 14 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 19.418475 21.956889 -13.072160 In\u00a0[13]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/parallelization-skforecast.html#parallelization-in-skforecast","title":"Parallelization in skforecast\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#parallelization-rules","title":"Parallelization rules\u00b6","text":"<p>The <code>n_jobs</code> argument facilitates the parallelization of specific functionalities to enhance speed within the skforecast library. Parallelization has been strategically integrated at two key levels: during the process of forecaster fitting and during the backtesting phase, which also encompasses hyperparameter search. When the <code>n_jobs</code> argument is set to its default value of <code>auto</code>, the library dynamically determines the number of jobs to employ, guided by the ensuing guidelines:</p> <p>Forecaster Fitting</p> <ul> <li><p>If the forecaster is either <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code>, and the underlying regressor happens to be a linear regressor, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>Otherwise, if none of the above conditions hold, the <code>n_jobs</code> value is determined as <code>cpu_count()</code>, aligning with the number of available CPU cores.</p> </li> </ul> <p>Backtesting</p> <ul> <li><p>If <code>refit</code> is an integer, then <code>n_jobs=1</code>. This is because parallelization doesn`t work with intermittent refit.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code> and the underlying regressor is linear, <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code>,  the underlying regressor regressor is not a linear regressor and <code>refit=True</code>, then <code>n_jobs</code> is set to  <code>cpu_count()</code>.</p> </li> <li><p>If forecaster is <code>ForecasterAutoreg</code> or <code>ForecasterAutoregCustom</code>, the underlying regressor is not linear and <code>refit=False</code>, n_jobs is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code> and refit=<code>True</code>, then <code>n_jobs</code> is set to <code>cpu_count()</code>.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregDirect</code> or <code>ForecasterAutoregMultiVariate</code> and refit=<code>False</code>, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterAutoregMultiseries</code>, then <code>n_jobs</code> is set to <code>cpu_count()</code>.</p> </li> </ul>"},{"location":"faq/parallelization-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoreg","title":"Benchmark ForecasterAutoreg\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoreg","title":"Benchmark ForecasterAutoreg\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoregmultiseries","title":"Benchmark ForecasterAutoregMultiSeries\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterautoregmultivariate","title":"Benchmark ForecasterAutoregMultivariate\u00b6","text":""},{"location":"faq/profiling-skforecast.html","title":"Profiling skforecast","text":"<p>This document shows the profiling of the main classes, methods and functions available in skforecast. Understanding the bottlenecks will help to:</p> <ul> <li>Use it more efficiently</li> <li>Improve the code for future releases</li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\n%load_ext pyinstrument\n</pre> # Libraries # ============================================================================== import time import numpy as np import pandas as pd import matplotlib.pyplot as plt  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import backtesting_forecaster  from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import HistGradientBoostingRegressor from lightgbm import LGBMRegressor  %load_ext pyinstrument <p>A time series of length 1000 with random values is created.</p> In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\nnp.random.seed(123)\nn = 1_000\ndata = pd.Series(data = np.random.normal(size=n))\n</pre> # Data # ============================================================================== np.random.seed(123) n = 1_000 data = pd.Series(data = np.random.normal(size=n)) <p>To isolate the training process of the regressor from the other parts of the code, a dummy regressor class is created. This dummy regressor has a fit method that does nothing, and a predict method that returns a constant value.</p> In\u00a0[3]: Copied! <pre>class DummyRegressor(LinearRegression):\n\"\"\"\n    Dummy regressor with dummy fit and predict methods.\n    \"\"\"\n    \n    def fit(self, X, y):\n        pass\n\n    def predict(self, y):\n        predictions = np.ones(shape = len(y))\n        return predictions\n</pre> class DummyRegressor(LinearRegression):     \"\"\"     Dummy regressor with dummy fit and predict methods.     \"\"\"          def fit(self, X, y):         pass      def predict(self, y):         predictions = np.ones(shape = len(y))         return predictions          In\u00a0[4]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) <p>Almost all of the time spent by <code>fit</code> is required by the <code>create_train_X_y</code> method.</p> In\u00a0[5]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) <p>When training a forecaster with a real machine learning regressor, the time spent by <code>create_train_X_y</code> is negligible compared to the time needed by the <code>fit</code> method of the regressor. Therefore, improving the speed of <code>create_train_X_y</code> will not have much impact.</p> <p>Understand how the <code>create_train_X_y</code> method is influenced by the length of the series and the number of lags.</p> In\u00a0[6]: Copied! <pre># Profiling `create_train_X_y` for different length of series and number of lags\n# ======================================================================================\nseries_length = np.linspace(1000, 1000000, num=5, dtype=int)\nn_lags = [5, 10, 50, 100, 200]\nresults = {}\n\nfor lags in n_lags:\n    execution_time = []\n    forecaster = ForecasterAutoreg(\n                     regressor = DummyRegressor(),\n                     lags      = lags\n                 )\n\n    for n in series_length:\n        y = pd.Series(data = np.random.normal(size=n))\n        tic = time.perf_counter()\n        _ = forecaster.create_train_X_y(y=y)\n        toc = time.perf_counter()\n        execution_time.append(toc-tic)\n\n    results[lags] = execution_time\n\nresults = pd.DataFrame(\n              data =  results,\n              index = series_length\n          )\n\nresults\n</pre> # Profiling `create_train_X_y` for different length of series and number of lags # ====================================================================================== series_length = np.linspace(1000, 1000000, num=5, dtype=int) n_lags = [5, 10, 50, 100, 200] results = {}  for lags in n_lags:     execution_time = []     forecaster = ForecasterAutoreg(                      regressor = DummyRegressor(),                      lags      = lags                  )      for n in series_length:         y = pd.Series(data = np.random.normal(size=n))         tic = time.perf_counter()         _ = forecaster.create_train_X_y(y=y)         toc = time.perf_counter()         execution_time.append(toc-tic)      results[lags] = execution_time  results = pd.DataFrame(               data =  results,               index = series_length           )  results Out[6]: 5 10 50 100 200 1000 0.001328 0.000788 0.000817 0.002116 0.003013 250750 0.008482 0.023412 0.126703 0.255135 0.508422 500500 0.015588 0.052422 0.255091 0.506566 0.998829 750250 0.024476 0.067499 0.383377 0.749951 1.481535 1000000 0.030307 0.091011 0.492627 0.991524 2.012482 In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 4))\nresults.plot(ax=ax, marker='.')\nax.set_xlabel('length of series')\nax.set_ylabel('time (seconds)')\nax.set_title('Profiling create_train_X_y()')\nax.legend(title='number of lags');\n</pre> fig, ax = plt.subplots(figsize=(7, 4)) results.plot(ax=ax, marker='.') ax.set_xlabel('length of series') ax.set_ylabel('time (seconds)') ax.set_title('Profiling create_train_X_y()') ax.legend(title='number of lags'); In\u00a0[8]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[9]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) In\u00a0[10]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[11]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) <p>Inside the <code>predict</code> method, the <code>append</code> action is the most expensive but, similar to what happen with <code>fit</code>, it is negligible compared to the time need by the <code>predict</code> method of the regressor.</p> In\u00a0[12]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/profiling-skforecast.html#profiling-skforecast","title":"Profiling skforecast\u00b6","text":""},{"location":"faq/profiling-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/profiling-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"faq/profiling-skforecast.html#dummy-regressor","title":"Dummy regressor\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-fit","title":"Profiling fit\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-create_train_x_y","title":"Profiling create_train_X_y\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-predict","title":"Profiling predict\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html","title":"Single vs multi series forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from statsmodels.graphics.tsaplots import plot_acf  from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries In\u00a0[3]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' +\n       'data/vic_elec.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation (aggregation at daily level)\n# ==============================================================================\ndata['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ')\ndata = data.set_index('Time')\ndata = data.asfreq('30min')\ndata = data.sort_index()\ndata = data.drop(columns='Date')\ndata = data.resample(rule='H', closed='left', label ='right')\\\n       .agg({'Demand': 'sum', 'Temperature': 'mean'})\n\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' +        'data/vic_elec.csv') data = pd.read_csv(url, sep=',')  # Data preparation (aggregation at daily level) # ============================================================================== data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ') data = data.set_index('Time') data = data.asfreq('30min') data = data.sort_index() data = data.drop(columns='Date') data = data.resample(rule='H', closed='left', label ='right')\\        .agg({'Demand': 'sum', 'Temperature': 'mean'})  data.head() Out[3]: Demand Temperature Time 2011-12-31 14:00:00 8646.190700 21.225 2011-12-31 15:00:00 7926.529376 20.625 2011-12-31 16:00:00 7901.826990 20.325 2011-12-31 17:00:00 7255.721350 19.850 2011-12-31 18:00:00 6792.503352 19.025 In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2014-06-01 00:00:00': '2014-07-31 23:59:00']\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2014-06-01 00:00:00': '2014-07-31 23:59:00'] end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Train dates : 2014-06-01 00:00:00 --- 2014-07-15 23:00:00  (n=1080)\nTest dates  : 2014-07-16 00:00:00 --- 2014-07-31 23:00:00  (n=384)\n</pre> In\u00a0[5]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['Demand'].plot(label='train', ax=axes[0])\ndata_test['Demand'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_train['Temperature'].plot(label='train', ax=axes[1])\ndata_test['Temperature'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('\u00baC')\naxes[1].set_title('Temperature')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)  data_train['Demand'].plot(label='train', ax=axes[0]) data_test['Demand'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_train['Temperature'].plot(label='train', ax=axes[1]) data_test['Temperature'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('\u00baC') axes[1].set_title('Temperature')  fig.tight_layout() plt.show(); In\u00a0[6]: Copied! <pre># Autocorrelation plot\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\nplot_acf(data['Demand'], ax=axes[0], lags=72)\naxes[0].set_title('Demand')\nplot_acf(data['Temperature'], ax=axes[1], lags=72)\naxes[1].set_title('Temperature')\nplt.show()\n</pre> # Autocorrelation plot # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True) plot_acf(data['Demand'], ax=axes[0], lags=72) axes[0].set_title('Demand') plot_acf(data['Temperature'], ax=axes[1], lags=72) axes[1].set_title('Temperature') plt.show() <p>For this first study, the forecasting performance of a multi-series forecaster (non-multivariate) will be compared with that of single forecasters. For all of them, a linear model will be used as regressor.</p> <p>First, an individual autoregressive forecaster is trained for each series and their forecasting capability is measured with backtesting.</p> In\u00a0[7]: Copied! <pre># ForecasterAutoreg with linear models\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = LinearRegression(),\n                 lags             = 24,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nmetric_demand, predictions_demand = backtesting_forecaster(\n                                        forecaster         = forecaster,\n                                        y                  = data['Demand'],\n                                        initial_train_size = len(data_train),\n                                        fixed_train_size   = True,\n                                        steps              = 24,\n                                        metric             = 'mean_absolute_error',\n                                        refit              = True,\n                                        verbose            = False\n                                )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster(\n                                                    forecaster         = forecaster,\n                                                    y                  = data['Temperature'],\n                                                    initial_train_size = len(data_train),\n                                                    fixed_train_size   = True,\n                                                    steps              = 24,\n                                                    metric             = 'mean_absolute_error',\n                                                    refit              = True,\n                                                    verbose            = False\n                                              )\n</pre> # ForecasterAutoreg with linear models # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = LinearRegression(),                  lags             = 24,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  metric_demand, predictions_demand = backtesting_forecaster(                                         forecaster         = forecaster,                                         y                  = data['Demand'],                                         initial_train_size = len(data_train),                                         fixed_train_size   = True,                                         steps              = 24,                                         metric             = 'mean_absolute_error',                                         refit              = True,                                         verbose            = False                                 )  metric_temperature, predictions_temperature = backtesting_forecaster(                                                     forecaster         = forecaster,                                                     y                  = data['Temperature'],                                                     initial_train_size = len(data_train),                                                     fixed_train_size   = True,                                                     steps              = 24,                                                     metric             = 'mean_absolute_error',                                                     refit              = True,                                                     verbose            = False                                               )  In\u00a0[8]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_test['Demand'].plot(label='test', ax=axes[0])\npredictions_demand.plot(label='pred', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_test['Temperature'].plot(label='test', ax=axes[1])\npredictions_temperature.plot(label='pred', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('\u00baC')\naxes[1].set_title('Temperature')\naxes[0].legend()\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)  data_test['Demand'].plot(label='test', ax=axes[0]) predictions_demand.plot(label='pred', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_test['Temperature'].plot(label='test', ax=axes[1]) predictions_temperature.plot(label='pred', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('\u00baC') axes[1].set_title('Temperature') axes[0].legend()  fig.tight_layout() plt.show(); In\u00a0[9]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics individual forecasters')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\nprint(f\"Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics individual forecasters') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") print(f\"Temperature: {metric_temperature:.2f}\") <pre>Backtesting metrics individual forecasters\n===================================================\nDemand: 571.22\nTemperature: 1.88\n</pre> <p>Now, a multi-series (non-multivariate) forecaster is trained to model both series at the same time. As for the single forecasters, its performance is measured with backtesting.</p> In\u00a0[10]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LinearRegression(),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster_multiseries(\n                                                 forecaster         = forecaster,\n                                                 series             = data[['Demand', 'Temperature']],\n                                                 level              = 'Temperature',\n                                                 initial_train_size = len(data_train),\n                                                 fixed_train_size   = True,\n                                                 steps              = 24,\n                                                 metric             = 'mean_absolute_error',\n                                                 refit              = True,\n                                                 verbose            = False\n                                              )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LinearRegression(),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     )  metric_temperature, predictions_temperature = backtesting_forecaster_multiseries(                                                  forecaster         = forecaster,                                                  series             = data[['Demand', 'Temperature']],                                                  level              = 'Temperature',                                                  initial_train_size = len(data_train),                                                  fixed_train_size   = True,                                                  steps              = 24,                                                  metric             = 'mean_absolute_error',                                                  refit              = True,                                                  verbose            = False                                               ) In\u00a0[11]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics using a multi-series forecaster')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\nprint(f\"Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics using a multi-series forecaster') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") print(f\"Temperature: {metric_temperature:.2f}\") <pre>Backtesting metrics using a multi-series forecaster\n===================================================\nDemand: 571.41\nTemperature: 1.78\n</pre> <p>The performance of both approaches is very similar, then, using a multi-series model has the advantage of maintaining onl one model instead of two.</p> <p>In the previous example, both time series had a similar dynamic: the current value is highly correlated with the lags 1, 2, 3, 24, 48. What would happen if one the series do not share the same internal dynamics?</p> <p>To better understand the behavior of the multi-series forecaster in this scenario, the variable temperature is replaced with random noise and then, the forecaster is fitted and evaluated.</p> In\u00a0[12]: Copied! <pre># Copy data and modify Temperature column as a normal distribution\n# ==============================================================================\nnp.random.seed(123)\ndata_modified = data.copy()\ndata_modified['Temperature'] = np.random.normal(size=len(data_modified), loc=10, scale=1)\n\n# Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train_modified = data_modified.loc[:end_train, :].copy()\ndata_test_modified  = data_modified.loc[end_train:, :].copy()\n\nprint(f\"Train dates : {data_train_modified.index.min()} --- {data_train_modified.index.max()}  (n={len(data_train_modified)})\")\nprint(f\"Test dates  : {data_test_modified.index.min()} --- {data_test_modified.index.max()}  (n={len(data_test_modified)})\")\n\n# Plot Temperature\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\ndata_train_modified['Demand'].plot(label='train', ax=axes[0])\ndata_test_modified['Demand'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_train_modified['Temperature'].plot(label='train', ax=axes[1])\ndata_test_modified['Temperature'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('MW')\naxes[1].set_title('Energy demand')\naxes[1].legend()\naxes[1].legend()\nplt.show();\n</pre> # Copy data and modify Temperature column as a normal distribution # ============================================================================== np.random.seed(123) data_modified = data.copy() data_modified['Temperature'] = np.random.normal(size=len(data_modified), loc=10, scale=1)  # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train_modified = data_modified.loc[:end_train, :].copy() data_test_modified  = data_modified.loc[end_train:, :].copy()  print(f\"Train dates : {data_train_modified.index.min()} --- {data_train_modified.index.max()}  (n={len(data_train_modified)})\") print(f\"Test dates  : {data_test_modified.index.min()} --- {data_test_modified.index.max()}  (n={len(data_test_modified)})\")  # Plot Temperature # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True) data_train_modified['Demand'].plot(label='train', ax=axes[0]) data_test_modified['Demand'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_train_modified['Temperature'].plot(label='train', ax=axes[1]) data_test_modified['Temperature'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('MW') axes[1].set_title('Energy demand') axes[1].legend() axes[1].legend() plt.show(); <pre>Train dates : 2014-06-01 00:00:00 --- 2014-07-15 23:00:00  (n=1080)\nTest dates  : 2014-07-16 00:00:00 --- 2014-07-31 23:00:00  (n=384)\n</pre> In\u00a0[13]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LinearRegression(),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data_modified[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train_modified),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LinearRegression(),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data_modified[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train_modified),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     ) In\u00a0[14]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics using a multi-series forecaster')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics using a multi-series forecaster') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") <pre>Backtesting metrics using a multi-series forecaster\n===================================================\nDemand: 793.37\n</pre> <p>The error of the model has increased remarkably. This is due two the fact that the model is constrained to learn a unique global pattern for both series. Since the time series have very different internal dynamics (relation between their current and past values) the model is no capable of learning a pattern that suits both series.</p> <p>The same experiment is repeated, but this time using a non lineal model.</p> In\u00a0[15]: Copied! <pre># ForecasterAutoreg with non linear models\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags             = 24,\n                 #transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nmetric_demand, predictions_demand = backtesting_forecaster(\n                                        forecaster         = forecaster,\n                                        y                  = data['Demand'],\n                                        initial_train_size = len(data_train),\n                                        fixed_train_size   = True,\n                                        steps              = 24,\n                                        metric             = 'mean_absolute_error',\n                                        refit              = True,\n                                        verbose            = False\n                                )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster(\n                                                    forecaster         = forecaster,\n                                                    y                  = data['Temperature'],\n                                                    initial_train_size = len(data_train),\n                                                    fixed_train_size   = True,\n                                                    steps              = 24,\n                                                    metric             = 'mean_absolute_error',\n                                                    refit              = True,\n                                                    verbose            = False\n                                              )\n</pre> # ForecasterAutoreg with non linear models # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags             = 24,                  #transformer_y    = StandardScaler(),                  transformer_exog = None              )  metric_demand, predictions_demand = backtesting_forecaster(                                         forecaster         = forecaster,                                         y                  = data['Demand'],                                         initial_train_size = len(data_train),                                         fixed_train_size   = True,                                         steps              = 24,                                         metric             = 'mean_absolute_error',                                         refit              = True,                                         verbose            = False                                 )  metric_temperature, predictions_temperature = backtesting_forecaster(                                                     forecaster         = forecaster,                                                     y                  = data['Temperature'],                                                     initial_train_size = len(data_train),                                                     fixed_train_size   = True,                                                     steps              = 24,                                                     metric             = 'mean_absolute_error',                                                     refit              = True,                                                     verbose            = False                                               )  In\u00a0[16]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\nprint(f\"Backtest error Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") print(f\"Backtest error Temperature: {metric_temperature:.2f}\") <pre>Backtest error Demand: 632.25\nBacktest error Temperature: 2.16\n</pre> In\u00a0[17]: Copied! <pre># ForecasterAutoregMultiSeries with non linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 #transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster_multiseries(\n                                                 forecaster         = forecaster,\n                                                 series             = data[['Demand', 'Temperature']],\n                                                 level              = 'Temperature',\n                                                 initial_train_size = len(data_train),\n                                                 fixed_train_size   = True,\n                                                 steps              = 24,\n                                                 metric             = 'mean_absolute_error',\n                                                 refit              = True,\n                                                 verbose            = False\n                                              )\n</pre> # ForecasterAutoregMultiSeries with non linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  #transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     )  metric_temperature, predictions_temperature = backtesting_forecaster_multiseries(                                                  forecaster         = forecaster,                                                  series             = data[['Demand', 'Temperature']],                                                  level              = 'Temperature',                                                  initial_train_size = len(data_train),                                                  fixed_train_size   = True,                                                  steps              = 24,                                                  metric             = 'mean_absolute_error',                                                  refit              = True,                                                  verbose            = False                                               ) In\u00a0[18]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\nprint(f\"Backtest error Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") print(f\"Backtest error Temperature: {metric_temperature:.2f}\") <pre>Backtest error Demand: 664.99\nBacktest error Temperature: 2.01\n</pre> <p>Again, the performance of moth approaches are very similar. Let's see what happen if the resies do not share the same dinamycs.</p> In\u00a0[24]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data_modified[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train_modified),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data_modified[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train_modified),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     ) In\u00a0[25]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") <pre>Backtest error Demand: 684.26\n</pre> <p>When using a non linear model, the forecasting performance is not harmed even though the time series do not follow the same dynamics. This is because the model is being able to identify that the series have more differences than similarities, and has flexibility enough to learn a different pattern for each one of the series.</p> <p>One way to verify this hypothesis is to analyze how the threes of the model are created.</p> In\u00a0[22]: Copied! <pre>from sklearn.tree import export_text\n\nforecaster.fit(series=data_modified[['Demand', 'Temperature']])\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n\nfor tree in forecaster.regressor.estimators_[:3]:\n    \n    print(export_text(\n        decision_tree = tree,\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> from sklearn.tree import export_text  forecaster.fit(series=data_modified[['Demand', 'Temperature']]) feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)  for tree in forecaster.regressor.estimators_[:3]:          print(export_text(         decision_tree = tree,         feature_names = feature_names,         max_depth = 2     ) )  <pre>|--- lag_1 &lt;= -0.01\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_24 &lt;= -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_3 &lt;= 1.66\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_3 &gt;  1.66\n|   |   |   |--- truncated branch of depth 9\n|--- lag_1 &gt;  -0.01\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_22 &lt;= 1.39\n|   |   |   |--- truncated branch of depth 26\n|   |   |--- lag_22 &gt;  1.39\n|   |   |   |--- truncated branch of depth 9\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_1 &lt;= 0.65\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.65\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_1 &lt;= -0.24\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_24 &lt;= -0.61\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  -0.61\n|   |   |   |--- truncated branch of depth 16\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_1 &lt;= -0.25\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_1 &gt;  -0.25\n|   |   |   |--- truncated branch of depth 3\n|--- lag_1 &gt;  -0.24\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_21 &lt;= 2.56\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_21 &gt;  2.56\n|   |   |   |--- truncated branch of depth 3\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_1 &lt;= 0.53\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.53\n|   |   |   |--- truncated branch of depth 19\n\n|--- lag_1 &lt;= 0.29\n|   |--- lag_24 &lt;= -0.76\n|   |   |--- Temperature &lt;= 0.50\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- Temperature &gt;  0.50\n|   |   |   |--- truncated branch of depth 15\n|   |--- lag_24 &gt;  -0.76\n|   |   |--- lag_24 &lt;= 0.18\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_24 &gt;  0.18\n|   |   |   |--- truncated branch of depth 22\n|--- lag_1 &gt;  0.29\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_1 &lt;= 1.05\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  1.05\n|   |   |   |--- truncated branch of depth 17\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_3 &lt;= 2.04\n|   |   |   |--- truncated branch of depth 24\n|   |   |--- lag_3 &gt;  2.04\n|   |   |   |--- truncated branch of depth 4\n\n</pre> In\u00a0[26]: Copied! <pre>forecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 transformer_exog   = None\n             )\n\n\nforecaster.fit(series=data_modified[['Demand', 'Temperature']])\n\nfor tree in forecaster.regressor.estimators_[:10]:\n    \n    print(export_text(\n        decision_tree = tree,\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  transformer_exog   = None              )   forecaster.fit(series=data_modified[['Demand', 'Temperature']])  for tree in forecaster.regressor.estimators_[:10]:          print(export_text(         decision_tree = tree,         feature_names = feature_names,         max_depth = 2     ) ) <pre>|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9965.84\n|   |   |--- lag_24 &lt;= 8731.67\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  8731.67\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9965.84\n|   |   |--- lag_1 &lt;= 10980.40\n|   |   |   |--- truncated branch of depth 14\n|   |   |--- lag_1 &gt;  10980.40\n|   |   |   |--- truncated branch of depth 16\n|--- Temperature &gt;  0.50\n|   |--- lag_17 &lt;= 10.41\n|   |   |--- lag_19 &lt;= 8.65\n|   |   |   |--- truncated branch of depth 11\n|   |   |--- lag_19 &gt;  8.65\n|   |   |   |--- truncated branch of depth 28\n|   |--- lag_17 &gt;  10.41\n|   |   |--- lag_3 &lt;= 10.28\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_3 &gt;  10.28\n|   |   |   |--- truncated branch of depth 19\n\n|--- Demand &lt;= 0.50\n|   |--- lag_16 &lt;= 10.74\n|   |   |--- lag_19 &lt;= 11.09\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_19 &gt;  11.09\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_16 &gt;  10.74\n|   |   |--- lag_11 &lt;= 10.99\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_11 &gt;  10.99\n|   |   |   |--- truncated branch of depth 10\n|--- Demand &gt;  0.50\n|   |--- lag_1 &lt;= 10028.92\n|   |   |--- lag_24 &lt;= 8855.49\n|   |   |   |--- truncated branch of depth 19\n|   |   |--- lag_24 &gt;  8855.49\n|   |   |   |--- truncated branch of depth 19\n|   |--- lag_1 &gt;  10028.92\n|   |   |--- lag_1 &lt;= 11282.66\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11282.66\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_5 &lt;= 3236.52\n|   |--- lag_15 &lt;= 12.22\n|   |   |--- lag_21 &lt;= 12.50\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_21 &gt;  12.50\n|   |   |   |--- truncated branch of depth 3\n|   |--- lag_15 &gt;  12.22\n|   |   |--- lag_11 &lt;= 9.68\n|   |   |   |--- truncated branch of depth 3\n|   |   |--- lag_11 &gt;  9.68\n|   |   |   |--- truncated branch of depth 3\n|--- lag_5 &gt;  3236.52\n|   |--- lag_1 &lt;= 9714.65\n|   |   |--- lag_24 &lt;= 8784.49\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8784.49\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9714.65\n|   |   |--- lag_1 &lt;= 11270.14\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11270.14\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_11 &lt;= 3236.52\n|   |--- lag_4 &lt;= 12.22\n|   |   |--- lag_24 &lt;= 9.37\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  9.37\n|   |   |   |--- truncated branch of depth 29\n|   |--- lag_4 &gt;  12.22\n|   |   |--- lag_24 &lt;= 10.36\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_24 &gt;  10.36\n|   |   |   |--- truncated branch of depth 3\n|--- lag_11 &gt;  3236.52\n|   |--- lag_1 &lt;= 9797.43\n|   |   |--- lag_24 &lt;= 8874.05\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_24 &gt;  8874.05\n|   |   |   |--- truncated branch of depth 19\n|   |--- lag_1 &gt;  9797.43\n|   |   |--- lag_1 &lt;= 11559.09\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_1 &gt;  11559.09\n|   |   |   |--- truncated branch of depth 14\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9798.85\n|   |   |--- lag_24 &lt;= 8788.75\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8788.75\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9798.85\n|   |   |--- lag_1 &lt;= 11236.58\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11236.58\n|   |   |   |--- truncated branch of depth 20\n|--- Temperature &gt;  0.50\n|   |--- lag_1 &lt;= 7.91\n|   |   |--- lag_5 &lt;= 9.75\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_5 &gt;  9.75\n|   |   |   |--- truncated branch of depth 5\n|   |--- lag_1 &gt;  7.91\n|   |   |--- lag_3 &lt;= 13.27\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_3 &gt;  13.27\n|   |   |   |--- value: [7.58]\n\n|--- lag_2 &lt;= 3236.21\n|   |--- lag_21 &lt;= 11.27\n|   |   |--- lag_3 &lt;= 11.13\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_3 &gt;  11.13\n|   |   |   |--- truncated branch of depth 14\n|   |--- lag_21 &gt;  11.27\n|   |   |--- lag_20 &lt;= 8.48\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_20 &gt;  8.48\n|   |   |   |--- truncated branch of depth 13\n|--- lag_2 &gt;  3236.21\n|   |--- lag_1 &lt;= 9764.90\n|   |   |--- lag_24 &lt;= 8486.44\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  8486.44\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9764.90\n|   |   |--- lag_1 &lt;= 11236.58\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11236.58\n|   |   |   |--- truncated branch of depth 17\n\n|--- lag_13 &lt;= 3236.21\n|   |--- lag_17 &lt;= 11.41\n|   |   |--- lag_18 &lt;= 11.56\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_18 &gt;  11.56\n|   |   |   |--- truncated branch of depth 12\n|   |--- lag_17 &gt;  11.41\n|   |   |--- lag_10 &lt;= 11.20\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_10 &gt;  11.20\n|   |   |   |--- truncated branch of depth 5\n|--- lag_13 &gt;  3236.21\n|   |--- lag_1 &lt;= 9384.82\n|   |   |--- lag_24 &lt;= 8669.37\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  8669.37\n|   |   |   |--- truncated branch of depth 13\n|   |--- lag_1 &gt;  9384.82\n|   |   |--- lag_1 &lt;= 11270.14\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_1 &gt;  11270.14\n|   |   |   |--- truncated branch of depth 14\n\n|--- lag_10 &lt;= 3236.52\n|   |--- lag_7 &lt;= 11.31\n|   |   |--- lag_17 &lt;= 8.35\n|   |   |   |--- truncated branch of depth 11\n|   |   |--- lag_17 &gt;  8.35\n|   |   |   |--- truncated branch of depth 24\n|   |--- lag_7 &gt;  11.31\n|   |   |--- lag_2 &lt;= 9.73\n|   |   |   |--- truncated branch of depth 13\n|   |   |--- lag_2 &gt;  9.73\n|   |   |   |--- truncated branch of depth 11\n|--- lag_10 &gt;  3236.52\n|   |--- lag_1 &lt;= 9797.43\n|   |   |--- lag_24 &lt;= 8673.36\n|   |   |   |--- truncated branch of depth 20\n|   |   |--- lag_24 &gt;  8673.36\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9797.43\n|   |   |--- lag_1 &lt;= 11297.26\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_1 &gt;  11297.26\n|   |   |   |--- truncated branch of depth 17\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9963.52\n|   |   |--- lag_24 &lt;= 8802.79\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8802.79\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9963.52\n|   |   |--- lag_1 &lt;= 11270.25\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11270.25\n|   |   |   |--- truncated branch of depth 16\n|--- Temperature &gt;  0.50\n|   |--- lag_21 &lt;= 11.23\n|   |   |--- lag_22 &lt;= 12.26\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_22 &gt;  12.26\n|   |   |   |--- truncated branch of depth 6\n|   |--- lag_21 &gt;  11.23\n|   |   |--- lag_9 &lt;= 10.55\n|   |   |   |--- truncated branch of depth 14\n|   |   |--- lag_9 &gt;  10.55\n|   |   |   |--- truncated branch of depth 9\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9170.38\n|   |   |--- lag_24 &lt;= 8648.16\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8648.16\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9170.38\n|   |   |--- lag_1 &lt;= 10517.94\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_1 &gt;  10517.94\n|   |   |   |--- truncated branch of depth 17\n|--- Temperature &gt;  0.50\n|   |--- lag_9 &lt;= 11.42\n|   |   |--- lag_9 &lt;= 11.35\n|   |   |   |--- truncated branch of depth 27\n|   |   |--- lag_9 &gt;  11.35\n|   |   |   |--- truncated branch of depth 6\n|   |--- lag_9 &gt;  11.42\n|   |   |--- lag_22 &lt;= 8.38\n|   |   |   |--- truncated branch of depth 3\n|   |   |--- lag_22 &gt;  8.38\n|   |   |   |--- truncated branch of depth 12\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(export_text(\n        decision_tree = forecaster.regressor.estimators_[0],\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> print(export_text(         decision_tree = forecaster.regressor.estimators_[0],         feature_names = feature_names,         max_depth = 2     ) ) <pre>|--- lag_1 &lt;= -0.01\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_3 &lt;= 1.66\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_3 &gt;  1.66\n|   |   |   |--- truncated branch of depth 9\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_24 &lt;= -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  -0.72\n|   |   |   |--- truncated branch of depth 17\n|--- lag_1 &gt;  -0.01\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_1 &lt;= 0.65\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.65\n|   |   |   |--- truncated branch of depth 16\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_22 &lt;= 1.39\n|   |   |   |--- truncated branch of depth 26\n|   |   |--- lag_22 &gt;  1.39\n|   |   |   |--- truncated branch of depth 9\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>r = export_text(\n        decision_tree = forecaster.regressor.estimators_[0],\n        feature_names = feature_names\n    )\n\nre.split(r\"|--- [a-zA-Z]\", r)\n</pre> r = export_text(         decision_tree = forecaster.regressor.estimators_[0],         feature_names = feature_names     )  re.split(r\"|--- [a-zA-Z]\", r) In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = []\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n</pre> top_3_nodes = [] feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns) In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = []\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n\nfor tree in forecaster.regressor.estimators_:\n    r = export_text(\n        decision_tree = tree,\n        feature_names = feature_names\n    )\n    r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')]\n    r = [re.sub('--- ', '', node) for node in r]\n    r = [re.sub('  +', '', node) for node in r]\n    r = [node.split(' ')[0]  for node in r]\n    r = [word for word in r if word in feature_names]\n    # remove duplicates\n    #r = list(dict.fromkeys(r))\n    top_3_nodes.append(r)\n</pre> top_3_nodes = [] feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)  for tree in forecaster.regressor.estimators_:     r = export_text(         decision_tree = tree,         feature_names = feature_names     )     r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')]     r = [re.sub('--- ', '', node) for node in r]     r = [re.sub('  +', '', node) for node in r]     r = [node.split(' ')[0]  for node in r]     r = [word for word in r if word in feature_names]     # remove duplicates     #r = list(dict.fromkeys(r))     top_3_nodes.append(r)  In\u00a0[\u00a0]: Copied! <pre>top_3_nodes[0]\n</pre> top_3_nodes[0] Out[\u00a0]: <pre>['lag_1',\n 'Demand',\n 'lag_3',\n 'lag_12',\n 'lag_4',\n 'lag_24',\n 'lag_24',\n 'lag_21',\n 'lag_21',\n 'lag_4',\n 'lag_9',\n 'lag_11',\n 'lag_10',\n 'lag_9',\n 'lag_9',\n 'lag_4',\n 'lag_4',\n 'lag_10',\n 'lag_19',\n 'lag_20',\n 'lag_15',\n 'lag_15',\n 'lag_20',\n 'lag_18',\n 'lag_18',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_11',\n 'lag_9',\n 'lag_14',\n 'lag_14',\n 'lag_7',\n 'lag_12',\n 'lag_12',\n 'lag_7',\n 'lag_17',\n 'lag_17',\n 'lag_9',\n 'lag_17',\n 'lag_7',\n 'lag_7',\n 'lag_17',\n 'lag_12',\n 'lag_9',\n 'lag_9',\n 'lag_12',\n 'lag_11',\n 'lag_11',\n 'lag_9',\n 'lag_19',\n 'lag_21',\n 'lag_18',\n 'lag_8',\n 'lag_4',\n 'lag_4',\n 'lag_8',\n 'lag_8',\n 'lag_8',\n 'lag_18',\n 'lag_15',\n 'lag_7',\n 'lag_7',\n 'lag_15',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_23',\n 'lag_12',\n 'lag_9',\n 'lag_9',\n 'lag_12',\n 'lag_16',\n 'lag_16',\n 'lag_23',\n 'lag_16',\n 'lag_4',\n 'lag_4',\n 'lag_16',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_9',\n 'lag_1',\n 'lag_17',\n 'lag_17',\n 'lag_1',\n 'lag_9',\n 'lag_20',\n 'lag_20',\n 'lag_8',\n 'lag_8',\n 'lag_4',\n 'lag_5',\n 'lag_3',\n 'lag_13',\n 'lag_13',\n 'lag_3',\n 'lag_5',\n 'lag_4',\n 'lag_14',\n 'lag_14',\n 'lag_4',\n 'lag_3',\n 'lag_3',\n 'lag_12',\n 'lag_10',\n 'lag_14',\n 'lag_15',\n 'lag_15',\n 'lag_14',\n 'lag_10',\n 'lag_2',\n 'lag_21',\n 'lag_10',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_22',\n 'lag_22',\n 'lag_20',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_22',\n 'lag_12',\n 'lag_12',\n 'lag_10',\n 'lag_12',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_12',\n 'lag_21',\n 'lag_12',\n 'lag_8',\n 'lag_8',\n 'lag_10',\n 'lag_10',\n 'lag_12',\n 'lag_8',\n 'lag_3',\n 'lag_3',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_2',\n 'lag_11',\n 'lag_24',\n 'lag_24',\n 'lag_19',\n 'lag_18',\n 'lag_2',\n 'lag_2',\n 'lag_18',\n 'lag_17',\n 'lag_17',\n 'lag_19',\n 'lag_7',\n 'lag_21',\n 'lag_21',\n 'lag_7',\n 'lag_11',\n 'lag_19',\n 'lag_21',\n 'lag_3',\n 'lag_19',\n 'lag_19',\n 'lag_3',\n 'lag_21',\n 'lag_4',\n 'lag_4',\n 'lag_23',\n 'lag_23',\n 'lag_19',\n 'lag_3',\n 'lag_18',\n 'lag_22',\n 'lag_15',\n 'lag_19',\n 'lag_23',\n 'lag_23',\n 'lag_19',\n 'lag_10',\n 'lag_10',\n 'lag_15',\n 'lag_18',\n 'lag_12',\n 'lag_12',\n 'lag_18',\n 'lag_22',\n 'lag_14',\n 'lag_18',\n 'lag_18',\n 'lag_3',\n 'lag_3',\n 'lag_14',\n 'lag_3',\n 'lag_3',\n 'lag_18',\n 'lag_15',\n 'lag_11',\n 'lag_14',\n 'lag_14',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_21',\n 'lag_2',\n 'lag_21',\n 'lag_21',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_9',\n 'lag_8',\n 'lag_8',\n 'lag_9',\n 'lag_21',\n 'lag_15',\n 'Demand',\n 'lag_24',\n 'lag_1',\n 'lag_8',\n 'lag_17',\n 'lag_15',\n 'lag_13',\n 'lag_13',\n 'lag_15',\n 'lag_17',\n 'lag_8',\n 'lag_12',\n 'lag_1',\n 'lag_5',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n 'lag_21',\n 'lag_21',\n 'lag_12',\n 'lag_5',\n 'lag_11',\n 'lag_1',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_11',\n 'lag_14',\n 'lag_14',\n 'lag_14',\n 'lag_14',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_23',\n 'lag_23',\n 'lag_5',\n 'lag_20',\n 'lag_20',\n 'lag_5',\n 'lag_11',\n 'lag_11',\n 'lag_23',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_12',\n 'lag_1',\n 'lag_7',\n 'lag_7',\n 'lag_7',\n 'lag_7',\n 'lag_1',\n 'lag_4',\n 'lag_8',\n 'lag_8',\n 'lag_4',\n 'lag_1',\n 'lag_11',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_1',\n 'lag_4',\n 'lag_1',\n 'lag_7',\n 'lag_2',\n 'lag_2',\n 'lag_7',\n 'lag_8',\n 'lag_8',\n 'lag_1',\n 'lag_3',\n 'lag_5',\n 'lag_5',\n 'lag_3',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_21',\n 'lag_1',\n 'lag_6',\n 'lag_6',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_21',\n 'lag_1',\n 'lag_3',\n 'lag_22',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_5',\n 'lag_22',\n 'lag_10',\n 'lag_24',\n 'lag_24',\n 'lag_11',\n 'lag_19',\n 'lag_19',\n 'lag_11',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_3',\n 'lag_24',\n 'lag_1',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_8',\n 'lag_16',\n 'lag_16',\n 'lag_1',\n 'lag_3',\n 'lag_24',\n 'lag_24',\n 'lag_3',\n 'lag_9',\n 'lag_23',\n 'lag_5',\n 'lag_8',\n 'lag_8',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_15',\n 'lag_15',\n 'lag_23',\n 'lag_1',\n 'lag_5',\n 'lag_1',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_2',\n 'lag_20',\n 'lag_20',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_4',\n 'lag_4',\n 'lag_24',\n 'lag_1',\n 'lag_13',\n 'lag_1',\n 'lag_23',\n 'lag_3',\n 'lag_3',\n 'lag_23',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_5',\n 'lag_18',\n 'lag_18',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_13',\n 'lag_21',\n 'lag_6',\n 'lag_6',\n 'lag_21',\n 'lag_1',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_24',\n 'lag_9',\n 'lag_12',\n 'lag_12',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_24',\n 'lag_3',\n 'lag_3',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_8',\n 'lag_11',\n 'lag_11',\n 'lag_8',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_5',\n 'lag_19',\n 'lag_19',\n 'lag_5',\n 'lag_1',\n 'lag_17',\n 'lag_16',\n 'lag_18',\n 'lag_18',\n 'lag_16',\n 'lag_19',\n 'lag_19',\n 'lag_17',\n 'lag_1',\n 'lag_15',\n 'lag_1',\n 'lag_24',\n 'lag_14',\n 'lag_24',\n 'lag_24',\n 'lag_14',\n 'lag_6',\n 'lag_9',\n 'lag_9',\n 'lag_6',\n 'lag_21',\n 'lag_21',\n 'lag_24',\n 'lag_19',\n 'lag_22',\n 'lag_9',\n 'lag_9',\n 'lag_22',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_14',\n 'lag_14',\n 'lag_1',\n 'lag_10',\n 'lag_20',\n 'lag_20',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_17',\n 'lag_17',\n 'lag_4',\n 'lag_15',\n 'lag_4',\n 'lag_23',\n 'lag_13',\n 'lag_23',\n 'lag_6',\n 'lag_6',\n 'lag_23',\n 'lag_24',\n 'lag_24',\n 'lag_13',\n 'lag_20',\n 'lag_15',\n 'lag_15',\n 'lag_20',\n 'lag_21',\n 'lag_21',\n 'lag_23',\n 'lag_11',\n 'lag_24',\n 'lag_5',\n 'lag_5',\n 'lag_24',\n 'lag_8',\n 'lag_8',\n 'lag_11',\n 'lag_10',\n 'lag_14',\n 'lag_14',\n 'lag_10',\n 'lag_4',\n 'lag_7',\n 'lag_6',\n 'lag_6',\n 'lag_7',\n 'lag_4',\n 'lag_6',\n 'lag_21',\n 'lag_21',\n 'lag_6',\n 'lag_23',\n 'lag_23',\n 'lag_4',\n 'lag_24',\n 'lag_1',\n 'lag_7',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_20',\n 'lag_24',\n 'lag_20',\n 'lag_20',\n 'lag_24',\n 'lag_20',\n 'lag_9',\n 'lag_6',\n 'lag_6',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_12',\n 'lag_14',\n 'lag_14',\n 'lag_12',\n 'lag_14',\n 'lag_6',\n 'lag_6',\n 'lag_14',\n 'lag_21',\n 'lag_21',\n 'lag_8',\n 'lag_7',\n 'lag_15',\n 'lag_9',\n 'lag_9',\n 'lag_15',\n 'lag_22',\n 'lag_22',\n 'lag_7',\n 'lag_14',\n 'lag_8',\n 'lag_8',\n 'lag_14',\n 'lag_7',\n 'lag_8',\n 'lag_12',\n 'lag_18',\n 'lag_13',\n 'lag_13',\n 'lag_18',\n 'lag_8',\n 'lag_18',\n 'lag_18',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_12',\n 'lag_1',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_20',\n 'lag_20',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_2',\n 'lag_24',\n 'lag_14',\n 'lag_23',\n 'lag_10',\n 'lag_23',\n 'lag_23',\n 'lag_10',\n 'lag_6',\n 'lag_6',\n 'lag_23',\n 'lag_14',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_11',\n 'lag_13',\n 'lag_13',\n 'lag_24',\n 'lag_4',\n 'lag_23',\n 'lag_23',\n 'lag_4',\n 'lag_2',\n 'lag_13',\n 'lag_16',\n 'lag_10',\n 'lag_10',\n 'lag_9',\n 'lag_9',\n 'lag_16',\n 'lag_14',\n 'lag_14',\n 'lag_13',\n 'lag_7',\n 'lag_24',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_20',\n 'lag_1',\n 'lag_1',\n 'lag_24',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_22',\n 'lag_7',\n 'lag_1',\n 'lag_24',\n 'lag_15',\n 'lag_15',\n 'lag_24',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_7',\n 'lag_7',\n 'lag_6',\n 'lag_6',\n 'Temperature',\n 'lag_1',\n 'lag_23',\n 'lag_20',\n 'lag_5',\n 'lag_4',\n 'lag_1',\n 'lag_14',\n 'lag_19',\n 'lag_11',\n 'lag_11',\n 'lag_19',\n 'lag_21',\n 'lag_21',\n 'lag_14',\n 'lag_6',\n 'lag_15',\n 'lag_15',\n 'lag_6',\n 'lag_1',\n 'lag_22',\n 'lag_3',\n 'lag_3',\n 'lag_22',\n 'lag_11',\n 'lag_11',\n 'lag_12',\n 'lag_12',\n 'lag_4',\n 'lag_20',\n 'lag_1',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_9',\n 'lag_1',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_20',\n 'lag_24',\n 'lag_1',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_24',\n 'lag_5',\n 'lag_1',\n 'lag_4',\n 'lag_18',\n 'lag_10',\n 'lag_10',\n 'lag_18',\n 'lag_4',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_21',\n 'lag_18',\n 'lag_12',\n 'lag_12',\n 'lag_18',\n 'lag_21',\n 'lag_11',\n 'lag_11',\n 'lag_20',\n 'lag_3',\n 'lag_3',\n 'lag_14',\n 'lag_20',\n 'lag_8',\n 'lag_8',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_8',\n 'lag_8',\n 'lag_14',\n 'lag_3',\n 'lag_21',\n 'lag_21',\n 'lag_20',\n 'lag_9',\n 'lag_18',\n 'lag_18',\n 'lag_9',\n 'lag_20',\n 'lag_3',\n 'lag_1',\n 'lag_20',\n 'lag_2',\n 'lag_19',\n 'lag_1',\n 'lag_1',\n 'lag_19',\n 'lag_2',\n 'lag_21',\n 'lag_21',\n 'lag_7',\n 'lag_7',\n 'lag_20',\n 'lag_1',\n 'lag_10',\n 'lag_14',\n 'lag_14',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_1',\n 'lag_19',\n 'lag_19',\n 'lag_20',\n 'lag_20',\n 'lag_1',\n 'lag_18',\n 'lag_22',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_1',\n 'lag_16',\n 'lag_16',\n 'lag_1',\n 'lag_3',\n 'lag_3',\n 'lag_18',\n 'lag_12',\n 'lag_1',\n 'lag_19',\n 'lag_19',\n 'lag_1',\n 'lag_13',\n 'lag_13',\n 'lag_12',\n 'lag_2',\n 'lag_14',\n 'lag_14',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_23',\n 'lag_10',\n 'lag_12',\n 'lag_14',\n 'lag_13',\n 'lag_9',\n 'lag_10',\n 'lag_10',\n 'lag_9',\n 'lag_1',\n 'lag_1',\n 'lag_15',\n 'lag_15',\n 'lag_13',\n 'lag_13',\n 'lag_24',\n 'lag_4',\n 'lag_4',\n 'lag_24',\n 'lag_22',\n 'lag_22',\n 'lag_13',\n 'lag_12',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n 'lag_18',\n 'lag_18',\n 'lag_14',\n 'lag_11',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_16',\n 'lag_16',\n 'lag_22',\n 'lag_1',\n 'lag_3',\n 'lag_3',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_11',\n 'lag_8',\n 'lag_1',\n 'lag_1',\n 'lag_8',\n 'lag_12',\n 'lag_17',\n 'lag_17',\n 'lag_11',\n 'lag_22',\n 'lag_22',\n 'lag_11',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_10',\n 'lag_11',\n 'lag_5',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_8',\n 'lag_13',\n 'lag_17',\n 'lag_17',\n 'lag_13',\n 'lag_7',\n 'lag_7',\n 'lag_5',\n 'lag_9',\n 'lag_6',\n 'lag_6',\n 'lag_4',\n 'lag_4',\n 'lag_9',\n 'lag_11',\n 'lag_1',\n 'lag_1',\n 'lag_18',\n 'lag_1',\n 'lag_16',\n 'lag_1',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_15',\n 'lag_15',\n 'lag_22',\n 'lag_9',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_8',\n 'lag_16',\n 'lag_1',\n 'lag_1',\n 'lag_21',\n 'lag_23',\n 'lag_23',\n 'lag_21',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_21',\n 'lag_5',\n 'lag_5',\n 'lag_21',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_1',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_21',\n 'lag_22',\n 'lag_22',\n 'lag_13',\n 'lag_13',\n 'lag_21',\n 'lag_19',\n 'lag_1',\n 'lag_20',\n 'lag_2',\n 'lag_2',\n 'lag_20',\n 'lag_1',\n 'lag_22',\n 'lag_1',\n 'lag_4',\n 'lag_4',\n 'lag_1',\n 'lag_22',\n 'lag_11',\n 'lag_11',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_10',\n 'lag_1',\n 'lag_9',\n 'lag_1',\n 'lag_13',\n 'lag_17',\n 'lag_17',\n 'lag_13',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_13',\n 'lag_13',\n 'lag_9',\n 'lag_22',\n 'lag_22',\n 'lag_21',\n 'lag_9',\n 'lag_9',\n 'lag_21',\n 'lag_23',\n 'lag_23',\n 'lag_1',\n 'lag_3',\n 'lag_6',\n 'lag_2',\n 'lag_2',\n 'lag_6',\n 'lag_22',\n 'lag_22',\n 'lag_3',\n 'lag_11',\n 'lag_4',\n 'lag_2',\n 'lag_2',\n 'lag_4',\n 'lag_5',\n 'lag_5',\n 'lag_11',\n 'lag_12',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>for feature in feature_names:\n    \n\ntop_3_nodes\n</pre> for feature in feature_names:       top_3_nodes <pre>\n  Cell In [118], line 4\n    top_3_nodes\n    ^\nIndentationError: expected an indented block\n</pre> In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = pd.DataFrame(\n                data = top_3_nodes,\n                columns= ['first_node', 'second_node', 'third_node']\n              )\ntop_3_nodes\n</pre> top_3_nodes = pd.DataFrame(                 data = top_3_nodes,                 columns= ['first_node', 'second_node', 'third_node']               ) top_3_nodes  Out[\u00a0]: first_node second_node third_node 0 lag_1 Demand lag_3 1 lag_1 Temperature lag_24 2 lag_1 lag_24 Temperature 3 lag_1 Temperature lag_24 4 lag_1 Demand lag_19 ... ... ... ... 95 lag_1 Demand lag_12 96 lag_1 Temperature lag_24 97 lag_1 Temperature lag_24 98 lag_1 Temperature lag_24 99 lag_1 Demand lag_18 <p>100 rows \u00d7 3 columns</p> In\u00a0[\u00a0]: Copied! <pre>for col in top_3_nodes.columns:\n    print(col)\n    print(\"---\")\n    print(top_3_nodes[col].value_counts())\n</pre> for col in top_3_nodes.columns:     print(col)     print(\"---\")     print(top_3_nodes[col].value_counts()) <pre>first_node\n---\nlag_1    100\nName: first_node, dtype: int64\nsecond_node\n---\nDemand         35\nTemperature    35\nlag_24         20\nlag_23         10\nName: second_node, dtype: int64\nthird_node\n---\nlag_24         35\nTemperature    19\nDemand         11\nlag_18          9\nlag_17          7\nlag_1           4\nlag_12          4\nlag_20          2\nlag_4           2\nlag_14          2\nlag_3           1\nlag_19          1\nlag_8           1\nlag_9           1\nlag_6           1\nName: third_node, dtype: int64\n</pre> In\u00a0[\u00a0]: Copied! <pre>import re\nr = [re.sub('\\|--- ', '', node) for node in r.split('|   |')[:3]]\nr = [re.sub('--- ', '', node) for node in r]\nr = [re.sub('  +', '', node) for node in r]\nr = [node.split(' ')[0]  for node in r]\nprint(r)\n</pre> import re r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')[:3]] r = [re.sub('--- ', '', node) for node in r] r = [re.sub('  +', '', node) for node in r] r = [node.split(' ')[0]  for node in r] print(r)  <pre>['lag_1', 'Demand', 'lag_19']\n</pre> In\u00a0[\u00a0]: Copied! <pre>r = [node.split(' ')[0]  for node in r]\n</pre> r = [node.split(' ')[0]  for node in r] Out[\u00a0]: <pre>['lag_1',\n 'Demand',\n 'lag_19',\n '',\n 'truncated',\n 'lag_19',\n '',\n 'truncated',\n 'Demand',\n 'lag_24',\n '',\n 'truncated',\n 'lag_24',\n '',\n 'truncated',\n 'Demand',\n 'lag_3',\n '',\n 'truncated',\n 'lag_3',\n '',\n 'truncated',\n 'Demand',\n 'lag_1',\n '',\n 'truncated',\n 'lag_1',\n '',\n 'truncated']</pre> In\u00a0[\u00a0]: Copied! <pre>[node.split(' ')[0]  for node in r]\n</pre> [node.split(' ')[0]  for node in r] Out[\u00a0]: <pre>['|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&lt;',\n '=',\n '',\n '-',\n '0',\n '.',\n '0',\n '1',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '9',\n '',\n '&lt;',\n '=',\n '',\n '1',\n '.',\n '1',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '2',\n '6',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '9',\n '',\n '&gt;',\n '',\n '',\n '1',\n '.',\n '1',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '2',\n '4',\n '',\n '&lt;',\n '=',\n '',\n '-',\n '0',\n '.',\n '6',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '6',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '2',\n '4',\n '',\n '&gt;',\n '',\n '',\n '-',\n '0',\n '.',\n '6',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '7',\n '\\n',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&gt;',\n '',\n '',\n '-',\n '0',\n '.',\n '0',\n '1',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '3',\n '',\n '&lt;',\n '=',\n '',\n '2',\n '.',\n '4',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '2',\n '5',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '3',\n '',\n '&gt;',\n '',\n '',\n '2',\n '.',\n '4',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '8',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '5',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '8',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '8',\n '\\n']</pre> In\u00a0[\u00a0]: Copied! <pre>list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n</pre> list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns) Out[\u00a0]: <pre>['lag_1',\n 'lag_2',\n 'lag_3',\n 'lag_4',\n 'lag_5',\n 'lag_6',\n 'lag_7',\n 'lag_8',\n 'lag_9',\n 'lag_10',\n 'lag_11',\n 'lag_12',\n 'lag_13',\n 'lag_14',\n 'lag_15',\n 'lag_16',\n 'lag_17',\n 'lag_18',\n 'lag_19',\n 'lag_20',\n 'lag_21',\n 'lag_22',\n 'lag_23',\n 'lag_24',\n 'Demand',\n 'Temperature']</pre> In\u00a0[\u00a0]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/single-vs-multi-series-forecasting.html#single-series-vs-multi-series-forecasting","title":"Single series vs multi series forecasting\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags. That is, the past values of the series are used to forecast its future. In multi-time series forecasting, two or more time series are modeled together using a single model.</p> <p>\u00bfIs it better to create a model for each series or a single one for all of them?</p>"},{"location":"faq/single-vs-multi-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#multi-series-forecaster-with-linear-models","title":"Multi series forecaster with linear models\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#multi-series-forecaster-with-non-linear-models","title":"Multi series forecaster with non linear models\u00b6","text":""},{"location":"faq/time-series-differentiation.html","title":"Time series differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p> <p>This methodology is very useful when dealing with time series that exhibit trends, as not all models can capture such trends properly. Among the many machine learning techniques, tree-based models, including decision trees, random forests, and gradient boosting machines (GBMs), stand out for their effectiveness and broad applicability to various domains. Nonetheless, these models are limited in their ability to extrapolate. Their inability to project values outside the observed range during training inevitably results in predicted values that deviate from the underlying trend.</p> <p>Skforecast, version 0.10.0 or higher, introduces a novel <code>differentiation</code> parameter within its forecasters. This parameter indicates that a differentiation process must be applied before training the model, and this task is performed through the internal use of a new class named <code>skforecast.preprocessing.TimeSeriesDifferentiator</code>. It is important to note that the entire differentiation process is automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p>  \u00a0 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from skforecast.preprocessing import TimeSeriesDifferentiator In\u00a0[2]: Copied! <pre># Differentiation with TimeSeriesDifferentiator\n# ==============================================================================\ny = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float)\ndiffenciator = TimeSeriesDifferentiator()\ndiffenciator.fit(y)\ny_diff = diffenciator.transform(y)\n\nprint(f\"Original time series   : {y}\")\nprint(f\"Differenced time series: {y_diff}\")\n</pre> # Differentiation with TimeSeriesDifferentiator # ============================================================================== y = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float) diffenciator = TimeSeriesDifferentiator() diffenciator.fit(y) y_diff = diffenciator.transform(y)  print(f\"Original time series   : {y}\") print(f\"Differenced time series: {y_diff}\") <pre>Original time series   : [ 5.  8. 12. 10. 14. 17. 21. 19.]\nDifferenced time series: [nan  3.  4. -2.  4.  3.  4. -2.]\n</pre> <p>The process of differencing can be reversed (integration) using the <code>inverse_transform</code> method.</p> In\u00a0[3]: Copied! <pre>diffenciator.inverse_transform(y_diff)\n</pre> diffenciator.inverse_transform(y_diff) Out[3]: <pre>array([ 5.,  8., 12., 10., 14., 17., 21., 19.])</pre> <p>  \u00a0 Warning </p> <p>The inverse transformation process, <code>inverse_transform</code>, is applicable only to the same time series that was previously differentiated using the same <code>TimeSeriesDifferentiator</code> object. This limitation arises from the need to use the initial n values of the time series (n equals the order of differentiation) to successfully reverse the differentiation. These values are stored when the <code>fit</code> method is executed.</p> <p>  \u00a0 Note </p> <p>An additional method <code>inverse_transform_next_window</code> is available in the <code>TimeSeriesDifferentiator</code>. This method is designed to be used inside the Forecasters to reverse the differentiation of the predicted values. If the Forecaster regressor is trained with a differentiated time series, then the predicted values will be differentiated as well. The <code>inverse_transform_next_window</code> method allows to return the predictions to the original scale, with the assumption that they start immediately after the last values observed (<code>last_window</code>).</p> In\u00a0[4]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import StandardScaler from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.preprocessing import TimeSeriesDifferentiator from skforecast.model_selection import backtesting_forecaster In\u00a0[5]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'\n    'master/data/AirPassengers.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y-%m')\ndata = data.set_index('Date')\ndata = data.asfreq('MS')\ndata = data['Passengers']\ndata = data.sort_index()\ndata.head(4)\n\n# Data partition train-test\n# ==============================================================================\nend_train = '1956-12-01 23:59:59'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 2.5))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'     'master/data/AirPassengers.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m') data = data.set_index('Date') data = data.asfreq('MS') data = data['Passengers'] data = data.sort_index() data.head(4)  # Data partition train-test # ============================================================================== end_train = '1956-12-01 23:59:59' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 2.5)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1949-01-01 00:00:00 --- 1956-12-01 00:00:00  (n=96)\nTest dates  : 1957-01-01 00:00:00 --- 1960-12-01 00:00:00  (n=48)\n</pre> <p>Two autoregressive forecasters are created, one with a scikit-learn <code>RandomForestRegressor</code> and the other with an <code>XGBoost</code>. Both are trained on data from 1949-01-01 to 1956-12-01 and produce forecasts for the next 48 months (4 years).</p> In\u00a0[6]: Copied! <pre># Forecasting without differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterAutoreg(\n                    regressor = RandomForestRegressor(random_state=963),\n                    lags      = 12\n                )\nforecaster_gb = ForecasterAutoreg(\n                    regressor = XGBRegressor(random_state=963),\n                    lags      = 12\n                )\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title(f'Forecasting without differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting without differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterAutoreg(                     regressor = RandomForestRegressor(random_state=963),                     lags      = 12                 ) forecaster_gb = ForecasterAutoreg(                     regressor = XGBRegressor(random_state=963),                     lags      = 12                 ) # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title(f'Forecasting without differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 54.79\nError (MAE) Gradient Boosting: 48.46\n</pre> <p>The plot shows that none of the models is capable of accurately predicting the trend. After a few steps, the predictions become nearly constant, close to the maximum values observed in the training data.</p> <p>Next, two new forecasters are trained using the same configuration, but with the argument <code>differentiation = 1</code>. This activates the internal process of differencing (order 1) the time series before training the model, and reverses the differentiation (also known as integration) for the predicted values.</p> In\u00a0[7]: Copied! <pre># Forecasting with differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterAutoreg(\n                    regressor       = RandomForestRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\nforecaster_gb = ForecasterAutoreg(\n                    regressor       = XGBRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title(f'Forecasting with differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting with differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterAutoreg(                     regressor       = RandomForestRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 ) forecaster_gb = ForecasterAutoreg(                     regressor       = XGBRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 ) # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title(f'Forecasting with differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 24.91\nError (MAE) Gradient Boosting: 24.60\n</pre> <p>This time, both models are able to follow the trend in their predictions.</p> In\u00a0[8]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\ndiferenciator = TimeSeriesDifferentiator(order=1)\ndata_diff = diferenciator.fit_transform(data)\ndata_diff = pd.Series(data_diff, index=data.index).dropna()\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=963),\n                 lags      = 15\n             )\nforecaster.fit(y=data_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\npredictions_1.head(5)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== diferenciator = TimeSeriesDifferentiator(order=1) data_diff = diferenciator.fit_transform(data) data_diff = pd.Series(data_diff, index=data.index).dropna()  forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=963),                  lags      = 15              ) forecaster.fit(y=data_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' predictions_1.head(5) Out[8]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[9]: Copied! <pre># Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1\n             )\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\npredictions_2.head(5)\n</pre> # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1              ) forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) predictions_2.head(5) Out[9]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[10]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>The predictions are the same for both approaches.</p> <p>The results are also equivalent when a transformation, such as standardization, is applied to the time series. In this case, the order of the steps is: transformation -&gt; differentiation -&gt; model fitting -&gt; prediction -&gt; inverse differentiation -&gt; inverse transformation.</p> In\u00a0[11]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nscaler = StandardScaler()\nscaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1))\ndata_scaled = scaler.transform(data.to_numpy().reshape(-1, 1))\ndata_scaled = pd.Series(data_scaled.flatten(), index=data.index)\ndata_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled)\ndata_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()\n\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=963),\n                 lags          = 15,\n             )\nforecaster.fit(y=data_scaled_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data_scaled.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\n# Revert the scaling\npredictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1))\npredictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy())\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\ndisplay(predictions_1.head(5))\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1,\n                 transformer_y = StandardScaler()\n             )\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\ndisplay(predictions_2.head(5))\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== scaler = StandardScaler() scaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1)) data_scaled = scaler.transform(data.to_numpy().reshape(-1, 1)) data_scaled = pd.Series(data_scaled.flatten(), index=data.index) data_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled) data_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()  forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=963),                  lags          = 15,              ) forecaster.fit(y=data_scaled_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data_scaled.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] # Revert the scaling predictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1)) predictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy()) predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' display(predictions_1.head(5))   # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1,                  transformer_y = StandardScaler()              ) forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) display(predictions_2.head(5)) <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>Next, the outcomes of the backtesting process are subjected to a comparative analysis. This comparison is more complex than the previous one, as the process of undoing the differentiation must be performed separately for each backtesting fold.</p> In\u00a0[13]: Copied! <pre># Backtesting with the time series differentiated by preprocessing before training\n# ==============================================================================\nsteps = 5\nforecaster_1 = ForecasterAutoreg(\n                   regressor = RandomForestRegressor(random_state=963),\n                   lags      = 15\n               )\n\n_, predictions_1 = backtesting_forecaster(\n                       forecaster            = forecaster_1,\n                       y                     = data_diff,\n                       steps                 = steps,\n                       metric                = 'mean_squared_error',\n                       initial_train_size    = len(data_diff.loc[:end_train]),\n                       fixed_train_size      = False,\n                       gap                   = 0,\n                       allow_incomplete_fold = True,\n                       refit                 = True,\n                       n_jobs                = 'auto',\n                       verbose               = False,\n                       show_progress         = True  \n                   )\n\n# Revert differentiation of predictions. Predictions of each fold must be reverted\n# individually. An id is added to each prediction to identify the fold to which it belongs.\npredictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'})\nfolds = len(predictions_1) / steps\nfolds = int(np.ceil(folds))\npredictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]\n\n# Add the previously observed value of the time series (only to the first prediction of each fold)\nprevious_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps]\nprevious_overved_values.name = 'previous_overved_value'\npredictions_1 = predictions_1.merge(\n                    previous_overved_values,\n                    left_index  = True,\n                    right_index = True,\n                    how         = 'left'\n                )\npredictions_1 = predictions_1.fillna(0)\npredictions_1['summed_value'] = (\n    predictions_1['pred_diff'] + predictions_1['previous_overved_value']\n)\n\n# Revert differentiation using the cumulative sum by fold\npredictions_1['pred'] = (\n    predictions_1\n    .groupby('backtesting_fold_id')\n    .apply(lambda x: x['summed_value'].cumsum())\n    .to_numpy()\n)\n\npredictions_1.head(5)\n</pre> # Backtesting with the time series differentiated by preprocessing before training # ============================================================================== steps = 5 forecaster_1 = ForecasterAutoreg(                    regressor = RandomForestRegressor(random_state=963),                    lags      = 15                )  _, predictions_1 = backtesting_forecaster(                        forecaster            = forecaster_1,                        y                     = data_diff,                        steps                 = steps,                        metric                = 'mean_squared_error',                        initial_train_size    = len(data_diff.loc[:end_train]),                        fixed_train_size      = False,                        gap                   = 0,                        allow_incomplete_fold = True,                        refit                 = True,                        n_jobs                = 'auto',                        verbose               = False,                        show_progress         = True                      )  # Revert differentiation of predictions. Predictions of each fold must be reverted # individually. An id is added to each prediction to identify the fold to which it belongs. predictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'}) folds = len(predictions_1) / steps folds = int(np.ceil(folds)) predictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]  # Add the previously observed value of the time series (only to the first prediction of each fold) previous_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps] previous_overved_values.name = 'previous_overved_value' predictions_1 = predictions_1.merge(                     previous_overved_values,                     left_index  = True,                     right_index = True,                     how         = 'left'                 ) predictions_1 = predictions_1.fillna(0) predictions_1['summed_value'] = (     predictions_1['pred_diff'] + predictions_1['previous_overved_value'] )  # Revert differentiation using the cumulative sum by fold predictions_1['pred'] = (     predictions_1     .groupby('backtesting_fold_id')     .apply(lambda x: x['summed_value'].cumsum())     .to_numpy() )  predictions_1.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[13]: pred_diff backtesting_fold_id previous_overved_value summed_value pred 1957-01-01 6.00 0 306.0 312.00 312.00 1957-02-01 -9.07 0 0.0 -9.07 302.93 1957-03-01 38.68 0 0.0 38.68 341.61 1957-04-01 -3.58 0 0.0 -3.58 338.03 1957-05-01 3.94 0 0.0 3.94 341.97 In\u00a0[14]: Copied! <pre># Backtesting with the time series differentiated internally\n# ==============================================================================\nforecaster_2 = ForecasterAutoreg(\n                   regressor       = RandomForestRegressor(random_state=963),\n                   lags            = 15,\n                   differentiation = 1\n               )\n\n_, predictions_2 = backtesting_forecaster(\n                       forecaster            = forecaster_2,\n                       y                     = data,\n                       steps                 = steps,\n                       metric                = 'mean_squared_error',\n                       initial_train_size    = len(data.loc[:end_train]),\n                       fixed_train_size      = False,\n                       gap                   = 0,\n                       allow_incomplete_fold = True,\n                       refit                 = True,\n                       n_jobs                = 'auto',\n                       verbose               = False,\n                       show_progress         = True  \n                   )\n\npredictions_2.head(5)\n</pre> # Backtesting with the time series differentiated internally # ============================================================================== forecaster_2 = ForecasterAutoreg(                    regressor       = RandomForestRegressor(random_state=963),                    lags            = 15,                    differentiation = 1                )  _, predictions_2 = backtesting_forecaster(                        forecaster            = forecaster_2,                        y                     = data,                        steps                 = steps,                        metric                = 'mean_squared_error',                        initial_train_size    = len(data.loc[:end_train]),                        fixed_train_size      = False,                        gap                   = 0,                        allow_incomplete_fold = True,                        refit                 = True,                        n_jobs                = 'auto',                        verbose               = False,                        show_progress         = True                      )  predictions_2.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[14]: pred 1957-01-01 312.00 1957-02-01 302.93 1957-03-01 341.61 1957-04-01 338.03 1957-05-01 341.97 In\u00a0[15]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred'])\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred']) <p>Finally, the validation is also performed for the predictions obtained with <code>predict_boostrapping</code>.</p> In\u00a0[16]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nforecaster_1 = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=963),\n                 lags      = 15\n             )\n\nforecaster_1.fit(y=data_diff.loc[:end_train])\nboot_predictions_diff = forecaster_1.predict_bootstrapping(\n                            steps  = steps,\n                            n_boot = 10\n                        )\n# Revert differentiation of predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\nboot_predictions_1 = boot_predictions_diff.copy()\nboot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0]\nboot_predictions_1 = boot_predictions_1.sort_index()\nboot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,]\nboot_predictions_1 = boot_predictions_1.asfreq('MS')\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster_2 = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1\n               )\n\nforecaster_2.fit(y=data.loc[:end_train])\nboot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== forecaster_1 = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=963),                  lags      = 15              )  forecaster_1.fit(y=data_diff.loc[:end_train]) boot_predictions_diff = forecaster_1.predict_bootstrapping(                             steps  = steps,                             n_boot = 10                         ) # Revert differentiation of predictions last_value_train = data.loc[:end_train].iloc[[-1]] boot_predictions_1 = boot_predictions_diff.copy() boot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0] boot_predictions_1 = boot_predictions_1.sort_index() boot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,] boot_predictions_1 = boot_predictions_1.asfreq('MS')   # Time series differentiated internally by the forecaster # ============================================================================== forecaster_2 = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1                )  forecaster_2.fit(y=data.loc[:end_train]) boot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10) In\u00a0[17]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2) In\u00a0[18]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/time-series-differentiation.html#time-series-differentiation","title":"Time series differentiation\u00b6","text":""},{"location":"faq/time-series-differentiation.html#timeseriesdifferentiator","title":"TimeSeriesDifferentiator\u00b6","text":"<p>TimeSeriesDifferentiator is a custom transformer that follows the preprocessing sklearn API. This means it has the method <code>fit</code>, <code>transform</code>, <code>fit_transform</code> and <code>inverse_transform</code>.</p>"},{"location":"faq/time-series-differentiation.html#forecasting-with-differentiation","title":"Forecasting with differentiation\u00b6","text":""},{"location":"faq/time-series-differentiation.html#internal-differentiation-vs-pre-processing","title":"Internal differentiation vs pre-processing\u00b6","text":"<p>Forecasters manage the differentiation process internally, so there is no need for additional pre-processing of the time series and post-processing of the predictions. This has several advantages, but before diving in, the results of both approaches are compared.</p>"},{"location":"faq/time-series-differentiation.html#conslusions","title":"Conslusions\u00b6","text":"<p>If, as demonstrated, the values are equivalent when differentiating the time series in a preprocessing step or when allowing the Forecaster to manage the differentiation internally, why the second alternative is better?</p> <ul> <li><p>Allowing the forecaster to manage all transformations internally guarantees that the same transformations are applied when the model is run on new data.</p> </li> <li><p>When the model is applied to new data that does not follow immediately after the training data (for example, if a model is not retrained for each prediction phase), the forecaster automatically increases the size of the last window needed to generate the predictors, as well as applying the differentiation to the incoming data and undoing it in the final predictions.</p> </li> </ul> <p>These transformations are non-trivial and very error-prone, so skforecast tries to avoid overcomplicating the already challenging task of forecasting time series.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html","title":"Forecaster Attributes","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n</pre> # Libraries # ============================================================================== import pandas as pd from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 5\n             )\n\nforecaster.fit(y=data['y'])\nforecaster\n</pre> # Download data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv') data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 5              )  forecaster.fit(y=data['y']) forecaster Out[2]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:26:16 \nLast fit date: 2023-09-09 16:26:16 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[3]: Copied! <pre># List of attributes\n# ==============================================================================\nfor attribute, value in forecaster.__dict__.items():\n    print(attribute)\n</pre> # List of attributes # ============================================================================== for attribute, value in forecaster.__dict__.items():     print(attribute) <pre>regressor\ntransformer_y\ntransformer_exog\nweight_func\ndifferentiation\ndifferentiator\nsource_code_weight_func\nlast_window\nindex_type\nindex_freq\ntraining_range\nincluded_exog\nexog_type\nexog_dtypes\nexog_col_names\nX_train_col_names\nin_sample_residuals\nout_sample_residuals\nfitted\ncreation_date\nfit_date\nskforcast_version\npython_version\nforecaster_id\nlags\nmax_lag\nwindow_size\nfit_kwargs\n</pre> In\u00a0[4]: Copied! <pre># Forecaster regressor\n# ==============================================================================\nforecaster.regressor\n</pre> # Forecaster regressor # ============================================================================== forecaster.regressor Out[4]: <pre>RandomForestRegressor(random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor<pre>RandomForestRegressor(random_state=123)</pre> In\u00a0[5]: Copied! <pre># Show regressor parameters\n# ==============================================================================\nforecaster.regressor.get_params(deep=True)\n</pre> # Show regressor parameters # ============================================================================== forecaster.regressor.get_params(deep=True) Out[5]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'criterion': 'squared_error',\n 'max_depth': None,\n 'max_features': 1.0,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 123,\n 'verbose': 0,\n 'warm_start': False}</pre> <p>  \u00a0 Note </p> <p>In the forecasters that follows a Direct Strategy, one instance of the regressor is trained for each step. All of them are stored in <code>self.regressors_</code></p> In\u00a0[6]: Copied! <pre># Forecaster lags\n# ==============================================================================\nforecaster.lags\n</pre> # Forecaster lags # ============================================================================== forecaster.lags Out[6]: <pre>array([1, 2, 3, 4, 5])</pre> In\u00a0[7]: Copied! <pre># Forecaster last window\n# ==============================================================================\nforecaster.last_window\n</pre> # Forecaster last window # ============================================================================== forecaster.last_window Out[7]: <pre>datetime\n2008-02-01    0.761822\n2008-03-01    0.649435\n2008-04-01    0.827887\n2008-05-01    0.816255\n2008-06-01    0.762137\nFreq: MS, Name: y, dtype: float64</pre> <p>  \u00a0 Tip </p> <p>Learn how to get your forecasters into production and get the most out of them with <code>last_window</code>. Using forecasting models in production.</p> In\u00a0[8]: Copied! <pre># Forecaster window size\n# ==============================================================================\nforecaster.window_size\n</pre> # Forecaster window size # ============================================================================== forecaster.window_size Out[8]: <pre>5</pre> <p>  \u00a0 Note </p> <p>In the forecasters that follows a Direct Strategy and in the  Independent multi-series forecasting this parameter is a <code>dict</code> containing the residuals for each regressor/serie.</p> In\u00a0[9]: Copied! <pre># Forecaster in-sample residuals\n# ==============================================================================\nprint(\"Length:\", len(forecaster.in_sample_residuals))\nforecaster.in_sample_residuals[:5]\n</pre> # Forecaster in-sample residuals # ============================================================================== print(\"Length:\", len(forecaster.in_sample_residuals)) forecaster.in_sample_residuals[:5] <pre>Length: 199\n</pre> Out[9]: <pre>array([ 0.02232928,  0.0597808 , -0.10463712, -0.03318622, -0.00291908])</pre> In\u00a0[10]: Copied! <pre># Forecaster out-of-sample residuals\n# ==============================================================================\nforecaster.out_sample_residuals\n</pre> # Forecaster out-of-sample residuals # ============================================================================== forecaster.out_sample_residuals In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"introduction-forecasting/forecaster-attributes.html#understanding-the-forecaster-attributes","title":"Understanding the forecaster attributes\u00b6","text":"<p>During the process of creating and training a forecaster, the object stores a lot of information in its attributes that can be useful to the user. We will explore the main attributes included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#create-and-train-a-forecaster","title":"Create and train a forecaster\u00b6","text":"<p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#regressor","title":"Regressor\u00b6","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#lags","title":"Lags\u00b6","text":"<p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#last-window","title":"Last window\u00b6","text":"<p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#window-size","title":"Window size\u00b6","text":"<p>The size of the data window needed to create the predictors. It is equal to <code>forecaster.max_lag</code>.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#in-sample-residuals","title":"In-sample residuals\u00b6","text":"<p>Residuals from models predicting training data. Only stored up to 1000 values. If <code>transformer_series</code> is not <code>None</code>, the residuals are stored in the transformed scale.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#out-of-sample-residuals","title":"Out-of-sample residuals\u00b6","text":"<p>Residuals from models predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <p>As no values have been added, the parameter is <code>None</code>.</p>"},{"location":"introduction-forecasting/forecaster-parameters.html","title":"Understanding the forecaster parameters","text":"<p>Understanding what can be done when initializing a forecaster with skforecast can have a significant impact on the accuracy and effectiveness of the model. This guide highlights key considerations to keep in mind when initializing a forecaster and how these functionalities can be used to create more powerful and accurate forecasting models in Python.</p> <p>We will explore the arguments that can be included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = None,\n                 lags             = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = None\n             )\n</code></pre> <p>Tip</p> <p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"introduction-forecasting/forecaster-parameters.html#general-parameters","title":"General parameters","text":""},{"location":"introduction-forecasting/forecaster-parameters.html#regressor","title":"Regressor","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API. Therefore, any of these regressors can be used to create a forecaster:</p> <ul> <li> <p>HistGradientBoostingRegressor</p> </li> <li> <p>LGBMRegressor</p> </li> <li> <p>XGBoost</p> </li> <li> <p>CatBoost</p> </li> </ul> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = None\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#lags","title":"Lags","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model. </p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <pre><code># Create a forecaster using 5 lags\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = 5\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#transformers","title":"Transformers","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <p>Both arguments expect an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform.</p> <p>More information: Scikit-learn transformers and pipelines.</p> <p>Example</p> <p>In this example, a scikit-learn <code>StandardScaler</code> preprocessor is used for both the time series and the exogenous variables.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = StandardScaler()\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#custom-weights","title":"Custom weights","text":"<p>The <code>weight_func</code> parameter allows the user to define custom weights for each observation in the time series. These custom weights can be used to assign different levels of importance to different time periods. For example, assign higher weights to recent data points and lower weights to older data points to emphasize the importance of recent observations in the forecast model.</p> <p>More information: Weighted time series forecasting.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = custom_weights,\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#differentiation","title":"Differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Skforecast, version 0.10.0 or higher, introduces a novel differentiation parameter within its Forecasters. </p> <p>More information: Time series differentiation.</p> <p>Warning</p> <p>The <code>differentiation</code> parameter is only available for the <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>, in the following versions it will be incorporated to the rest of the Forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = 1,\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#inclusion-of-kwargs-in-the-regressor-fit-method","title":"Inclusion of kwargs in the regressor fit method","text":"<p>Some regressors include the possibility to add some additional configuration during the fitting method. The predictor parameter <code>fit_kwargs</code> allows these arguments to be set when the forecaster is declared.</p> <p>Danger</p> <p>To add weights to the forecaster, it must be done through the <code>weight_func</code> argument and not through a <code>fit_kwargs</code>.</p> <p>Example</p> <p>The following example demonstrates the inclusion of categorical features in an LGBM regressor. This must be done during the <code>LGBMRegressor</code> fit method. Fit parameters lightgbm</p> <p>More information: Categorical features.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = {'categorical_feature': ['exog_1', 'exog_2']}\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#forecaster-id","title":"Forecaster ID","text":"<p>Name used as an identifier of the forecaster. It may be used, for example to identify the time series being modeled.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#direct-multi-step-parameters","title":"Direct multi-step parameters","text":"<p>For the Forecasters that follow a direct multi-step strategy (<code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>), there are two additional parameters in addition to those mentioned above.</p>"},{"location":"introduction-forecasting/forecaster-parameters.html#steps","title":"Steps","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. </p> <p>The number of models to be trained is specified by the <code>steps</code> parameter.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoregDirect(\n                 regressor        = RandomForestRegressor(),\n                 steps            = 5,\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 n_jobs           = 'auto',\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#number-of-jobs","title":"Number of jobs","text":"<p>The <code>n_jobs</code> parameter allows multi-process parallelization to train regressors for all <code>steps</code> simultaneously. </p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoregDirect(\n                 regressor        = RandomForestRegressor(),\n                 steps            = 5,\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 n_jobs           = 'auto',\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"introduction-forecasting/introduction-forecasting.html","title":"Introduction to forecasting","text":""},{"location":"introduction-forecasting/introduction-forecasting.html#time-series-and-forecasting","title":"Time series and forecasting","text":"<p>A time series is a sequence of data arranged chronologically and spaced at equal or irregular intervals. The forecasting process consists of predicting the future value of a time series, either by modeling the series solely based on its past behavior (autoregressive) or by incorporating other external variables.</p> <p> </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#machine-learning-for-forecasting","title":"Machine learning for forecasting","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model.</p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <p>This type of transformation also allows to include additional variables.</p> <p> Time series transformation including an exogenous variable. </p> <p>Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ... p are considered predictors for the target quantity of the time series at time step p+1. </p> <p> Diagram of training a machine learning model with time series data. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#single-step-forecasting","title":"Single-step forecasting","text":"<p>Single-step prediction is used when the goal is to predict only the next value of the series.</p> <p> Diagram of single-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-step-forecasting","title":"Multi-step forecasting","text":"<p>When working with time series, it is seldom needed to predict only the next element in the series (t+1). Instead, the most common goal is to predict a whole future interval (t+1, ..., t+n)  or a far point in time (t+n). Several strategies allow generating this type of prediction.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting","text":"<p>Since the value t(n-1) is required to predict t(n), and t(n-1) is unknown, a recursive process is applied in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting and can be easily generated with the <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> classes.</p> <p> Diagram of recursive multi-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#direct-multi-step-forecasting","title":"Direct multi-step forecasting","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. This entire process is automated in the <code>ForecasterAutoregDirect</code> class. </p> <p> Diagram of direct multi-step forecasting. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multiple-output-forecasting","title":"Multiple output forecasting","text":"<p>Some machine learning models, such as long short-term memory (LSTM) neural network, can predict simultaneously several values of a sequence (one-shot). This strategy is not currently implemented in skforecast library.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-time-series-forecasting","title":"Multi-time series forecasting","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model. There are two distinct strategies for multi-series forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#independent-multi-series-forecasting","title":"Independent Multi-Series Forecasting","text":"<p>A single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied</p> <p> Diagram of recursive forecasting with multiple independent time series. </p> <p>The <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes cover this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#dependent-multi-series-forecasting-multivariate-time-series","title":"Dependent Multi-Series Forecasting (multivariate time series)","text":"<p>All series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-variate-series context. </p> <p>The <code>ForecasterAutoregMultiVariate</code> class covers this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive prediction Probabilistic prediction Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-forecasting-models","title":"Backtesting forecasting models","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-without-refit","title":"Backtesting without refit","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)","text":"<p>In this approach, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin). </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin). </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit","text":"<p>The model is retrained every n iterations, a method often used when the model retraining is limited to certain time intervals, such as weekly, but a different time window, such as a day, needs to be used for prediction.</p> <p>This refit strategy can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p> Backtesting with intermittent refit. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-including-gap","title":"Backtesting including gap","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#which-strategy-should-i-use","title":"Which strategy should I use?","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy.</p> <p>To determine an appropriate strategy, factors such as use case characteristics, available computing resources, and time intervals between predictions must be considered. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li> <p>Prediction horizon: Suppose you need to predict the users of an App every Monday for the entire week. In this case, each iteration would be a seven-step prediction representing the seven days of the week.</p> </li> <li> <p>Refit strategy: Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to retrain the model when the error metric shows a consistent upward trend. This behavior can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with different <code>refit</code> strategies: <code>False</code> (no refit between predictions), refit every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (refit after every predictions). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>For a code example illustrating the backtesting process, refer to the Backtesting user guide.</p>"},{"location":"releases/releases.html","title":"Change Log","text":"<p>All significant changes to this project are documented in this release file.</p>"},{"location":"releases/releases.html#0101-2023-09-26","title":"[0.10.1] - [2023-09-26]","text":"<p>This is a minor release to fix a bug when using <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</p> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Bug fix <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0100-2023-09-07","title":"[0.10.0] - [2023-09-07]","text":"<p>The main changes in this release are:</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API and can be used with the <code>ForecasterSarimax</code>.</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series using the new skforecast preprocessor <code>TimeSeriesDifferentiator</code>.</p> </li> </ul> <p>Added</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API.</p> </li> <li> <p>Added <code>skforecast.preprocessing.TimeSeriesDifferentiator</code> to preprocess time series by differentiating or integrating them (reverse differentiation).</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Refactor <code>ForecasterSarimax</code> to work with both skforecast Sarimax and pmdarima ARIMA models.</p> </li> <li> <p>Replace <code>setup.py</code> with <code>pyproject.toml</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#091-2023-07-14","title":"[0.9.1] - [2023-07-14]","text":"<p>The main changes in this release are:</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul>"},{"location":"releases/releases.html#090-2023-07-09","title":"[0.9.0] - [2023-07-09]","text":"<p>The main changes in this release are:</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> include the <code>n_jobs</code> argument in their <code>fit</code> method, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>All backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> can be trained using series of different lengths. This means that the model can handle datasets with different numbers of data points in each series.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>scikit-learn 1.3.x</code>.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to <code>fit</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> allow to use series of different lengths for training.</p> </li> <li> <p>Added <code>show_progress</code> to grid search functions.</p> </li> <li> <p>Added functions <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> to <code>utils</code> to select the number of jobs to use during multi-process parallelization.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Remove <code>get_feature_importance</code> in favor of <code>get_feature_importances</code> in all Forecasters, (deprecated since 0.8.0).</p> </li> <li> <p>The <code>model_selection._create_backtesting_folds</code> function now also returns the last window indices and whether or not to train the forecaster.</p> </li> <li> <p>The <code>model_selection</code> functions <code>_backtesting_forecaster_refit</code> and <code>_backtesting_forecaster_no_refit</code> have been unified in <code>_backtesting_forecaster</code>.</p> </li> <li> <p>The <code>model_selection_multiseries</code> functions <code>_backtesting_forecaster_multiseries_refit</code> and <code>_backtesting_forecaster_multiseries_no_refit</code> have been unified in <code>_backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>The <code>model_selection_sarimax</code> functions <code>_backtesting_refit_sarimax</code> and <code>_backtesting_no_refit_sarimax</code> have been unified in <code>_backtesting_sarimax</code>.</p> </li> <li> <p><code>utils.preprocess_y</code> allows a pandas DataFrame as input.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Ensure reproducibility of Direct Forecasters when using <code>predict_bootstrapping</code>, <code>predict_dist</code> and <code>predict_interval</code> with a <code>list</code> of steps.</p> </li> <li> <p>The <code>create_train_X_y</code> method returns a dict of pandas Series as <code>y_train</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. This ensures that each series has the appropriate index according to the step to be trained.</p> </li> <li> <p>The <code>filter_train_X_y_for_step</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> now updates the index of <code>X_train_step</code> to ensure correct alignment with <code>y_train_step</code>.</p> </li> </ul>"},{"location":"releases/releases.html#081-2023-05-27","title":"[0.8.1] - [2023-05-27]","text":"<p>Added</p> <ul> <li>Argument <code>store_in_sample_residuals=True</code> in <code>fit</code> method added to all forecasters to speed up functions such as backtesting.</li> </ul> <p>Changed</p> <ul> <li>Refactor <code>utils.exog_to_direct</code> and <code>utils.exog_to_direct_numpy</code> to increase performance.</li> </ul> <p>Fixed</p> <ul> <li><code>utils.check_exog_dtypes</code> now compares the <code>dtype.name</code> instead of the <code>dtype</code>. (suggested by Metaming https://github.com/Metaming)</li> </ul>"},{"location":"releases/releases.html#080-2023-05-16","title":"[0.8.0] - [2023-05-16]","text":"<p>Added</p> <ul> <li> <p>Added the <code>fit_kwargs</code> argument to all forecasters to allow the inclusion of additional keyword arguments passed to the regressor's <code>fit</code> method.</p> </li> <li> <p>Added the <code>set_fit_kwargs</code> method to set the <code>fit_kwargs</code> attribute.</p> </li> <li> <p>Support for <code>pandas 2.0.x</code>.</p> </li> <li> <p>Added <code>exceptions</code> module with custom warnings.</p> </li> <li> <p>Added function <code>utils.check_exog_dtypes</code> to issue a warning if exogenous variables are one of type <code>init</code>, <code>float</code>, or <code>category</code>. Raise Exception if <code>exog</code> has categorical columns with non integer values.</p> </li> <li> <p>Added function <code>utils.get_exog_dtypes</code> to get the data types of the exogenous variables included during the training of the forecaster model. </p> </li> <li> <p>Added function <code>utils.cast_exog_dtypes</code> to cast data types of the exogenous variables using a dictionary as a mapping.</p> </li> <li> <p>Added function <code>utils.check_select_fit_kwargs</code> to check if the argument <code>fit_kwargs</code> is a dictionary and select only the keys used by the <code>fit</code> method of the regressor.</p> </li> <li> <p>Added function <code>model_selection._create_backtesting_folds</code> to provide train/test indices (position) for backtesting functions.</p> </li> <li> <p>Added argument <code>gap</code> to functions in <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to omit observations between training and prediction.</p> </li> <li> <p>Added argument <code>show_progress</code> to functions <code>model_selection.backtesting_forecaster</code>, <code>model_selection_multiseries.backtesting_forecaster_multiseries</code> and <code>model_selection_sarimax.backtesting_forecaster_sarimax</code> to indicate weather to show a progress bar.</p> </li> <li> <p>Added argument <code>remove_suffix</code>, default <code>False</code>, to the method <code>filter_train_X_y_for_step()</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the column names of the training matrices.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename optional dependency package <code>statsmodels</code> to <code>sarimax</code>. Now only <code>pmdarima</code> will be installed, <code>statsmodels</code> is no longer needed.</p> </li> <li> <p>Rename <code>get_feature_importance()</code> to <code>get_feature_importances()</code> in all Forecasters. <code>get_feature_importance()</code> method will me removed in skforecast 0.9.0.</p> </li> <li> <p>Refactor <code>get_feature_importances()</code> in all Forecasters.</p> </li> <li> <p>Remove <code>model_selection_statsmodels</code> in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>, (deprecated since 0.7.0).</p> </li> <li> <p>Remove attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> in favor of <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>, (deprecated since 0.7.0).</p> </li> <li> <p>The <code>utils.check_exog</code> function now includes a new optional parameter, <code>allow_nan</code>, that controls whether a warning should be issued if the input <code>exog</code> contains NaN values. </p> </li> <li> <p><code>utils.check_exog</code> is applied before and after <code>exog</code> transformations.</p> </li> <li> <p>The <code>utils.preprocess_y</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>The <code>utils.preprocess_exog</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>Replaced <code>tqdm.tqdm</code> by <code>tqdm.auto.tqdm</code>.</p> </li> <li> <p>Refactor <code>utils.exog_to_direct</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>The dtypes of exogenous variables are maintained when generating the training matrices with the <code>create_train_X_y</code> method in all the Forecasters.</li> </ul>"},{"location":"releases/releases.html#070-2023-03-21","title":"[0.7.0] - [2023-03-21]","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultiSeriesCustom</code>.</p> </li> <li> <p>Class <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code> (wrapper of pmdarima).</p> </li> <li> <p>Method <code>predict_interval()</code> to <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li> <p>Method <code>predict_bootstrapping()</code> to all forecasters, generate multiple forecasting predictions using a bootstrapping process.</p> </li> <li> <p>Method <code>predict_dist()</code> to all forecasters, fit a given probability distribution for each step using a bootstrapping process.</p> </li> <li> <p>Function <code>plot_prediction_distribution</code> in module <code>plot</code>.</p> </li> <li> <p>Alias <code>backtesting_forecaster_multivariate</code> for <code>backtesting_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>grid_search_forecaster_multivariate</code> for <code>grid_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>random_search_forecaster_multivariate</code> for <code>random_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Attribute <code>forecaster_id</code> to all Forecasters.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.7</code> compatibility.</p> </li> <li> <p>Added <code>python 3.11</code> compatibility.</p> </li> <li> <p><code>model_selection_statsmodels</code> is deprecated in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>. It will be removed in version 0.8.0.</p> </li> <li> <p>Remove <code>levels_weights</code> argument in <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, deprecated since version 0.6.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> renamed to <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>. Old names will be removed in version 0.8.0.</p> </li> <li> <p>Remove engine <code>'skopt'</code> in <code>bayesian_search_forecaster</code> in favor of engine <code>'optuna'</code>. To continue using it, use skforecast 0.6.0.</p> </li> <li> <p><code>in_sample_residuals</code> and <code>out_sample_residuals</code> are stored as numpy ndarrays instead of pandas series.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>set_out_sample_residuals()</code> is now expecting a <code>dict</code> for the <code>residuals</code> argument instead of a <code>pandas DataFrame</code>.</p> </li> <li> <p>Remove the <code>scikit-optimize</code> dependency.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Remove operator <code>**</code> in <code>set_params()</code> method for all forecasters.</p> </li> <li> <p>Replace <code>getfullargspec</code> in favor of <code>inspect.signature</code> (contribution by @jordisilv).</p> </li> </ul>"},{"location":"releases/releases.html#060-2022-11-30","title":"[0.6.0] - [2022-11-30]","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultivariate</code>.</p> </li> <li> <p>Function <code>initialize_lags</code> in <code>utils</code> module  to create lags values in the initialization of forecasters (applies to all forecasters).</p> </li> <li> <p>Function <code>initialize_weights</code> in <code>utils</code> module to check and initialize arguments <code>series_weights</code>and <code>weight_func</code> (applies to all forecasters).</p> </li> <li> <p>Argument <code>weights_func</code> in all Forecasters to allow weighted time series forecasting. Individual time based weights can be assigned to each value of the series during the model training.</p> </li> <li> <p>Argument <code>series_weights</code> in <code>ForecasterAutoregMultiSeries</code> to define individual weights each series.</p> </li> <li> <p>Include argument <code>random_state</code> in all Forecasters <code>set_out_sample_residuals</code> methods for random sampling with reproducible output.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>predict</code> and <code>predict_interval</code> methods allow the simultaneous prediction of multiple levels.</p> </li> <li> <p><code>backtesting_forecaster_multiseries</code> allows backtesting multiple levels simultaneously.</p> </li> <li> <p><code>metric</code> argument can be a list in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Function <code>multivariate_time_series_corr</code> in module <code>utils</code>.</p> </li> <li> <p>Function <code>plot_multivariate_time_series_corr</code> in module <code>plot</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> allows to predict specific steps.</p> </li> <li> <p>Remove <code>ForecasterAutoregMultiOutput</code> in favor of <code>ForecasterAutoregDirect</code>, (deprecated since 0.5.0).</p> </li> <li> <p>Rename function <code>exog_to_multi_output</code> to <code>exog_to_direct</code> in <code>utils</code> module.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, rename parameter <code>series_levels</code> to <code>series_col_names</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code> change type of <code>out_sample_residuals</code> to a <code>dict</code> of numpy ndarrays.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, delete argument <code>level</code> from method <code>set_out_sample_residuals</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>check_predict_input</code> function, argument <code>level</code> renamed to <code>levels</code> and <code>series_levels</code> renamed to <code>series_col_names</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>metrics_levels</code> output is now a pandas DataFrame.</p> </li> <li> <p>In <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, argument <code>levels_weights</code> is deprecated since version 0.6.0, and will be removed in version 0.7.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Refactor <code>_create_lags_</code> in <code>ForecasterAutoreg</code>, <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiSeries</code>. (suggested by Bennett https://github.com/Bennett561)</p> </li> <li> <p>Refactor <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, <code>filter_train_X_y_for_step</code> now starts at 1 (before 0).</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, DataFrame <code>y_train</code> now start with 1, <code>y_step_1</code> (before <code>y_step_0</code>).</p> </li> <li> <p>Remove <code>cv_forecaster</code> from module <code>model_selection</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, argument <code>last_window</code> predict method now works when it is a pandas DataFrame.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, fix bug transformers initialization.</p> </li> </ul>"},{"location":"releases/releases.html#051-2022-10-05","title":"[0.5.1] - [2022-10-05]","text":"<p>Added</p> <ul> <li> <p>Check that <code>exog</code> and <code>y</code> have the same length in <code>_evaluate_grid_hyperparameters</code> and <code>bayesian_search_forecaster</code> to avoid fit exception when <code>return_best</code>.</p> </li> <li> <p>Check that <code>exog</code> and <code>series</code> have the same length in <code>_evaluate_grid_hyperparameters_multiseries</code> to avoid fit exception when <code>return_best</code>.</p> </li> </ul> <p>Changed</p> <ul> <li>Argument <code>levels_list</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code> renamed to <code>levels</code>.</li> </ul> <p>Fixed</p> <ul> <li> <p><code>ForecasterAutoregMultiOutput</code> updated to match <code>ForecasterAutoregDirect</code>.</p> </li> <li> <p>Fix Exception to raise when <code>level_weights</code> does not add up to a number close to 1.0 (before was exactly 1.0) in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p><code>Create_train_X_y</code> in <code>ForecasterAutoregMultiSeries</code> now works when the forecaster is not fitted.</p> </li> </ul>"},{"location":"releases/releases.html#050-2022-09-23","title":"[0.5.0] - [2022-09-23]","text":"<p>Added</p> <ul> <li> <p>New arguments <code>transformer_y</code> (<code>transformer_series</code> for multiseries) and <code>transformer_exog</code> in all forecaster classes. It is for transforming (scaling, max-min, ...) the modeled time series and exogenous variables inside the forecaster.</p> </li> <li> <p>Functions in utils <code>transform_series</code> and <code>transform_dataframe</code> to carry out the transformation of the modeled time series and exogenous variables.</p> </li> <li> <p>Functions <code>_backtesting_forecaster_verbose</code>, <code>random_search_forecaster</code>, <code>_evaluate_grid_hyperparameters</code>, <code>bayesian_search_forecaster</code>, <code>_bayesian_search_optuna</code> and <code>_bayesian_search_skopt</code> in model_selection.</p> </li> <li> <p>Created <code>ForecasterAutoregMultiSeries</code> class for modeling multiple time series simultaneously.</p> </li> <li> <p>Created module <code>model_selection_multiseries</code>. Functions: <code>_backtesting_forecaster_multiseries_refit</code>, <code>_backtesting_forecaster_multiseries_no_refit</code>, <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p>Function <code>_check_interval</code> in utils. (suggested by Thomas Karaouzene https://github.com/tkaraouzene)</p> </li> <li> <p><code>metric</code> can be a list in <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, <code>backtesting_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Skforecast works with python 3.10.</p> </li> <li> <p>Functions <code>save_forecaster</code> and <code>load_forecaster</code> to module utils.</p> </li> <li> <p><code>get_feature_importance()</code> method checks if the forecast is fitted.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster</code> change default value of argument <code>fixed_train_size: bool=True</code>.</p> </li> <li> <p>Remove argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster</code> (deprecated since 0.4.2).</p> </li> <li> <p><code>backtesting_forecaster</code> verbose now includes fold size.</p> </li> <li> <p><code>grid_search_forecaster</code> results include the name of the used metric as column name.</p> </li> <li> <p>Remove <code>get_coef</code> method from <code>ForecasterAutoreg</code>, <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiOutput</code> (deprecated since 0.4.3).</p> </li> <li> <p><code>_get_metric</code> now allows <code>mean_squared_log_error</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput</code> has been renamed to <code>ForecasterAutoregDirect</code>. <code>ForecasterAutoregMultiOutput</code> will be removed in version 0.6.0.</p> </li> <li> <p><code>check_predict_input</code> updated to check <code>ForecasterAutoregMultiSeries</code> inputs.</p> </li> <li> <p><code>set_out_sample_residuals</code> has a new argument <code>transform</code> to transform the residuals before being stored.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p><code>fit</code> now stores <code>last_window</code> values with len = forecaster.max_lag in ForecasterAutoreg and ForecasterAutoregCustom.</p> </li> <li> <p><code>in_sample_residuals</code> stored as a <code>pd.Series</code> when <code>len(residuals) &gt; 1000</code>.</p> </li> </ul>"},{"location":"releases/releases.html#043-2022-03-18","title":"[0.4.3] - [2022-03-18]","text":"<p>Added</p> <ul> <li> <p>Checks if all elements in lags are <code>int</code> when creating ForecasterAutoreg and ForecasterAutoregMultiOutput.</p> </li> <li> <p>Add <code>fixed_train_size: bool=False</code> argument to <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code></p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename <code>get_metric</code> to <code>_get_metric</code>.</p> </li> <li> <p>Functions in model_selection module allow custom metrics.</p> </li> <li> <p>Functions in model_selection_statsmodels module allow custom metrics.</p> </li> <li> <p>Change function <code>set_out_sample_residuals</code> (ForecasterAutoreg and ForecasterAutoregCustom), <code>residuals</code> argument must be a <code>pandas Series</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p>Returned value of backtesting functions (model_selection and model_selection_statsmodels) is now a <code>float</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p><code>get_coef</code> and <code>get_feature_importance</code> methods unified in <code>get_feature_importance</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Requirements versions.</p> </li> <li> <p>Method <code>fit</code> doesn't remove <code>out_sample_residuals</code> each time the forecaster is fitted.</p> </li> <li> <p>Added random seed to residuals downsampling (ForecasterAutoreg and ForecasterAutoregCustom)</p> </li> </ul>"},{"location":"releases/releases.html#042-2022-01-08","title":"[0.4.2] - [2022-01-08]","text":"<p>Added</p> <ul> <li> <p>Increased verbosity of function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Random state argument in <code>backtesting_forecaster()</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Function <code>backtesting_forecaster()</code> do not modify the original forecaster.</p> </li> <li> <p>Deprecated argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Function <code>model_selection.time_series_spliter</code> renamed to <code>model_selection.time_series_splitter</code></p> </li> </ul> <p>Fixed</p> <ul> <li>Methods <code>get_coef</code> and <code>get_feature_importance</code> of <code>ForecasterAutoregMultiOutput</code> class return proper feature names.</li> </ul>"},{"location":"releases/releases.html#041-2021-12-13","title":"[0.4.1] - [2021-12-13]","text":"<p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li><code>fit</code> and <code>predict</code> transform pandas series and dataframes to numpy arrays if regressor is XGBoost.</li> </ul>"},{"location":"releases/releases.html#040-2021-12-10","title":"[0.4.0] - [2021-12-10]","text":"<p>Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit).</p> <p>Added</p> <ul> <li><code>ForecasterBase</code> as parent class</li> </ul> <p>Changed</p> <ul> <li> <p>Argument <code>y</code> must be pandas Series. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Argument <code>exog</code> must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Output of <code>predict</code> is a pandas Series with index according to the steps predicted.</p> </li> <li> <p>Scikitlearn pipelines are allowed as regressors.</p> </li> <li> <p><code>backtesting_forecaster</code> and <code>backtesting_forecaster_intervals</code> have been combined in a single function.</p> <ul> <li>It is possible to backtest forecasters already trained.</li> <li><code>ForecasterAutoregMultiOutput</code> allows incomplete folds.</li> <li>It is possible to update <code>out_sample_residuals</code> with backtesting residuals.</li> </ul> </li> <li> <p><code>cv_forecaster</code> has the option to update <code>out_sample_residuals</code> with backtesting residuals.</p> </li> <li> <p><code>backtesting_sarimax_statsmodels</code> and <code>cv_sarimax_statsmodels</code> have been combined in a single function.</p> </li> <li> <p><code>gridsearch_forecaster</code> use backtesting as validation strategy with the option of refit.</p> </li> <li> <p>Extended information when printing <code>Forecaster</code> object.</p> </li> <li> <p>All static methods for checking and preprocessing inputs moved to module utils.</p> </li> <li> <p>Remove deprecated class <code>ForecasterCustom</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#030-2021-09-01","title":"[0.3.0] - [2021-09-01]","text":"<p>Added</p> <ul> <li> <p>New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library:</p> <ul> <li><code>backtesting_autoreg_statsmodels</code></li> <li><code>cv_autoreg_statsmodels</code></li> <li><code>backtesting_sarimax_statsmodels</code></li> <li><code>cv_sarimax_statsmodels</code></li> <li><code>grid_search_sarimax_statsmodels</code></li> </ul> </li> <li> <p>Added attribute window_size to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>. It is equal to <code>max_lag</code>.</p> </li> </ul> <p>Changed</p> <ul> <li><code>cv_forecaster</code> returns cross-validation metrics and cross-validation predictions.</li> <li>Added an extra column for each parameter in the dataframe returned by <code>grid_search_forecaster</code>.</li> <li>statsmodels 0.12.2 added to requirements</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#020-2021-08-26","title":"[0.2.0] - [2021-08-26]","text":"<p>Added</p> <ul> <li> <p>Multiple exogenous variables can be passed as pandas DataFrame.</p> </li> <li> <p>Documentation at https://joaquinamatrodrigo.github.io/skforecast/</p> </li> <li> <p>New unit test</p> </li> <li> <p>Increased typing</p> </li> </ul> <p>Changed</p> <ul> <li>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#019-2021-07-27","title":"[0.1.9] - [2021-07-27]","text":"<p>Added</p> <ul> <li> <p>Logging total number of models to fit in <code>grid_search_forecaster</code>.</p> </li> <li> <p>Class <code>ForecasterAutoregCustom</code>.</p> </li> <li> <p>Method <code>create_train_X_y</code> to facilitate access to the training data matrix created from <code>y</code> and <code>exog</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p>Class <code>ForecasterCustom</code> has been renamed to <code>ForecasterAutoregCustom</code>. However, <code>ForecasterCustom</code> will still remain to keep backward compatibility.</p> </li> <li> <p>Argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p>Check if argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p><code>time_series_spliter</code> doesn't include the remaining observations in the last complete fold but in a new one when <code>allow_incomplete_fold=True</code>. Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.</p> </li> </ul> <p>Fixed</p> <ul> <li>Update lags of  <code>ForecasterAutoregMultiOutput</code> after <code>grid_search_forecaster</code>.</li> </ul>"},{"location":"releases/releases.html#0181-2021-05-17","title":"[0.1.8.1] - [2021-05-17]","text":"<p>Added</p> <ul> <li><code>set_out_sample_residuals</code> method to store or update out of sample residuals used by <code>predict_interval</code>.</li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster_intervals</code> and <code>backtesting_forecaster</code> print number of steps per fold.</p> </li> <li> <p>Only stored up to 1000 residuals.</p> </li> <li> <p>Improved verbose in <code>backtesting_forecaster_intervals</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Warning of inclompleted folds when using <code>backtesting_forecast</code> with a  <code>ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput.predict</code> allow exog data longer than needed (steps).</p> </li> <li> <p><code>backtesting_forecast</code> prints correctly the number of folds when remainder observations are cero.</p> </li> <li> <p>Removed named argument X in <code>self.regressor.predict(X)</code> to allow using XGBoost regressor.</p> </li> <li> <p>Values stored in <code>self.last_window</code> when training <code>ForecasterAutoregMultiOutput</code>. </p> </li> </ul>"},{"location":"releases/releases.html#018-2021-04-02","title":"[0.1.8] - [2021-04-02]","text":"<p>Added</p> <ul> <li>Class <code>ForecasterAutoregMultiOutput.py</code>: forecaster with direct multi-step predictions.</li> <li>Method <code>ForecasterCustom.predict_interval</code> and  <code>ForecasterAutoreg.predict_interval</code>: estimate prediction interval using bootstrapping.</li> <li><code>skforecast.model_selection.backtesting_forecaster_intervals</code> perform backtesting and return prediction intervals.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"releases/releases.html#017-2021-03-19","title":"[0.1.7] - [2021-03-19]","text":"<p>Added</p> <ul> <li>Class <code>ForecasterCustom</code>: same functionalities as <code>ForecasterAutoreg</code> but allows custom definition of predictors.</li> </ul> <p>Changed</p> <ul> <li><code>grid_search forecaster</code> adapted to work with objects <code>ForecasterCustom</code> in addition to <code>ForecasterAutoreg</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#016-2021-03-14","title":"[0.1.6] - [2021-03-14]","text":"<p>Added</p> <ul> <li>Method <code>get_feature_importances</code> to <code>skforecast.ForecasterAutoreg</code>.</li> <li>Added backtesting strategy in <code>grid_search_forecaster</code>.</li> <li>Added <code>backtesting_forecast</code> to <code>skforecast.model_selection</code>.</li> </ul> <p>Changed</p> <ul> <li>Method <code>create_lags</code> return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor.</li> <li>Renamed argument <code>X</code> to <code>last_window</code> in method <code>predict</code>.</li> <li>Renamed <code>ts_cv_forecaster</code> to <code>cv_forecaster</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#014-2021-02-15","title":"[0.1.4] - [2021-02-15]","text":"<p>Added</p> <ul> <li>Method <code>get_coef</code> to <code>skforecast.ForecasterAutoreg</code>.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"user_guides/autoregresive-forecaster.html","title":"Recursive multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:48:14 \nLast fit date: 2023-09-09 16:48:14 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[4]: <pre>2005-07-01    0.921840\n2005-08-01    0.954921\n2005-09-01    1.101716\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.00429855684785846\n</pre> In\u00a0[7]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100) predictions.head(3) Out[7]: pred lower_bound upper_bound 2005-07-01 0.921840 0.884362 0.960898 2005-08-01 0.954921 0.923163 0.980901 2005-09-01 1.101716 1.057791 1.143083 In\u00a0[8]: Copied! <pre># Plot prediction interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_test.plot(ax=ax, label=\"test\")\npredictions['pred'].plot(ax=ax, label=\"prediction\")\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'blue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.legend(loc=\"upper left\");\n</pre> # Plot prediction interval # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_test.plot(ax=ax, label=\"test\") predictions['pred'].plot(ax=ax, label=\"prediction\") ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'blue',     alpha = 0.3,     label = '80% interval' ) ax.legend(loc=\"upper left\"); In\u00a0[9]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[9]: feature importance 0 lag_1 0.012340 1 lag_2 0.085160 2 lag_3 0.013407 3 lag_4 0.004374 4 lag_5 0.003188 5 lag_6 0.003436 6 lag_7 0.003136 7 lag_8 0.007141 8 lag_9 0.007831 9 lag_10 0.012751 10 lag_11 0.009019 11 lag_12 0.807098 12 lag_13 0.004811 13 lag_14 0.016328 14 lag_15 0.009979 In\u00a0[10]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) X_train.head() Out[10]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1992-12-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1993-01-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 1993-02-01 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 In\u00a0[11]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[11]: <pre>datetime\n1992-10-01    0.568606\n1992-11-01    0.595223\n1992-12-01    0.771258\n1993-01-01    0.751503\n1993-02-01    0.387554\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) In\u00a0[13]: Copied! <pre># Predict using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressor.predict(X_train)\npredictions_training[:4]\n</pre> # Predict using the internal regressor # ============================================================================== predictions_training = forecaster.regressor.predict(X_train) predictions_training[:4] Out[13]: <pre>array([0.55271468, 0.56539424, 0.72799541, 0.73649315])</pre> <p>  \u00a0 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[14]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 lags            = 15,\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = RandomForestRegressor(random_state=123),                  lags            = 15,                  differentiation = 1              )  forecaster.fit(y=data_train) forecaster Out[14]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 16 \nWeight function included: False \nDifferentiation order: 1 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:48:26 \nLast fit date: 2023-09-09 16:48:27 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[15]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[15]: <pre>2005-07-01    0.894926\n2005-08-01    0.909836\n2005-09-01    1.017546\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[16]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting\u00b6","text":"<p>Recursive multi-step forecasting involves using the predicted values from previous time steps as input to forecast the values for the subsequent time steps. The model initially predicts one time step ahead and then uses that forecast as an input for the next time step, continuing this recursive process until the desired forecast horizon is reached.</p> <p>To clarify, since the value of $t_{n-1}$ is required to predict $t_{n}$, and $t_{n-1}$ is unknown, a recursive process is applied in which, each new prediction is based on the previous one.</p> <p> Diagram of recursive multi-step forecasting. </p> <p>When using machine learning models for recursive multi-step forecasting, one of the major challenges is transforming the time series data into a matrix format that relates each value of the series to the preceding time window (lags). This process can be complex and time-consuming, requiring careful feature engineering to ensure the model can accurately capture the underlying patterns in the data.</p> <p> Time series transformation including an exogenous variable. </p> <p>By using the classes <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>, it is possible to build machine learning models for recursive multi-step forecasting with ease. These classes can automatically transform the time series data into a matrix format suitable for input to a machine learning algorithm, and provide a range of options for tuning the model parameters to achieve the best possible performance.</p>"},{"location":"user_guides/autoregresive-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction-intervals","title":"Prediction intervals\u00b6","text":"<p>A prediction interval defines the interval within which the true value of <code>y</code> is expected to be found with a given probability. For example, the prediction interval (1, 99) can be expected to contain the true prediction value with 98% probability.</p> <p>By default, every forecaster can estimate prediction intervals using the method <code>predict_interval</code>. For a detailed explanation of the different prediction intervals available in skforecast visit Probabilistic forecasting.</p>"},{"location":"user_guides/autoregresive-forecaster.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p>"},{"location":"user_guides/autoregresive-forecaster.html#prediction-on-training-data","title":"Prediction on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data, the <code>predict()</code> method of the regressor stored inside the forecaster object can be accessed, and the training matrix can be passed as an input to this method. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p>"},{"location":"user_guides/autoregresive-forecaster.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/backtesting.html","title":"Backtesting forecaster","text":"<p>  \u00a0 Note </p> <p>Since skforecast 0.9.0, all backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance. This applies to all functions of the different <code>model_selection</code> modules.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='validation')\nax.legend()\nplt.show()\n\ndisplay(data.head(4))\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='validation') ax.legend() plt.show()  display(data.head(4)) <pre>Train dates      : 1991-07-01 00:00:00 --- 2002-01-01 00:00:00  (n=127)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 In\u00a0[3]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          n_jobs                = 'auto',\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           n_jobs                = 'auto',                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[4]: Copied! <pre>print(f\"Backtest error: {metric}\")\npredictions.head(4)\n</pre> print(f\"Backtest error: {metric}\") predictions.head(4) <pre>Backtest error: 0.00818535931502708\n</pre> Out[4]: pred 2002-02-01 0.594506 2002-03-01 0.785886 2002-04-01 0.698925 2002-05-01 0.790560 In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax) predictions.plot(ax=ax) ax.legend(); <p>  \u00a0 Note </p> <p>With an intermittent refit, the model is trained every <code>refit</code> * <code>steps</code> observations. In this case, 3 * 10 = 30 observations. This can be easily observed in the <code>verbose</code> logs.</p> In\u00a0[6]: Copied! <pre># Backtesting with intermittent refit\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          allow_incomplete_fold = True,\n                          refit                 = 3,\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting with intermittent refit # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           allow_incomplete_fold = True,                           refit                 = 3,                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <p>  \u00a0 Note </p> <p>In this example, although only the last 10 predictions (steps) are stored for model evaluation, the total number of predicted steps in each fold is 15 (steps + gap).</p> In\u00a0[7]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 5,\n                          allow_incomplete_fold = False,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = False  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 5,                           allow_incomplete_fold = False,                           refit                 = True,                           verbose               = True,                           show_progress         = False                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 7\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 5\n    Last fold has been excluded because it was incomplete.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-07-01 00:00:00 -- 2003-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2003-05-01 00:00:00 -- 2004-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2004-03-01 00:00:00 -- 2004-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2005-01-01 00:00:00 -- 2005-10-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-11-01 00:00:00 -- 2006-08-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-09-01 00:00:00 -- 2007-06-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-07-01 00:00:00 -- 2008-04-01 00:00:00  (n=10)\n\n</pre> In\u00a0[8]: Copied! <pre>print(f\"Backtest error: {metric}\")\npredictions.head(4)\n</pre> print(f\"Backtest error: {metric}\") predictions.head(4) <pre>Backtest error: 0.011194317866594718\n</pre> Out[8]: pred 2002-07-01 0.870897 2002-08-01 0.920207 2002-09-01 0.942503 2002-10-01 1.022695 In\u00a0[9]: Copied! <pre># Backtesting forecaster with prediction intervals\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          interval              = [5, 95],\n                          n_boot                = 500,\n                          verbose               = True,\n                          show_progress         = False\n                      )\n</pre> # Backtesting forecaster with prediction intervals # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           interval              = [5, 95],                           n_boot                = 500,                           verbose               = True,                           show_progress         = False                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> In\u00a0[10]: Copied! <pre>predictions.head()\n</pre> predictions.head() Out[10]: pred lower_bound upper_bound 2002-02-01 0.703579 0.599636 0.834616 2002-03-01 0.673522 0.572605 0.787942 2002-04-01 0.698319 0.589196 0.815748 2002-05-01 0.703042 0.593719 0.831795 2002-06-01 0.733776 0.632476 0.842865 In\u00a0[11]: Copied! <pre># Plot prediction intervals\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'red',\n    alpha = 0.2,\n    label = 'prediction interval'\n)\nax.legend();\n</pre> # Plot prediction intervals # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'red',     alpha = 0.2,     label = 'prediction interval' ) ax.legend(); In\u00a0[12]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(6, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(6, 3)) data.plot(ax=ax); <pre>Train dates      : 1992-04-01 00:00:00 --- 2002-01-01 00:00:00  (n=118)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> In\u00a0[13]: Copied! <pre># Backtest forecaster exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = False\n                      )\n</pre> # Backtest forecaster exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = True,                           show_progress         = False                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 118\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2002-01-01 00:00:00  (n=118)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2002-11-01 00:00:00  (n=128)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2003-09-01 00:00:00  (n=138)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1992-04-01 00:00:00 -- 2004-07-01 00:00:00  (n=148)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1992-04-01 00:00:00 -- 2005-05-01 00:00:00  (n=158)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1992-04-01 00:00:00 -- 2006-03-01 00:00:00  (n=168)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1992-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=178)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1992-04-01 00:00:00 -- 2007-11-01 00:00:00  (n=188)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> In\u00a0[14]: Copied! <pre>print(f\"Backtest error with exogenous variables: {metric}\")\n</pre> print(f\"Backtest error with exogenous variables: {metric}\") <pre>Backtest error with exogenous variables: 0.007800037462113706\n</pre> In\u00a0[15]: Copied! <pre>fig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend(loc=\"upper left\");\n</pre> fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:].plot(ax=ax) predictions.plot(ax=ax) ax.legend(loc=\"upper left\"); In\u00a0[16]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = custom_metric,\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = False,\n                          show_progress         = False\n                      )\n\nprint(f\"Backtest error custom metric: {metric}\")\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric   metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = custom_metric,                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = False,                           show_progress         = False                       )  print(f\"Backtest error custom metric: {metric}\") <pre>Backtest error custom metric: 0.005482666107945366\n</pre> In\u00a0[17]: Copied! <pre>metrics, predictions = backtesting_forecaster(\n                           forecaster            = forecaster,\n                           y                     = data['y'],\n                           steps                 = 10,\n                           metric                = ['mean_squared_error', 'mean_absolute_error'],\n                           initial_train_size    = len(data.loc[:end_train]),\n                           fixed_train_size      = False,\n                           gap                   = 0,\n                           allow_incomplete_fold = True,\n                           refit                 = True,\n                           verbose               = False,\n                           show_progress         = False\n                       )\n\nprint(f\"Backtest error metrics: {metrics}\")\n</pre> metrics, predictions = backtesting_forecaster(                            forecaster            = forecaster,                            y                     = data['y'],                            steps                 = 10,                            metric                = ['mean_squared_error', 'mean_absolute_error'],                            initial_train_size    = len(data.loc[:end_train]),                            fixed_train_size      = False,                            gap                   = 0,                            allow_incomplete_fold = True,                            refit                 = True,                            verbose               = False,                            show_progress         = False                        )  print(f\"Backtest error metrics: {metrics}\") <pre>Backtest error metrics: [0.00804575658744146, 0.06502216041298702]\n</pre> In\u00a0[18]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  forecaster.fit(y=data['y']) In\u00a0[19]: Copied! <pre># Backtesting on training data\n# ==============================================================================\nmetric, predictions_training = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data['y'],\n                                   steps              = 1,\n                                   metric             = 'mean_squared_error',\n                                   initial_train_size = None,\n                                   refit              = False,\n                                   verbose            = False,\n                                   show_progress      = False\n                               )\n\nprint(f\"Backtest training error: {metric}\")\n</pre> # Backtesting on training data # ============================================================================== metric, predictions_training = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data['y'],                                    steps              = 1,                                    metric             = 'mean_squared_error',                                    initial_train_size = None,                                    refit              = False,                                    verbose            = False,                                    show_progress      = False                                )  print(f\"Backtest training error: {metric}\") <pre>Backtest training error: 0.0005647902993476632\n</pre> In\u00a0[20]: Copied! <pre>predictions_training.head(4)\n</pre> predictions_training.head(4) Out[20]: pred 1993-07-01 0.515375 1993-08-01 0.558055 1993-09-01 0.597601 1993-10-01 0.618857 <p>It is important to note that the first 15 observations are excluded from the predictions since they are required to generate the lags that serve as predictors in the model.</p> In\u00a0[21]: Copied! <pre># Plot training predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata['y'].plot(ax=ax)\npredictions_training.plot(ax=ax)\nax.set_title(\"Backtesting on training data\")\nax.legend();\n</pre> # Plot training predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data['y'].plot(ax=ax) predictions_training.plot(ax=ax) ax.set_title(\"Backtesting on training data\") ax.legend(); In\u00a0[22]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/backtesting.html#backtesting","title":"Backtesting\u00b6","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"user_guides/backtesting.html#backtesting-without-refit","title":"Backtesting without refit\u00b6","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit. </p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)\u00b6","text":"<p>In this strategy, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin). </p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)\u00b6","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin). </p>"},{"location":"user_guides/backtesting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit\u00b6","text":"<p>The model is retrained every $n$ iterations of prediction. This method is often used when the frequency of retraining and prediction is different. It can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p>  \u00a0 Tip </p> <p>This strategy usually achieves a good balance between the computational cost of retraining and avoiding model degradation.</p> <p> Backtesting with intermittent refit. </p>"},{"location":"user_guides/backtesting.html#backtesting-including-gap","title":"Backtesting including gap\u00b6","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap. </p>"},{"location":"user_guides/backtesting.html#which-strategy-should-i-use","title":"Which strategy should I use?\u00b6","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy. Factors such as use case characteristics, available computing resources and time intervals between predictions need to be considered to determine which strategy to use. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li><p>Prediction horizon (<code>steps</code>): suppose you need to predict the users of an application every Monday for the whole week. In this case, each iteration of backtesting would be a seven-step prediction, representing the seven days of the week.</p> </li> <li><p>Refit strategy (<code>refit</code>): Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to compare different retrain frequencies and select the one for which the error metric shows a consistent upward trend. This behaviour can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with <code>steps=24</code> (predict 24 hours) and different <code>refit</code> strategies: <code>False</code> (no re-fitting between predictions), re-fitting every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (re-fitting after every prediction). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>In general, the more closely the backtesting process resembles the actual scenario in which the model is used, the more reliable the estimated metric will be.</p>"},{"location":"user_guides/backtesting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/backtesting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/backtesting.html#backtest","title":"Backtest\u00b6","text":"<p>An example of a backtesting with refit consists of the following steps:</p> <ol> <li><p>Train the model using an initial training set, the length of which is specified by <code>initial_train_size</code>.</p> </li> <li><p>Once the model is trained, it is used to make predictions for the next 10 steps (<code>steps=10</code>) in the data. These predictions are saved for later evaluation.</p> </li> <li><p>As <code>refit</code> is set to <code>True</code>, the size of the training set is increased by adding the lats 10 data points (the previously predicted 10 steps), while the next 10 steps are used as test data.</p> </li> <li><p>After expanding the training set, the model is retrained using the updated training data and then used to predict the next 10 steps.</p> </li> <li><p>Repeat steps 3 and 4 until the entire series has been tested.</p> </li> </ol> <p>By following these steps, you can ensure that the model is evaluated on multiple sets of test data, thereby providing a more accurate assessment of its predictive power.</p>"},{"location":"user_guides/backtesting.html#backtesting-intermittent-refit","title":"Backtesting intermittent refit\u00b6","text":"<p>The same backtesting as above is repeated, but this time with <code>refit = 3</code>. The training of the model is done every 3 folds instead of every fold.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-gap","title":"Backtesting with gap\u00b6","text":"<p>The gap size can be adjusted with the <code>gap</code> argument. In addition, the <code>allow_incomplete_fold</code> parameter allows the last fold to be excluded from the analysis if it doesn't have the same size as the required number of <code>steps</code>.</p> <p>These arguments can be used in conjunction with either refit set to <code>True</code> or <code>False</code>, depending on the needs and objectives of the use case.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>Backtesting can be used not only to obtain point estimate predictions but also to obtain prediction intervals. Prediction intervals provide a range of values within which the actual values are expected to fall with a certain level of confidence. By estimating prediction intervals during backtesting, one can get a better understanding of the uncertainty associated with your model's predictions. This information can be used to assess the reliability of the model's predictions and to make more informed decisions. To learn more about probabilistic forecasting, visit Probabilistic forecasting.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies discussed in this document can also be applied when incorporating exogenous variables in the forecasting model.</p> <p>Exogenous variables are additional independent variables that can impact the value of the target variable being forecasted. These variables can provide valuable information to the model and improve its forecasting accuracy.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-custom-metric","title":"Backtesting with custom metric\u00b6","text":"<p>In addition to the commonly used metrics such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-multiple-metrics","title":"Backtesting with multiple metrics\u00b6","text":"<p>The <code>backtesting_forecaster</code> function provides a convenient way to estimate multiple metrics simultaneously by accepting a list of metrics as an input argument. This list can include any combination of built-in metrics, such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, as well as user-defined custom metrics.</p> <p>By specifying multiple metrics, users can obtain a more comprehensive evaluation of the model's predictive performance, which can help in selecting the best model for a particular task. Additionally, the ability to include custom metrics allows users to tailor the evaluation to specific use cases and domain-specific requirements.</p>"},{"location":"user_guides/backtesting.html#backtesting-on-training-data","title":"Backtesting on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data,the forecaster must first be fitted using the training dataset. Then, backtesting can be performed using  <code>backtesting_forecaster</code> and specifying the arguments <code>initial_train_size=None</code> and <code>refit=False</code>. This configuration enables backtesting on the same data that was used to train the model.</p>"},{"location":"user_guides/categorical-features.html","title":"Categorical features","text":"<p>In the field of machine learning, categorical features play a crucial role in determining the predictive ability of a model. Categorical features are features that can take a limited number of values, such as color, gender or location. While these features can provide useful insights into patterns and relationships within data, they also present unique challenges for machine learning models.</p> <p>One of these challenges is the need to transform categorical features before they can be used by most models. This transformation involves converting categorical values into numerical values that can be processed by machine learning algorithms.</p> <p>Another challenge is dealing with infrequent categories, which can lead to biased models. If a categorical feature has a large number of categories, but some of them are rare or appear infrequently in the data, the model may not be able to learn accurately from these categories, resulting in biased predictions and inaccurate results.</p> <p>Despite these difficulties, categorical features are still an essential component in many use cases. When properly encoded and handled, machine learning models can effectively learn from patterns and relationships in categorical data, leading to better predictions.</p> <p>This document provides an overview of three of the most commonly used transformations: one-hot encoding, ordinal encoding, and target encoding. It explains how to apply them in the skforecast package using scikit-learn encoders, which provide a convenient and flexible way to pre-process data. It also shows how to use the native implementation of three popular gradient boosting frameworks \u2013 LightGBM, scikit-learn's HistogramGradientBoosting, and XGBoost \u2013 to handle categorical features directly in the model.</p> <p>For a comprehensive demonstration of the use of categorical features in time series forecasting, check out the article Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost.</p> <p>  \u00a0 Note </p> <p>All of the transformations described in this document can be applied to the entire dataset, regardless of the forecaster. However, it is important to ensure that the transformations are learned only from the training data to avoid information leakage. Furthermore, the same transformation should be applied to the input data during prediction. To reduce the likelihood of errors and to ensure consistent application of the transformations, it is advisable to include the transformation within the forecaster object, so that it is handled internally.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import make_pipeline\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd  import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5  from lightgbm import LGBMRegressor from sklearn.ensemble import HistGradientBoostingRegressor from xgboost import XGBRegressor from catboost import CatBoostRegressor from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import make_column_transformer from sklearn.compose import make_column_selector from sklearn.pipeline import make_pipeline  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'\n       'learning-python/master/data/bike_sharing_dataset_clean.csv')\ndata = pd.read_csv(url)\n\n# Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('H')\ndata = data.sort_index()\ndata['holiday'] = data['holiday'].astype(int)\ndata = data[['holiday', 'weather', 'temp', 'hum', 'users']]\ndata[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str)\nprint(data.dtypes)\ndata.head(3)\n</pre> # Downloading data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'        'learning-python/master/data/bike_sharing_dataset_clean.csv') data = pd.read_csv(url)  # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('H') data = data.sort_index() data['holiday'] = data['holiday'].astype(int) data = data[['holiday', 'weather', 'temp', 'hum', 'users']] data[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str) print(data.dtypes) data.head(3) <pre>holiday     object\nweather     object\ntemp       float64\nhum        float64\nusers      float64\ndtype: object\n</pre> Out[2]: holiday weather temp hum users date_time 2011-01-01 00:00:00 0 clear 9.84 81.0 16.0 2011-01-01 01:00:00 0 clear 9.02 80.0 40.0 2011-01-01 02:00:00 0 clear 9.02 80.0 32.0 <p>Only part of the data is used to simplify the example.</p> In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nstart_train = '2012-06-01 00:00:00'\nend_train = '2012-07-31 23:59:00'\nend_test = '2012-08-15 23:59:00'\ndata_train = data.loc[start_train:end_train, :]\ndata_test  = data.loc[end_train:end_test, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split train-test # ============================================================================== start_train = '2012-06-01 00:00:00' end_train = '2012-07-31 23:59:00' end_test = '2012-08-15 23:59:00' data_train = data.loc[start_train:end_train, :] data_test  = data.loc[end_train:end_test, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Dates train : 2012-06-01 00:00:00 --- 2012-07-31 23:00:00  (n=1464)\nDates test  : 2012-08-01 00:00:00 --- 2012-08-15 23:00:00  (n=360)\n</pre> In\u00a0[4]: Copied! <pre># ColumnTransformer with one-hot encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical features (no numerical)\n# using one-hot encoding. Numeric features are left untouched. For binary\n# features, only one column is created.\none_hot_encoder = make_column_transformer(\n                    (\n                        OneHotEncoder(sparse_output=False, drop='if_binary'),\n                        make_column_selector(dtype_exclude=np.number)\n                    ),\n                    remainder=\"passthrough\",\n                    verbose_feature_names_out=False,\n                ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with one-hot encoding # ============================================================================== # A ColumnTransformer is used to transform categorical features (no numerical) # using one-hot encoding. Numeric features are left untouched. For binary # features, only one column is created. one_hot_encoder = make_column_transformer(                     (                         OneHotEncoder(sparse_output=False, drop='if_binary'),                         make_column_selector(dtype_exclude=np.number)                     ),                     remainder=\"passthrough\",                     verbose_feature_names_out=False,                 ).set_output(transform=\"pandas\") In\u00a0[5]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123),\n                 lags = 5,\n                 transformer_exog = one_hot_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123),                  lags = 5,                  transformer_exog = one_hot_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f5ca12da090&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: H \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-07-09 13:39:36 \nLast fit date: 2023-07-09 13:39:36 \nSkforecast version: 0.9.0 \nPython version: 3.11.4 \nForecaster id: None </pre> <p>Once the forecaster has been trained, the transformer can be inspected (feature_names_in, feature_names_out, ...) by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[6]: Copied! <pre># Access to the transformer used for exogenous features\n# ==============================================================================\nprint(forecaster.transformer_exog.get_feature_names_out())\nforecaster.transformer_exog\n</pre> # Access to the transformer used for exogenous features # ============================================================================== print(forecaster.transformer_exog.get_feature_names_out()) forecaster.transformer_exog <pre>['holiday_1' 'weather_clear' 'weather_mist' 'weather_rain' 'temp' 'hum']\n</pre> Out[6]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f5ca12da090&gt;)],\n                  verbose_feature_names_out=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f5ca12da090&gt;)],\n                  verbose_feature_names_out=False)</pre>onehotencoder<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f5ca12da090&gt;</pre>OneHotEncoder<pre>OneHotEncoder(drop='if_binary', sparse_output=False)</pre>remainder<pre>['temp', 'hum']</pre>passthrough<pre>passthrough</pre> In\u00a0[7]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[7]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>  \u00a0 Note </p> <p>It is possible to apply a transformation to the entire dataset independent of the forecaster. However, it is crucial to ensure that the transformations are only learned from the training data to avoid information leakage. In addition, the same transformation should be applied to the input data during prediction. It is therefore advisable to incorporate the transformation into the forecaster, so that it is handled internally. This approach ensures consistency in the application of transformations and reduces the likelihood of errors.</p> <p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y()</code> method to generate the matrices used by the forecaster to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p> In\u00a0[8]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) print(X_train.dtypes) X_train.head() <pre>lag_1            float64\nlag_2            float64\nlag_3            float64\nlag_4            float64\nlag_5            float64\nholiday_1        float64\nweather_clear    float64\nweather_mist     float64\nweather_rain     float64\ntemp             float64\nhum              float64\ndtype: object\n</pre> Out[8]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 0.0 1.0 0.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 1.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 1.0 0.0 0.0 13.12 76.0 In\u00a0[9]: Copied! <pre># Transform exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features])\nexog_transformed.head()\n</pre> # Transform exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features]) exog_transformed.head() Out[9]: holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 00:00:00 0.0 1.0 0.0 0.0 9.84 81.0 2011-01-01 01:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 02:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 03:00:00 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 04:00:00 0.0 1.0 0.0 0.0 9.84 75.0 In\u00a0[10]: Copied! <pre># ColumnTransformer with ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\nordinal_encoder = make_column_transformer(\n                    (\n                        OrdinalEncoder(\n                            handle_unknown='use_encoded_value',\n                            unknown_value=-1,\n                            encoded_missing_value=-1\n                        ),\n                        make_column_selector(dtype_exclude=np.number)\n                    ),\n                    remainder=\"passthrough\",\n                    verbose_feature_names_out=False,\n                ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. ordinal_encoder = make_column_transformer(                     (                         OrdinalEncoder(                             handle_unknown='use_encoded_value',                             unknown_value=-1,                             encoded_missing_value=-1                         ),                         make_column_selector(dtype_exclude=np.number)                     ),                     remainder=\"passthrough\",                     verbose_feature_names_out=False,                 ).set_output(transform=\"pandas\") In\u00a0[11]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                regressor = LGBMRegressor(random_state=123),\n                lags = 5,\n                transformer_exog = ordinal_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                 regressor = LGBMRegressor(random_state=123),                 lags = 5,                 transformer_exog = ordinal_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[11]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f5ca135f4d0&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: H \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-07-09 13:39:36 \nLast fit date: 2023-07-09 13:39:37 \nSkforecast version: 0.9.0 \nPython version: 3.11.4 \nForecaster id: None </pre> In\u00a0[12]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) print(X_train.dtypes) X_train.head() <pre>lag_1      float64\nlag_2      float64\nlag_3      float64\nlag_4      float64\nlag_5      float64\nholiday    float64\nweather    float64\ntemp       float64\nhum        float64\ndtype: object\n</pre> Out[12]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 1.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 0.0 13.12 76.0 <p>Once the forecaster has been trained, the transformer can be inspected by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[13]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[13]: <pre>2012-08-01 00:00:00    89.096098\n2012-08-01 01:00:00    57.749964\n2012-08-01 02:00:00    29.263922\nFreq: H, Name: pred, dtype: float64</pre> <p>  \u00a0 Warning </p> <p>We are currently working on adapting this encoding strategy to time series to avoid data leakage.</p> <p>  \u00a0 Warning </p> <p>When deploying models in production, it is strongly recommended to avoid using automatic detection based on pandas <code>category</code> type columns. Although pandas provides an internal coding for these columns, it is not consistent across different datasets and may vary depending on the categories present in each one. It is therefore crucial to be aware of this issue and to take appropriate measures to ensure consistency in the coding of categorical features when deploying models in production.</p> <p>At the time of writing, the only thing the authors have observed is that LightGBM internally manages changes in the coding of categories. More details on this issue can be found in  github issue and  stackoverflow.</p> <p>If the user still wishes to rely on automatic detection of categorical features based on pandas data types, categorical variables must first be encoded as integers (ordinal encoding) and then stored as category type. This is necessary because skforecast uses a numeric numpy array internally to speed up the calculation.</p> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features (recommended)</p> <p>When creating a forecaster with <code>LGBMRegressor</code>, it is necessary to specify the names of the categorical columns using the <code>fit_kwargs</code> argument. This is because the <code>categorical_feature</code> argument is only specified in the <code>fit</code> method of <code>LGBMRegressor</code>, and not during its initialization.</p> In\u00a0[14]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[15]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=963),\n                 lags = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs = {'categorical_feature': categorical_features}\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=963),                  lags = 5,                  transformer_exog = transformer_exog,                  fit_kwargs = {'categorical_feature': categorical_features}              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) <pre>/home/ubuntu/anaconda3/envs/skforecast_09_py11/lib/python3.11/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n</pre> <p>The UserWarning raised indicates that categorical features have been included. To suppress this warning use <code>warnings.filterwarnings('ignore')</code>. Warnings can then be restored using <code>warnings.filterwarnings('default')</code> or <code>warnings.resetwarnings()</code></p> In\u00a0[16]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[16]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>Allow the model to automatically detect categorical features (not recommended)</p> <p>  \u00a0 Warning </p> <p>Handling categorical variables by relying on the automatic detection of the <code>category</code> datatype can be achieved by setting <code>categorical_features='auto'</code> during model initialization. However, this approach can lead to significant problems when the model is exposed to new datasets that have a different pandas encoding for categorical columns than the one used during training. Therefore, it's crucial to ensure that the encoding is consistent between the training and the testing datasets to avoid any potential errors.</p> In\u00a0[17]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            FunctionTransformer(\n                                func=lambda x: x.astype('category'),\n                                feature_names_out= 'one-to-one'\n                            )\n                       )\n\ntransformer_exog = make_column_transformer(\n                        (\n                            pipeline_categorical,\n                            make_column_selector(dtype_exclude=np.number)\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             FunctionTransformer(                                 func=lambda x: x.astype('category'),                                 feature_names_out= 'one-to-one'                             )                        )  transformer_exog = make_column_transformer(                         (                             pipeline_categorical,                             make_column_selector(dtype_exclude=np.number)                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[18]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=963),\n                 lags = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs = {'categorical_feature': 'auto'}\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=963),                  lags = 5,                  transformer_exog = transformer_exog,                  fit_kwargs = {'categorical_feature': 'auto'}              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[19]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[19]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>As with any other forecaster, the matrices used during model training can be created with <code>create_train_X_y</code>.</p> In\u00a0[20]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) X_train.head() Out[20]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0 0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0 0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0 0 13.12 76.0 <p>When creating a forecaster using <code>HistogramGradientBoosting</code>, the names of the categorical columns should be specified during the instantiation by passing them as a list to the <code>categorical_feature</code> argument.</p> In\u00a0[21]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[22]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(\n                                 categorical_features = categorical_features,\n                                 random_state = 963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(                                  categorical_features = categorical_features,                                  random_state = 963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) <p><code>HistGradientBoostingRegressor</code> stores a boolean mask indicating which features were considered categorical. It will be <code>None</code> if there are no categorical features.</p> In\u00a0[23]: Copied! <pre>forecaster.regressor.is_categorical_\n</pre> forecaster.regressor.is_categorical_ Out[23]: <pre>array([False, False, False, False, False,  True,  True, False, False])</pre> In\u00a0[24]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[24]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: H, Name: pred, dtype: float64</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features (recommended)</p> <p>At the time of writing, the <code>XGBRegressor</code> module does not provide an option to specify the names of categorical features. Instead, the feature types are specified by passing a list of strings to the <code>feature_types</code> argument, where 'c' denotes categorical and 'q' numeric features. The <code>enable_categorical</code> argument must also be set to <code>True</code>.</p> <p>Determining the positions of each column to create a list of feature types can be a challenging task. The shape of the data matrix depends on two factors, the number of lags used and the transformations applied to the exogenous variables. However, there is a workaround to this problem. First, create a forecaster without specifying the <code>feature_types</code> argument. Next, the <code>create_train_X_y</code> method can be used with a small sample of data to determine the position of each feature. Once the position of each feature has been determined, the <code>set_params()</code> method can be used to specify the values of <code>feature_types</code>. By following this approach it is possible to ensure that the feature types are correctly specified, thus avoiding any errors that may occur due to incorrect specification.</p> In\u00a0[25]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") <p>A forecaster is created without specifying the <code>feature_types</code> argument.</p> In\u00a0[26]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                               tree_method='hist',\n                               random_state=12345,\n                               enable_categorical=True,\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                tree_method='hist',                                random_state=12345,                                enable_categorical=True,                              ),                  lags = 5,                  transformer_exog = transformer_exog              ) <p>Once the forecaster is instantiated, its <code>create_train_X_y()</code> method is used to generate the training matrices that allow the user to identify the positions of the variables.</p> In\u00a0[27]: Copied! <pre># Create training matrices using a sample of the training data\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'][:10],\n                        exog = data.loc[:end_train, exog_features][:10]\n                   )\nX_train.head(2)\n</pre> # Create training matrices using a sample of the training data # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'][:10],                         exog = data.loc[:end_train, exog_features][:10]                    ) X_train.head(2) Out[27]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 <p>Create a list to identify which columns in the training matrix are numeric ('q') and categorical ('c').</p> In\u00a0[28]: Copied! <pre>feature_types = ['c' if col in categorical_features else 'q' for col in X_train.columns]\nfeature_types\n</pre> feature_types = ['c' if col in categorical_features else 'q' for col in X_train.columns] feature_types Out[28]: <pre>['q', 'q', 'q', 'q', 'q', 'c', 'c', 'q', 'q']</pre> <p>Update the regressor parameters using the forecaster's <code>set_params</code> method and fit.</p> In\u00a0[29]: Copied! <pre># Update regressor parameters\n# ==============================================================================\nforecaster.set_params({'feature_types': feature_types})\n</pre> # Update regressor parameters # ============================================================================== forecaster.set_params({'feature_types': feature_types}) In\u00a0[30]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[31]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[31]: <pre>2012-08-01 00:00:00    77.470787\n2012-08-01 01:00:00    40.735706\n2012-08-01 02:00:00    13.448755\nFreq: H, Name: pred, dtype: float64</pre> <p>Allow the model to automatically detect categorical features (not recommended)</p> <p>  \u00a0 Warning </p> <p>Handling categorical variables by relying on the automatic detection of the <code>category</code> datatype can be achieved by setting <code>enable_categorical=True</code> during model initialization. However, this approach can lead to significant problems when the model is exposed to new datasets that have a different pandas encoding for categorical columns than the one used during training. Therefore, it's crucial to ensure that the encoding is consistent between the training and the testing datasets to avoid any potential errors.</p> In\u00a0[32]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After the encoding, the features are converted back to category type so \n# that they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            FunctionTransformer(\n                                func=lambda x: x.astype('category'),\n                                feature_names_out= 'one-to-one'\n                            )\n                       )\n\ntransformer_exog = make_column_transformer(\n                        (\n                            pipeline_categorical,\n                            make_column_selector(dtype_exclude=np.number)\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After the encoding, the features are converted back to category type so  # that they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             FunctionTransformer(                                 func=lambda x: x.astype('category'),                                 feature_names_out= 'one-to-one'                             )                        )  transformer_exog = make_column_transformer(                         (                             pipeline_categorical,                             make_column_selector(dtype_exclude=np.number)                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[33]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 enable_categorical=True,\n                                 tree_method='hist',\n                                 random_state=963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  enable_categorical=True,                                  tree_method='hist',                                  random_state=963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[34]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[34]: <pre>2012-08-01 00:00:00    77.470787\n2012-08-01 01:00:00    40.735706\n2012-08-01 02:00:00    13.448755\nFreq: H, Name: pred, dtype: float64</pre> In\u00a0[35]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/categorical-features.html#categorical-features","title":"Categorical features\u00b6","text":""},{"location":"user_guides/categorical-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":"<p>The dataset used in this user guide consists of information on the number of users of a bicycle rental service, in addition to weather variables and holiday data. Two of the variables in the dataset, <code>holiday</code> and <code>weather</code>, are categorical.</p>"},{"location":"user_guides/categorical-features.html#one-hot-encoding","title":"One Hot Encoding\u00b6","text":"<p>One hot encoding, also known as dummy encoding or one-of-K encoding, consists of replacing the categorical variable with a set of binary variables that take the value 0 or 1 to indicate whether a particular category is present in an observation. For example, suppose a dataset contains a categorical variable called \"color\" with the possible values of \"red,\" \"blue,\" and \"green\". Using one hot encoding, this variable is converted into three binary variables such as <code>color_red</code>, <code>color_blue</code>, and <code>color_green</code>, where each variable takes a value of 0 or 1 depending on the category.</p> <p>The OneHotEncoder class in scikit-learn can be used to transform any categorical feature with n possible values into n new binary features, where one of them takes the value 1, and all the others take the value 0. The <code>OneHotEncoder</code> can be configured to handle certain corner cases, including unknown categories, missing values, and infrequent categories.</p> <ul> <li><p>When <code>handle_unknown='ignore'</code> and <code>drop</code> is not <code>None</code>, unknown categories are encoded as zeros. Additionally, if a feature contains both <code>np.nan</code> and <code>None</code>, they are considered separate categories.</p> </li> <li><p>It supports the aggregation of infrequent categories into a single output for each feature. The parameters to enable the aggregation of infrequent categories are <code>min_frequency</code> and <code>max_categories</code>. By setting <code>handle_unknown</code> to 'infrequent_if_exist', unknown categories are considered infrequent.</p> </li> <li><p>To avoid collinearity between features, it is possible to drop one of the categories per feature using the <code>drop</code> argument. This is especially important when using linear models.</p> </li> </ul> <p>ColumnTransformers in scikit-learn provide a powerful way to define transformations and apply them to specific features. By encapsulating the <code>OneHotEncoder</code> in a <code>ColumnTransformer</code> object, it can be passed to a forecaster using the <code>transformer_exog</code> argument.</p>"},{"location":"user_guides/categorical-features.html#ordinal-encoding","title":"Ordinal encoding\u00b6","text":"<p>Ordinal encoding is a technique used to convert categorical variables into numerical variables. Each category is assigned a unique numerical value based on its order or rank, as determined by a chosen criterion such as frequency or importance. This encoding method is particularly useful when categories have a natural order or ranking, such as educational qualifications. However, it is important to note that the numerical values assigned to each category do not represent any inherent numerical difference between them, but simply provide a numerical representation.</p> <p>The scikit-learn library provides the OrdinalEncoder class, which allows users to replace categorical variables with ordinal numbers ranging from 0 to n_categories-1. In addition, this class includes the <code>encoded_missing_value</code> parameter, which allows for the encoding of missing values. It is important to note that this implementation arbitrarily assigns numbers to categories on a first-seen-first-served basis. Users should therefore exercise caution when interpreting the numerical values assigned to the categories. Other implementations, such as the Feature-engine, numbers can be ordered based on the mean of the target.</p>"},{"location":"user_guides/categorical-features.html#target-encoding","title":"Target encoding\u00b6","text":"<p>Target encoding is a technic that encodes categorical variables based on the relationship between the categories and the target variable. Each category is encoded based on a shrinked estimate of the average target values for observations belonging to the category. The encoding scheme mixes the global target mean with the target mean conditioned on the value of the category.</p> <p>For example, suppose a categorical variable \"City\" with categories \"New York,\" \"Los Angeles,\" and \"Chicago,\" and a target variable \"Salary.\" One can calculate the mean salary for each city category based on the training data, and use these mean values to encode the categories.</p> <p>This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories are location based such as zip code or region.</p> <p>The TargetEncoder class is available in Scikit-learn (since version 1.3). <code>TargetEncoder</code> considers missing values, such as <code>np.nan</code> or <code>None</code>, as another category and encodes them like any other category. Categories that are not seen during fit are encoded with the target mean, i.e. <code>target_mean_</code>. A more detailed description of target encoding can be found in the scikit-learn user guide.</p>"},{"location":"user_guides/categorical-features.html#native-implementation-for-categorical-features","title":"Native implementation for categorical features\u00b6","text":"<p>Some machine learning models, including XGBoost, LightGBM, CatBoost, and HistGradientBoostingRegressor, provide built-in methods to handle categorical features, but they assume that the input categories are integers starting from 0 up to the number of categories [0, 1, ..., n_categories-1]. In practice, categorical variables are not coded with numbers but with strings, so an intermediate transformation step is necessary. Two options are:</p> <ul> <li><p>Set columns with categorical variables to the type <code>category</code>. For each column, the data structure consists of an array of categories and an array of integer values (codes) that point to the actual value of the array of categories. That is, internally it is a numeric array with a mapping that relates each value to a category. Models are able to automatically identify the columns of type <code>category</code> and access their internal codes. This approach is applicable to XGBoost, LightGBM and CatBoost.</p> </li> <li><p>Preprocess the categorical columns with an <code>OrdinalEncoder</code> to transform their values to integers and explicitly indicate that the columns should be treated as categorical. Skforecast allows this by using the <code>fit_kwargs</code> argument.</p> </li> </ul>"},{"location":"user_guides/categorical-features.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/categorical-features.html#scikit-learn-histogramgradientboosting","title":"Scikit-learn HistogramGradientBoosting\u00b6","text":""},{"location":"user_guides/categorical-features.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/categorical-features.html#catboost","title":"CatBoost\u00b6","text":"<p>Unfortunately, the current version of skforecast is not compatible with CatBoost's built-in handling of categorical features. The issue arises because CatBoost only accepts categorical features as integers, while skforecast converts input data to floats for faster computation using numpy arrays in the internal prediction process. If a CatBoost model is required, an external encoder should be used for the categorical variables.</p>"},{"location":"user_guides/custom-predictors.html","title":"Custom predictors","text":"<p>  \u00a0 Warning </p> <p>The <code>window_size</code> parameter specifies the size of the data window that <code>fun_predictors</code> uses to generate each row of predictors. Choosing the appropriate value for this parameter is crucial to avoid losing data when constructing the training matrices. Be sure to provide the correct value for <code>window_size</code> when using <code>fun_predictors</code>.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' \n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'      'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Custom function to create predictors\n# ==============================================================================\ndef create_predictors(y):\n\"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    \"\"\"\n\n    lags = y[-1:-11:-1]     # window_size = 10\n    mean = np.mean(y[-20:]) # window_size = 20\n    predictors = np.hstack([lags, mean])\n\n    return predictors\n</pre> # Custom function to create predictors # ============================================================================== def create_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     \"\"\"      lags = y[-1:-11:-1]     # window_size = 10     mean = np.mean(y[-20:]) # window_size = 20     predictors = np.hstack([lags, mean])      return predictors In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 fun_predictors  = create_predictors,\n                 name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],\n                 window_size     = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor       = RandomForestRegressor(random_state=123),                  fun_predictors  = create_predictors,                  name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],                  window_size     = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(y=data_train) forecaster Out[4]: <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 20 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:49:49 \nLast fit date: 2023-09-09 16:49:50 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps) predictions.head(3) Out[5]: <pre>2005-07-01    0.926598\n2005-08-01    0.948202\n2005-09-01    1.020947\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.04487765885818191\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[8]: feature importance 0 lag 1 0.539720 1 lag 2 0.119097 2 lag 3 0.046404 3 lag 4 0.024165 4 lag 5 0.030567 5 lag 6 0.015139 6 lag 7 0.042883 7 lag 8 0.012742 8 lag 9 0.018938 9 lag 10 0.108639 10 moving_avg_20 0.041707 In\u00a0[9]: Copied! <pre>X, y = forecaster.create_train_X_y(data_train)\nX.head()\n</pre> X, y = forecaster.create_train_X_y(data_train) X.head() Out[9]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 datetime 1993-03-01 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.496401 1993-04-01 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.496275 1993-05-01 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.496924 1993-06-01 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.496759 1993-07-01 0.470126 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.495638 In\u00a0[10]: Copied! <pre>y.head()\n</pre> y.head() Out[10]: <pre>datetime\n1993-03-01    0.427283\n1993-04-01    0.413890\n1993-05-01    0.428859\n1993-06-01    0.470126\n1993-07-01    0.509210\nFreq: MS, Name: y, dtype: float64</pre> <p>  \u00a0 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[11]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 fun_predictors  = create_predictors,\n                 name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],\n                 window_size     = 20, # window_size needed by the mean is the most restrictive one\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor       = RandomForestRegressor(random_state=123),                  fun_predictors  = create_predictors,                  name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],                  window_size     = 20, # window_size needed by the mean is the most restrictive one                  differentiation = 1              )  forecaster.fit(y=data_train) forecaster Out[11]: <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 21 \nWeight function included: False \nDifferentiation order: 1 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:49:50 \nLast fit date: 2023-09-09 16:49:50 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[12]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[12]: <pre>2005-07-01    0.912886\n2005-08-01    0.927379\n2005-09-01    0.985755\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[13]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/simulated_items_sales.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/simulated_items_sales.csv') data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[13]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[14]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00  (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00  (n=170)\n</pre> In\u00a0[15]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); In\u00a0[16]: Copied! <pre># Custom function to create predictors\n# ==============================================================================\ndef create_complex_predictors(y):\n\"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    \"\"\"\n\n    lags = y[-1:-11:-1]     # window_size = 10  \n    mean = np.mean(y[-20:]) # window_size = 20\n    predictors = np.hstack([lags, mean])\n\n    return predictors\n</pre> # Custom function to create predictors # ============================================================================== def create_complex_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     \"\"\"      lags = y[-1:-11:-1]     # window_size = 10       mean = np.mean(y[-20:]) # window_size = 20     predictors = np.hstack([lags, mean])      return predictors In\u00a0[17]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeriesCustom(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 fun_predictors  = create_complex_predictors,\n                 name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],\n                 window_size     = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeriesCustom(                  regressor       = RandomForestRegressor(random_state=123),                  fun_predictors  = create_complex_predictors,                  name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],                  window_size     = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(series=data_train) forecaster Out[17]: <pre>================================== \nForecasterAutoregMultiSeriesCustom \n================================== \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_complex_predictors \nTransformer for series: None \nTransformer for exog: None \nWindow size: 20 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:49:52 \nLast fit date: 2023-09-09 16:49:58 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[18]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps, levels=None)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps, levels=None) predictions.head(3) Out[18]: item_1 item_2 item_3 2014-07-16 25.737550 11.265323 11.328083 2014-07-17 25.649209 10.784593 12.283007 2014-07-18 25.602333 11.273438 12.722012 In\u00a0[19]: Copied! <pre>X_train, y_train, _, _ = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> X_train, y_train, _, _ = forecaster.create_train_X_y(data_train) X_train.head() Out[19]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 item_1 item_2 item_3 0 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 21.717691 21.954065 1.0 0.0 0.0 1 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 22.666583 1.0 0.0 0.0 2 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 22.567441 1.0 0.0 0.0 3 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 22.389038 1.0 0.0 0.0 4 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 22.495203 1.0 0.0 0.0 In\u00a0[20]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[20]: <pre>0    22.503533\n1    20.794986\n2    23.981037\n3    28.018830\n4    28.747482\nName: y, dtype: float64</pre> In\u00a0[21]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/custom-predictors.html#custom-predictors-for-recursive-multi-step-forecasting","title":"Custom predictors for recursive multi-step forecasting\u00b6","text":"<p>In forecasting time series data, it can be useful to consider additional characteristics beyond just the lagged values. For instance, the moving average of the previous n values can help capture the trend in the series.</p> <p>The <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes are similar to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregMultiSeries</code>, respectively, but they allow the user to define their own function for generating predictors.</p> <p>To create a custom predictor function, the user needs to write a function that takes a time series as input (a NumPy ndarray) and outputs another NumPy ndarray containing the predictors. The Forecaster parameters used to specify this function are <code>fun_predictors</code> and <code>window_size</code>.</p>"},{"location":"user_guides/custom-predictors.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/custom-predictors.html#forecasterautoregcustom","title":"ForecasterAutoregCustom\u00b6","text":""},{"location":"user_guides/custom-predictors.html#custom-predictors","title":"Custom predictors\u00b6","text":""},{"location":"user_guides/custom-predictors.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/custom-predictors.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/custom-predictors.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/custom-predictors.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":""},{"location":"user_guides/custom-predictors.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/custom-predictors.html#forecasterautoregmultiseriescustom","title":"ForecasterAutoregMultiSeriesCustom\u00b6","text":"<p>All the methods available for <code>ForecasterAutoregMultiSeries</code> can also be applied to ForecasterAutoregMultiSeriesCustom. To learn more about these methods, please refer to the Multi-series forecasting page.</p>"},{"location":"user_guides/custom-predictors.html#train","title":"Train\u00b6","text":""},{"location":"user_guides/custom-predictors.html#predict","title":"Predict\u00b6","text":"<p>If no value is specified for the <code>levels</code> argument, predictions will be computed for all available levels.</p>"},{"location":"user_guides/custom-predictors.html#training-matrices","title":"Training matrices\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html","title":"Dependent multivariate series forecasting","text":"<p>  \u00a0 Note </p> <p>Starting from version skforecast 0.9.0, the <code>ForecasterAutoregMultiVariate</code> now includes the <code>n_jobs</code> parameter, allowing multi-process parallelization. This allows to train regressors for all steps simultaneously.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> <p>  \u00a0 Note </p> <p>See ForecasterAutoregMultiSeries for independent multi-series forecasting.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import random_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import random_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/guangyuan_air_pollution.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata = data[['CO', 'SO2', 'PM2.5']]\ndata.head()\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/guangyuan_air_pollution.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data = data[['CO', 'SO2', 'PM2.5']] data.head() Out[2]: CO SO2 PM2.5 date 2013-03-01 9600.0 204.0 181.0 2013-03-02 20198.0 674.0 633.0 2013-03-03 47195.0 1661.0 1956.0 2013-03-04 15000.0 485.0 438.0 2013-03-05 59594.0 2001.0 3388.0 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2016-05-31 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2013-03-01 00:00:00 --- 2016-05-31 00:00:00(n=1188)\nTest dates  : 2016-06-01 00:00:00 --- 2017-02-28 00:00:00(n=273)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['CO'].plot(label='train', ax=axes[0])\ndata_test['CO'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('Concentration (ug/m^3)')\naxes[0].set_title('CO')\naxes[0].legend()\n\ndata_train['SO2'].plot(label='train', ax=axes[1])\ndata_test['SO2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('Concentration (ug/m^3)')\naxes[1].set_title('SO2')\n\ndata_train['PM2.5'].plot(label='train', ax=axes[2])\ndata_test['PM2.5'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('Concentration (ug/m^3)')\naxes[2].set_title('PM2.5')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['CO'].plot(label='train', ax=axes[0]) data_test['CO'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('Concentration (ug/m^3)') axes[0].set_title('CO') axes[0].legend()  data_train['SO2'].plot(label='train', ax=axes[1]) data_test['SO2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('Concentration (ug/m^3)') axes[1].set_title('SO2')  data_train['PM2.5'].plot(label='train', ax=axes[2]) data_test['PM2.5'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('Concentration (ug/m^3)') axes[2].set_title('PM2.5')  fig.tight_layout() plt.show(); In\u00a0[5]: Copied! <pre># Create and fit forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 n_jobs             = 'auto'\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  n_jobs             = 'auto'              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: None \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:53:25 \nLast fit date: 2023-09-09 16:53:25 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul> In\u00a0[6]: Copied! <pre># Predict with forecaster MultiVariate\n# ==============================================================================\n# Predict as many steps as defined in the forecaster initialization\npredictions = forecaster.predict()\ndisplay(predictions)\n</pre> # Predict with forecaster MultiVariate # ============================================================================== # Predict as many steps as defined in the forecaster initialization predictions = forecaster.predict() display(predictions) CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 2016-06-04 23111.643246 2016-06-05 24686.327128 2016-06-06 24023.210710 2016-06-07 24015.607873 In\u00a0[7]: Copied! <pre># Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) CO 2016-06-01 20244.442039 2016-06-05 24686.327128 In\u00a0[8]: Copied! <pre># Predict with prediction intervals\npredictions = forecaster.predict_interval(random_state=9871)\ndisplay(predictions)\n</pre> # Predict with prediction intervals predictions = forecaster.predict_interval(random_state=9871) display(predictions) CO lower_bound upper_bound 2016-06-01 20244.442039 -56.145312 48679.447542 2016-06-02 23307.697174 1339.464819 63081.735422 2016-06-03 22487.345075 -427.660902 62653.088738 2016-06-04 23111.643246 1062.725713 61154.741013 2016-06-05 24686.327128 2936.856407 64075.756388 2016-06-06 24023.210710 3016.266495 66136.706478 2016-06-07 24015.607873 800.257958 60082.397338 In\u00a0[9]: Copied! <pre># Backtesting MultiVariate\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data,\n                                           steps                 = 7,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_train),\n                                           fixed_train_size      = False,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = False,\n                                           n_jobs                = 'auto',\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting MultiVariate # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data,                                            steps                 = 7,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_train),                                            fixed_train_size      = False,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = False,                                            n_jobs                = 'auto',                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/39 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 CO 14931.121425 <pre>\nBacktest predictions\n</pre> Out[9]: CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 2016-06-04 23111.643246 In\u00a0[10]: Copied! <pre># Create and forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = RandomForestRegressor(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n</pre> # Create and forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = RandomForestRegressor(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None              ) In\u00a0[11]: Copied! <pre># Random search MultiVariate\n# ==============================================================================\nlags_grid = [7, 14]\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),\n    'max_depth': np.arange(start=3, stop=6, step=1, dtype=int)\n}\n\nresults = random_search_forecaster_multiseries(\n              forecaster            = forecaster,\n              series                = data,\n              exog                  = None,\n              lags_grid             = lags_grid,\n              param_distributions   = param_distributions,\n              steps                 = 7,\n              metric                = 'mean_absolute_error',\n              initial_train_size    = len(data_train),\n              fixed_train_size      = False,\n              gap                   = 0,\n              allow_incomplete_fold = True,\n              refit                 = False,\n              n_iter                = 5,\n              return_best           = False,\n              n_jobs                = 'auto',\n              verbose               = False,\n              show_progress         = True\n          )\n\nresults\n</pre> # Random search MultiVariate # ============================================================================== lags_grid = [7, 14] param_distributions = {     'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),     'max_depth': np.arange(start=3, stop=6, step=1, dtype=int) }  results = random_search_forecaster_multiseries(               forecaster            = forecaster,               series                = data,               exog                  = None,               lags_grid             = lags_grid,               param_distributions   = param_distributions,               steps                 = 7,               metric                = 'mean_absolute_error',               initial_train_size    = len(data_train),               fixed_train_size      = False,               gap                   = 0,               allow_incomplete_fold = True,               refit                 = False,               n_iter                = 5,               return_best           = False,               n_jobs                = 'auto',               verbose               = False,               show_progress         = True           )  results <pre>10 models compared for 1 level(s). Number of iterations: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[11]: levels lags params mean_absolute_error n_estimators max_depth 7 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 15, 'max_depth': 3} 15991.360494 15 3 5 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 17, 'max_depth': 3} 16001.390244 17 3 9 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 18, 'max_depth': 3} 16058.419766 18 3 8 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'max_depth': 5} 16160.441414 16 5 6 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 19, 'max_depth': 5} 16192.047506 19 5 3 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 16, 'max_depth': 5} 16195.444749 16 5 4 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 18, 'max_depth': 3} 16203.381611 18 3 1 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 19, 'max_depth': 5} 16269.039511 19 5 0 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 17, 'max_depth': 3} 16303.094588 17 3 2 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 15, 'max_depth': 3} 16354.631284 15 3 In\u00a0[12]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\ndisplay(predictions)\n</pre> # Create and fit forecaster MultiVariate Custom lags # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) display(predictions) CO 2016-06-01 19421.982426 2016-06-02 21856.950600 2016-06-03 21910.011418 2016-06-04 23197.353798 2016-06-05 23294.891733 2016-06-06 22478.158677 2016-06-07 23393.641623 <p>It is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific lags that have been created.</p> In\u00a0[13]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step    = 1,\n               X_train = X,\n               y_train = y,\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step    = 1,                X_train = X,                y_train = y,            )  X_1.head(4) Out[13]: CO_lag_1 CO_lag_2 CO_lag_3 CO_lag_4 CO_lag_5 CO_lag_6 CO_lag_7 SO2_lag_1 SO2_lag_7 PM2.5_lag_1 PM2.5_lag_2 date 2013-03-08 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 9600.0 2369.0000 204.0 5952.0 4934.0 2013-03-09 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 1535.5424 674.0 5039.0 5952.0 2013-03-10 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 881.0000 1661.0 1242.0 5039.0 2013-03-11 17600.0 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 553.0000 485.0 818.0 1242.0 In\u00a0[14]: Copied! <pre># Weights in MultiVariate\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = custom_weights\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=7).head(3)\n</pre> # Weights in MultiVariate # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = custom_weights              )  forecaster.fit(series=data_train) forecaster.predict(steps=7).head(3) Out[14]: CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 <p>  \u00a0 Warning </p> <p>The <code>weight_func</code> argument will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>.</p> In\u00a0[15]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> In\u00a0[16]: Copied! <pre>forecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},\n                 transformer_exog   = None,\n                 weight_func        = None,\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},                  transformer_exog   = None,                  weight_func        = None,              )  forecaster.fit(series=data_train) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py:496: IgnoredArgumentWarning: {'PM2.5'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> Out[16]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: {'CO': StandardScaler(), 'SO2': StandardScaler()} \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:53:35 \nLast fit date: 2023-09-09 16:53:35 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[17]: Copied! <pre># Grid search MultiVariate with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7\n             )    \n\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [7, 14]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              exog                = None,\n              steps               = 7,\n              metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              lags_grid           = lags_grid,\n              param_grid          = param_grid,\n              initial_train_size  = len(data_train),\n              refit               = False,\n              fixed_train_size    = False,\n              return_best         = True,\n              n_jobs              = 'auto',\n              verbose             = False,\n              show_progress       = True\n          )\n\nresults\n</pre> # Grid search MultiVariate with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7              )      def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [7, 14] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               exog                = None,               steps               = 7,               metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],               lags_grid           = lags_grid,               param_grid          = param_grid,               initial_train_size  = len(data_train),               refit               = False,               fixed_train_size    = False,               return_best         = True,               n_jobs              = 'auto',               verbose             = False,               show_progress       = True           )  results <pre>6 models compared for 1 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7]\n  Parameters: {'alpha': 1}\n  Backtesting metric: 14931.12142535687\n  Levels: ['CO']\n\n</pre> Out[17]: levels lags params mean_absolute_error custom_metric mean_squared_error alpha 2 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 1} 14931.121425 21210.876663 5.519094e+08 1.00 1 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.1} 14931.121426 21210.876659 5.519094e+08 0.10 0 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.01} 14931.121426 21210.876659 5.519094e+08 0.01 5 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 1} 15019.782237 20996.998181 5.649121e+08 1.00 4 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.1} 15019.782238 20996.998178 5.649121e+08 0.10 3 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.01} 15019.782238 20996.998178 5.649121e+08 0.01 In\u00a0[18]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[18]: feature importance 0 CO_lag_1 0.655461 1 CO_lag_2 -0.072108 2 CO_lag_3 0.097851 3 CO_lag_4 0.055877 4 CO_lag_5 0.067536 5 CO_lag_6 -0.122215 6 CO_lag_7 0.115945 7 SO2_lag_1 5.336658 8 SO2_lag_2 -1.074241 9 SO2_lag_3 -3.647177 10 SO2_lag_4 0.020614 11 SO2_lag_5 1.483432 12 SO2_lag_6 2.971133 13 SO2_lag_7 -1.764458 14 PM2.5_lag_1 -1.000016 15 PM2.5_lag_2 -0.902531 16 PM2.5_lag_3 0.373182 17 PM2.5_lag_4 -0.767187 18 PM2.5_lag_5 -0.828407 19 PM2.5_lag_6 0.546431 20 PM2.5_lag_7 -0.401235 In\u00a0[19]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step          = 1,\n               X_train       = X,\n               y_train       = y,\n               remove_suffix = False\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step          = 1,                X_train       = X,                y_train       = y,                remove_suffix = False            )  X_1.head(4) Out[19]: CO_lag_1 CO_lag_2 CO_lag_3 CO_lag_4 CO_lag_5 CO_lag_6 CO_lag_7 SO2_lag_1 SO2_lag_2 SO2_lag_3 ... SO2_lag_5 SO2_lag_6 SO2_lag_7 PM2.5_lag_1 PM2.5_lag_2 PM2.5_lag_3 PM2.5_lag_4 PM2.5_lag_5 PM2.5_lag_6 PM2.5_lag_7 date 2013-03-08 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 9600.0 2369.0000 3052.0000 2001.0000 ... 1661.0 674.0 204.0 5952.0 4934.0 3388.0 438.0 1956.0 633.0 181.0 2013-03-09 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 1535.5424 2369.0000 3052.0000 ... 485.0 1661.0 674.0 5039.0 5952.0 4934.0 3388.0 438.0 1956.0 633.0 2013-03-10 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 881.0000 1535.5424 2369.0000 ... 2001.0 485.0 1661.0 1242.0 5039.0 5952.0 4934.0 3388.0 438.0 1956.0 2013-03-11 17600.0 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 553.0000 881.0000 1535.5424 ... 3052.0 2001.0 485.0 818.0 1242.0 5039.0 5952.0 4934.0 3388.0 438.0 <p>4 rows \u00d7 21 columns</p> In\u00a0[20]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[20]: <pre>date\n2013-03-08    78194.0\n2013-03-09    24596.0\n2013-03-10    17600.0\n2013-03-11    40794.0\nFreq: D, Name: CO_step_1, dtype: float64</pre> <p>  \u00a0 Tip </p> <p><code>bayesian_search_forecaster_multiseries</code> will be released in a future version of skforecast.</p> <p>Stay tuned!</p> In\u00a0[21]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#dependent-multi-series-forecasting-multivariate-forecasting","title":"Dependent multi-series forecasting (Multivariate forecasting)\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model.</p> <p>In dependent multi-series forecasting (multivariate time series), all series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p> Internal Forecaster time series transformation to train a forecaster with multiple dependent time series. </p> <p>Since as many training matrices are created as there are series in the dataset, it must be decided on which level the forecasting will be performed. To predict the next n steps a model is trained for each step to be predicted, the selected level in the figure is <code>Series 1</code>. This strategy is of the type direct multi-step forecasting.</p> <p> Diagram of direct forecasting with multiple dependent time series. </p> <p>Using the <code>ForecasterAutoregMultiVariate</code> class, it is possible to easily build machine learning models for dependent multi-series forecasting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#train-and-predict-forecasterautoregmultivariate","title":"Train and predict ForecasterAutoregMultiVariate\u00b6","text":"<p>When initializing the forecaster, the <code>level</code> to be predicted and the maximum number of <code>steps</code> must be indicated since a different model will be created for each step.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#backtesting-multivariate","title":"Backtesting MultiVariate\u00b6","text":"<p>See the backtesting user guide to learn more about backtesting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#hyperparameter-tuning-and-lags-selection-multivariate","title":"Hyperparameter tuning and lags selection MultiVariate\u00b6","text":"<p>Functions <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code> in the module <code>model_selection_multiseries</code> allow for lag and hyperparameter optimization. The optimization is performed in the same way as in the other forecasters, see the user guide here.</p> <p>The following example shows how to use <code>random_search_forecaster_multiseries</code> to find the best lags and model hyperparameters for all time series:</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#different-lags-for-each-time-series","title":"Different lags for each time series\u00b6","text":"<p>If a <code>dict</code> is passed to the <code>lags</code> argument, it allows setting different lags for each of the series.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#weights-in-multivariate","title":"Weights in MultiVariate\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model.</p> <p>Learn more about weighted time series forecasting with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#scikit-learn-transformers-in-multivariate","title":"Scikit-learn transformers in MultiVariate\u00b6","text":"<p>Learn more about using scikit-learn transformers with skforecast.</p> <ul> <li>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</li> <li>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them.</li> </ul>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, and <code>random_search_forecaster_multiseries</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregMultiVariate</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#extract-training-matrix","title":"Extract training matrix\u00b6","text":"<p>Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/direct-multi-step-forecasting.html","title":"Direct multi-step forecasting","text":"<p>  \u00a0 Note </p> <p>Starting from version skforecast 0.9.0, the <code>ForecasterAutoregDirect</code> now includes the <code>n_jobs</code> parameter, allowing multi-process parallelization. This allows to train regressors for all steps simultaneously.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor     = Ridge(),\n                 steps         = 36,\n                 lags          = 15,\n                 transformer_y = None,\n                 n_jobs        = 'auto'\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor     = Ridge(),                  steps         = 36,                  lags          = 15,                  transformer_y = None,                  n_jobs        = 'auto'              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>======================= \nForecasterAutoregDirect \n======================= \nRegressor: Ridge() \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWeight function included: False \nWindow size: 15 \nMaximum steps predicted: 36 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:54:14 \nLast fit date: 2023-09-09 16:54:14 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\n# Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict # ============================================================================== # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) <pre>2005-07-01    0.952051\n2005-11-01    1.179922\nName: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Predict all steps defined in the initialization.\npredictions = forecaster.predict()\ndisplay(predictions.head(3))\n</pre> # Predict all steps defined in the initialization. predictions = forecaster.predict() display(predictions.head(3)) <pre>2005-07-01    0.952051\n2005-08-01    1.004145\n2005-09-01    1.114590\nName: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\n\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n            \nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== predictions = forecaster.predict(steps=36)  error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )              print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.008419597278831958\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[8]: feature importance 0 lag_1 0.139299 1 lag_2 0.051089 2 lag_3 0.044192 3 lag_4 -0.019868 4 lag_5 -0.017935 5 lag_6 -0.013233 6 lag_7 -0.021063 7 lag_8 -0.012591 8 lag_9 0.011918 9 lag_10 0.020511 10 lag_11 0.154030 11 lag_12 0.551652 12 lag_13 0.057513 13 lag_14 -0.071071 14 lag_15 -0.035237 In\u00a0[9]: Copied! <pre># Create the whole train matrix\nX, y = forecaster.create_train_X_y(data_train)\n\n# Extract X and y to train the model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step          = 1,\n               X_train       = X,\n               y_train       = y,\n               remove_suffix = False\n           )\n\nX_1.head(4)\n</pre> # Create the whole train matrix X, y = forecaster.create_train_X_y(data_train)  # Extract X and y to train the model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step          = 1,                X_train       = X,                y_train       = y,                remove_suffix = False            )  X_1.head(4) Out[9]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1992-12-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1993-01-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 In\u00a0[10]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[10]: <pre>datetime\n1992-10-01    0.568606\n1992-11-01    0.595223\n1992-12-01    0.771258\n1993-01-01    0.751503\nFreq: MS, Name: y_step_1, dtype: float64</pre> In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","title":"Direct multi-step forecaster\u00b6","text":"<p>This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the <code>ForecasterAutoregDirect</code> class and can also include one or multiple exogenous variables.</p> <p>Direct multi-step forecasting is a time series forecasting strategy in which a separate model is trained to predict each step in the forecast horizon. This is in contrast to recursive multi-step forecasting, where a single model is used to make predictions for all future time steps by recursively using its own output as input.</p> <p>Direct multi-step forecasting can be more computationally expensive than recursive forecasting since it requires training multiple models. However, it can often achieve better accuracy in certain scenarios, particularly when there are complex patterns and dependencies in the data that are difficult to capture with a single model.</p> <p>Direct multi-step forecasting can be performed using the <code>ForecasterAutoregDirect</code> class, which can also incorporate one or multiple exogenous variables to improve the accuracy of the forecasts.</p> <p> Diagram of direct multi-step forecasting. </p> <p>To train a <code>ForecasterAutoregDirect</code> a different training matrix is created for each model.</p> <p> Transformation of a time series into matrices to train a direct multi-step forecasting model. </p>"},{"location":"user_guides/direct-multi-step-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#prediction","title":"Prediction\u00b6","text":"<p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul>"},{"location":"user_guides/direct-multi-step-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregDirect</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/direct-multi-step-forecasting.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>Two steps are needed to extract the training matrices. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/exogenous-variables.html","title":"Exogenous variables","text":"<p>  \u00a0 Note </p> <p>When exogenous variables are used, their values must be aligned so that y[i] regresses on exog[i].</p> <p> Time series transformation including an exogenous variable. </p> <p>  \u00a0 Warning </p> <p>When exogenous variables are included in a forecasting model, it is assumed that all exogenous inputs are known in the future. Do not include exogenous variables as predictors if their future value will not be known when making predictions.</p> <p>  \u00a0 Note </p> <p>For a detailed guide on how to include categorical exogenous variables, please visit Categorical Features.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data.plot(ax=ax); In\u00a0[3]: Copied! <pre># Split data in train and test\n# ==============================================================================\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Split data in train and test # ============================================================================== steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(\n    y    = data_train['y'],\n    exog = data_train[['exog_1', 'exog_2']]\n)\n\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(     y    = data_train['y'],     exog = data_train[['exog_1', 'exog_2']] )  forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:55:01 \nLast fit date: 2023-09-09 16:55:02 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(\n                  steps = steps,\n                  exog = data_test[['exog_1', 'exog_2']]\n              )\n\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(                   steps = steps,                   exog = data_test[['exog_1', 'exog_2']]               )  predictions.head(3) Out[5]: <pre>2005-07-01    0.908832\n2005-08-01    0.953925\n2005-09-01    1.100887\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.004022228812838391\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[8]: feature importance 0 lag_1 0.013354 1 lag_2 0.061120 2 lag_3 0.009086 3 lag_4 0.002721 4 lag_5 0.002478 5 lag_6 0.003155 6 lag_7 0.002179 7 lag_8 0.008154 8 lag_9 0.010319 9 lag_10 0.020587 10 lag_11 0.007036 11 lag_12 0.773389 12 lag_13 0.004583 13 lag_14 0.018127 14 lag_15 0.008732 15 exog_1 0.010364 16 exog_2 0.044616 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/exogenous-variables.html#exogenous-variables-features","title":"Exogenous variables  (features)\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In Skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p>"},{"location":"user_guides/exogenous-variables.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#prediction","title":"Prediction\u00b6","text":"<p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p>"},{"location":"user_guides/exogenous-variables.html#feature-importances","title":"Feature importances\u00b6","text":"<p>If exogenous variables are included as predictors, they have a value of feature importances.</p>"},{"location":"user_guides/feature-importances.html","title":"Feature importances","text":"<p>  \u00a0 Warning </p> <p>The <code>get_feature_importances()</code> method will only return values if the forecaster's regressor has either the <code>coef_</code> or <code>feature_importances_</code> attribute, which is the default in scikit-learn. If your regressor does not follow this naming convention, please consider opening an issue on GitHub and we will strive to include it in future updates.</p> <p>  \u00a0 Note </p> <p>See also: SHAP values in skforecast models</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') In\u00a0[3]: Copied! <pre># Create and fit forecaster using a RandomForest regressor\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\n# Predictors importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Create and fit forecaster using a RandomForest regressor # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  # Predictors importances # ============================================================================== forecaster.get_feature_importances() Out[3]: feature importance 0 lag_1 0.530186 1 lag_2 0.100529 2 lag_3 0.023620 3 lag_4 0.070458 4 lag_5 0.063155 5 exog_1 0.047043 6 exog_2 0.165009 In\u00a0[4]: Copied! <pre># Create and fit forecaster using a linear regressor\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\nforecaster.get_feature_importances()\n</pre> # Create and fit forecaster using a linear regressor # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  forecaster.get_feature_importances() Out[4]: feature importance 0 lag_1 0.327688 1 lag_2 -0.073593 2 lag_3 -0.152202 3 lag_4 -0.217106 4 lag_5 -0.145800 5 exog_1 0.379798 6 exog_2 0.668162 <p>To properly retrieve the feature importances in the <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>, it is essential to specify the model from which to extract the feature importances are to be extracted. This is because Direct Strategy Forecasters fit one model per step, and each model may have different important features. Therefore, the user must explicitly specify which model's feature importances wish to extract to ensure that the correct features are used.</p> In\u00a0[5]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor = Ridge(random_state=123),\n                 steps = 10,\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\n# Predictors importances of model for step 1\n# ==============================================================================\nforecaster.get_feature_importances(step=1)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor = Ridge(random_state=123),                  steps = 10,                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  # Predictors importances of model for step 1 # ============================================================================== forecaster.get_feature_importances(step=1) Out[5]: feature importance 0 lag_1 0.326827 1 lag_2 -0.055386 2 lag_3 -0.155098 3 lag_4 -0.220415 4 lag_5 -0.138252 5 exog_1 0.386103 6 exog_2 0.635972 In\u00a0[6]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/feature-importances.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Feature importance is a technique used in machine learning to determine the relevance or importance of each feature (or variable) in a model's prediction. In other words, it measures how much each feature contributes to the model's output.</p> <p>Feature importance can be used for several purposes, such as identifying the most relevant features for a given prediction, understanding the behavior of a model, and selecting the best set of features for a given task. It can also help to identify potential biases or errors in the data used to train the model. It is important to note that feature importance is not a definitive measure of causality. Just because a feature is identified as important does not necessarily mean that it causes the outcome. Other factors, such as confounding variables, may also be at play.</p> <p>The method used to calculate feature importance may vary depending on the type of machine learning model being used. Different machine learning models may have different assumptions and characteristics that affect the calculation of feature importance. For example, decision tree-based models such as Random Forest and Gradient Boosting typically use mean decrease impurity or permutation feature importance methods to calculate feature importance.</p> <p>Linear regression models typically use coefficients or standardized coefficients to determine the importance of a feature. The magnitude of the coefficient reflects the strength and direction of the relationship between the feature and the target variable.</p> <p>The importance of the predictors included in a forecaster can be obtained using the method <code>get_feature_importances()</code>. This method accesses the <code>coef_</code> and <code>feature_importances_</code> attributes of the internal regressor.</p>"},{"location":"user_guides/feature-importances.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/feature-importances.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/feature-importances.html#extract-feature-importances-from-trained-forecaster","title":"Extract feature importances from trained forecaster\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html","title":"Forecaster in production","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata_train = data.loc[:'2005-01-01']\ndata_train.tail()\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data_train = data.loc[:'2005-01-01'] data_train.tail() Out[2]: y date 2004-09-01 1.134432 2004-10-01 1.181011 2004-11-01 1.216037 2004-12-01 1.257238 2005-01-01 1.170690 In\u00a0[3]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster.predict(steps=3) Out[4]: <pre>2005-02-01    0.927480\n2005-03-01    0.756215\n2005-04-01    0.692595\nFreq: MS, Name: pred, dtype: float64</pre> <p>As expected, predictions follow directly from the end of training data.</p> <p>When the <code>last_window</code> argument is provided, the forecaster uses this data to generate the necessary lags as predictors and starts the prediction thereafter.</p> In\u00a0[5]: Copied! <pre># Predict with last_window\n# ==============================================================================\nlast_window = data['y'].tail(5)\nforecaster.predict(steps=3, last_window=last_window)\n</pre> # Predict with last_window # ============================================================================== last_window = data['y'].tail(5) forecaster.predict(steps=3, last_window=last_window) Out[5]: <pre>2008-07-01    0.803853\n2008-08-01    0.870858\n2008-09-01    0.905003\nFreq: MS, Name: pred, dtype: float64</pre> <p>Since the provided <code>last_window</code> contains values from 2008-02-01 to 2008-06-01, the forecaster can create the needed lags and predict the next 5 steps.</p> <p>  \u00a0 Warning </p> <p>It is important to note that <code>last_window</code>'s length must be enough to include the maximum lag used by the forecaster. For example, if the forecaster uses lags 1, 24 and 48, <code>last_window</code> must include the last 48 values of the series.</p> <p>When using the <code>last_window</code> argument, it is crucial to ensure that the length of <code>last_window</code> is sufficient to include the maximum lag (or custom predictor) used by the forecaster. For instance, if the forecaster employs lags 1, 24, and 48, <code>last_window</code> must include the most recent 48 values of the series. Failing to include the required number of past observations may result in an error or incorrect predictions.</p> In\u00a0[6]: Copied! <pre># Split data\n# ==============================================================================\ndata_train = data.loc[:'2005-01-01'].copy()\ndata_last_window = data.loc['2005-08-01':'2005-12-01'].copy()\n\nprint(\n    f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"\n    f\"  (n={len(last_window)})\"\n)\n</pre> # Split data # ============================================================================== data_train = data.loc[:'2005-01-01'].copy() data_last_window = data.loc['2005-08-01':'2005-12-01'].copy()  print(     f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"     f\"  (n={len(last_window)})\" ) <pre>Train dates       : 1991-07-01 00:00:00 --- 2005-01-01 00:00:00  (n=163)\nLast window dates : 2005-08-01 00:00:00 --- 2005-12-01 00:00:00  (n=5)\n</pre> In\u00a0[7]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 3))\ndata_train['y'].plot(label='train', ax=ax)\ndata_last_window['y'].plot(label='last window', ax=ax)\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(9, 3)) data_train['y'].plot(label='train', ax=ax) data_last_window['y'].plot(label='last window', ax=ax) ax.set_xlabel('') ax.legend(); In\u00a0[8]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) <p>The size of the window needed to make predictions and the last window stored in the forecaster to make predictions immediately after the training data can be observed with the <code>window_size</code> and <code>last_window</code> attributes.</p> In\u00a0[9]: Copied! <pre># Forecaster Attributes\n# ==============================================================================\nprint('Window size:', forecaster.window_size)\nprint(\"Forecaster last window:\")\nprint(forecaster.last_window)\n</pre> # Forecaster Attributes # ============================================================================== print('Window size:', forecaster.window_size) print(\"Forecaster last window:\") print(forecaster.last_window) <pre>Window size: 5\nForecaster last window:\ndate\n2004-09-01    1.134432\n2004-10-01    1.181011\n2004-11-01    1.216037\n2004-12-01    1.257238\n2005-01-01    1.170690\nFreq: MS, Name: y, dtype: float64\n</pre> <p>The model is saved for future use.</p> <p>  \u00a0 Note </p> <p>Learn more about how to Save and load forecasters.</p> In\u00a0[10]: Copied! <pre># Save Forecaster\n# ==============================================================================\nsave_forecaster(forecaster, file_name='forecaster_001.py', verbose=False)\n</pre> # Save Forecaster # ============================================================================== save_forecaster(forecaster, file_name='forecaster_001.py', verbose=False) <p>  \u00a0 Tip </p> <p>Since the Forecaster has already been trained, there is no need to re-fit the model.</p> In\u00a0[11]: Copied! <pre># Load Forecaster\n# ==============================================================================\nforecaster_loaded = load_forecaster('forecaster_001.py', verbose=True)\n</pre> # Load Forecaster # ============================================================================== forecaster_loaded = load_forecaster('forecaster_001.py', verbose=True) <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:55:37 \nLast fit date: 2023-09-09 16:55:38 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: forecasting_series_y \n\n</pre> <p>The forecaster's training range ends at '2005-01-01'. Using a <code>last_window</code>, the forecaster will be able to make predictions for '2006-01-01', 1 year later, without having to re-fit the model.</p> In\u00a0[12]: Copied! <pre># 1 year later last window\n# ==============================================================================\ndata_last_window\n</pre> # 1 year later last window # ============================================================================== data_last_window Out[12]: y date 2005-08-01 1.006497 2005-09-01 1.094736 2005-10-01 1.027043 2005-11-01 1.149232 2005-12-01 1.160712 In\u00a0[13]: Copied! <pre># Predict with last_window\n# ==============================================================================\nforecaster.predict(steps=3, last_window=data_last_window['y'])\n</pre> # Predict with last_window # ============================================================================== forecaster.predict(steps=3, last_window=data_last_window['y']) Out[13]: <pre>2006-01-01    0.979303\n2006-02-01    0.760421\n2006-03-01    0.634806\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[14]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecaster-in-production.html#using-forecaster-models-in-production","title":"Using forecaster models in production\u00b6","text":"<p>When using a trained model in production, regular predictions need to be generated, for example, on a weekly basis every Monday. By default, the <code>predict</code> method on a trained forecaster object generates predictions starting right after the last training observation. Therefore, the model could be retrained weekly, just before the first prediction is needed, and its predict method called. However, this approach may not be practical due to reasons such as: expensive model training, unavailability of the training data history, or high prediction frequency.</p> <p>In such scenarios, the model must be able to predict at any time, even if it has not been recently trained. Fortunately, every model generated using skforecast has the <code>last_window</code> argument in its <code>predict</code> method. This argument allows providing only the past values needed to create the autoregressive predictors (lags or custom predictors), enabling prediction without the need to retrain the model. This feature is particularly useful when there are limitations in retraining the model regularly or when dealing with high-frequency predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#predicting-with-last-window","title":"Predicting with last window\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#real-use-case","title":"Real Use Case\u00b6","text":"<p>The main advantage of using the <code>last_window</code> argument is that it can be used to predict at any time, even if the Forecaster has not been trained recently.</p> <p>Imagine a use case where a model is trained, stored, and 1 year later the company wants to use it to make some predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#data","title":"Data\u00b6","text":"<p>A gap is created between the end of the training data and the last window data to simulate this behavior.</p>"},{"location":"user_guides/forecaster-in-production.html#forecaster-initial-train","title":"Forecaster initial train\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#future-predictions","title":"Future predictions\u00b6","text":"<p>The model is loaded to make new predictions.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html","title":"ARIMA and SARIMAX forecasting","text":"<p>  \u00a0 Tip </p> <p>To learn more about modeling time series with ARIMA models, visit our example: ARIMA and SARIMAX models with Python.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.Sarimax import Sarimax\nfrom skforecast.ForecasterSarimax import ForecasterSarimax\nfrom skforecast.model_selection_sarimax import backtesting_sarimax\nfrom skforecast.model_selection_sarimax import grid_search_sarimax\nfrom pmdarima import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.Sarimax import Sarimax from skforecast.ForecasterSarimax import ForecasterSarimax from skforecast.model_selection_sarimax import backtesting_sarimax from skforecast.model_selection_sarimax import grid_search_sarimax from pmdarima import ARIMA from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Train-test dates\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nprint(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\")\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax)\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Train-test dates # ============================================================================== end_train = '2005-06-01 23:59:59' print(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\") data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.plot(ax=ax) ax.legend(); <pre>Train dates : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> <p>The following section focus on how to train an ARIMA model and forecast future values with each of the three libraries.</p> <p>statsmodels</p> In\u00a0[3]: Copied! <pre># ARIMA model with statsmodels.Sarimax\n# ==============================================================================\narima = SARIMAX(endog = data_train['y'], order = (1, 1, 1))\narima_res = arima.fit(disp=0)\narima_res.summary()\n</pre> # ARIMA model with statsmodels.Sarimax # ============================================================================== arima = SARIMAX(endog = data_train['y'], order = (1, 1, 1)) arima_res = arima.fit(disp=0) arima_res.summary() Out[3]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Sat, 09 Sep 2023   AIC                 -173.869 Time: 16:56:02   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.421  0.000     0.352     0.912 ma.L1    -0.9535     0.054   -17.821  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.06 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[4]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima_res.get_forecast(steps=12)\npredictions.predicted_mean.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima_res.get_forecast(steps=12) predictions.predicted_mean.head(4) Out[4]: <pre>2005-07-01    0.859450\n2005-08-01    0.870306\n2005-09-01    0.877163\n2005-10-01    0.881494\nFreq: MS, Name: predicted_mean, dtype: float64</pre> <p>pmdarima</p> In\u00a0[5]: Copied! <pre># ARIMA model with pmdarima.ARIMA\n# ==============================================================================\narima = ARIMA(order=(1, 1, 1), suppress_warnings=True)\narima.fit(y=data_train['y'])\narima.summary()\n</pre> # ARIMA model with pmdarima.ARIMA # ============================================================================== arima = ARIMA(order=(1, 1, 1), suppress_warnings=True) arima.fit(y=data_train['y']) arima.summary() Out[5]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      93.643 Date: Sat, 09 Sep 2023   AIC                 -179.287 Time: 16:56:02   BIC                 -167.036 Sample: 04-01-1992   HQIC                -174.312 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] intercept     0.0011     0.000     2.183  0.029     0.000     0.002 ar.L1     0.6019     0.108     5.593  0.000     0.391     0.813 ma.L1    -0.9998     5.468    -0.183  0.855   -11.717     9.718 sigma2     0.0175     0.096     0.183  0.855    -0.170     0.205 Ljung-Box (L1) (Q): 1.58   Jarque-Bera (JB):   129.76 Prob(Q): 0.21   Prob(JB):           0.00 Heteroskedasticity (H): 2.19   Skew:               -1.49 Prob(H) (two-sided): 0.01   Kurtosis:           6.29 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[6]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima.predict(n_periods=12)\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima.predict(n_periods=12) predictions.head(4) Out[6]: <pre>2005-07-01    0.891865\n2005-08-01    0.922805\n2005-09-01    0.942514\n2005-10-01    0.955461\nFreq: MS, dtype: float64</pre> <p>skforecast</p> In\u00a0[7]: Copied! <pre># ARIMA model with skforecast.Sarimax\n# ==============================================================================\narima = Sarimax(order=(1, 1, 1))\narima.fit(y=data_train['y'])\narima.summary()\n</pre> # ARIMA model with skforecast.Sarimax # ============================================================================== arima = Sarimax(order=(1, 1, 1)) arima.fit(y=data_train['y']) arima.summary() Out[7]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Sat, 09 Sep 2023   AIC                 -173.869 Time: 16:56:02   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.421  0.000     0.352     0.912 ma.L1    -0.9535     0.054   -17.821  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.06 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[8]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima.predict(steps=12)\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima.predict(steps=12) predictions.head(4) Out[8]: pred 2005-07-01 0.859450 2005-08-01 0.870306 2005-09-01 0.877163 2005-10-01 0.881494 <p>The previous section introduced the construction of ARIMA-SARIMAX models using three different implementations. In order to seamlessly integrate these models with the various functionalities provided by skforecast, the next step is to encapsulate these models within a <code>ForecasterSarimax</code> object. This encapsulation harmonizes the intricacies of the model and allows for the coherent use of skforecast's extensive capabilities.</p> <p>The following code is done using the skforecast <code>Sarimax</code> model, but the same code can be applied to the pmdarima <code>ARIMA</code> model.</p> In\u00a0[9]: Copied! <pre># Create and fit ForecasterSarimax\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\nforecaster\n</pre> # Create and fit ForecasterSarimax # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(y=data_train['y'], suppress_warnings=True) forecaster Out[9]: <pre>================= \nForecasterSarimax \n================= \nRegressor: Sarimax(12,1,1)(0,0,0)[0] \nRegressor parameters: {'concentrate_scale': False, 'dates': None, 'disp': False, 'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None, 'hamilton_representation': False, 'maxiter': 200, 'measurement_error': False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True, 'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing': False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {}, 'start_params': None, 'time_varying_regression': False, 'trend': None, 'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2023-09-09 16:56:02 \nLast fit date: 2023-09-09 16:56:04 \nIndex seen by the forecaster: DatetimeIndex(['1992-04-01', '1992-05-01', '1992-06-01', '1992-07-01',\n               '1992-08-01', '1992-09-01', '1992-10-01', '1992-11-01',\n               '1992-12-01', '1993-01-01',\n               ...\n               '2004-09-01', '2004-10-01', '2004-11-01', '2004-12-01',\n               '2005-01-01', '2005-02-01', '2005-03-01', '2005-04-01',\n               '2005-05-01', '2005-06-01'],\n              dtype='datetime64[ns]', name='datetime', length=159, freq='MS') \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[10]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[10]: <pre>2005-07-01    0.957888\n2005-08-01    0.960101\n2005-09-01    1.108663\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[11]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[12]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.07124801395226059\n</pre> In\u00a0[13]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, alpha=0.05)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, alpha=0.05) predictions.head(3) Out[13]: pred lower_bound upper_bound 2005-07-01 0.957888 0.857849 1.057926 2005-08-01 0.960101 0.853867 1.066336 2005-09-01 1.108663 0.998261 1.219064 <p>  \u00a0 Tip </p> <p>To learn more about exogenous variables and how to correctly manage them with skforecast visit: Exogenous variables (features) user guide.</p> In\u00a0[14]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps = 36,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog # ============================================================================== predictions = forecaster.predict(                   steps = 36,                   exog  = data_test[['exog_1', 'exog_2']]               ) predictions.head(3) Out[14]: <pre>2005-07-01    0.904036\n2005-08-01    0.932258\n2005-09-01    1.088950\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[15]: Copied! <pre># Split data Train - Last window - Test\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nend_last_window = '2007-06-01 23:59:59'\n\nprint(\n    f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"\n    f\"(n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"\n    f\"(n={len(data.loc[end_train:end_last_window])})\"\n)\nprint(\n    f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_last_window:])})\"\n)\ndata_train       = data.loc[:end_train]\ndata_last_window = data.loc[end_train:end_last_window]\ndata_test        = data.loc[end_last_window:]\n\n# Plot\n# ======================================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Split data Train - Last window - Test # ============================================================================== end_train = '2005-06-01 23:59:59' end_last_window = '2007-06-01 23:59:59'  print(     f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"     f\"(n={len(data.loc[:end_train])})\" ) print(     f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"     f\"(n={len(data.loc[end_train:end_last_window])})\" ) print(     f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_last_window:])})\" ) data_train       = data.loc[:end_train] data_last_window = data.loc[end_train:end_last_window] data_test        = data.loc[end_last_window:]  # Plot # ====================================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates       : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nLast window dates : 2005-07-01 00:00:00 --- 2007-06-01 00:00:00  (n=24)\nTest dates        : 2007-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=12)\n</pre> <p>Since exogenous variables have been included in the Forecaster tuning, it is necessary to pass both past values and their future values to the <code>predict</code> method using the <code>last_window_exog</code> and <code>exog</code> parameters when making predictions.</p> In\u00a0[16]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog and last window\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps            = 12,\n                  exog             = data_test[['exog_1', 'exog_2']],\n                  last_window      = data_last_window['y'],\n                  last_window_exog = data_last_window[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog and last window # ============================================================================== predictions = forecaster.predict(                   steps            = 12,                   exog             = data_test[['exog_1', 'exog_2']],                   last_window      = data_last_window['y'],                   last_window_exog = data_last_window[['exog_1', 'exog_2']]               ) predictions.head(3) Out[16]: <pre>2007-07-01    0.883503\n2007-08-01    1.041713\n2007-09-01    1.071409\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[17]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.06296272219417774\n</pre> In\u00a0[18]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[19]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[19]: feature importance 0 exog_1 -0.541309 1 exog_2 1.525838 2 ar.L1 -0.162700 3 ar.L2 -0.236563 4 ar.L3 -0.194586 5 ar.L4 -0.284359 6 ar.L5 -0.190579 7 ar.L6 -0.211254 8 ar.L7 -0.247428 9 ar.L8 -0.212840 10 ar.L9 -0.211725 11 ar.L10 -0.229331 12 ar.L11 -0.151496 13 ar.L12 0.672440 14 ma.L1 -0.978456 15 sigma2 0.001588 In\u00a0[20]: Copied! <pre># Backtest forecaster\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nmetric, predictions = backtesting_sarimax(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          initial_train_size    = len(data_train),\n                          fixed_train_size      = False,\n                          steps                 = 12,\n                          metric                = 'mean_absolute_error',\n                          refit                 = True,\n                          n_jobs                = 'auto',\n                          suppress_warnings_fit = True,\n                          verbose               = True,\n                          show_progress         = True\n                      )\n\nprint(f\"Error backtest: {metric}\")\npredictions.head(4)\n</pre> # Backtest forecaster # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  metric, predictions = backtesting_sarimax(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           initial_train_size    = len(data_train),                           fixed_train_size      = False,                           steps                 = 12,                           metric                = 'mean_absolute_error',                           refit                 = True,                           n_jobs                = 'auto',                           suppress_warnings_fit = True,                           verbose               = True,                           show_progress         = True                       )  print(f\"Error backtest: {metric}\") predictions.head(4) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 159\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number of steps per fold: 12\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2005-06-01 00:00:00  (n=159)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2006-06-01 00:00:00  (n=171)\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2007-06-01 00:00:00  (n=183)\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Error backtest: 0.0561092485592625\n</pre> Out[20]: pred 2005-07-01 0.904036 2005-08-01 0.932258 2005-09-01 1.088950 2005-10-01 1.112871 In\u00a0[21]: Copied! <pre># Train-validation-test data\n# ======================================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\")\nprint(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")\n\n# Plot\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation')\ndata.loc[end_val:, 'y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Train-validation-test data # ====================================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\") print(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")  # Plot # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation') data.loc[end_val:, 'y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1992-04-01 00:00:00 --- 2001-01-01 00:00:00  (n=106)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=29)\n</pre> In\u00a0[22]: Copied! <pre># Grid search hyperparameter\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), maxiter=200),\n             )\n\nparam_grid = {\n    'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],\n    'trend': [None, 'n', 'c']\n}\n\nresults_grid = grid_search_sarimax(\n                   forecaster            = forecaster,\n                   y                     = data.loc[:end_val, 'y'],\n                   param_grid            = param_grid,\n                   steps                 = 12,\n                   refit                 = False,\n                   metric                = 'mean_absolute_error',\n                   initial_train_size    = len(data_train),\n                   fixed_train_size      = False,\n                   return_best           = True,\n                   n_jobs                = 'auto',\n                   suppress_warnings_fit = True,\n                   verbose               = False,\n                   show_progress         = True\n               )\n\nresults_grid.head(5)\n</pre> # Grid search hyperparameter # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), maxiter=200),              )  param_grid = {     'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],     'trend': [None, 'n', 'c'] }  results_grid = grid_search_sarimax(                    forecaster            = forecaster,                    y                     = data.loc[:end_val, 'y'],                    param_grid            = param_grid,                    steps                 = 12,                    refit                 = False,                    metric                = 'mean_absolute_error',                    initial_train_size    = len(data_train),                    fixed_train_size      = False,                    return_best           = True,                    n_jobs                = 'auto',                    suppress_warnings_fit = True,                    verbose               = False,                    show_progress         = True                )  results_grid.head(5) <pre>Number of models compared: 9.\n</pre> <pre>params grid:   0%|          | 0/9 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found parameters, and the whole data set: \n  Parameters: {'order': (12, 1, 1), 'trend': None}\n  Backtesting metric: 0.06867231274352846\n\n</pre> Out[22]: params mean_absolute_error order trend 6 {'order': (12, 1, 1), 'trend': None} 0.068672 (12, 1, 1) None 7 {'order': (12, 1, 1), 'trend': 'n'} 0.068672 (12, 1, 1) n 2 {'order': (12, 0, 0), 'trend': 'c'} 0.077006 (12, 0, 0) c 0 {'order': (12, 0, 0), 'trend': None} 0.077562 (12, 0, 0) None 1 {'order': (12, 0, 0), 'trend': 'n'} 0.077562 (12, 0, 0) n <p>Since <code>return_best = True</code>, the Forecaster object is updated with the most optimal configuration found and trained with the entire dataset. This means that the grid search will yield the lowest error model with the best hyperparameters that lead to the highest performance metric. This last model can subsequently be utilized for forecasts on new data.</p> In\u00a0[23]: Copied! <pre>forecaster\n</pre> forecaster Out[23]: <pre>================= \nForecasterSarimax \n================= \nRegressor: Sarimax(12,1,1)(0,0,0)[0] \nRegressor parameters: {'concentrate_scale': False, 'dates': None, 'disp': False, 'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None, 'hamilton_representation': False, 'maxiter': 200, 'measurement_error': False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True, 'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing': False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {}, 'start_params': None, 'time_varying_regression': False, 'trend': None, 'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2023-09-09 16:56:27 \nLast fit date: 2023-09-09 16:56:49 \nIndex seen by the forecaster: DatetimeIndex(['1992-04-01', '1992-05-01', '1992-06-01', '1992-07-01',\n               '1992-08-01', '1992-09-01', '1992-10-01', '1992-11-01',\n               '1992-12-01', '1993-01-01',\n               ...\n               '2005-04-01', '2005-05-01', '2005-06-01', '2005-07-01',\n               '2005-08-01', '2005-09-01', '2005-10-01', '2005-11-01',\n               '2005-12-01', '2006-01-01'],\n              dtype='datetime64[ns]', name='datetime', length=166, freq='MS') \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[24]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecasting-sarimax-arima.html#arima-and-sarimax","title":"ARIMA and SARIMAX\u00b6","text":"<p>ARIMA (AutoRegressive Integrated Moving Average) and SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) are prominent and widely used statistical forecasting models. While ARIMA models are more widely known, SARIMAX models extend the ARIMA framework by seamlessly integrating seasonal patterns and exogenous variables.</p> <p>In the ARIMA-SARIMAX model notation, the parameters $p$, $d$, and $q$ represent the autoregressive, differencing, and moving-average components, respectively. $P$, $D$, and $Q$ denote the same components for the seasonal part of the model, with $m$ representing the number of periods in each season.</p> <ul> <li><p>$p$ is the order (number of time lags) of the autoregressive part of the model.</p> </li> <li><p>$d$ is the degree of differencing (the number of times that past values have been subtracted from the data).</p> </li> <li><p>$q$ is the order of the moving average part of the model.</p> </li> <li><p>$P$ is the order (number of time lags) of the seasonal part of the model.</p> </li> <li><p>$D$ is the degree of differencing (the number of times the data have had past values subtracted) of the seasonal part of the model.</p> </li> <li><p>$Q$ is the order of the moving average of the seasonal part of the model.</p> </li> <li><p>$m$ refers to the number of periods in each season.</p> </li> </ul> <p>When the terms $P$, $D$, $Q$, and $m$ are zero and no exogenous variables are included in the model, the SARIMAX model is equivalent to an ARIMA.</p> <p>When two out of the three terms are zero, the model can be referred to based on the non-zero parameter, dropping \"AR\", \"I\" or \"MA\" from the acronym describing the model. For example, $ARIMA(1,0,0)$ is $AR(1)$, $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$.</p> <p>ARIMA implementations</p> <p>Several Python libraries implement ARIMA-SARIMAX models. Three of them are:</p> <ul> <li><p>statsmodels: this is one of the most complete libraries for statistical modeling. While the functional paradigm may be intuitive for those coming from the R environment, those accustomed to the object-oriented API of scikit-learn may need a short period of adaptation.</p> </li> <li><p>pmdarima: This is a wrapper for <code>statsmodels SARIMAX</code>. Its distinguishing feature is its seamless integration with the scikit-learn API, allowing users familiar with scikit-learn's conventions to seamlessly dive into time series modeling.</p> </li> <li><p>skforecast: a novel wrapper for <code>statsmodels SARIMAX</code> that also follows the scikit-learn API. This implementation is very similar to that of pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p> </li> </ul> <p>ForecasterSarimax</p> <p>The <code>ForecasterSarimax</code> class allows training and validation of ARIMA and SARIMAX models using the skforecast API. <code>ForecasterSarimax</code> is compatible with two ARIMA-SARIMAX implementations:</p> <ul> <li><p><code>ARIMA</code> from pmdarima: a wrapper for statsmodels SARIMAX that follows the scikit-learn API.</p> </li> <li><p><code>Sarimax</code> from skforecast: a novel wrapper for statsmodels SARIMAX that also follows the sklearn API. This implementation is very similar to pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p> </li> </ul>"},{"location":"user_guides/forecasting-sarimax-arima.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#statsmodels-pmdarima-and-skforecast","title":"Statsmodels, pmdarima and skforecast\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#forecastersarimax","title":"ForecasterSarimax\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#training","title":"Training\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#interval-prediction","title":"Interval prediction\u00b6","text":"<p>Either <code>alpha</code> or <code>interval</code> can be used to indicate the confidence of the estimated prediction interval.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#exogenous-variables","title":"Exogenous variables\u00b6","text":"<p>The addition of exogenous variables is done using the <code>exog</code> argument. The only requirement for including an exogenous variable is the need to know the value of the variable also during the forecast period.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#using-an-already-trained-arima","title":"Using an already trained ARIMA\u00b6","text":"<p>Forecasting with an ARIMA model becomes challenging when the forecast horizon data does not immediately follow the last observed value during the training phase. This complexity is due to the moving average (MA) component, which relies on past forecast errors as predictors. Thus, to predict at time 't', the error of the 't-1' prediction becomes a necessity. In situations where this prediction isn't available, the corresponding error remains unavailable.</p> <p>For this reason, in most cases, ARIMA models are retrained each time predictions need to be made. Despite considerable efforts and advances to speed up the training process for these models, it is not always feasible to retrain the model between predictions, either due to time constraints or insufficient computational resources for repeated access to historical data. An intermediate approach is to feed the model with data from the last training observation to the start of the prediction phase. This technique enables the estimation of intermediate predictions and, as a result, the necessary errors.</p> <p>For example, imagine a situation where a model was trained 20 days ago with daily data from the past three years. When generating new predictions, only the 20 most recent values would be needed, rather than the complete historical dataset (365 * 3 + 20).</p> <p>Integrating new data into the model can be complex, but the <code>ForecasterSarimax</code> class simplifies this considerably by automating the process through the <code>last_window</code> argument in its <code>predict</code> method.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Returns the parameters of the model.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#backtesting","title":"Backtesting\u00b6","text":"<p>SARIMAX models can be evaluated using any of the backtesting strategies implemented in skforecast.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#model-tunning","title":"Model tunning\u00b6","text":"<p>To find the optimal hyperparameters for the SARIMAX model, various hyperparameter optimization strategies can be used. It is crucial to conduct hyperparameter optimization using a validation dataset, rather than the test dataset, to ensure accurate evaluation of model performance.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html","title":"Forecasting with XGBoost and LightGBM","text":"<p>  \u00a0 Note </p> <p>All of the gradient boosting libraries mentioned above - XGBoost, Lightgbm, HistGradientBoostingRegressor, and CatBoost - can handle categorical features natively, but they require specific encoding techniques that may not be entirely intuitive. Detailed information can be found in categorical features and in this example.</p> <p>  \u00a0 Note </p> <p>Tree-based models, including decision trees, random forests and gradient boosting machines (GBMs) have limitations when it comes to extrapolation, i.e. making predictions or estimates beyond the range of observed data. This limitation becomes particularly critical when forecasting time series data with a trend. Because these models cannot predict values beyond the observed range during training, their predicted values will deviate from the underlying trend.</p> <p>Several strategies have been proposed to address this challenge, with one of the most common approaches being the use of differentiation. The differentiation process involves computing the differences between consecutive observations in the time series. Instead of directly modeling the absolute values, the focus shifts to modeling the relative change ratios. Skforecast, version 0.10.0 or higher, introduces a novel <code>differentiation</code> parameter within its forecaster classes to indicate that a differentiation process must be applied before training the model. It is worth noting that the entire differentiation process has been automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p>For more details in this topic visit: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from xgboost import XGBRegressor from lightgbm import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n        url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(         url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = LGBMRegressor(random_state=123),\n                 lags            = 8,\n                 differentiation = None\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = LGBMRegressor(random_state=123),                  lags            = 8,                  differentiation = None              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:58:11 \nLast fit date: 2023-09-09 16:58:11 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[4]: <pre>2005-07-01    0.939158\n2005-08-01    0.931943\n2005-09-01    1.072937\n2005-10-01    1.090429\n2005-11-01    1.087492\n2005-12-01    1.170073\n2006-01-01    0.964073\n2006-02-01    0.760841\n2006-03-01    0.829831\n2006-04-01    0.800095\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[5]: feature importance 0 lag_1 61 1 lag_2 91 2 lag_3 14 3 lag_4 38 4 lag_5 35 5 lag_6 49 6 lag_7 25 7 lag_8 26 8 exog_1 43 9 exog_2 127 In\u00a0[6]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor       = XGBRegressor(random_state=123),\n                 lags            = 8,\n                 differentiation = None\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor       = XGBRegressor(random_state=123),                  lags            = 8,                  differentiation = None              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[6]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=123, ...) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None} \nfit_kwargs: {} \nCreation date: 2023-09-09 16:58:12 \nLast fit date: 2023-09-09 16:58:13 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[7]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[7]: <pre>2005-07-01    0.882285\n2005-08-01    0.971786\n2005-09-01    1.106107\n2005-10-01    1.064638\n2005-11-01    1.094615\n2005-12-01    1.139401\n2006-01-01    0.948508\n2006-02-01    0.784839\n2006-03-01    0.774227\n2006-04-01    0.789593\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[8]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[8]: feature importance 0 lag_1 0.286422 1 lag_2 0.125064 2 lag_3 0.001548 3 lag_4 0.027828 4 lag_5 0.075020 5 lag_6 0.011337 6 lag_7 0.058954 7 lag_8 0.045198 8 exog_1 0.075610 9 exog_2 0.293018 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecasting-with-xgboost-lightgbm-and-other-gradient-boosting-models","title":"Forecasting with XGBoost, LightGBM and other Gradient Boosting models\u00b6","text":"<p>Gradient boosting models have gained popularity in the machine learning community due to their ability to achieve excellent results in a wide range of use cases, including both regression and classification. Although these models have traditionally been less common in forecasting, recent research has shown that they can be highly effective in this domain. Some of the key advantages of using gradient boosting models for forecasting include:</p> <ul> <li><p>The ease with which exogenous variables, in addition to autoregressive variables, can be incorporated into the model.</p> </li> <li><p>The ability to capture non-linear relationships between variables.</p> </li> <li><p>High scalability, which enables the models to handle large volumes of data.</p> </li> </ul> <p>There are several popular implementations of gradient boosting in Python, with four of the most popular being XGBoost, LightGBM, scikit-learn HistGradientBoostingRegressor and CatBoost. All of these libraries follow the scikit-learn API, making them compatible with skforecast.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-lightgbm","title":"Forecaster LightGBM\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-xgboost","title":"Forecaster XGBoost\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html","title":"Hyperparameter tuning and lags selection","text":"<p>  \u00a0 Tip </p> <p>The computational cost of hyperparameter tuning depends heavily on the backtesting approach chosen to evaluate each hyperparameter combination. In general, the duration of the tuning process increases with the number of re-trains involved in the backtesting.</p> <p>To effectively speed up the prototyping phase, it is highly recommended to adopt a two-step strategy. First, use <code>refit=False</code> during the initial search to narrow down the range of values. Then, focus on the identified region of interest and apply a tailored backtesting strategy that meets the specific requirements of the use case.</p> <p>For additional tips on backtesting strategies, refer to the following resource: Which backtesting strategy should I use?.</p> <p>  \u00a0 Note </p> <p>Since skforecast 0.9.0, all backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance. This applies to all functions of the different <code>model_selection</code> modules.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import random_search_forecaster\nfrom skforecast.model_selection import bayesian_search_forecaster\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt  from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import random_search_forecaster from skforecast.model_selection import bayesian_search_forecaster from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-val-test dates\n# ==============================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"\n    f\"  (n={len(data.loc[end_train:end_val])})\"\n)\nprint(\n    f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"\n    f\" (n={len(data.loc[end_val:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:end_val].plot(ax=ax, label='validation')\ndata.loc[end_val:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-val-test dates # ============================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"     f\"  (n={len(data.loc[end_train:end_val])})\" ) print(     f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"     f\" (n={len(data.loc[end_val:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:end_val].plot(ax=ax, label='validation') data.loc[end_val:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00 (n=29)\n</pre> In\u00a0[3]: Copied! <pre># Grid search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val, 'y'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 12,\n                   refit              = False,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   n_jobs             = 'auto',\n                   verbose            = False,\n                   show_progress      = True\n               )\n</pre> # Grid search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data.loc[:end_val, 'y'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 12,                    refit              = False,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    n_jobs             = 'auto',                    verbose            = False,                    show_progress      = True                ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 15, 'n_estimators': 50}\n  Backtesting metric: 0.015325449516605583\n\n</pre> In\u00a0[4]: Copied! <pre>results_grid\n</pre> results_grid Out[4]: lags params mean_squared_error max_depth n_estimators 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.015325 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.019538 5 50 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.021915 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.023351 15 100 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.024818 10 100 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.028400 15 100 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.029157 5 50 13 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.030666 5 100 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.030762 15 50 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.030913 5 100 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.031989 10 50 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.035752 10 50 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.036509 5 100 17 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.039121 15 100 15 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.043974 10 100 16 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.049995 15 50 12 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.051199 5 50 14 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.051979 10 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[5]: Copied! <pre>forecaster\n</pre> forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(max_depth=15, n_estimators=50, random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 15, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:00:52 \nLast fit date: 2023-09-09 17:00:56 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[6]: Copied! <pre># Random search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),\n    'max_depth': np.arange(start=5, stop=30, step=1, dtype=int)\n}\n\nresults = random_search_forecaster(\n              forecaster           = forecaster,\n              y                    = data.loc[:end_val, 'y'],\n              steps                = 12,\n              lags_grid            = lags_grid,\n              param_distributions  = param_distributions,\n              n_iter               = 5,\n              metric               = 'mean_squared_error',\n              refit                = False,\n              initial_train_size   = len(data.loc[:end_train]),\n              fixed_train_size     = False,\n              return_best          = True,\n              random_state         = 123,\n              n_jobs               = 'auto',\n              verbose              = False,\n              show_progress        = True\n          )\n\nresults_grid.head(4)\n</pre> # Random search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters param_distributions = {     'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),     'max_depth': np.arange(start=5, stop=30, step=1, dtype=int) }  results = random_search_forecaster(               forecaster           = forecaster,               y                    = data.loc[:end_val, 'y'],               steps                = 12,               lags_grid            = lags_grid,               param_distributions  = param_distributions,               n_iter               = 5,               metric               = 'mean_squared_error',               refit                = False,               initial_train_size   = len(data.loc[:end_train]),               fixed_train_size     = False,               return_best          = True,               random_state         = 123,               n_jobs               = 'auto',               verbose              = False,               show_progress        = True           )  results_grid.head(4) <pre>Number of models compared: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'n_estimators': 96, 'max_depth': 19}\n  Backtesting metric: 0.027234146319364036\n\n</pre> Out[6]: lags params mean_squared_error max_depth n_estimators 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.015325 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.019538 5 50 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.021915 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.023351 15 100 In\u00a0[7]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters search space\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1., 10),\n        'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    } \n    return search_space\n\nresults, frozen_trial = bayesian_search_forecaster(\n                            forecaster            = forecaster,\n                            y                     = data.loc[:end_val, 'y'],\n                            lags_grid             = lags_grid,\n                            search_space          = search_space,\n                            steps                 = 12,\n                            metric                = 'mean_absolute_error',\n                            refit                 = False,\n                            initial_train_size    = len(data.loc[:end_train]),\n                            fixed_train_size      = True,\n                            n_trials              = 10,\n                            random_state          = 123,\n                            return_best           = False,\n                            n_jobs                = 'auto',\n                            verbose               = False,\n                            show_progress         = True,\n                            engine                = 'optuna',\n                            kwargs_create_study   = {},\n                            kwargs_study_optimize = {}\n                        )\n\nresults_grid.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters search space def search_space(trial):     search_space  = {         'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1., 10),         'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  results, frozen_trial = bayesian_search_forecaster(                             forecaster            = forecaster,                             y                     = data.loc[:end_val, 'y'],                             lags_grid             = lags_grid,                             search_space          = search_space,                             steps                 = 12,                             metric                = 'mean_absolute_error',                             refit                 = False,                             initial_train_size    = len(data.loc[:end_train]),                             fixed_train_size      = True,                             n_trials              = 10,                             random_state          = 123,                             return_best           = False,                             n_jobs                = 'auto',                             verbose               = False,                             show_progress         = True,                             engine                = 'optuna',                             kwargs_create_study   = {},                             kwargs_study_optimize = {}                         )  results_grid.head(4) <pre>Number of models compared: 20,\n         10 bayesian search in each lag configuration.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> Out[7]: lags params mean_squared_error max_depth n_estimators 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.015325 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.019538 5 50 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.021915 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.023351 15 100 <p><code>frozen_trial</code> contains information of the trial which achived the best results: See more in Study class.</p> In\u00a0[8]: Copied! <pre>frozen_trial\n</pre> frozen_trial Out[8]: <pre>FrozenTrial(number=3, state=TrialState.COMPLETE, values=[0.13263306663095237], datetime_start=datetime.datetime(2023, 9, 9, 17, 0, 59, 501915), datetime_complete=datetime.datetime(2023, 9, 9, 17, 0, 59, 548942), params={'n_estimators': 14, 'min_samples_leaf': 1, 'max_features': 'sqrt'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=3, value=None)</pre> <p>  \u00a0 Warning </p> <p>This engine is deprecated since skforecast 0.7.0 in favor of optuna engine. To continue using it, install skforecast 0.6.0.  User guide: https://joaquinamatrodrigo.github.io/skforecast/0.6.0/user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize</p> In\u00a0[9]: Copied! <pre># Custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n</pre> # Custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric In\u00a0[10]: Copied! <pre># Grid search hyperparameter and lags with custom metric\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val, 'y'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 12,\n                   refit              = False,\n                   metric             = custom_metric,\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   n_jobs             = 'auto',\n                   verbose            = False,\n                   show_progress      = True\n               )\n\nresults_grid.head(4)\n</pre> # Grid search hyperparameter and lags with custom metric # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data.loc[:end_val, 'y'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 12,                    refit              = False,                    metric             = custom_metric,                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    n_jobs             = 'auto',                    verbose            = False,                    show_progress      = True                )  results_grid.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 15, 'n_estimators': 100}\n  Backtesting metric: 0.030341102602751614\n\n</pre> Out[10]: lags params custom_metric max_depth n_estimators 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.030341 15 100 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.032391 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.034509 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.038321 10 50 In\u00a0[11]: Copied! <pre># Grid search hyperparameter and lags with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Metrics\nmetrics = ['mean_absolute_error', mean_squared_error, custom_metric]\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults_grid = grid_search_forecaster(\n                    forecaster         = forecaster,\n                    y                  = data.loc[:end_val, 'y'],\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    steps              = 12,\n                    refit              = False,\n                    metric             = metrics,\n                    initial_train_size = len(data.loc[:end_train]),\n                    fixed_train_size   = False,\n                    return_best        = True,\n                    n_jobs             = 'auto',\n                    verbose            = False,\n                    show_progress      = True\n               )\n\nresults_grid.head(4)\n</pre> # Grid search hyperparameter and lags with multiple metrics # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Metrics metrics = ['mean_absolute_error', mean_squared_error, custom_metric]  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results_grid = grid_search_forecaster(                     forecaster         = forecaster,                     y                  = data.loc[:end_val, 'y'],                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     steps              = 12,                     refit              = False,                     metric             = metrics,                     initial_train_size = len(data.loc[:end_train]),                     fixed_train_size   = False,                     return_best        = True,                     n_jobs             = 'auto',                     verbose            = False,                     show_progress      = True                )  results_grid.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 15, 'n_estimators': 50}\n  Backtesting metric: 0.09876770647333354\n\n</pre> Out[11]: lags params mean_absolute_error mean_squared_error custom_metric max_depth n_estimators 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.098768 0.015325 0.032391 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.108105 0.019538 0.034509 5 50 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.115013 0.024818 0.038799 10 100 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.115177 0.028400 0.030341 15 100 In\u00a0[12]: Copied! <pre># Models to compare\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\nmodels = [RandomForestRegressor(random_state=123), \n          GradientBoostingRegressor(random_state=123),\n          Ridge(random_state=123)]\n\n# Hyperparameter to search for each model\nparam_grids = {'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},\n               'GradientBoostingRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},\n               'Ridge': {'alpha': [0.01, 0.1, 1]}}\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\ndf_results = pd.DataFrame()\nfor i, model in enumerate(models):\n\n    print(f\"Grid search for regressor: {model}\")\n    print(f\"-------------------------\")\n\n    forecaster = ForecasterAutoreg(\n                     regressor = model,\n                     lags      = 3\n                 )\n\n    # Regressor hyperparameters\n    param_grid = param_grids[list(param_grids)[i]]\n\n    results_grid = grid_search_forecaster(\n                       forecaster         = forecaster,\n                       y                  = data.loc[:end_val, 'y'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 3,\n                       refit              = False,\n                       metric             = 'mean_squared_error',\n                       initial_train_size = len(data.loc[:end_train]),\n                       fixed_train_size   = True,\n                       return_best        = False,\n                       n_jobs             = 'auto',\n                       verbose            = False,\n                       show_progress      = True\n                   )\n    \n    # Create a column with model name\n    results_grid['model'] = list(param_grids)[i]\n    \n    df_results = pd.concat([df_results, results_grid])\n\ndf_results = df_results.sort_values(by='mean_squared_error')\ndf_results.head(10)\n</pre> # Models to compare from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.linear_model import Ridge  models = [RandomForestRegressor(random_state=123),            GradientBoostingRegressor(random_state=123),           Ridge(random_state=123)]  # Hyperparameter to search for each model param_grids = {'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},                'GradientBoostingRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},                'Ridge': {'alpha': [0.01, 0.1, 1]}}  # Lags used as predictors lags_grid = [3, 5]  df_results = pd.DataFrame() for i, model in enumerate(models):      print(f\"Grid search for regressor: {model}\")     print(f\"-------------------------\")      forecaster = ForecasterAutoreg(                      regressor = model,                      lags      = 3                  )      # Regressor hyperparameters     param_grid = param_grids[list(param_grids)[i]]      results_grid = grid_search_forecaster(                        forecaster         = forecaster,                        y                  = data.loc[:end_val, 'y'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 3,                        refit              = False,                        metric             = 'mean_squared_error',                        initial_train_size = len(data.loc[:end_train]),                        fixed_train_size   = True,                        return_best        = False,                        n_jobs             = 'auto',                        verbose            = False,                        show_progress      = True                    )          # Create a column with model name     results_grid['model'] = list(param_grids)[i]          df_results = pd.concat([df_results, results_grid])  df_results = df_results.sort_values(by='mean_squared_error') df_results.head(10) <pre>Grid search for regressor: RandomForestRegressor(random_state=123)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: GradientBoostingRegressor(random_state=123)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: Ridge(random_state=123)\n-------------------------\nNumber of models compared: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[12]: lags params mean_squared_error max_depth n_estimators model alpha 0 [1, 2, 3] {'alpha': 0.01} 0.059814 NaN NaN Ridge 0.01 1 [1, 2, 3] {'alpha': 0.1} 0.060078 NaN NaN Ridge 0.10 3 [1, 2, 3, 4, 5] {'alpha': 0.01} 0.061481 NaN NaN Ridge 0.01 4 [1, 2, 3, 4, 5] {'alpha': 0.1} 0.061765 NaN NaN Ridge 0.10 2 [1, 2, 3] {'alpha': 1} 0.063084 NaN NaN Ridge 1.00 5 [1, 2, 3, 4, 5] {'alpha': 1} 0.064954 NaN NaN Ridge 1.00 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.076140 5.0 100.0 RandomForestRegressor NaN 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.078051 5.0 50.0 RandomForestRegressor NaN 3 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.081714 15.0 100.0 RandomForestRegressor NaN 2 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.082889 15.0 50.0 RandomForestRegressor NaN In\u00a0[13]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search, that can be combined with backtesting to identify the optimal combination of lags and hyperparameters that achieve the best prediction performance.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#grid-search","title":"Grid search\u00b6","text":"<p>Grid search is a popular hyperparameter tuning technique that evaluate an exaustive list of combinations of hyperparameters and lags to find the optimal configuration for a forecasting model. To perform a grid search with the Skforecast library, two grids are needed: one with different lags (<code>lags_grid</code>) and another with the hyperparameters (<code>param_grid</code>).</p> <p>The grid search process involves the following steps:</p> <ol> <li><p><code>grid_search_forecaster</code> creates a copy of the forecaster object and replaces the <code>lags</code> argument with the first option appearing in <code>lags_grid</code>.</p> </li> <li><p>The function validates all combinations of hyperparameters presented in <code>param_grid</code> using backtesting.</p> </li> <li><p>The function repeats these two steps until it has evaluated all possible combinations of lags and hyperparameters.</p> </li> <li><p>If <code>return_best = True</code>, the original forecaster is trained with the best lags and hyperparameters configuration found during the grid search process.</p> </li> </ol>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#random-search","title":"Random search\u00b6","text":"<p>Random search is another hyperparameter tuning strategy available in the Skforecast library. In contrast to grid search, which tries out all possible combinations of hyperparameters and lags, randomized search samples a fixed number of values from the specified possibilities. The number of combinations that are evaluated is given by <code>n_iter</code>.</p> <p>It is important to note that random sampling is only applied to the model hyperparameters, but not to the lags. All lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#bayesian-search","title":"Bayesian search\u00b6","text":"<p>Grid and random search can generate good results, especially when the search range is narrowed down. However, neither of them takes into account the results obtained so far, which prevents them from focusing the search on the regions of greatest interest while avoiding unnecessary ones.</p> <p>An alternative is to use Bayesian optimization methods to search for hyperparameters. In general terms, bayesian hyperparameter optimization consists of creating a probabilistic model in which the objective function is the model validation metric (RMSE, AUC, accuracy...). With this strategy, the search is redirected at each iteration to the regions of greatest interest. The ultimate goal is to reduce the number of hyperparameter combinations with which the model is evaluated, choosing only the best candidates. This approach is particularly advantageous when the search space is very large or the model evaluation is very slow.</p> <p>Skforecast offers two Bayesian optimization engines: Scikit-Optimize and Optuna. It is worth noting that, in the context of skforecast, bayesian search is only applied to the hyperparameters of the model, and not to the lags, as all lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#optuna","title":"Optuna\u00b6","text":"<p>In skforecast, Bayesian optimization with Optuna is performed using its Study object. The objective of the optimization is to minimize the metric generated by backtesting.</p> <p>Additional parameters can be included by passing a dictionary to <code>kwargs_create_study</code> and <code>kwargs_study_optimize</code> arguments to create_study and optimize method, respectively. These arguments are used to configure the study object and optimization algorithm.</p> <p>To use Optuna in skforecast, the <code>search_space</code> argument must be a python function that defines the hyperparameters to optimize over. Optuna uses the Trial object object to generate each search space.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize","title":"Scikit-optimize\u00b6","text":"<p>Skopt performs bayesian optimization with Gaussian processes. This is done with the function gp_minimize where the objective value to be minimized is calculated by backtesting.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-with-custom-metric","title":"Hyperparameter tuning with custom metric\u00b6","text":"<p>Besides to the commonly used metrics such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p> <p>The example below demonstrates how to use hyperparameter optimization to find the optimal parameters for a custom metric that considers only the last three months of each year.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p> <p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) enable users to calculate multiple metrics for each forecaster configuration if a list is provided. This list may include any combination of built-in metrics, such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, as well as user-defined custom metrics.</p> <p>Note that if multiple metrics are specified, these functions will select the best model based on the first metric in the list.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-regressors","title":"Compare multiple regressors\u00b6","text":"<p>The grid search process can be easily extended to compare several machine learning models. This can be achieved by using a simple for loop that iterates over each regressor and applying the <code>grid_search_forecaster</code> function. This approach allows for a more thorough exploration and can help you select the best model.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html","title":"Independent multi-time series forecasting","text":"<p>  \u00a0 Tip </p> <p>To learn more about independent multi-series forecasting visit our example: Multi-series forecasting with Python and Skforecast.</p> <p>  \u00a0 Note </p> <p>See ForecasterAutoregMultiVariate for dependent multi-series forecasting (multivariate time series).</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/simulated_items_sales.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/simulated_items_sales.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00   (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00   (n=170)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); In\u00a0[5]: Copied! <pre># Create and fit forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 forecaster_id      = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  forecaster_id      = None              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: Ridge(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:01:38 \nLast fit date: 2023-09-09 17:01:38 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>Two methods can be use to predict the next n steps: <code>predict()</code> or <code>predict_interval()</code>. The argument <code>levels</code> is used to indicate for which series estimate predictions. If <code>None</code> all series will be predicted.</p> In\u00a0[6]: Copied! <pre># Predict and predict_interval\n# ==============================================================================\nsteps = 24\n\n# Predictions for item_1\npredictions_item_1 = forecaster.predict(steps=steps, levels='item_1')\ndisplay(predictions_item_1.head(3))\n\n# Interval predictions for item_1 and item_2\npredictions_intervals = forecaster.predict_interval(steps=steps, levels=['item_1', 'item_2'])\ndisplay(predictions_intervals.head(3))\n</pre> # Predict and predict_interval # ============================================================================== steps = 24  # Predictions for item_1 predictions_item_1 = forecaster.predict(steps=steps, levels='item_1') display(predictions_item_1.head(3))  # Interval predictions for item_1 and item_2 predictions_intervals = forecaster.predict_interval(steps=steps, levels=['item_1', 'item_2']) display(predictions_intervals.head(3)) item_1 2014-07-16 25.497376 2014-07-17 24.866972 2014-07-18 24.281173 item_1 item_1_lower_bound item_1_upper_bound item_2 item_2_lower_bound item_2_upper_bound 2014-07-16 25.497376 23.220087 28.226068 10.694506 7.093046 15.518896 2014-07-17 24.866972 22.141168 27.389805 11.080091 6.467676 16.534679 2014-07-18 24.281173 21.688393 26.981395 11.490882 7.077863 16.762530 In\u00a0[7]: Copied! <pre># Backtesting Multi Series\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data,\n                                           exog                  = None,\n                                           levels                = None,\n                                           steps                 = 24,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_train),\n                                           fixed_train_size      = True,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = True,\n                                           n_jobs                = 'auto',\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting Multi Series # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data,                                            exog                  = None,                                            levels                = None,                                            steps                 = 24,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_train),                                            fixed_train_size      = True,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = True,                                            n_jobs                = 'auto',                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.360675 1 item_2 2.332392 2 item_3 3.155592 <pre>\nBacktest predictions\n</pre> Out[7]: item_1 item_2 item_3 2014-07-16 25.497376 10.694506 11.275026 2014-07-17 24.866972 11.080091 11.313510 2014-07-18 24.281173 11.490882 13.030112 2014-07-19 23.515499 11.548922 13.378282 In\u00a0[8]: Copied! <pre># Create Forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n</pre> # Create Forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              ) In\u00a0[9]: Copied! <pre># Grid search Multi Series\n# ==============================================================================\nlags_grid = [24, 48]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nlevels = ['item_1', 'item_2', 'item_3']\n\nresults = grid_search_forecaster_multiseries(\n              forecaster         = forecaster,\n              series             = data,\n              exog               = None,\n              levels             = levels, # Same as levels=None\n              lags_grid          = lags_grid,\n              param_grid         = param_grid,\n              steps              = 24,\n              metric             = 'mean_absolute_error',\n              initial_train_size = len(data_train),\n              refit              = True,\n              fixed_train_size   = True,\n              return_best        = False,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search Multi Series # ============================================================================== lags_grid = [24, 48] param_grid = {'alpha': [0.01, 0.1, 1]}  levels = ['item_1', 'item_2', 'item_3']  results = grid_search_forecaster_multiseries(               forecaster         = forecaster,               series             = data,               exog               = None,               levels             = levels, # Same as levels=None               lags_grid          = lags_grid,               param_grid         = param_grid,               steps              = 24,               metric             = 'mean_absolute_error',               initial_train_size = len(data_train),               refit              = True,               fixed_train_size   = True,               return_best        = False,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>6 models compared for 3 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[9]: levels lags params mean_absolute_error alpha 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.207648 1.00 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.207700 0.10 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.207706 0.01 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.335039 1.00 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.335149 0.10 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.335161 0.01 <p>  \u00a0 Tip </p> <p>To learn more about exogenous variables in skforecast visit the exogenous variables user guide.</p> In\u00a0[10]: Copied! <pre># Generate exogenous variable month\n# ==============================================================================\ndata_exog = data.copy()\ndata_exog['month'] = data_exog.index.month\n\n# Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_exog_train = data_exog.loc[:end_train, :].copy()\ndata_exog_test  = data_exog.loc[end_train:, :].copy()\n\ndata_exog_train.head(3)\n</pre> # Generate exogenous variable month # ============================================================================== data_exog = data.copy() data_exog['month'] = data_exog.index.month  # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_exog_train = data_exog.loc[:end_train, :].copy() data_exog_test  = data_exog.loc[end_train:, :].copy()  data_exog_train.head(3) Out[10]: item_1 item_2 item_3 month date 2012-01-01 8.253175 21.047727 19.429739 1 2012-01-02 22.777826 26.578125 28.009863 1 2012-01-03 27.549099 31.751042 32.078922 1 In\u00a0[11]: Copied! <pre># Create and fit forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24,\n             )\n\nforecaster.fit(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = data_exog_train[['month']]\n)\nforecaster\n</pre> # Create and fit forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor = Ridge(random_state=123),                  lags      = 24,              )  forecaster.fit(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = data_exog_train[['month']] ) forecaster Out[11]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: Ridge(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['month'] \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:01:42 \nLast fit date: 2023-09-09 17:01:42 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p> In\u00a0[12]: Copied! <pre># Predict with exogenous variables\n# ==============================================================================\npredictions = forecaster.predict(steps=24, exog=data_exog_test[['month']])\npredictions.head(3)\n</pre> # Predict with exogenous variables # ============================================================================== predictions = forecaster.predict(steps=24, exog=data_exog_test[['month']]) predictions.head(3) Out[12]: item_1 item_2 item_3 2014-07-16 25.493029 10.698000 11.278556 2014-07-17 24.860186 11.085074 11.318098 2014-07-18 24.273013 11.497196 13.035015 <p>As mentioned earlier, the <code>month</code> exogenous variable is replicated for each of the series. This can be easily demonstrated using the <code>create_train_X_y</code> method, which returns the matrix used in the <code>fit</code> method.</p> In\u00a0[13]: Copied! <pre># X_train matrix\n# ==============================================================================\nX_train = forecaster.create_train_X_y(\n    series = data_exog_train[['item_1', 'item_2', 'item_3']], \n    exog   = data_exog_train[['month']]\n)[0]\n</pre> # X_train matrix # ============================================================================== X_train = forecaster.create_train_X_y(     series = data_exog_train[['item_1', 'item_2', 'item_3']],      exog   = data_exog_train[['month']] )[0] In\u00a0[14]: Copied! <pre># X_train slice for item_1\n# ==============================================================================\nX_train.loc[X_train['item_1'] == 1.].head(3)\n</pre> # X_train slice for item_1 # ============================================================================== X_train.loc[X_train['item_1'] == 1.].head(3) Out[14]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 ... lag_19 lag_20 lag_21 lag_22 lag_23 lag_24 month item_1 item_2 item_3 0 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 ... 21.106643 21.379238 25.895533 27.549099 22.777826 8.253175 1 1.0 0.0 0.0 1 28.747482 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 ... 20.533871 21.106643 21.379238 25.895533 27.549099 22.777826 1 1.0 0.0 0.0 2 23.908368 28.747482 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 ... 20.069327 20.533871 21.106643 21.379238 25.895533 27.549099 1 1.0 0.0 0.0 <p>3 rows \u00d7 28 columns</p> In\u00a0[15]: Copied! <pre># X_train slice for item_2\n# ==============================================================================\nX_train.loc[X_train['item_2'] == 1.].head(3)\n</pre> # X_train slice for item_2 # ============================================================================== X_train.loc[X_train['item_2'] == 1.].head(3) Out[15]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 ... lag_19 lag_20 lag_21 lag_22 lag_23 lag_24 month item_1 item_2 item_3 903 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 19.255208 17.096875 ... 17.812500 18.191667 24.567708 31.751042 26.578125 21.047727 1 0.0 1.0 0.0 904 26.611458 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 19.255208 ... 19.510417 17.812500 18.191667 24.567708 31.751042 26.578125 1 0.0 1.0 0.0 905 19.759375 26.611458 26.675000 25.332292 21.690625 19.688542 19.178125 19.265625 28.779167 28.060417 ... 24.098958 19.510417 17.812500 18.191667 24.567708 31.751042 1 0.0 1.0 0.0 <p>3 rows \u00d7 28 columns</p> <p>To use exogenous variables in backtesting or hyperparameter tuning, they must be specified with the <code>exog</code> argument.</p> In\u00a0[16]: Copied! <pre># Backtesting Multi Series with exog\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data_exog[['item_1', 'item_2', 'item_3']],\n                                           exog                  = data_exog[['month']],\n                                           levels                = None,\n                                           steps                 = 24,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_exog_train),\n                                           fixed_train_size      = True,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = True,\n                                           n_jobs                = 'auto',\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions with exogenous variables\")\nbacktest_predictions.head(4)\n</pre> # Backtesting Multi Series with exog # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data_exog[['item_1', 'item_2', 'item_3']],                                            exog                  = data_exog[['month']],                                            levels                = None,                                            steps                 = 24,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_exog_train),                                            fixed_train_size      = True,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = True,                                            n_jobs                = 'auto',                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions with exogenous variables\") backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.368685 1 item_2 2.335076 2 item_3 3.152820 <pre>\nBacktest predictions with exogenous variables\n</pre> Out[16]: item_1 item_2 item_3 2014-07-16 25.493029 10.698000 11.278556 2014-07-17 24.860186 11.085074 11.318098 2014-07-18 24.273013 11.497196 13.035015 2014-07-19 23.506739 11.555868 13.383641 In\u00a0[17]: Copied! <pre>forecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': StandardScaler()},\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = {'item_1': StandardScaler(), 'item_2': StandardScaler()},                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              )  forecaster.fit(series=data_train) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:420: IgnoredArgumentWarning: {'item_3'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> Out[17]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: Ridge(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: {'item_1': StandardScaler(), 'item_2': StandardScaler()} \nTransformer for exog: None \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:01:43 \nLast fit date: 2023-09-09 17:01:43 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[18]: Copied! <pre># Create NaNs in columns `item_2` and `item_3` \n# ==============================================================================\ndata_train_with_nan = data_train.copy()\ndata_train_with_nan['item_2'].iloc[:300] = np.nan\ndata_train_with_nan['item_3'].iloc[:500] = np.nan\n\nprint('Data train shape:', data_train_with_nan.shape)\ndata_train_with_nan.isna().sum()\n</pre> # Create NaNs in columns `item_2` and `item_3`  # ============================================================================== data_train_with_nan = data_train.copy() data_train_with_nan['item_2'].iloc[:300] = np.nan data_train_with_nan['item_3'].iloc[:500] = np.nan  print('Data train shape:', data_train_with_nan.shape) data_train_with_nan.isna().sum() <pre>Data train shape: (927, 3)\n</pre> Out[18]: <pre>item_1      0\nitem_2    300\nitem_3    500\ndtype: int64</pre> In\u00a0[19]: Copied! <pre># Create and fit forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n\nforecaster.fit(series=data_train)\n</pre> # Create and fit forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              )  forecaster.fit(series=data_train) In\u00a0[20]: Copied! <pre># Predict all levels\n# ==============================================================================\npredictions = forecaster.predict(steps=24)\npredictions.head(3)\n</pre> # Predict all levels # ============================================================================== predictions = forecaster.predict(steps=24) predictions.head(3) Out[20]: item_1 item_2 item_3 2014-07-16 25.864018 10.742220 11.569140 2014-07-17 25.229255 11.151981 10.844131 2014-07-18 24.584602 11.662202 12.608430 <p>When creating the training matrix, the forecaster will use a different number of observations depending on the length of the series. This number can be calculated as: $SeriesLength - LengthMissing - MaxLag$</p> In\u00a0[21]: Copied! <pre># Understanding training matrix\n# ==============================================================================\nX_train = forecaster.create_train_X_y(data_train_with_nan)[0]\n\n# Data train shape\nprint('Data train shape:', data_train_with_nan.shape)\nprint('Max lag used:', forecaster.max_lag)\nprint(\"\")\n\n# Determine the number of observations per series\nlen_1 = len(X_train.loc[X_train['item_1'] == 1.])\nlen_2 = len(X_train.loc[X_train['item_2'] == 1.])\nlen_3 = len(X_train.loc[X_train['item_3'] == 1.])\n\nprint(\"X_train shape:\", X_train.shape)\nprint('item_1 length:', len_1) # 927 - 24\nprint('item_2 length:', len_2) # 927 - 300 - 24\nprint('item_3 length:', len_3) # 927 - 500 - 24\nprint(\"Total length:\", len_1 + len_2 + len_3)\n</pre> # Understanding training matrix # ============================================================================== X_train = forecaster.create_train_X_y(data_train_with_nan)[0]  # Data train shape print('Data train shape:', data_train_with_nan.shape) print('Max lag used:', forecaster.max_lag) print(\"\")  # Determine the number of observations per series len_1 = len(X_train.loc[X_train['item_1'] == 1.]) len_2 = len(X_train.loc[X_train['item_2'] == 1.]) len_3 = len(X_train.loc[X_train['item_3'] == 1.])  print(\"X_train shape:\", X_train.shape) print('item_1 length:', len_1) # 927 - 24 print('item_2 length:', len_2) # 927 - 300 - 24 print('item_3 length:', len_3) # 927 - 500 - 24 print(\"Total length:\", len_1 + len_2 + len_3) <pre>Data train shape: (927, 3)\nMax lag used: 24\n\nX_train shape: (1909, 27)\nitem_1 length: 903\nitem_2 length: 603\nitem_3 length: 403\nTotal length: 1909\n</pre> In\u00a0[22]: Copied! <pre># Weights in Multi-Series\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = custom_weights,\n                 series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.} # Same as {'item_2': 2.}\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=24).head(3)\n</pre> # Weights in Multi-Series # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = custom_weights,                  series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.} # Same as {'item_2': 2.}              )  forecaster.fit(series=data_train) forecaster.predict(steps=24).head(3) Out[22]: item_1 item_2 item_3 2014-07-16 25.547560 10.527454 11.195018 2014-07-17 24.779779 10.987891 11.424717 2014-07-18 24.182702 11.375158 13.090853 <p>  \u00a0 Warning </p> <p>The <code>weight_func</code> and <code>series_weights</code> arguments will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>. If <code>weight_func</code> is a <code>dict</code>, it will be a <code>dict</code> of the form <code>{'series_column_name': source_code_weight_func}</code> .</p> In\u00a0[23]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> In\u00a0[24]: Copied! <pre># Grid search Multi-Series with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24\n             )\n\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [24, 48]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster         = forecaster,\n              series             = data,\n              lags_grid          = lags_grid,\n              param_grid         = param_grid,\n              steps              = 24,\n              metric             = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              initial_train_size = len(data_train),\n              fixed_train_size   = True,\n              levels             = None,\n              exog               = None,\n              refit              = True,\n              return_best        = False,\n              n_jobs             = 'auto',\n              verbose            = False,\n              show_progress      = True\n          )\n\nresults\n</pre> # Grid search Multi-Series with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24              )  def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [24, 48] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster         = forecaster,               series             = data,               lags_grid          = lags_grid,               param_grid         = param_grid,               steps              = 24,               metric             = [mean_absolute_error, custom_metric, 'mean_squared_error'],               initial_train_size = len(data_train),               fixed_train_size   = True,               levels             = None,               exog               = None,               refit              = True,               return_best        = False,               n_jobs             = 'auto',               verbose            = False,               show_progress      = True           )  results <pre>6 models compared for 3 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[24]: levels lags params mean_absolute_error custom_metric mean_squared_error alpha 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.190420 2.290771 9.250861 1.00 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.190493 2.290853 9.251522 0.10 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.190500 2.290861 9.251589 0.01 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.282886 2.358415 9.770826 1.00 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.282948 2.358494 9.771567 0.10 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.282954 2.358502 9.771641 0.01 <p>  \u00a0 Tip </p> <p><code>bayesian_search_forecaster_multiseries</code> will be released in a future version of skforecast.</p> <p>Stay tuned!</p> In\u00a0[25]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/independent-multi-time-series-forecasting.html#independent-multi-series-forecasting","title":"Independent multi-series forecasting\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model.</p> <p>In independent multi-series forecasting a single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p> Internal Forecaster transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied, with the only difference being that the series name for which to estimate the predictions needs to be indicated.</p> <p> Diagram of recursive forecasting with multiple independent time series. </p> <p>Using the <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes, it is possible to easily build machine learning models for independent multi-series forecasting.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#train-and-predict-forecasterautoregmultiseries","title":"Train and predict ForecasterAutoregMultiSeries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#backtesting-multi-series","title":"Backtesting Multi Series\u00b6","text":"<p>As in the <code>predict</code> method, the <code>levels</code> at which backtesting is performed must be indicated. The argument can also be set to <code>None</code> to perform backtesting at all levels.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#hyperparameter-tuning-and-lags-selection-multi-series","title":"Hyperparameter tuning and lags selection Multi Series\u00b6","text":"<p>The <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code> functions in the <code>model_selection_multiseries</code> module allow for lag and hyperparameter optimization. The optimization is performed using the same strategy as in other Forecasters, see the user guide here, except for the <code>levels</code> argument:</p> <ul> <li><p><code>levels</code>: level(s) at which the forecaster is optimized, for example:</p> <ul> <li><p>If <code>levels = ['item_1', 'item_2', 'item_3']</code> (Same as <code>levels = None</code>), the function will search for the lags and hyperparameters that minimize the average error of the predictions of all the time series. The resulting metric will be the average of all levels.</p> </li> <li><p>If <code>levels = 'item_1'</code> (Same as <code>levels = ['item_1']</code>), the function will search for the lags and hyperparameters that minimize the error of the <code>item_1</code> predictions. The resulting metric will be the one calculated for <code>item_1</code>.</p> </li> </ul> </li> </ul> <p>The following example shows how to use <code>grid_search_forecaster_multiseries</code> to find the best lags and model hyperparameters for all time series (all <code>levels</code>):</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#exogenous-variables-in-multi-series","title":"Exogenous variables in multi-series\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process.</p> <p>In the <code>ForecasterAutoregMultiSeries</code>, the same exogenous variables are replicated for each of the series. Here is an example of adding the month of the series as an exogenous variable.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#scikit-learn-transformers-in-multi-series","title":"Scikit-learn transformers in multi-series\u00b6","text":"<p>Learn more about using scikit-learn transformers with skforecast.</p> <ul> <li>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</li> <li>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them.</li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-with-different-lengths","title":"Series with different lengths\u00b6","text":"<p>When faced with a multi-series forecasting problem, it is common for the series to have varying lengths due to differences in the starting times of data recording. To address this scenario, the <code>ForecasterAutoregMultiSeries</code> provides a solution taking into account the following points:</p> <ul> <li>The input for the <code>series</code> parameter must be a pandas DataFrame containing all series where missing values must be represented as <code>NaNs</code>.</li> <li>Missing values are only expected to occur at the beginning of each series.</li> <li>All series should conclude at the same index and possess a length greater than or equal to the maximum lag used.</li> </ul> <p>Examples:</p> Series values Allowed <code>[NaN, NaN, NaN, NaN, 4, 5, 6, 7, 8, 9]</code> \u2714\ufe0f <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, NaN]</code> \u274c <code>[0, 1, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u274c <code>[NaN, NaN, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u274c"},{"location":"user_guides/independent-multi-time-series-forecasting.html#weights-in-multi-series","title":"Weights in multi-series\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights:</p> <ul> <li><p><code>series_weights</code> controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.</p> </li> <li><p><code>weight_func</code> controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.</p> </li> </ul> <p>If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <p> Weights in multi-series. </p> <ul> <li><p><code>series_weights</code> is a dict of the form <code>{'series_column_name': float}</code>. If a series is used during <code>fit</code> and is not present in <code>series_weights</code>, it will have a weight of 1.</p> </li> <li><p><code>weight_func</code> is a function that defines the individual weights of each sample based on the index.</p> <ul> <li><p>If it is a <code>callable</code>, the same function will apply to all series.</p> </li> <li><p>If it is a <code>dict</code> of the form <code>{'series_column_name': callable}</code>, a different function can be used for each series. A weight of 1 is given to all series not present in <code>weight_func</code>.</p> </li> </ul> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, and <code>random_search_forecaster_multiseries</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p>"},{"location":"user_guides/input-data.html","title":"Input data","text":"<p>Skforecast only allows pandas series and dataframes as input (although numpy arrays are used internally for better performance). The type of pandas index is used to determine how the data is processed:</p> <ul> <li><p>If the index is not of type DatetimeIndex, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex but has no frequency, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex and has a frequency, it remains unchanged.</p> </li> </ul> <p>  \u00a0 Note </p> <p>Although it is possible to use data without an associated date/time index, when using a pandas series with an associated frequency prediction results will have a more useful index.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date']) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data Out[2]: y date 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 1991-11-01 0.502369 ... ... 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'])\n\n# Predictions\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'])  # Predictions # ============================================================================== forecaster.predict(steps=5) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\n2008-10-01    0.845027\n2008-11-01    0.914621\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Data without datetime index\n# ==============================================================================\ndata = data.reset_index(drop=True)\ndata\n</pre> # Data without datetime index # ============================================================================== data = data.reset_index(drop=True) data Out[4]: y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 ... ... 199 0.761822 200 0.649435 201 0.827887 202 0.816255 203 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[5]: Copied! <pre># Fit - Predict\n# ==============================================================================\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=5)\n</pre> # Fit - Predict # ============================================================================== forecaster.fit(y=data['y']) forecaster.predict(steps=5) Out[5]: <pre>204    0.714526\n205    0.789144\n206    0.818433\n207    0.845027\n208    0.914621\nName: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/input-data.html#input-data","title":"Input data\u00b6","text":""},{"location":"user_guides/input-data.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/input-data.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-with-datetime-and-frequency-index","title":"Train and predict using input with datetime and frequency index\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-without-datetime-index","title":"Train and predict using input without datetime index\u00b6","text":""},{"location":"user_guides/plot-forecaster-residuals.html","title":"Plot forecaster residuals","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import plot_residuals\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.plot import plot_residuals In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\n# Plot data\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  # Plot data # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.plot(ax=ax); In\u00a0[3]: Copied! <pre># Train and backtest forecaster\n# ==============================================================================\nn_backtest = 36*3\ndata_train = data[:-n_backtest]\ndata_test  = data[-n_backtest:]\n\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(),\n                 lags      = 5 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.y,\n                        initial_train_size = len(data_train),\n                        steps              = 36,\n                        metric             = 'mean_squared_error',\n                        verbose            = True\n                     )\n \npredictions.head()\n</pre> # Train and backtest forecaster # ============================================================================== n_backtest = 36*3 data_train = data[:-n_backtest] data_test  = data[-n_backtest:]  forecaster = ForecasterAutoreg(                  regressor = Ridge(),                  lags      = 5               )  metric, predictions = backtesting_forecaster(                         forecaster         = forecaster,                         y                  = data.y,                         initial_train_size = len(data_train),                         steps              = 36,                         metric             = 'mean_squared_error',                         verbose            = True                      )   predictions.head() <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 96\nNumber of observations used for backtesting: 108\n    Number of folds: 3\n    Number of steps per fold: 36\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 1999-07-01 00:00:00 -- 2002-06-01 00:00:00  (n=36)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2002-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=36)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2005-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=36)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[3]: pred 1999-07-01 0.667651 1999-08-01 0.655759 1999-09-01 0.652177 1999-10-01 0.641377 1999-11-01 0.635245 <p>The <code>predictions_backtest</code> function can be used in two ways. Firstly, it can be utilized with pre-calculated residuals to perform a backtest on the forecast model. Secondly, it can be used with both predicted values and actual values of the series to evaluate the accuracy of the forecast.</p> In\u00a0[4]: Copied! <pre># Plot residuals\n# ======================================================================================\nresiduals = predictions['pred'] - data_test['y']\n_ = plot_residuals(residuals=residuals)\n</pre> # Plot residuals # ====================================================================================== residuals = predictions['pred'] - data_test['y'] _ = plot_residuals(residuals=residuals) In\u00a0[5]: Copied! <pre>_ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'])\n</pre> _ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred']) In\u00a0[6]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n_ = plot_residuals(residuals=residuals, fig=fig)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 5)) _ = plot_residuals(residuals=residuals, fig=fig) In\u00a0[7]: Copied! <pre>_ = plot_residuals(residuals=residuals, edgecolor=\"black\", linewidth=15, figsize=(8, 5))\n</pre> _ = plot_residuals(residuals=residuals, edgecolor=\"black\", linewidth=15, figsize=(8, 5)) <p>It is also possible to customize the internal aesthetics of the plots by changing <code>plt.rcParams</code>.</p> In\u00a0[8]: Copied! <pre>plt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor'  : '#212946',\n    'axes.facecolor'    : '#212946',\n    'savefig.facecolor' :'#212946',\n    'axes.grid'         : True,\n    'axes.grid.which'   : 'both',\n    'axes.spines.left'  : False,\n    'axes.spines.right' : False,\n    'axes.spines.top'   : False,\n    'axes.spines.bottom': False,\n    'grid.color'        : '#2A3459',\n    'grid.linewidth'    : '1',\n    'text.color'        : '0.9',\n    'axes.labelcolor'   : '0.9',\n    'xtick.color'       : '0.9',\n    'ytick.color'       : '0.9',\n    'font.size'         : 12\n}\nplt.rcParams.update(dark_style)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\n_ = plot_residuals(residuals=residuals, fig=fig)\n</pre> plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 dark_style = {     'figure.facecolor'  : '#212946',     'axes.facecolor'    : '#212946',     'savefig.facecolor' :'#212946',     'axes.grid'         : True,     'axes.grid.which'   : 'both',     'axes.spines.left'  : False,     'axes.spines.right' : False,     'axes.spines.top'   : False,     'axes.spines.bottom': False,     'grid.color'        : '#2A3459',     'grid.linewidth'    : '1',     'text.color'        : '0.9',     'axes.labelcolor'   : '0.9',     'xtick.color'       : '0.9',     'ytick.color'       : '0.9',     'font.size'         : 12 } plt.rcParams.update(dark_style)  fig, ax = plt.subplots(1, 1, figsize=(8, 5)) _ = plot_residuals(residuals=residuals, fig=fig) In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/plot-forecaster-residuals.html#plot-forecasting-residuals","title":"Plot forecasting residuals\u00b6","text":"<p>Analyzing the residuals (errors) of predictions is useful to understand the behavior of a forecaster. The function <code>skforecast.plot.plot_residuals</code> creates 3 plots:</p> <ul> <li><p>A time-ordered plot of residual values</p> </li> <li><p>A distribution plot that showcases the distribution of residuals</p> </li> <li><p>A plot showcasing the autocorrelation of residuals</p> </li> </ul> <p>By examining the residual values over time, you can determine whether there is a pattern in the errors made by the forecast model. The distribution plot helps you understand whether the residuals are normally distributed, and the autocorrelation plot helps you identify whether there are any dependencies or relationships between the residuals.</p>"},{"location":"user_guides/plot-forecaster-residuals.html#customizing-plots","title":"Customizing plots\u00b6","text":"<p>It is possible to customize the plot by by either passing a pre-existing matplotlib figure object or using additional keyword arguments that are passed to <code>matplotlib.pyplot.figure()</code>.</p>"},{"location":"user_guides/probabilistic-forecasting.html","title":"Probabilistic forecasting","text":"<p>  \u00a0 Warning </p> <p>As Rob J Hyndman explains in his blog, in real-world problems, almost all prediction intervals are too narrow. For example, nominal 95% intervals may only provide coverage between 71% and 87%. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. With forecasting models, there are at least four sources of uncertainty:</p> <ul> <li>The random error term</li> <li>The parameter estimates</li> <li>The choice of model for the historical data</li> <li>The continuation of the historical data generating process into the future</li> </ul> <p>When producing prediction intervals for time series models, generally only the first of these sources is taken into account. Therefore, it is advisable to use test data to validate the empirical coverage of the interval and not solely rely on the expected coverage.</p> <p>  \u00a0 Tip </p> <p>The following example shows how to use these methods with a <code>ForecasterAutoreg</code>, but note that this works for all other types of forecasters as well.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.plot import plot_prediction_distribution\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_pinball_loss\nfrom skforecast.exceptions import LongTrainingWarning\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.plot import plot_prediction_distribution from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from lightgbm import LGBMRegressor from sklearn.metrics import mean_pinball_loss from skforecast.exceptions import LongTrainingWarning  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/vic_elec.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation (aggregation at daily level)\n# ==============================================================================\ndata['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ')\ndata = data.set_index('Time')\ndata = data.asfreq('30min')\ndata = data.sort_index()\ndata = data.drop(columns='Date')\ndata = data.resample(rule='D', closed='left', label ='right')\\\n       .agg({'Demand': 'sum', 'Temperature': 'mean', 'Holiday': 'max'})\n\ndata.head(3)\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/vic_elec.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation (aggregation at daily level) # ============================================================================== data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ') data = data.set_index('Time') data = data.asfreq('30min') data = data.sort_index() data = data.drop(columns='Date') data = data.resample(rule='D', closed='left', label ='right')\\        .agg({'Demand': 'sum', 'Temperature': 'mean', 'Holiday': 'max'})  data.head(3) Out[2]: Demand Temperature Holiday Time 2012-01-01 82531.745918 21.047727 True 2012-01-02 227778.257304 26.578125 True 2012-01-03 275490.988882 31.751042 True In\u00a0[3]: Copied! <pre># Split data into train-validation-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\nend_validation = '2014-9-30 23:59:00'\ndata_train = data.loc[: end_train, :].copy()\ndata_val   = data.loc[end_train:end_validation, :].copy()\ndata_test  = data.loc[end_validation:, :].copy()\n\nprint(\n    f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"\n    f\"  (n={len(data_val)})\"\n)\nprint(\n    f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split data into train-validation-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' end_validation = '2014-9-30 23:59:00' data_train = data.loc[: end_train, :].copy() data_val   = data.loc[end_train:end_validation, :].copy() data_test  = data.loc[end_validation:, :].copy()  print(     f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"     f\"  (n={len(data_val)})\" ) print(     f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Train dates      : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00  (n=731)\nValidation dates : 2014-01-01 00:00:00 --- 2014-09-30 00:00:00  (n=273)\nTest dates       : 2014-10-01 00:00:00 --- 2014-12-30 00:00:00  (n=91)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 3))\ndata_train['Demand'].plot(label='train', ax=ax)\ndata_val['Demand'].plot(label='validation', ax=ax)\ndata_test['Demand'].plot(label='test', ax=ax)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylim(bottom=160_000)\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(9, 3)) data_train['Demand'].plot(label='train', ax=ax) data_val['Demand'].plot(label='validation', ax=ax) data_test['Demand'].plot(label='test', ax=ax) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylim(bottom=160_000) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand') ax.legend(); In\u00a0[5]: Copied! <pre># Create and train a ForecasterAutoreg\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(\n                                 learning_rate = 0.01,\n                                 max_depth     = 10,\n                                 n_estimators  = 500,\n                                 random_state  = 123\n                             ),\n                 lags      = 7\n             )\n\nforecaster.fit(y=data.loc[end_train:end_validation, 'Demand'])\nforecaster\n</pre> # Create and train a ForecasterAutoreg # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(                                  learning_rate = 0.01,                                  max_depth     = 10,                                  n_estimators  = 500,                                  random_state  = 123                              ),                  lags      = 7              )  forecaster.fit(y=data.loc[end_train:end_validation, 'Demand']) forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(learning_rate=0.01, max_depth=10, n_estimators=500,\n              random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 7 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-09-30 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.01, 'max_depth': 10, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:04:47 \nLast fit date: 2023-09-09 17:04:47 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[6]: Copied! <pre># Predict 10 different forecasting sequences of 7 steps each using bootstrapping\n# ==============================================================================\nboot_predictions = forecaster.predict_bootstrapping(steps=7, n_boot=10)\nboot_predictions\n</pre> # Predict 10 different forecasting sequences of 7 steps each using bootstrapping # ============================================================================== boot_predictions = forecaster.predict_bootstrapping(steps=7, n_boot=10) boot_predictions Out[6]: pred_boot_0 pred_boot_1 pred_boot_2 pred_boot_3 pred_boot_4 pred_boot_5 pred_boot_6 pred_boot_7 pred_boot_8 pred_boot_9 2014-10-01 208783.097577 204764.999003 201367.203420 212280.661192 204622.100582 202889.246579 210078.609982 210151.742392 200990.348257 203137.232359 2014-10-02 212340.130913 208320.030667 217221.147681 209996.152477 218464.949614 212545.697772 212061.596039 216579.376162 226902.326444 212847.017644 2014-10-03 221380.131363 223890.630246 224625.204840 207260.018970 206099.826214 288939.369743 220085.831843 229514.853352 230476.360893 224059.729506 2014-10-04 209943.773973 212946.528553 200027.296792 203592.696069 194120.730680 262225.653280 212516.896678 222042.271003 215236.033624 222487.580478 2014-10-05 193372.607408 177195.227257 194351.934752 206606.909536 202654.435499 297740.735825 192454.576530 199372.141645 208527.690318 197830.624380 2014-10-06 199626.367544 203338.054671 207145.334747 208993.695577 204766.177714 258050.416951 184903.916795 201473.420885 204790.425542 183312.907014 2014-10-07 201546.003371 212477.910879 209639.468951 204102.852012 225036.945159 257847.869040 197235.769115 196990.134684 200007.936217 207116.299541 <p>Using a ridge plot is a useful way to visualize the uncertainty of a forecasting model. This plot estimates a kernel density for each step by using the bootstrapped predictions.</p> In\u00a0[7]: Copied! <pre># Ridge plot of bootstrapping predictions\n# ==============================================================================\n_ = plot_prediction_distribution(boot_predictions, figsize=(7, 4))\n</pre> # Ridge plot of bootstrapping predictions # ============================================================================== _ = plot_prediction_distribution(boot_predictions, figsize=(7, 4)) In\u00a0[8]: Copied! <pre># Predict intervals for next 7 steps, quantiles 10th and 90th\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=7, interval=[10, 90], n_boot=1000)\npredictions\n</pre> # Predict intervals for next 7 steps, quantiles 10th and 90th # ============================================================================== predictions = forecaster.predict_interval(steps=7, interval=[10, 90], n_boot=1000) predictions Out[8]: pred lower_bound upper_bound 2014-10-01 205723.923923 195483.862851 214883.472075 2014-10-02 215167.163121 204160.738452 225623.750301 2014-10-03 225144.443075 212599.136391 237675.514362 2014-10-04 211406.440681 196863.841123 234950.293307 2014-10-05 194848.766987 185544.868289 225497.131653 2014-10-06 201901.819903 188334.486578 226720.998742 2014-10-07 208648.526025 191797.716429 231948.852094 In\u00a0[9]: Copied! <pre># Predict the parameters of a normal distribution for the next 7 steps\n# ==============================================================================\nfrom scipy.stats import norm\n\npredictions = forecaster.predict_dist(steps=7, distribution=norm, n_boot=1000)\npredictions\n</pre> # Predict the parameters of a normal distribution for the next 7 steps # ============================================================================== from scipy.stats import norm  predictions = forecaster.predict_dist(steps=7, distribution=norm, n_boot=1000) predictions Out[9]: loc scale 2014-10-01 205374.632063 12290.061732 2014-10-02 215791.488070 13607.935838 2014-10-03 225631.039103 14206.970100 2014-10-04 215210.987777 18980.776421 2014-10-05 202613.715687 19980.302998 2014-10-06 205391.493409 19779.732794 2014-10-07 208579.269999 18950.289467 In\u00a0[10]: Copied! <pre># Backtesting on test data \n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['Demand'],\n                          steps                 = 7,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train) + len(data_val),\n                          fixed_train_size      = True,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          interval              = [10, 90],\n                          n_boot                = 1000,\n                          random_state          = 123,\n                          verbose               = False,\n                          show_progress         = True\n                      )\npredictions.head(4)\n</pre> # Backtesting on test data  # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['Demand'],                           steps                 = 7,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train) + len(data_val),                           fixed_train_size      = True,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           interval              = [10, 90],                           n_boot                = 1000,                           random_state          = 123,                           verbose               = False,                           show_progress         = True                       ) predictions.head(4) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> Out[10]: pred lower_bound upper_bound 2014-10-01 223264.432587 214633.692784 231376.785240 2014-10-02 233596.210854 223051.862727 244752.538248 2014-10-03 242653.671597 226947.654906 258361.695192 2014-10-04 220842.603128 209967.438912 236404.408845 In\u00a0[11]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend();\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(); In\u00a0[12]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[predictions.index, 'Demand'] &gt;= predictions['lower_bound']) &amp; \\\n                      (data.loc[predictions.index, 'Demand'] &lt;= predictions['upper_bound']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval on test data: {100 * coverage}\")\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[predictions.index, 'Demand'] &gt;= predictions['lower_bound']) &amp; \\                       (data.loc[predictions.index, 'Demand'] &lt;= predictions['upper_bound']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval on test data: {100 * coverage}\") <pre>Coverage of the predicted interval on test data: 65.93406593406593\n</pre> <p>The coverage of the predicted interval is much lower than expected (80%).</p> In\u00a0[13]: Copied! <pre># Backtesting using validation data\n# ==============================================================================\nmetric, backtest_predictions = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data.loc[:end_validation, 'Demand'],\n                                   initial_train_size = len(data_train),\n                                   steps              = 7,\n                                   refit              = True,\n                                   metric             = 'mean_squared_error',\n                                   verbose            = False,\n                                   show_progress      = True\n                               )\n\n# Calculate residuals in validation data\n# ==============================================================================\nresiduals = (data_val['Demand'] - backtest_predictions['pred']).to_numpy()\nresiduals[:5]\n</pre> # Backtesting using validation data # ============================================================================== metric, backtest_predictions = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data.loc[:end_validation, 'Demand'],                                    initial_train_size = len(data_train),                                    steps              = 7,                                    refit              = True,                                    metric             = 'mean_squared_error',                                    verbose            = False,                                    show_progress      = True                                )  # Calculate residuals in validation data # ============================================================================== residuals = (data_val['Demand'] - backtest_predictions['pred']).to_numpy() residuals[:5] <pre>  0%|          | 0/39 [00:00&lt;?, ?it/s]</pre> Out[13]: <pre>array([-16084.02669559, -14168.47600418,  -4416.31618072, -15101.78356121,\n       -31133.70782794])</pre> <p>Once the residuals of the validation set are calculated, they can be stored inside the forecaster. This allows the forecaster to use the out-of-sample residuals for prediction interval calculation instead of the default in-sample residuals. By doing so, the estimated prediction intervals will be less optimistic and more representative of the performance of the model on new observations.</p> In\u00a0[14]: Copied! <pre># Set out-sample residuals\n# ==============================================================================\nforecaster.set_out_sample_residuals(residuals=residuals)\n</pre> # Set out-sample residuals # ============================================================================== forecaster.set_out_sample_residuals(residuals=residuals) <p>Once the new residuals have been added to the forecaster, use <code>in_sample_residuals = False</code> when calling the prediction methods.</p> In\u00a0[15]: Copied! <pre># Predict intervals using out-sample residuals\n# ==============================================================================\nforecaster.predict_interval(\n    steps               = 7, \n    interval            = [10, 90], \n    n_boot              = 1000,\n    in_sample_residuals = False\n)\n</pre> # Predict intervals using out-sample residuals # ============================================================================== forecaster.predict_interval(     steps               = 7,      interval            = [10, 90],      n_boot              = 1000,     in_sample_residuals = False ) Out[15]: pred lower_bound upper_bound 2014-10-01 205723.923923 186442.948366 219301.240056 2014-10-02 215167.163121 193309.527919 237664.343916 2014-10-03 225144.443075 202621.185338 248592.853063 2014-10-04 211406.440681 190686.942976 252492.488196 2014-10-05 194848.766987 182470.870866 246304.519999 2014-10-06 201901.819903 183721.031607 243358.781198 2014-10-07 208648.526025 183541.873516 244359.947649 <p>Also, a backtesting process can be applied to the test data with <code>in_sample_residuals = False</code> to use the out-of-sample residuals in the interval estimation.</p> In\u00a0[16]: Copied! <pre># Backtesting using test data and out-sample residuals\n# ==============================================================================\nmetric, predictions_2 = backtesting_forecaster(\n                            forecaster          = forecaster,\n                            y                   = data['Demand'],\n                            initial_train_size  = len(data_train) + len(data_val),\n                            steps               = 7,\n                            refit               = True,\n                            interval            = [10, 90],\n                            n_boot              = 1000,\n                            in_sample_residuals = False,\n                            random_state        = 123,\n                            metric              = 'mean_squared_error',\n                            verbose             = False,\n                            show_progress       = True\n                        )\n\npredictions_2.head(4)\n</pre> # Backtesting using test data and out-sample residuals # ============================================================================== metric, predictions_2 = backtesting_forecaster(                             forecaster          = forecaster,                             y                   = data['Demand'],                             initial_train_size  = len(data_train) + len(data_val),                             steps               = 7,                             refit               = True,                             interval            = [10, 90],                             n_boot              = 1000,                             in_sample_residuals = False,                             random_state        = 123,                             metric              = 'mean_squared_error',                             verbose             = False,                             show_progress       = True                         )  predictions_2.head(4) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> Out[16]: pred lower_bound upper_bound 2014-10-01 223264.432587 203983.457029 236841.748720 2014-10-02 233596.210854 207273.919217 255429.172682 2014-10-03 242653.671597 211668.258043 265757.229322 2014-10-04 220842.603128 195869.206779 245921.530106 In\u00a0[17]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 4))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'orange',\n    alpha = 0.3,\n    label = '80% interval in-sample residuals'\n)\nax.fill_between(\n    predictions_2.index,\n    predictions_2['lower_bound'],\n    predictions_2['upper_bound'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval out-sample residuals'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend(fontsize=10);\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 4)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'orange',     alpha = 0.3,     label = '80% interval in-sample residuals' ) ax.fill_between(     predictions_2.index,     predictions_2['lower_bound'],     predictions_2['upper_bound'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval out-sample residuals' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(fontsize=10); In\u00a0[18]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[predictions_2.index, 'Demand'] &gt;= predictions_2['lower_bound']) &amp; \\\n                      (data.loc[predictions_2.index, 'Demand'] &lt;= predictions_2['upper_bound']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval on test data: {100 * coverage}\")\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[predictions_2.index, 'Demand'] &gt;= predictions_2['lower_bound']) &amp; \\                       (data.loc[predictions_2.index, 'Demand'] &lt;= predictions_2['upper_bound']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval on test data: {100 * coverage}\") <pre>Coverage of the predicted interval on test data: 92.3076923076923\n</pre> <p>Using out-of-sample residuals, the interval coverage is much closer to that expected than when using in-sample residuals.</p> <p>  \u00a0 Note </p> <p>In the case of <code>ForecasterAutoregDirect</code>, <code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>, the residuals must be provided using a dictionary to indicate to which step or level (series) they belong to.</p> In\u00a0[19]: Copied! <pre># Create forecasters: one for each bound of the interval\n# ==============================================================================\n# The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence\n# interval (90% - 10% = 80%).\n\n# Forecaster for quantile 10%\nforecaster_q10 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.1,\n                                     learning_rate = 0.1,\n                                     max_depth     = 10,\n                                     n_estimators  = 100\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n                  \n# Forecaster for quantile 90%\nforecaster_q90 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.9,\n                                     learning_rate = 0.1,\n                                     max_depth     = 10,\n                                     n_estimators  = 100\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n\nforecaster_q10.fit(y=data['Demand'])\nforecaster_q90.fit(y=data['Demand'])\n</pre> # Create forecasters: one for each bound of the interval # ============================================================================== # The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence # interval (90% - 10% = 80%).  # Forecaster for quantile 10% forecaster_q10 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.1,                                      learning_rate = 0.1,                                      max_depth     = 10,                                      n_estimators  = 100                                  ),                      lags = 7,                      steps = 7                  )                    # Forecaster for quantile 90% forecaster_q90 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.9,                                      learning_rate = 0.1,                                      max_depth     = 10,                                      n_estimators  = 100                                  ),                      lags = 7,                      steps = 7                  )  forecaster_q10.fit(y=data['Demand']) forecaster_q90.fit(y=data['Demand']) In\u00a0[20]: Copied! <pre># Predict intervals for next 7 steps\n# ==============================================================================\npredictions_q10 = forecaster_q10.predict(steps=7)\npredictions_q90 = forecaster_q90.predict(steps=7)\npredictions = pd.DataFrame({\n                  'lower_bound': predictions_q10,\n                  'upper_bound': predictions_q90,\n              })\npredictions\n</pre> # Predict intervals for next 7 steps # ============================================================================== predictions_q10 = forecaster_q10.predict(steps=7) predictions_q90 = forecaster_q90.predict(steps=7) predictions = pd.DataFrame({                   'lower_bound': predictions_q10,                   'upper_bound': predictions_q90,               }) predictions  Out[20]: lower_bound upper_bound 2014-12-31 177607.329878 218885.714906 2015-01-01 178062.602517 242582.788349 2015-01-02 186213.019530 220677.491829 2015-01-03 184901.085939 204261.638256 2015-01-04 193237.899189 235310.200573 2015-01-05 196673.050873 284881.068713 2015-01-06 184148.733152 293018.478848 <p>When validating a quantile regression model, a custom metric must be provided depending on the quantile being estimated. These metrics will be used again when tuning the hyper-parameters of each model.</p> In\u00a0[21]: Copied! <pre># Loss function for each quantile (pinball_loss)\n# ==============================================================================\ndef mean_pinball_loss_q10(y_true, y_pred):\n\"\"\"\n    Pinball loss for quantile 10.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.1)\n\n\ndef mean_pinball_loss_q90(y_true, y_pred):\n\"\"\"\n    Pinball loss for quantile 90.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.9)\n</pre> # Loss function for each quantile (pinball_loss) # ============================================================================== def mean_pinball_loss_q10(y_true, y_pred):     \"\"\"     Pinball loss for quantile 10.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.1)   def mean_pinball_loss_q90(y_true, y_pred):     \"\"\"     Pinball loss for quantile 90.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.9) In\u00a0[22]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nwarnings.simplefilter('ignore', category=LongTrainingWarning)\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster         = forecaster_q10,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = True,\n                                  metric             = mean_pinball_loss_q10,\n                                  verbose            = False,\n                                  show_progress      = False\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster         = forecaster_q90,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = True,\n                                  metric             = mean_pinball_loss_q90,\n                                  verbose            = False,\n                                  show_progress      = False\n                              )\n\nprint(\"Quantile 10 metric\", metric_q10)\nprint(\"Quantile 90 metric\", metric_q90)\nprint(\"\")\ndisplay(predictions_q10.head(4))\nprint(\"\")\ndisplay(predictions_q90.head(4))\n</pre> # Backtesting on test data # ============================================================================== warnings.simplefilter('ignore', category=LongTrainingWarning) metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster         = forecaster_q10,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = True,                                   metric             = mean_pinball_loss_q10,                                   verbose            = False,                                   show_progress      = False                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster         = forecaster_q90,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = True,                                   metric             = mean_pinball_loss_q90,                                   verbose            = False,                                   show_progress      = False                               )  print(\"Quantile 10 metric\", metric_q10) print(\"Quantile 90 metric\", metric_q90) print(\"\") display(predictions_q10.head(4)) print(\"\") display(predictions_q90.head(4)) <pre>Quantile 10 metric 3408.28324086281\nQuantile 90 metric 2616.1339689106367\n\n</pre> pred 2014-10-01 209699.327721 2014-10-02 218271.081905 2014-10-03 221150.120318 2014-10-04 208837.320910 <pre>\n</pre> pred 2014-10-01 248739.650047 2014-10-02 241249.713258 2014-10-03 270670.830049 2014-10-04 220208.757206 <p>Predictions generated for each model are used to define the upper and lower limits of the interval.</p> In\u00a0[23]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\\n                      (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval: {100 * coverage}\")\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\                       (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval: {100 * coverage}\") <pre>Coverage of the predicted interval: 61.53846153846154\n</pre> <p>The coverage of the predicted interval is much lower than expected (80%).</p> In\u00a0[24]: Copied! <pre># Grid search of hyper-parameters and lags for each quantile forecaster\n# ==============================================================================\n# Regressor hyper-parameters\nparam_grid = {\n    'n_estimators': [100, 500],\n    'max_depth': [3, 5, 10],\n    'learning_rate': [0.01, 0.1]\n}\n\n# Lags used as predictors\nlags_grid = [7]\n\nresults_grid_q10 = grid_search_forecaster(\n                       forecaster         = forecaster_q10,\n                       y                  = data.loc[:end_validation, 'Demand'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 7,\n                       refit              = False,\n                       metric             = mean_pinball_loss_q10,\n                       initial_train_size = int(len(data_train)),\n                       return_best        = True,\n                       verbose            = False,\n                       show_progress      = True\n                   )\n\nresults_grid_q90 = grid_search_forecaster(\n                       forecaster         = forecaster_q90,\n                       y                  = data.loc[:end_validation, 'Demand'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 7,\n                       refit              = False,\n                       metric             = mean_pinball_loss_q90,\n                       initial_train_size = int(len(data_train)),\n                       return_best        = True,\n                       verbose            = False,\n                       show_progress      = True\n                   )\n</pre> # Grid search of hyper-parameters and lags for each quantile forecaster # ============================================================================== # Regressor hyper-parameters param_grid = {     'n_estimators': [100, 500],     'max_depth': [3, 5, 10],     'learning_rate': [0.01, 0.1] }  # Lags used as predictors lags_grid = [7]  results_grid_q10 = grid_search_forecaster(                        forecaster         = forecaster_q10,                        y                  = data.loc[:end_validation, 'Demand'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 7,                        refit              = False,                        metric             = mean_pinball_loss_q10,                        initial_train_size = int(len(data_train)),                        return_best        = True,                        verbose            = False,                        show_progress      = True                    )  results_grid_q90 = grid_search_forecaster(                        forecaster         = forecaster_q90,                        y                  = data.loc[:end_validation, 'Demand'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 7,                        refit              = False,                        metric             = mean_pinball_loss_q90,                        initial_train_size = int(len(data_train)),                        return_best        = True,                        verbose            = False,                        show_progress      = True                    ) <pre>Number of models compared: 12.\n</pre> <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7] \n  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n  Backtesting metric: 2713.0192469016706\n\nNumber of models compared: 12.\n</pre> <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7] \n  Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500}\n  Backtesting metric: 4094.3047516967745\n\n</pre> In\u00a0[25]: Copied! <pre># Grid search results\n# ==============================================================================\ndisplay(results_grid_q10.head(4))\nprint(\"\")\ndisplay(results_grid_q90.head(4))\n</pre> # Grid search results # ============================================================================== display(results_grid_q10.head(4)) print(\"\") display(results_grid_q90.head(4)) lags params mean_pinball_loss_q10 learning_rate max_depth n_estimators 1 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.01, 'max_depth': 3, 'n_est... 2713.019247 0.01 3.0 500.0 3 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.01, 'max_depth': 5, 'n_est... 2724.174361 0.01 5.0 500.0 5 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.01, 'max_depth': 10, 'n_es... 2794.381391 0.01 10.0 500.0 6 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.1, 'max_depth': 3, 'n_esti... 2831.048035 0.10 3.0 100.0 <pre>\n</pre> lags params mean_pinball_loss_q90 learning_rate max_depth n_estimators 5 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.01, 'max_depth': 10, 'n_es... 4094.304752 0.01 10.0 500.0 10 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.1, 'max_depth': 10, 'n_est... 4185.354890 0.10 10.0 100.0 3 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.01, 'max_depth': 5, 'n_est... 4198.557508 0.01 5.0 500.0 8 [1, 2, 3, 4, 5, 6, 7] {'learning_rate': 0.1, 'max_depth': 5, 'n_esti... 4202.854953 0.10 5.0 100.0 In\u00a0[26]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster         = forecaster_q10,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = False,\n                                  metric             = mean_pinball_loss_q10,\n                                  verbose            = False\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster         = forecaster_q90,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = False,\n                                  metric             = mean_pinball_loss_q90,\n                                  verbose            = False\n                              )\n</pre> # Backtesting on test data # ============================================================================== metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster         = forecaster_q10,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = False,                                   metric             = mean_pinball_loss_q10,                                   verbose            = False                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster         = forecaster_q90,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = False,                                   metric             = mean_pinball_loss_q90,                                   verbose            = False                               ) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[27]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    data.loc[end_validation:].index,\n    predictions_q10['pred'],\n    predictions_q90['pred'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend();\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     data.loc[end_validation:].index,     predictions_q10['pred'],     predictions_q90['pred'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(); In\u00a0[28]: Copied! <pre># Interval coverage\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\\n                      (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval: {100 * coverage}\")\n</pre> # Interval coverage # ============================================================================== inside_interval = np.where(                       (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\                       (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval: {100 * coverage}\") <pre>Coverage of the predicted interval: 75.82417582417582\n</pre> <p>After optimizing the hyper-parameters of each quantile forecaster, the coverage is closer to the expected one (80%).</p> In\u00a0[29]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/probabilistic-forecasting.html#probabilistic-forecasting-prediction-intervals-and-prediction-distribution","title":"Probabilistic forecasting: prediction intervals and prediction distribution\u00b6","text":"<p>When trying to anticipate future values, most forecasting models focus on predicting the most likely value, which is called point-forecasting. Although knowing the expected value of a time series is useful for most business cases, this type of forecasting does not provide any information about the model's confidence or the prediction's uncertainty.</p> <p>Probabilistic forecasting, as opposed to point-forecasting, is a family of techniques that enable the prediction of the expected distribution of the outcome, rather than a single future value. This type of forecasting provides much richer information, as it reports the range of probable values into which the true value may fall, allowing the estimation of prediction intervals.</p> <p>A prediction interval is the interval within which the true value of the response variable is expected to be found with a given probability. There are multiple ways to estimate prediction intervals, most of which require that the residuals (errors) of the model follow a normal distribution. However, when this assumption cannot be made, two commonly used alternatives are bootstrapping and quantile regression.</p> <p>To illustrate how skforecast enables the estimation of prediction intervals for multi-step forecasting, examples attempting to predict energy demand for a 7-day horizon are provided. Two strategies are showcased:</p> <ul> <li><p>Prediction intervals based on bootstrapped residuals.</p> </li> <li><p>Prediction intervals based on quantile regression.</p> </li> </ul>"},{"location":"user_guides/probabilistic-forecasting.html#predicting-distribution-and-intervals-using-bootstrapped-residuals","title":"Predicting distribution and intervals using bootstrapped residuals\u00b6","text":"<p>The error of a one-step-ahead forecast is defined as the difference between the actual value and the predicted value ($e_t = y_t - \\hat{y}_{t|t-1}$). By assuming that future errors will be similar to past errors, it is possible to simulate different predictions by taking samples from the collection of errors previously seen in the past (i.e., the residuals) and adding them to the predictions.</p> <p> Diagram bootstrapping prediction process. </p> <p>Repeatedly performing this process creates a collection of slightly different predictions, which represent the distribution of possible outcomes due to the expected variance in the forecasting process.</p> <p> Bootstrapping predictions. </p> <p>Using the outcome of the bootstrapping process, prediction intervals can be computed by calculating the $\u03b1/2$ and $1 \u2212 \u03b1/2$ percentiles at each forecasting horizon.</p> <p> </p> <p>Alternatively, it is also possible to fit a parametric distribution for each forecast horizon.</p> <p>One of the main advantages of this strategy is that it requires only a single model to estimate any interval. However, performing hundreds or thousands of bootstrapping iterations can be computationally expensive and may not always be feasible.</p>"},{"location":"user_guides/probabilistic-forecasting.html#probabilistic-prediction-methods","title":"Probabilistic prediction methods\u00b6","text":"<p>In skforecast, all forecasters have three different methods that allow for probabilistic forecasting:</p> <ul> <li><p><code>predict_bootstrapping</code>: this method generates multiple forecasting predictions through a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each bootstrapping iteration generates a different set of predictions. The output is a <code>pandas DataFrame</code> with one row for each predicted step and one column for each bootstrapping iteration.</p> </li> <li><p><code>predict_intervals</code>: this method estimates quantile predictions intervals using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_dist</code>: this method fits a parametric distribution using the values generated with <code>predict_bootstrapping</code>. Any of the continuous distributions available in scipy.stats can be used.</p> </li> </ul>"},{"location":"user_guides/probabilistic-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#predict-bootstraping","title":"Predict Bootstraping\u00b6","text":"<p>Once the Forecaster has been trained, the method <code>predict_bootstraping</code> can be use to simulate <code>n_boot</code> values of the forecasting distribution.</p>"},{"location":"user_guides/probabilistic-forecasting.html#predict-interval","title":"Predict Interval\u00b6","text":"<p>In most cases, the user is interested in a specific interval rather than the entire bootstrapping simulation matrix. To address this need, skforecast provides the <code>predict_interval</code> method. This method internally uses <code>predict_bootstrapping</code> to obtain the bootstrapping matrix and estimates the upper and lower quantiles for each step, thus providing the user with the desired prediction intervals.</p>"},{"location":"user_guides/probabilistic-forecasting.html#predict-distribution","title":"Predict Distribution\u00b6","text":"<p>The intervals estimated so far are distribution-free, which means that no assumptions are made about a particular distribution. The <code>predict_dist</code> method in skforecast allows fitting a parametric distribution to the bootstrapped prediction samples obtained with <code>predict_bootstrapping</code>. This is useful when there is reason to believe that the forecast errors follow a particular distribution, such as the normal distribution or the student's t-distribution. The <code>predict_dist</code> method allows the user to specify any continuous distribution from the scipy.stats module.</p>"},{"location":"user_guides/probabilistic-forecasting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>A backtesting process can be applied to evaluate the performance of the forecaster over a period of time, for example on test data, and calculate the actual coverage of the estimated range.</p>"},{"location":"user_guides/probabilistic-forecasting.html#prediction-with-out-sample-residuals","title":"Prediction with out-sample residuals\u00b6","text":"<p>By default, the training residuals are used to create the prediction intervals. However, this is not the most recommended option as the estimated interval tends to be too narrow. This is because the model's performance on training data is usually better than on new observations, leading to smaller residuals and, in turn, narrower prediction intervals.</p> <p>To overcome this issue, it is advisable to use other residuals, such as those obtained from a validation set. This can be achieved by storing the new residuals inside the forecaster using the <code>set_out_sample_residuals</code> method.</p>"},{"location":"user_guides/probabilistic-forecasting.html#prediction-intervals-using-quantile-regression-models","title":"Prediction intervals using quantile regression models\u00b6","text":"<p>As opposed to linear regression, which is intended to estimate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating the conditional quantiles of the response variable. For a continuous distribution function, the $\\alpha$-quantile $Q_{\\alpha}(x)$ is defined such that the probability of $Y$ being smaller than $Q_{\\alpha}(x)$ is, for a given $X=x$, equal to $\\alpha$. For example, 36% of the population values are lower than the quantile  $Q=0.36$. The most known quantile is the 50%-quantile, more commonly called the median.</p> <p>By combining the predictions of two quantile regressors, it is possible to build an interval. Each model estimates one of the limits of the interval. For example, the models obtained for $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval (90% - 10% = 80%).</p> <p>Several machine learning algorithms are capable of modeling quantiles. Some of them are:</p> <ul> <li><p>LightGBM</p> </li> <li><p>XGBoost</p> </li> <li><p>CatBoost</p> </li> <li><p>sklearn GradientBoostingRegressor</p> </li> <li><p>sklearn QuantileRegressor</p> </li> <li><p>skranger quantile RandomForest</p> </li> </ul> <p>Just as the squared-error loss function is used to train models that predict the mean value, a specific loss function is needed in order to train models that predict quantiles. The most common metric used for quantile regression is calles quantile loss  or pinball loss:</p> $$\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)$$<p>where $\\alpha$ is the target quantile, $y$ the real value and $\\hat{y}$ the quantile prediction.</p> <p>It can be seen that loss differs depending on the evaluated quantile. The higher the quantile, the more the loss function penalizes underestimates, and the less it penalizes overestimates. As with MSE and MAE, the goal is to minimize its values (the lower loss, the better).</p> <p>Two disadvantages of quantile regression, compared to the bootstrap approach to prediction intervals, are that each quantile needs its regressor and quantile regression is not available for all types of regression models. However, once the models are trained, the inference is much faster since no iterative process is needed.</p> <p>This type of prediction intervals can be easily estimated using ForecasterAutoregDirect and ForecasterAutoregMultiVariate models.</p>"},{"location":"user_guides/probabilistic-forecasting.html#forecasters","title":"Forecasters\u00b6","text":"<p>As in the previous example, an 80% prediction interval is estimated for 7 steps-ahead predictions but, this time, using quantile regression. In this example a LightGBM gradient boosting model is trained, however, the reader can use any other model by simply substituting the regressor definition.</p>"},{"location":"user_guides/probabilistic-forecasting.html#predictions","title":"Predictions\u00b6","text":"<p>Once the quantile forecasters are trained, they can be used to predict each of the bounds of the forecasting interval.</p>"},{"location":"user_guides/probabilistic-forecasting.html#backtesting","title":"Backtesting\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#grid-search","title":"Grid Search\u00b6","text":"<p>The hyperparameters of the model were hand-tuned and there is no reason that the same hyperparameters are suitable for the 10th and 90th percentiles regressors. Therefore, a grid search is carried out for each forecaster.</p>"},{"location":"user_guides/probabilistic-forecasting.html#backtesting-using-test-data","title":"Backtesting using test data\u00b6","text":"<p>Once the best hyperparameters have been found for each forecaster, a backtesting process is applied again using the test data.</p>"},{"location":"user_guides/quick-start-skforecast.html","title":"Quick start","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Data partition train-test\n# ==============================================================================\nend_train = '2005-06-01 23:59:00'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Data partition train-test # ============================================================================== end_train = '2005-06-01 23:59:00' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data.loc[:end_train])\nforecaster\n</pre> # Create and fit a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(y=data.loc[:end_train]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:08:47 \nLast fit date: 2023-09-09 17:08:48 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>  \u00a0 Tip </p> <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data.loc[end_train:]))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data.loc[end_train:])) predictions.head(3) Out[4]: <pre>2005-07-01    0.921840\n2005-08-01    0.954921\n2005-09-01    1.101716\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error on test data\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data.loc[end_train:],\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error on test data # ============================================================================== error_mse = mean_squared_error(                 y_true = data.loc[end_train:],                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.00429855684785846\n</pre> In\u00a0[7]: Copied! <pre># Backtesting\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster            = forecaster,\n                                   y                     = data,\n                                   steps                 = 10,\n                                   metric                = 'mean_squared_error',\n                                   initial_train_size    = len(data.loc[:end_train]),\n                                   fixed_train_size      = False,\n                                   gap                   = 0,\n                                   allow_incomplete_fold = True,\n                                   refit                 = True,\n                                   n_jobs                = 'auto',\n                                   verbose               = True,\n                                   show_progress         = True\n                               )\n\nprint(f\"Backtest error: {metric}\")\n</pre> # Backtesting # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster            = forecaster,                                    y                     = data,                                    steps                 = 10,                                    metric                = 'mean_squared_error',                                    initial_train_size    = len(data.loc[:end_train]),                                    fixed_train_size      = False,                                    gap                   = 0,                                    allow_incomplete_fold = True,                                    refit                 = True,                                    n_jobs                = 'auto',                                    verbose               = True,                                    show_progress         = True                                )  print(f\"Backtest error: {metric}\") <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 4\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 6 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=178)\n    Validation: 2006-05-01 00:00:00 -- 2007-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2007-02-01 00:00:00  (n=188)\n    Validation: 2007-03-01 00:00:00 -- 2007-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2007-12-01 00:00:00  (n=198)\n    Validation: 2008-01-01 00:00:00 -- 2008-06-01 00:00:00  (n=6)\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error: 0.004810878299401304\n</pre> In\u00a0[8]: Copied! <pre># Grid search hyperparameter and lags\n# ==============================================================================\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data,\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 10,\n                   refit              = False,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   n_jobs             = 'auto',\n                   verbose            = False,\n                   show_progress      = True\n               )\n</pre> # Grid search hyperparameter and lags # ============================================================================== # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data,                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 10,                    refit              = False,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    n_jobs             = 'auto',                    verbose            = False,                    show_progress      = True                ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 10, 'n_estimators': 100}\n  Backtesting metric: 0.025302666452954245\n\n</pre> In\u00a0[9]: Copied! <pre># Grid results\n# ==============================================================================\nresults_grid\n</pre> # Grid results # ============================================================================== results_grid Out[9]: lags params mean_squared_error max_depth n_estimators 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.025303 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.025797 15 100 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.026126 15 50 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.027727 5 100 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.028581 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.028700 10 50 17 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.036246 15 100 14 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.039460 10 50 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.040298 15 100 13 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.040301 5 100 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.041785 10 100 15 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.042480 10 100 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.044865 10 50 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.045543 5 100 12 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.046820 5 50 16 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.048899 15 50 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.051129 15 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.051601 5 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[10]: Copied! <pre>forecaster\n</pre> forecaster Out[10]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(max_depth=10, random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 10, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:08:47 \nLast fit date: 2023-09-09 17:08:54 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/quick-start-skforecast.html#quick-start-skforecast","title":"Quick start skforecast\u00b6","text":"<p>Welcome to a quick start guide to using skforecast! In this guide, we will provide you with a code example that demonstrates how to create, validate, and optimize a recursive multi-step forecaster, <code>ForecasterAutoreg</code>, using skforecast.</p> <p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>If you need more detailed documentation or guidance, you can visit the User Guides section.</p> <p>Without further ado, let's jump into the code example!</p>"},{"location":"user_guides/quick-start-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/quick-start-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/quick-start-skforecast.html#train-a-forecaster","title":"Train a forecaster\u00b6","text":"<p>Let's start by training a forecaster! For a more in-depth guide to using <code>ForecasterAutoreg</code>, visit the User guide ForecasterAutoreg.</p>"},{"location":"user_guides/quick-start-skforecast.html#prediction","title":"Prediction\u00b6","text":"<p>After training the forecaster, the <code>predict</code> method can be used to make predictions for the future $n$ steps.</p>"},{"location":"user_guides/quick-start-skforecast.html#backtesting-forecaster-validation","title":"Backtesting: forecaster validation\u00b6","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data. For more detailed documentation on backtesting, visit: User guide Backtesting forecaster.</p>"},{"location":"user_guides/quick-start-skforecast.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The Skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search. For more detailed documentation on Hyperparameter tuning, visit: Hyperparameter tuning and lags selection.</p>"},{"location":"user_guides/save-load-forecaster.html","title":"Save and load forecaster","text":"<p>  \u00a0 Tip </p> <p>Since version 0.7.0, when initializing the forecaster, a <code>forecaster_id</code> can be included to help identify the target of the forecaster.</p> <p>  \u00a0 Note </p> <p>Learn how to use forecaster models in production.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date']) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') In\u00a0[3]: Copied! <pre># Create and train forecaster\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = \"forecaster_001\"\n             )\n\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=3)\n</pre> # Create and train forecaster forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = \"forecaster_001\"              )  forecaster.fit(y=data['y']) forecaster.predict(steps=3) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Save model\nsave_forecaster(forecaster, file_name='forecaster_001.py', verbose=False)\n</pre> # Save model save_forecaster(forecaster, file_name='forecaster_001.py', verbose=False) In\u00a0[5]: Copied! <pre># Load model\nforecaster_loaded = load_forecaster('forecaster_001.py', verbose=True)\n</pre> # Load model forecaster_loaded = load_forecaster('forecaster_001.py', verbose=True) <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nDifferentiation order: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:09:24 \nLast fit date: 2023-09-09 17:09:25 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: forecaster_001 \n\n</pre> In\u00a0[6]: Copied! <pre># Predict\nforecaster_loaded.predict(steps=3)\n</pre> # Predict forecaster_loaded.predict(steps=3) Out[6]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[7]: Copied! <pre># Forecaster identifier\n# ==============================================================================\nforecaster.forecaster_id\n</pre> # Forecaster identifier # ============================================================================== forecaster.forecaster_id Out[7]: <pre>'forecaster_001'</pre> In\u00a0[8]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecasters","title":"Save and load forecasters\u00b6","text":"<p>Skforecast models can be easily saved and loaded from disk using the pickle or joblib library. To streamline this process, two convenient functions, <code>save_forecaster</code> and <code>load_forecaster</code>, are available. See below for a simple example.</p> <p>A <code>forecaster_id</code> has been included when initializing the forecaster, this may help to identify the target of the forecaster.</p>"},{"location":"user_guides/save-load-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecaster-model","title":"Save and load forecaster model\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html","title":"Shap-values","text":"<p>  \u00a0 Note </p> <p>See also: Feature importances</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport shap\nshap.initjs()\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import shap shap.initjs() from sklearn.ensemble import HistGradientBoostingRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg <pre>Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n</pre> In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index() data.head(3) Out[2]: y exog_1 exog_2 datetime 1992-04-01 0.379808 0.958792 1.166029 1992-05-01 0.361801 0.951993 1.117859 1992-06-01 0.410534 0.952955 1.067942 In\u00a0[3]: Copied! <pre># Create recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = 5\n             )\n\nforecaster.fit(\n    y    = data['y'],\n    exog = data[['exog_1', 'exog_2']]\n)\n</pre> # Create recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = 5              )  forecaster.fit(     y    = data['y'],     exog = data[['exog_1', 'exog_2']] ) In\u00a0[4]: Copied! <pre># Training matrices used by the forecaster to fit the internal regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y    = data['y'],\n                        exog = data[['exog_1', 'exog_2']]\n                    )\ndisplay(X_train.head(3))\ndisplay(y_train.head(3))\n</pre> # Training matrices used by the forecaster to fit the internal regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y    = data['y'],                         exog = data[['exog_1', 'exog_2']]                     ) display(X_train.head(3)) display(y_train.head(3)) lag_1 lag_2 lag_3 lag_4 lag_5 exog_1 exog_2 datetime 1992-09-01 0.475463 0.483389 0.410534 0.361801 0.379808 0.959610 1.153190 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.956205 1.194551 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.949715 1.231489 <pre>datetime\n1992-09-01    0.534761\n1992-10-01    0.568606\n1992-11-01    0.595223\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Create SHAP explainer\n# ==============================================================================\nexplainer = shap.TreeExplainer(forecaster.regressor)\nshap_values = explainer.shap_values(X_train)\n</pre> # Create SHAP explainer # ============================================================================== explainer = shap.TreeExplainer(forecaster.regressor) shap_values = explainer.shap_values(X_train) In\u00a0[6]: Copied! <pre>shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n</pre> shap.summary_plot(shap_values, X_train, plot_type=\"bar\") In\u00a0[7]: Copied! <pre>shap.summary_plot(shap_values, X_train)\n</pre> shap.summary_plot(shap_values, X_train) <pre>No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n</pre> <p>Visualize a single prediction</p> In\u00a0[8]: Copied! <pre># visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n</pre> # visualize the first prediction's explanation shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:]) Out[8]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>Visualize many predictions</p> In\u00a0[9]: Copied! <pre># visualize the first 200 training set predictions\nshap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:])\n</pre> # visualize the first 200 training set predictions shap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:]) Out[9]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  In\u00a0[10]: Copied! <pre>shap.dependence_plot(\"exog_2\", shap_values, X_train)\n</pre> shap.dependence_plot(\"exog_2\", shap_values, X_train) In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/shap-values-skforecast.html#shap-values-for-skforecast-models","title":"SHAP values for skforecast models\u00b6","text":"<p>SHAP (SHapley Additive exPlanations) values are a popular method for explaining machine learning models, as they help to understand how variables and values influence predictions visually and quantitatively.</p> <p>It is possible to generate SHAP-values explanations from Skforecast models with just two essential elements:</p> <ul> <li><p>The internal regressor of the forecaster.</p> </li> <li><p>The training matrices created from the time series and used to fit the forecaster.</p> </li> </ul> <p>By leveraging these two components, users can create insightful and interpretable explanations for their skforecast models. These explanations can be used to verify the reliability of the model, identify the most significant factors that contribute to model predictions, and gain a deeper understanding of the underlying relationship between the input variables and the target variable.</p>"},{"location":"user_guides/shap-values-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#training-matrices","title":"Training matrices\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-values","title":"Shap Values\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-explainer","title":"Shap explainer\u00b6","text":"<p>The python implementation of SHAP is built along the explainers. These explainers are appropriate only for certain types or classes of algorithms. For example, the TreeExplainer is used for tree-based models.</p>"},{"location":"user_guides/shap-values-skforecast.html#shap-summary-plot","title":"SHAP Summary Plot\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#explain-predictions","title":"Explain predictions\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-dependence-plots","title":"SHAP Dependence Plots\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html","title":"Skforecast in GPU","text":"<p>Traditionally, machine learning algorithms are executed on CPUs (Central Processing Units), which are general-purpose processors that are designed to handle a wide range of tasks. However, CPUs are not optimized for the highly parallelized matrix operations that are required by many machine learning algorithms, which can result in slow training times and limited scalability. GPUs, on the other hand, are designed specifically for parallel processing and can perform thousands of mathematical operations simultaneously, making them ideal for training and deploying large-scale machine learning models.</p> <p>Two popular machine learning libraries that have implemented GPU acceleration are XGBoost and LightGBM. These libraries are used for building gradient boosting models, which are a type of machine learning algorithm that is highly effective for a wide range of tasks, including forecasting. With GPU acceleration, these libraries can significantly reduce the training time required to build these models and improve their scalability.</p> <p>Despite the significant advantages offered by GPUs (specifically Nvidia GPUs) in accelerating machine learning computations, access to them is often limited due to high costs or other practical constraints. Fortunatelly, Google Colaboratory (Colab), a free Jupyter notebook environment, allows users to run Python code in the cloud, with access to powerful hardware resources such as GPUs. This makes it an excellent platform for experimenting with machine learning models, especially those that require intensive computations.</p> <p>The following sections demonstrate how to install and use XGBoost and LightGBM with GPU acceleration to create powerful forecasting models.</p> <p>  \u00a0 Warning </p> <p>The following code assumes that the user is executing it in Google Colab with an activated GPU runtime.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom xgboost import XGBRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport torch\nimport os\nimport sys\nimport psutil\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg import torch import os import sys import psutil In\u00a0[14]: Copied! <pre># Setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# GPU info\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n\n# CPU info\nprint(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n</pre> # Setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device)  # GPU info if device.type == 'cuda':     print(torch.cuda.get_device_name(0))     print('Memory Usage:')     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')     print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')  # CPU info print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\") <pre>Using device: cpu\nCPU RAM Free: 6.81 GB\n</pre> In\u00a0[16]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a XGBRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                regressor = XGBRegressor(\n                              n_estimators=5000,\n                              tree_method='gpu_hist',\n                              gpu_id=0\n                            ),\n                lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a XGBRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                 regressor = XGBRegressor(                               n_estimators=5000,                               tree_method='gpu_hist',                               gpu_id=0                             ),                 lags = 20              )  forecaster.fit(y=data) In\u00a0[\u00a0]: Copied! <pre>!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n</pre> !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm In\u00a0[\u00a0]: Copied! <pre>!git clone --recursive https://github.com/Microsoft/LightGBM\n</pre> !git clone --recursive https://github.com/Microsoft/LightGBM In\u00a0[\u00a0]: Copied! <pre>!apt-get install -y -qq libboost-all-dev\n</pre> !apt-get install -y -qq libboost-all-dev In\u00a0[\u00a0]: Copied! <pre>%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)\n</pre> %%bash cd LightGBM rm -r build mkdir build cd build cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc) In\u00a0[\u00a0]: Copied! <pre>!cd LightGBM/python-package/;python3 setup.py install --precompile\n</pre> !cd LightGBM/python-package/;python3 setup.py install --precompile In\u00a0[\u00a0]: Copied! <pre>!mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM\n</pre> !mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd !rm -r LightGBM <p>Once all the above installation has been executed, it is necessary to restart the runtime (kernel).</p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom lightgbm  import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from lightgbm  import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[\u00a0]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a LGBMRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),\n                 lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a LGBMRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),                  lags = 20              )  forecaster.fit(y=data) In\u00a0[\u00a0]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/skforecast-in-GPU.html#skforecast-in-gpu","title":"Skforecast in GPU\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#xgboost","title":"XGBoost\u00b6","text":"<p>When creating the model, only two arguments are need to indicate XGBoost to GPU, if it available: <code>tree_method='gpu_hist'</code> and <code>gpu_id=0</code>.</p>"},{"location":"user_guides/skforecast-in-GPU.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html","title":"Scikit-learn Transformers and Pipelines","text":"<p>  \u00a0 Note </p> <p>When using ForecasterAutoregMultiSeries or ForecasterAutoregMultiVariate the <code>transformer_series</code> argument replaces <code>transformer_y</code>. If it is a <code>transformer</code>, the same transformation will be applied to all series. If it is a <code>dict</code> a different transformation can be set for each series.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n           url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n# Add an extra categorical variable\ndata['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1))\ndata.head()\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(            url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') # Add an extra categorical variable data['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1)) data.head() Out[2]: y exog_1 exog_2 exog_3 date 1992-04-01 0.379808 0.958792 1.166029 A 1992-05-01 0.361801 0.951993 1.117859 A 1992-06-01 0.410534 0.952955 1.067942 A 1992-07-01 0.483389 0.958078 1.097376 A 1992-08-01 0.475463 0.956370 1.122199 A In\u00a0[3]: Copied! <pre># Create and fit forecaster that scales the input series\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster that scales the input series # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: StandardScaler() \nTransformer for exog: None \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:09:45 \nLast fit date: 2023-09-09 17:09:45 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Create and fit forecaster with same transformer for all exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = StandardScaler()\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster with same transformer for all exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = StandardScaler()              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: StandardScaler() \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:09:45 \nLast fit date: 2023-09-09 17:09:45 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>It is also possible to apply a different transformation to each exogenous variable making use of <code>ColumnTransformer</code>.</p> In\u00a0[5]: Copied! <pre># Create and fit forecaster with different transformer for each exog variable\n# ==============================================================================\ntransformer_exog = ColumnTransformer(\n                       [('scale_1', StandardScaler(), ['exog_1']),\n                        ('scale_2', StandardScaler(), ['exog_2']),\n                        ('onehot', OneHotEncoder(), ['exog_3']),\n                       ],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = transformer_exog\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']])\nforecaster\n</pre> # Create and fit forecaster with different transformer for each exog variable # ============================================================================== transformer_exog = ColumnTransformer(                        [('scale_1', StandardScaler(), ['exog_1']),                         ('scale_2', StandardScaler(), ['exog_2']),                         ('onehot', OneHotEncoder(), ['exog_3']),                        ],                        remainder = 'passthrough',                        verbose_feature_names_out = False                    )  forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = transformer_exog              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']]) forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('scale_1', StandardScaler(), ['exog_1']),\n                                ('scale_2', StandardScaler(), ['exog_2']),\n                                ('onehot', OneHotEncoder(), ['exog_3'])],\n                  verbose_feature_names_out=False) \nWindow size: 3 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2', 'exog_3'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:09:45 \nLast fit date: 2023-09-09 17:09:45 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>It is possible to verify if the data transformation has been applied correctly by examining the training matrices. The training matrices should reflect the data transformation that was specified using the <code>transformer_y</code> or <code>transformer_exog</code> arguments.</p> In\u00a0[6]: Copied! <pre>X_train, y_train = forecaster.create_train_X_y(\n                       y    = data['y'],\n                       exog = data[['exog_1', 'exog_2', 'exog_3']]\n                   )\n</pre> X_train, y_train = forecaster.create_train_X_y(                        y    = data['y'],                        exog = data[['exog_1', 'exog_2', 'exog_3']]                    ) In\u00a0[7]: Copied! <pre>X_train.head(4)\n</pre> X_train.head(4) Out[7]: lag_1 lag_2 lag_3 exog_1 exog_2 exog_3_A exog_3_B date 1992-07-01 0.410534 0.361801 0.379808 -2.119529 -2.135088 1.0 0.0 1992-08-01 0.483389 0.410534 0.361801 -2.131024 -1.996017 1.0 0.0 1992-09-01 0.475463 0.483389 0.410534 -2.109222 -1.822392 1.0 0.0 1992-10-01 0.534761 0.475463 0.483389 -2.132137 -1.590667 1.0 0.0 In\u00a0[8]: Copied! <pre>y_train.head(4)\n</pre> y_train.head(4) Out[8]: <pre>date\n1992-07-01    0.483389\n1992-08-01    0.475463\n1992-09-01    0.534761\n1992-10-01    0.568606\nFreq: MS, Name: y, dtype: float64</pre> <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation.</p> <p>Scikit-learn's FunctionTransformer can be used to incorporate custom transformers, such as a logarithmic transformation, in the forecaster object. To implement this, a user-defined transformation function can be created and then passed to the <code>FunctionTransformer</code>. Detailed information on how to use FunctionTransformer can be found in the scikit-learn documentation.</p> <p>  \u00a0 Warning </p> <p>For versions 1.1.0 &gt;= scikit-learn &lt;= 1.2.0 <code>sklearn.preprocessing.FunctionTransformer.inverse_transform</code> does not support DataFrames that are all numerical when <code>check_inverse=True</code>. It will raise an Exception which is fixed in scikit-learn 1.2.1.</p> <p>More info: https://scikit-learn.org/stable/whats_new/v1.2.html#version-1-2-1</p> In\u00a0[9]: Copied! <pre># Create custom transformer\n# =============================================================================\ndef log_transform(x):\n\"\"\" \n    Calculate log adding 1 to avoid calculation errors if x is very close to 0.\n    \"\"\"\n    return np.log(x+1)\n\ndef exp_transform(x):\n\"\"\"\n    Inverse of log_transform.\n    \"\"\"\n    return np.exp(x) - 1\n\ntransformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)\n\n# Create forecaster and train\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 lags          = 3,\n                 transformer_y = transformer_y\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create custom transformer # ============================================================================= def log_transform(x):     \"\"\"      Calculate log adding 1 to avoid calculation errors if x is very close to 0.     \"\"\"     return np.log(x+1)  def exp_transform(x):     \"\"\"     Inverse of log_transform.     \"\"\"     return np.exp(x) - 1  transformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)  # Create forecaster and train # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  lags          = 3,                  transformer_y = transformer_y              )  forecaster.fit(y=data['y']) <p>If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[10]: Copied! <pre>forecaster.predict(steps=4)\n</pre> forecaster.predict(steps=4) Out[10]: <pre>2008-07-01    0.776206\n2008-08-01    0.775471\n2008-09-01    0.777200\n2008-10-01    0.777853\nFreq: MS, Name: pred, dtype: float64</pre> <p>  \u00a0 Warning </p> <p>Starting from version 0.4.0, skforecast permits the usage of scikit-learn pipelines as regressors. It is important to note that ColumnTransformer cannot be included in the pipeline; thus, the same transformation will be applied to both the modeled series and all exogenous variables. However, if the preprocessing transformations only apply to specific columns, then they need to be applied separately using <code>transformer_y</code> and <code>transformer_exog</code>.</p> In\u00a0[11]: Copied! <pre>pipe = make_pipeline(StandardScaler(), Ridge())\npipe\n</pre> pipe = make_pipeline(StandardScaler(), Ridge()) pipe Out[11]: <pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre>StandardScaler<pre>StandardScaler()</pre>Ridge<pre>Ridge()</pre> In\u00a0[12]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags      = 10\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags      = 10              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[12]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nDifferentiation order: None \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-09-09 17:09:45 \nLast fit date: 2023-09-09 17:09:45 \nSkforecast version: 0.10.0 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>When performing a grid search over a scikit-learn pipeline, the model's name precedes the parameters' name.</p> In\u00a0[13]: Copied! <pre># Hyperparameter grid search using a scikit-learn pipeline\n# ==============================================================================\npipe = make_pipeline(StandardScaler(), Ridge())\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags = 10  # This value will be replaced in the grid search\n             )\n\n# Regressor's hyperparameters\nparam_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}\n\n# Lags used as predictors\nlags_grid = [5, 24, [1, 2, 3, 23, 24]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data['y'],\n                   exog               = data[['exog_1', 'exog_2']],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 5,\n                   metric             = 'mean_absolute_error',\n                   refit              = False,\n                   initial_train_size = len(data.loc[:'2000-04-01']),\n                   return_best        = True,\n                   verbose            = False,\n                   show_progress      = True\n               )\n\nresults_grid.head(4)\n</pre> # Hyperparameter grid search using a scikit-learn pipeline # ============================================================================== pipe = make_pipeline(StandardScaler(), Ridge()) forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags = 10  # This value will be replaced in the grid search              )  # Regressor's hyperparameters param_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}  # Lags used as predictors lags_grid = [5, 24, [1, 2, 3, 23, 24]]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data['y'],                    exog               = data[['exog_1', 'exog_2']],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 5,                    metric             = 'mean_absolute_error',                    refit              = False,                    initial_train_size = len(data.loc[:'2000-04-01']),                    return_best        = True,                    verbose            = False,                    show_progress      = True                )  results_grid.head(4) <pre>Number of models compared: 30.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'ridge__alpha': 0.001}\n  Backtesting metric: 6.845311709632476e-05\n\n</pre> Out[13]: lags params mean_absolute_error ridge__alpha 0 [1, 2, 3, 4, 5] {'ridge__alpha': 0.001} 0.000068 0.001000 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.001} 0.000188 0.001000 1 [1, 2, 3, 4, 5] {'ridge__alpha': 0.007742636826811269} 0.000526 0.007743 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.007742636826811269} 0.001413 0.007743 In\u00a0[14]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#scikit-learn-transformers-and-pipelines","title":"Scikit-learn transformers and pipelines\u00b6","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <ul> <li><p><code>transformer_y</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform. Scikit-learn ColumnTransformer is not allowed since they do not have the inverse_transform method.</p> </li> <li><p><code>transformer_exog</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. Scikit-learn ColumnTransformer can be used if the preprocessing transformations only apply to some specific columns or if different transformations are needed for different columns. For example, scale numeric features and one hot encode categorical ones.</p> </li> </ul> <p>Transformations are learned and applied before training the forecaster and are automatically used when calling <code>predict</code>. The output of <code>predict</code> is always on the same scale as the original series y.</p> <p>Although skforecast has allowed using scikit-learn pipelines as regressors since version 0.4.0, it is recommended to use <code>transformer_y</code> and <code>transformer_exog</code> instead.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-input-series","title":"Transforming input series\u00b6","text":"<p>The following example shows how include a transformer that scales the input series y.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-exogenous-variables","title":"Transforming exogenous variables\u00b6","text":"<p>The following example shows how to apply the same transformation (scaling) to all exogenous variables.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#custom-transformers","title":"Custom transformers\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#scikit-learn-pipelines","title":"Scikit-learn pipelines\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html","title":"Weighted time series forecasting","text":"<p>  \u00a0 Note </p> <p>The examples that follow demonstrate how a portion of the time series can be excluded from model training by assigning it a weight of zero. However, the use of weights extends beyond the inclusion or exclusion of observations and can also balance the degree of influence that each observation has on the forecasting model. For instance, an observation assigned a weight of 10 will have ten times more impact on the model training than an observation assigned a weight of 1.</p> <p>  \u00a0 Warning </p> <p>In most gradient boosting implementations, such as LightGBM, XGBoost, and CatBoost, samples with zero weight are typically excluded when calculating gradients and Hessians. However, these samples are still taken into account when constructing the feature histograms, which can result in a model that differs from one trained without zero-weighted samples. For more information on this issue, please refer to this GitHub issue.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid') from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/energy_production_shutdown.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/energy_production_shutdown.csv') data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: production date 2012-01-01 375.1 2012-01-02 474.5 2012-01-03 573.9 2012-01-04 539.5 2012-01-05 445.4 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Dates train : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00   (n=731)\nDates test  : 2014-01-01 00:00:00 --- 2014-12-30 00:00:00   (n=364)\n</pre> In\u00a0[4]: Copied! <pre># Time series plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train.production.plot(ax=ax, label='train', linewidth=1)\ndata_test.production.plot(ax=ax, label='test', linewidth=1)\nax.axvspan(\n    pd.to_datetime('2012-06'),\n    pd.to_datetime('2012-10'), \n    label=\"Shutdown\",\n    color=\"red\",\n    alpha=0.1\n)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Time series plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train.production.plot(ax=ax, label='train', linewidth=1) data_test.production.plot(ax=ax, label='test', linewidth=1) ax.axvspan(     pd.to_datetime('2012-06'),     pd.to_datetime('2012-10'),      label=\"Shutdown\",     color=\"red\",     alpha=0.1 ) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[5]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between 2012-06-01 and 2012-10-21.     \"\"\"     weights = np.where(                   (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),                    0,                    1               )      return weights <p>A <code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[6]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = Ridge(random_state=123),\n                 lags        = 21,\n                 weight_func = custom_weights\n             )\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = Ridge(random_state=123),                  lags        = 21,                  weight_func = custom_weights              ) <p>  \u00a0 Warning </p> <p>If the regressor used in the model's fitting method does not support <code>sample_weight</code> within its <code>fit</code> method, the <code>weight_func</code> argument will be ignored.</p> <p>The <code>source_code_weight_func</code> argument stores the source code of the <code>weight_func</code> added to the forecaster.</p> In\u00a0[7]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p>After creating the forecaster, a backtesting process is performed to simulate its behavior if the test set had been predicted in batches of 12 days.</p> In\u00a0[8]: Copied! <pre># Backtesting: predict batches of 12 days\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data.production,\n                                   initial_train_size = len(data.loc[:end_train]),\n                                   fixed_train_size   = False,\n                                   steps              = 12,\n                                   metric             = 'mean_absolute_error',\n                                   refit              = False,\n                                   verbose            = False,\n                                   show_progress      = True\n                               )\n\nprint(f\"Backtest error: {metric}\")\npredictions_backtest.head()\n</pre> # Backtesting: predict batches of 12 days # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data.production,                                    initial_train_size = len(data.loc[:end_train]),                                    fixed_train_size   = False,                                    steps              = 12,                                    metric             = 'mean_absolute_error',                                    refit              = False,                                    verbose            = False,                                    show_progress      = True                                )  print(f\"Backtest error: {metric}\") predictions_backtest.head() <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error: 26.82118940346796\n</pre> Out[8]: pred 2014-01-01 406.122211 2014-01-02 444.103631 2014-01-03 469.424876 2014-01-04 449.407001 2014-01-05 414.945674 In\u00a0[9]: Copied! <pre># Predictions plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test.production.plot(ax=ax, label='test', linewidth=1)\npredictions_backtest.plot(ax=ax, label='predictions', linewidth=1)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Predictions plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test.production.plot(ax=ax, label='test', linewidth=1) predictions_backtest.plot(ax=ax, label='predictions', linewidth=1) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[10]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/weighted-time-series-forecasting.html#weighted-time-series-forecasting","title":"Weighted time series forecasting\u00b6","text":"<p>In many real-world scenarios, historical data is available for forecasting, but not all of it is reliable. For example, IoT sensors capture raw data from the physical world, but they are often prone to failure, malfunction, and attrition due to harsh deployment environments, leading to unusual or erroneous readings. Similarly, factories may shut down for maintenance, repair, or overhaul, resulting in gaps in the data. The Covid-19 pandemic has also affected population behavior, impacting many time series such as production, sales, and transportation.</p> <p>The presence of unreliable or unrepresentative values in the data history poses a significant challenge, as it hinders model learning. However, most forecasting algorithms require complete time series data, making it impossible to remove these observations. An alternative solution is to reduce the weight of the affected observations during model training. This document demonstrates how skforecast makes it easy to implement this strategy with two examples.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html#data","title":"Data\u00b6","text":"<p>Power plants used for energy generation are complex installations that require a high level of maintenance. It is typical for power plants to require periodic shutdowns for maintenance, repair, or overhaul activities. The following data set simulates these events, where energy production experiences a decrease.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#exclude-part-of-the-time-series","title":"Exclude part of the time series\u00b6","text":"<p>Between 2012-06-01 and 2012-09-30, the factory underwent a shutdown. To reduce the impact of these dates on the model, a custom function is created. This function assigns a value of 0 to any index date that falls within the shutdown period or up to 21 days later (lags used by the model), and a value of 1 to all other dates. Observations assigned a weight of 0 have no influence on the model training.</p>"}]}