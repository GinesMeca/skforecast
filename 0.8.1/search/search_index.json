{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to skforecast","text":""},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>Skforecast is a Python library that eases using scikit-learn regressors as single and multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (LightGBM, XGBoost, CatBoost, ...).</p> <p>Why use skforecast?</p> <p>The fields of statistics and machine learning have developed many excellent regression algorithms that can be useful for forecasting, but applying them effectively to time series analysis can still be a challenge. To address this issue, the skforecast library provides a comprehensive set of tools for training, validation and prediction in a variety of scenarios commonly encountered when working with time series. The library is built using the widely used scikit-learn API, making it easy to integrate into existing workflows. With skforecast, users have access to a wide range of functionalities such as feature engineering, model selection, hyperparameter tuning and many others. This allows users to focus on the essential aspects of their projects and leave the intricacies of time series analysis to skforecast. In addition, skforecast is developed according to the following priorities:</p> <ul> <li>Fast and robust prototyping. </li> <li>Validation and backtesting methods to have a realistic assessment of model performance. </li> <li>Models must be deployed in production. </li> <li>Models must be interpretable. </li> </ul> <p>Share Your Thoughts with Us</p> <p>Thank you for choosing skforecast! We value your suggestions, bug reports and recommendations as they help us identify areas for improvement and ensure that skforecast meets the needs of the community. Please consider sharing your experiences, reporting bugs, making suggestions or even contributing to the codebase on GitHub. Together, let's make time series forecasting more accessible and accurate for everyone.</p> <p>Documentation</p> <p>https://skforecast.org** </p>"},{"location":"index.html#installation","title":"Installation","text":"<p>The default installation of skforecast only installs hard dependencies.</p> <pre><code>pip install skforecast\n</code></pre> <p>Specific version:</p> <pre><code>pip install skforecast==0.8.1\n</code></pre> <p>Latest (unstable):</p> <pre><code>pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master\n</code></pre> <p>Install the full version (all dependencies):</p> <pre><code>pip install skforecast[full]\n</code></pre> <p>Install optional dependencies:</p> <pre><code>pip install skforecast[sarimax]\n</code></pre> <pre><code>pip install skforecast[plotting]\n</code></pre>"},{"location":"index.html#dependencies","title":"Dependencies","text":"<ul> <li>Python &gt;= 3.8</li> </ul>"},{"location":"index.html#hard-dependencies","title":"Hard dependencies","text":"<ul> <li>numpy&gt;=1.20, &lt;1.25</li> <li>pandas&gt;=1.2, &lt;2.1</li> <li>tqdm&gt;=4.57.0, &lt;4.65</li> <li>scikit-learn&gt;=1.0, &lt;1.3</li> <li>optuna&gt;=2.10.0, &lt;3.2</li> <li>joblib&gt;=1.1.0, &lt;1.3.0</li> </ul>"},{"location":"index.html#optional-dependencies","title":"Optional dependencies","text":"<ul> <li>matplotlib&gt;=3.3, &lt;3.8</li> <li>seaborn&gt;=0.11, &lt;0.13</li> <li>statsmodels&gt;=0.12, &lt;0.14</li> <li>pmdarima&gt;=2.0, &lt;2.1</li> </ul>"},{"location":"index.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"index.html#features","title":"Features","text":"<ul> <li>Create Forecasters from any regressor that follows the scikit-learn API</li> <li>Include exogenous variables as predictors</li> <li>Include custom predictors (rolling mean, rolling variance ...)</li> <li>Multiple backtesting methods for model validation</li> <li>Grid search, random search and Bayesian search to find optimal lags (predictors) and best hyperparameters</li> <li>Prediction interval estimated by bootstrapping and quantile regression</li> <li>Include custom metrics for model validation and grid search</li> <li>Get predictor importance</li> <li>Forecaster in production</li> </ul>"},{"location":"index.html#examples-and-tutorials","title":"Examples and tutorials","text":""},{"location":"index.html#english","title":"English","text":"<p> Skforecast: time series forecasting with Python and Scikit-learn </p> <p> Forecasting electricity demand with Python </p> <p> Forecasting web traffic with machine learning and Python </p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost </p> <p> Bitcoin price prediction with Python</p> <p> Prediction intervals in forecasting models</p> <p> Multi-series forecasting</p> <p> Reducing the influence of Covid-19 on time series forecasting models</p> <p> Forecasting time series with missing values</p> <p> Intermittent demand forecasting</p>"},{"location":"index.html#espanol","title":"Espa\u00f1ol","text":"<p> Skforecast: forecasting series temporales con Python y Scikit-learn </p> <p> Forecasting de la demanda el\u00e9ctrica </p> <p> Forecasting de las visitas a una p\u00e1gina web </p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost </p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p> <p> Intervalos de predicci\u00f3n en modelos de forecasting</p> <p> Multi-series forecasting</p> <p> Predicci\u00f3n demanda intermitente</p>"},{"location":"index.html#how-to-contribute","title":"How to contribute","text":"<p>For more information on how to contribute to skforecast, see our Contribution Guide.</p>"},{"location":"index.html#donating","title":"Donating","text":"<p>If you found skforecast useful, you can support us with a donation. Your contribution will help to continue developing and improving this project. Many thanks!  </p> <p></p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use this software, please cite it using the following metadata.</p> <p>APA: <pre><code>Amat Rodrigo, J., &amp; Escobar Ortiz, J. skforecast (Version 0.8.1) [Computer software]\n</code></pre></p> <p>BibTeX: <pre><code>@software{skforecast,\nauthor = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},\nlicense = {BSD 3-Clause License},\nmonth = {5},\ntitle = {{skforecast}},\nversion = {0.8.1},\nyear = {2023}\n}\n</code></pre></p> <p>View the citation file.</p>"},{"location":"index.html#license","title":"License","text":"<p>BSD 3-Clause License</p>"},{"location":"api/ForecasterAutoreg.html","title":"<code>ForecasterAutoreg</code>","text":""},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg","title":"<code>ForecasterAutoreg(regressor, lags, transformer_y=None, transformer_exog=None, weight_func=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_y</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Union[int, np.ndarray, list],\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.source_code_weight_func = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.X_train_col_names       = None\n    self.in_sample_residuals     = None\n    self.out_sample_residuals    = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"       \n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series ({len(y)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag: -lag]\n\n    y_data = y[self.max_lag:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors). Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag, )</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f'`exog` must have same number of samples as `y`. '\n                 f'length `exog`: ({len(exog)}), length `y`: ({len(y)})')\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_train, y_train = self._create_lags(y=y_values)\n    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag: ]\n              )\n\n    if exog is not None:\n        # The first `self.max_lag` positions have to be removed from exog\n        # since they are not in X_train.\n        exog_to_train = exog.iloc[self.max_lag:, ]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = y_index[self.max_lag: ],\n                  name  = 'y'\n              )\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight,\n                           **self.fit_kwargs)\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = (y_train - self.regressor.predict(X_train)).to_numpy()\n\n        if len(residuals) &gt; 1000:\n            # Only up to 1000 residuals are stored\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(\n                            a       = residuals, \n                            size    = 1000, \n                            replace = False\n                        )\n\n        self.in_sample_residuals = residuals\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated.\n    self.last_window = y.iloc[-self.max_lag:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg._recursive_predict","title":"<code>_recursive_predict(steps, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = last_window[-self.lags].reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    ) \n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    predictions = self._recursive_predict(\n                      steps       = steps,\n                      last_window = copy(last_window_values),\n                      exog        = copy(exog_values)\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = expand_index(\n                                  index = last_window_index,\n                                  steps = steps\n                              ),\n                      name = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if not in_sample_residuals and self.out_sample_residuals is None:\n        raise ValueError(\n            (\"`forecaster.out_sample_residuals` is `None`. Use \"\n             \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n             \"before `predict_interval()`, `predict_bootstrapping()` or \"\n             \"`predict_dist()`.\")\n        )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           dtype      = float\n                       )\n    rng = np.random.default_rng(seed=random_state)\n    seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n    if in_sample_residuals:\n        residuals = self.in_sample_residuals\n    else:\n        residuals = self.out_sample_residuals\n\n    for i in range(n_boot):\n        # In each bootstraping iteration the initial last_window and exog \n        # need to be restored.\n        last_window_boot = last_window_values.copy()\n        exog_boot = exog_values.copy() if exog is not None else None\n\n        rng = np.random.default_rng(seed=seeds[i])\n        sample_residuals = rng.choice(\n                               a       = residuals,\n                               size    = steps,\n                               replace = True\n                           )\n\n        for step in range(steps):\n\n            prediction = self._recursive_predict(\n                             steps       = 1,\n                             last_window = last_window_boot,\n                             exog        = exog_boot \n                         )\n\n            prediction_with_residual  = prediction + sample_residuals[step]\n            boot_predictions[step, i] = prediction_with_residual\n\n            last_window_boot = np.append(\n                                   last_window_boot[1:],\n                                   prediction_with_residual\n                               )\n\n            if exog is not None:\n                exog_boot = exog_boot[1:]\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = expand_index(last_window_index, steps=steps),\n                           columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                       )\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which each prediction is used as a predictor for the next step, and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which each prediction is used as a predictor\n    for the next step, and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.predict_dist","title":"<code>predict_dist(steps, distribution, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).  \n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"\n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = max(self.lags)\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>numpy ndarray</code> <p>Values of residuals. If len(residuals) &gt; 1000, only a random sample of 1000 values are stored.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: np.ndarray, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : numpy ndarray\n        Values of residuals. If len(residuals) &gt; 1000, only a random sample\n        of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, np.ndarray):\n        raise TypeError(\n            f\"`residuals` argument must be `numpy ndarray`. Got {type(residuals)}.\"\n        )\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new residuals \"\n             f\"are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure that the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n\n        residuals = transform_series(\n                        series            = pd.Series(residuals, name='residuals'),\n                        transformer       = self.transformer_y,\n                        fit               = False,\n                        inverse_transform = False\n                    ).to_numpy()\n\n    if len(residuals) &gt; 1000:\n        rng = np.random.default_rng(seed=random_state)\n        residuals = rng.choice(a=residuals, size=1000, replace=False)\n\n    if append and self.out_sample_residuals is not None:\n        free_space = max(0, 1000 - len(self.out_sample_residuals))\n        if len(residuals) &lt; free_space:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals\n                        ))\n        else:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals[:free_space]\n                        ))\n\n    self.out_sample_residuals = residuals\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoreg/ForecasterAutoreg.py</code> <pre><code>def get_feature_importance(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances(). \"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances()\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html","title":"<code>ForecasterAutoregCustom</code>","text":""},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom","title":"<code>ForecasterAutoregCustom(regressor, fun_predictors, window_size, name_predictors=None, transformer_y=None, transformer_exog=None, weight_func=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns  another numpy ndarray with the predictors.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> required <code>name_predictors</code> <code>list, default</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the position the predictor has in the returned array of <code>fun_predictors</code>. <code>None</code> <code>transformer_y</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor compatible with the scikit-learn API</code> <p>An instance of a regressor compatible with the scikit-learn API.</p> <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. New in version 0.7.0</p> <code>source_code_fun_predictors</code> <code>str</code> <p>Source code of the custom function used to create the predictors. New in version 0.7.0</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the position the predictor has in the returned array of <code>fun_predictors</code>. <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int, default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def __init__(\n    self, \n    regressor: object, \n    fun_predictors: Callable, \n    window_size: int,\n    name_predictors: Optional[list]=None,\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor                  = regressor\n    self.fun_predictors             = fun_predictors\n    self.source_code_fun_predictors = None\n    self.window_size                = window_size\n    self.name_predictors            = name_predictors\n    self.transformer_y              = transformer_y\n    self.transformer_exog           = transformer_exog\n    self.weight_func                = weight_func\n    self.source_code_weight_func    = None\n    self.last_window                = None\n    self.index_type                 = None\n    self.index_freq                 = None\n    self.training_range             = None\n    self.included_exog              = False\n    self.exog_type                  = None\n    self.exog_dtypes                = None\n    self.exog_col_names             = None\n    self.X_train_col_names          = None\n    self.in_sample_residuals        = None\n    self.out_sample_residuals       = None\n    self.fitted                     = False\n    self.creation_date              = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                   = None\n    self.skforcast_version          = skforecast.__version__\n    self.python_version             = sys.version.split(\" \")[0]\n    self.forecaster_id              = forecaster_id\n\n    if not isinstance(window_size, int):\n        raise TypeError(\n            f\"Argument `window_size` must be an int. Got {type(window_size)}.\"\n        )\n\n    if not isinstance(fun_predictors, Callable):\n        raise TypeError(\n            f\"Argument `fun_predictors` must be a Callable. Got {type(fun_predictors)}.\"\n        )\n\n    self.source_code_fun_predictors = inspect.getsource(fun_predictors)\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Pandas DataFrame with the training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n\n    \"\"\"\n\n    if len(y) &lt; self.window_size + 1:\n        raise ValueError(\n            (f\"`y` must have as many values as the windows_size needed by \"\n             f\"{self.fun_predictors.__name__}. For this Forecaster the \"\n             f\"minimum length is {self.window_size + 1}\")\n        )\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                f'`exog` must have same number of samples as `y`. '\n                f'length `exog`: ({len(exog)}), length `y`: ({len(y)})'\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                        series            = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                        df                = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")      \n            )\n\n    X_train  = []\n    y_train  = []\n\n    for i in range(len(y) - self.window_size):\n\n        train_index = np.arange(i, self.window_size + i)\n        test_index  = self.window_size + i\n\n        X_train.append(self.fun_predictors(y=y_values[train_index]))\n        y_train.append(y_values[test_index])\n\n    X_train = np.vstack(X_train)\n    y_train = np.array(y_train)\n\n    if self.name_predictors is None:\n        X_train_col_names = [f\"custom_predictor_{i}\" for i in range(X_train.shape[1])]\n    else:\n        if len(self.name_predictors) != X_train.shape[1]:\n            raise ValueError(\n                (\"The length of provided predictors names (`name_predictors`) do not \"\n                 \"match the number of columns created by `fun_predictors()`.\")\n            )\n        X_train_col_names = self.name_predictors.copy()\n\n    if np.isnan(X_train).any():\n        raise ValueError(\n            \"`fun_predictors()` is returning `NaN` values.\"\n        )\n\n    expected = self.fun_predictors(y_values[:-1])\n    observed = X_train[-1, :]\n\n    if expected.shape != observed.shape or not (expected == observed).all():\n        raise ValueError(\n            (f\"The `window_size` argument ({self.window_size}), declared when \"\n             f\"initializing the forecaster, does not correspond to the window \"\n             f\"used by `fun_predictors()`.\")\n        )\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.window_size: ]\n              )\n\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train.\n        exog_to_train = exog.iloc[self.window_size:, ]\n        check_exog_dtypes(exog_to_train)\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = y_index[self.window_size: ],\n                  name  = 'y'\n              )\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight,\n                           **self.fit_kwargs)\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = (y_train - self.regressor.predict(X_train)).to_numpy()\n\n        if len(residuals) &gt; 1000:\n            # Only up to 1000 residuals are stored\n            rng = np.random.default_rng(seed=123)\n            residuals = rng.choice(\n                            a       = residuals, \n                            size    = 1000, \n                            replace = False\n                        )                            \n\n        self.in_sample_residuals = residuals\n\n    # The last time window of training data is stored so that predictors in\n    # the first iteration of `predict()` can be calculated.\n    self.last_window = y.iloc[-self.window_size:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom._recursive_predict","title":"<code>_recursive_predict(steps, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = self.fun_predictors(y=last_window).reshape(1, -1)\n        if np.isnan(X).any():\n            raise ValueError(\n                \"`fun_predictors()` is returning `NaN` values.\"\n            )\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict","title":"<code>predict(steps, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    predictions = self._recursive_predict(\n                      steps       = steps,\n                      last_window = copy(last_window_values),\n                      exog        = copy(exog_values)\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = expand_index(\n                                  index = last_window_index,\n                                  steps = steps\n                              ),\n                      name = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if not in_sample_residuals and self.out_sample_residuals is None:\n        raise ValueError(\n            ('`forecaster.out_sample_residuals` is `None`. Use '\n             '`in_sample_residuals=True` or method `set_out_sample_residuals()` '\n             'before `predict_interval()`, `predict_bootstrapping()` or '\n             '`predict_dist()`.')\n        )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           dtype      = float\n                       )\n    rng = np.random.default_rng(seed=random_state)\n    seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n    if in_sample_residuals:\n        residuals = self.in_sample_residuals\n    else:\n        residuals = self.out_sample_residuals\n\n    for i in range(n_boot):\n        # In each bootstraping iteration the initial last_window and exog \n        # need to be restored.\n        last_window_boot = last_window_values.copy()\n        exog_boot = exog_values.copy() if exog is not None else None\n\n        rng = np.random.default_rng(seed=seeds[i])\n        sample_residuals = rng.choice(\n                               a       = residuals,\n                               size    = steps,\n                               replace = True\n                           )\n\n        for step in range(steps):\n\n            prediction = self._recursive_predict(\n                             steps       = 1,\n                             last_window = last_window_boot,\n                             exog        = exog_boot \n                         )\n\n            prediction_with_residual  = prediction + sample_residuals[step]\n            boot_predictions[step, i] = prediction_with_residual\n\n            last_window_boot = np.append(\n                                   last_window_boot[1:],\n                                   prediction_with_residual\n                               )\n\n            if exog is not None:\n                exog_boot = exog_boot[1:]\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = expand_index(last_window_index, steps=steps),\n                           columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                       )\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval","title":"<code>predict_interval(steps, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which each prediction is used as a predictor for the next step, and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which each prediction is used as a predictor\n    for the next step, and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_dist","title":"<code>predict_dist(steps, distribution, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>numpy ndarray</code> <p>Values of residuals. If len(residuals) &gt; 1000, only a random sample of 1000 values are stored.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: np.ndarray, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : numpy ndarray\n        Values of residuals. If len(residuals) &gt; 1000, only a random sample\n        of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, np.ndarray):\n        raise TypeError(\n            f\"`residuals` argument must be `numpy ndarray`. Got {type(residuals)}.\"\n        )\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new residuals \"\n             f\"are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure that the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n\n        residuals = transform_series(\n                        series            = pd.Series(residuals, name='residuals'),\n                        transformer       = self.transformer_y,\n                        fit               = False,\n                        inverse_transform = False\n                    ).to_numpy()\n\n    if len(residuals) &gt; 1000:\n        rng = np.random.default_rng(seed=random_state)\n        residuals = rng.choice(a=residuals, size=1000, replace=False)\n\n    if append and self.out_sample_residuals is not None:\n        free_space = max(0, 1000 - len(self.out_sample_residuals))\n        if len(residuals) &lt; free_space:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals\n                        ))\n        else:\n            residuals = np.hstack((\n                            self.out_sample_residuals,\n                            residuals[:free_space]\n                        ))\n\n    self.out_sample_residuals = residuals\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py</code> <pre><code>def get_feature_importance(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances(). \"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances()\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html","title":"<code>ForecasterAutoregDirect</code>","text":""},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect","title":"<code>ForecasterAutoregDirect(regressor, steps, lags, transformer_y=None, transformer_exog=None, weight_func=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive direct multi-step forecaster. A separate model is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_y</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_y</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster.</p> <code>fit_kwargs</code> <code>dict, default `None`</code> <p>Additional parameters passed to the <code>fit</code> method of the regressor.</p>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect--notes","title":"Notes","text":"<p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def __init__(\n    self, \n    regressor: object,\n    steps: int,\n    lags: Union[int, np.ndarray, list],\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None,\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.steps                   = steps\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.source_code_weight_func = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.X_train_col_names       = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals = None\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"       \n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - (self.steps - 1) # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series minus the number of steps ({len(y)-(self.steps-1)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n\n    y_data = np.full(shape=(n_splits, self.steps), fill_value=np.nan, dtype=float)\n    for step in range(self.steps):\n        y_data[:, step] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_train_X_y","title":"<code>create_train_X_y(y, exog=None)</code>","text":"<p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>y_train</code> <code>pandas DataFrame</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step. Shape: (len(y) - self.max_lag, )</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step.\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    y_train : pandas DataFrame\n        Values (target) of the time series related to each row of `X_train` \n        for each step.\n        Shape: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    if len(y) &lt; self.max_lag + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `y` for training this forecaster is \"\n             f\"{self.max_lag + self.steps}. Got {len(y)}. Reduce the \"\n             f\"number of predicted steps, {self.steps}, or the maximum \"\n             f\"lag, {self.max_lag}, if no more data is available.\")\n        )\n\n    check_y(y=y)\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n    y_values, y_index = preprocess_y(y=y)\n\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.included_exog = True\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(y_index)] == y_index).all():\n            raise ValueError(\n                (\"Different index for `y` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")      \n            )\n\n    X_train, y_train = self._create_lags(y=y_values)\n    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag + (self.steps -1): ]\n              )\n\n    if exog is not None:\n        # Transform exog to match direct format\n        # The first `self.max_lag` positions have to be removed from X_exog\n        # since they are not in X_lags.\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        ).iloc[-X_train.shape[0]:, :]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train_col_names = [f\"y_step_{i+1}\" for i in range(self.steps)]\n    y_train = pd.DataFrame(\n                  data    = y_train,\n                  index   = y_index[self.max_lag + (self.steps -1): ],\n                  columns = y_train_col_names,\n              )\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y()</code>. If  <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the  column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <code>y_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, second return.</p> required <code>remove_suffix</code> <code>bool, default</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    remove_suffix: bool=False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y()`. If \n    `remove_suffix=True` the suffix \"_step_i\" will be removed from the \n    column names. \n\n    Parameters\n    ----------\n    step : int\n        Step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n    y_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    # Matrices X_train and y_train start at index 0.\n    y_train_step = y_train.iloc[:, step - 1] \n\n    if not self.included_exog:\n        X_train_step = X_train\n    else:\n        idx_columns_lags = np.arange(len(self.lags))\n        n_exog = (len(self.X_train_col_names) - len(self.lags)) / self.steps\n        idx_columns_exog = (\n            np.arange((step-1)*n_exog, (step)*n_exog) + idx_columns_lags[-1] + 1\n        )\n        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    if remove_suffix:\n        X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n                                for col_name in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return  X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the methods <code>create_train_X_y</code> and  <code>filter_train_X_y_for_step</code>, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe generated with the methods `create_train_X_y` and \n        `filter_train_X_y_for_step`, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.fit","title":"<code>fit(y, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = {step: None for step in range(1, self.steps + 1)}\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n\n    # Train one regressor for each step \n    for step in range(1, self.steps + 1): \n        # self.regressors_ and self.filter_train_X_y_for_step expect\n        # first step to start at value 1\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            self.regressors_[step].fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressors_[step].fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n\n            residuals = (\n                (y_train_step - self.regressors_[step].predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n\n            self.in_sample_residuals[step] = residuals\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    self.last_window = y.iloc[-self.max_lag:].copy()\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict","title":"<code>predict(steps=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n\n    if last_window is None:\n        last_window = copy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = self.steps,\n        levels           = None,\n        series_col_names = None\n    ) \n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n    else:\n        exog_values = None\n\n    last_window = transform_series(\n                      series            = last_window,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    last_window_values, last_window_index = preprocess_last_window(\n                                                last_window = last_window\n                                            )\n\n    X_lags = last_window_values[-self.lags].reshape(1, -1)\n\n    if exog is None:\n        Xs = [X_lags] * len(steps)\n    else:\n        n_exog = exog.shape[1] if isinstance(exog, pd.DataFrame) else 1\n        Xs = [\n            np.hstack([X_lags, exog_values[(step-1)*n_exog:(step)*n_exog].reshape(1, -1)])\n            for step in steps\n        ]\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.simplefilter(\"ignore\")\n        predictions = [\n            regressor.predict(X)[0] for regressor, X in zip(regressors, Xs)\n        ]\n\n    idx = expand_index(index=last_window_index, steps=max(steps))\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = idx[np.array(steps)-1],\n                      name  = 'pred'\n                  )\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    if in_sample_residuals:\n        if not set(steps).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for steps: \"\n                 f\"{set(steps) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n                 \"before `predict_interval()`, `predict_bootstrapping()` or \"\n                 \"`predict_dist()`.\")\n            )\n        else:\n            if not set(steps).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for steps: \"\n                     f\"{set(steps) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for step in steps:\n        if residuals[step] is None:\n            raise ValueError(\n                (f\"forecaster residuals for step {step} are `None`. \"\n                 f\"Check {check_residuals}.\")\n            )\n        elif (residuals[step] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for step {step} contains `None` values. \"\n                 f\"Check {check_residuals}.\")\n            )\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog \n                  )\n\n    # Predictions must be in the transformed scale before adding residuals\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    boot_predictions = pd.concat([predictions] * n_boot, axis=1)\n    boot_predictions.columns= [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    rng = np.random.default_rng(seed=random_state)\n    for i, step in enumerate(steps):\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions.iloc[i, :] = boot_predictions.iloc[i, :] + sample_residuals\n\n    if self.transformer_y:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_y,\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Bootstrapping based prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Bootstrapping based prediction intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.Series]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = max(self.lags)\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {step: None \n                                     for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            f\"\"\"\n            Only residuals of models (steps) \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    if not transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_y}. Ensure that the new \"\n             f\"residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_y is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used \"\n             f\"when training the forecaster ({self.transformer_y}). Ensure the \"\n             f\"new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_series(\n                                 series            = pd.Series(value, name='residuals'),\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             ).to_numpy()\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[key]))\n            if len(value) &lt; free_space:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value\n                        ))\n            else:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals[key] = value\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.get_feature_importances","title":"<code>get_feature_importances(step)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def get_feature_importances(\n    self, \n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f'`step` must be an integer. Got {type(step)}.'\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    idx_columns_lags = np.arange(len(self.lags))\n    if self.included_exog:\n        idx_columns_exog = np.flatnonzero(\n                            [name.endswith(f\"step_{step}\") for name in self.X_train_col_names]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_col_names[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterAutoregDirect.html#skforecast.ForecasterAutoregDirect.ForecasterAutoregDirect.ForecasterAutoregDirect.get_feature_importance","title":"<code>get_feature_importance(step)</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregDirect/ForecasterAutoregDirect.py</code> <pre><code>def get_feature_importance(\n    self, \n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances().\"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances(step=step)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html","title":"<code>ForecasterAutoregMultiSeries</code>","text":""},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries","title":"<code>ForecasterAutoregMultiSeries(regressor, lags, transformer_series=None, transformer_exog=None, weight_func=None, series_weights=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_series</code> <code>transformer(preprocessor), dict, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>None</code> <code>transformer_exog</code> <code>transformer, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, dict, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>None</code> <code>series_weights</code> <code>dict, default</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict, default `None`</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series.It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>index_values</code> <code>pandas Index</code> <p>Values of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series (levels) used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries--notes","title":"Notes","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <pre><code>- `series_weights` : controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.\n- `weight_func` : controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.\n</code></pre> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: Union[int, np.ndarray, list],\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Union[Callable, dict]]=None,\n    series_weights: Optional[dict]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.transformer_series      = transformer_series\n    self.transformer_series_     = None\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.weight_func_            = None\n    self.source_code_weight_func = None\n    self.series_weights          = series_weights\n    self.series_weights_         = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.index_values            = None\n    self.training_range          = None\n    self.last_window             = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.series_col_names        = None\n    self.X_train_col_names       = None\n    self.in_sample_residuals     = None\n    self.out_sample_residuals    = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    self.lags = initialize_lags(type(self).__name__, lags)\n    self.max_lag = max(self.lags)\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = series_weights\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._create_lags","title":"<code>_create_lags(y)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"       \n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series ({len(y)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n\n    for i, lag in enumerate(self.lags):\n        X_data[:, i] = y[self.max_lag - lag: -lag]\n\n    y_data = y[self.max_lag:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(series) - self.max_lag, )</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>series</code>.</p> <code>y_train_index</code> <code>pandas Index</code> <p>Index of <code>y_train</code>.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(series) - self.max_lag, )\n    y_index : pandas Index\n        Index of `series`.\n    y_train_index: pandas Index\n        Index of `y_train`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    series_col_names = list(series.columns)\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items() if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                 f\" No transformation is applied to these series.\"),\n                 IgnoredArgumentWarning\n            )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_levels = []\n    X_train_col_names = [f\"lag_{lag}\" for lag in self.lags]\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        check_y(y=y)\n        y = transform_series(\n                series            = y,\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values, y_index = preprocess_y(y=y)\n        X_train_values, y_train_values = self._create_lags(y=y_values)\n\n        if i == 0:\n            X_train = X_train_values\n            y_train = y_train_values\n        else:\n            X_train = np.vstack((X_train, X_train_values))\n            y_train = np.append(y_train, y_train_values)\n\n        X_level = [serie]*len(X_train_values)\n        X_levels.extend(X_level)\n\n    X_levels = pd.Series(X_levels)\n    X_levels = pd.get_dummies(X_levels, dtype=float)\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names\n              )\n\n    if exog is not None:\n        # The first `self.max_lag` positions have to be removed from exog\n        # since they are not in X_train. Then exog is cloned as many times\n        # as series.\n        exog_to_train = exog.iloc[self.max_lag:, ]\n        exog_to_train = pd.concat([exog_to_train]*len(series_col_names)).reset_index(drop=True)\n    else:\n        exog_to_train = None\n\n    X_train = pd.concat([X_train, exog_to_train, X_levels], axis=1)\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = pd.Series(\n                  data = y_train,\n                  name = 'y'\n              )\n\n    y_train_index = pd.Index(\n                        np.tile(\n                            y_index[self.max_lag: ].to_numpy(),\n                            reps = len(series_col_names)\n                        )\n                    )\n\n    return X_train, y_train, y_index, y_train_index\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.create_sample_weights","title":"<code>create_sample_weights(series, X_train, y_train_index)</code>","text":"<p>Crate weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Time series used to create <code>X_train</code> with the method <code>create_train_X_y</code>.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <code>y_train_index</code> <code>pandas Index</code> <p>Index generated with the method <code>create_train_X_y</code>, fourth return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def create_sample_weights(\n    self,\n    series: pd.DataFrame,\n    X_train: pd.DataFrame,\n    y_train_index: pd.Index,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Time series used to create `X_train` with the method `create_train_X_y`.\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n    y_train_index : pandas Index\n        Index generated with the method `create_train_X_y`, fourth return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    weights_series = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = set(series.columns) - set(self.series_weights.keys())\n        if series_not_in_series_weights:\n            warnings.warn(\n                (f\"{series_not_in_series_weights} not present in `series_weights`. \"\n                 f\"A weight of 1 is given to all their samples.\"),\n                 IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series.columns}\n        self.series_weights_.update((k, v) for k, v in self.series_weights.items() if k in self.series_weights_)\n        weights_series = [np.repeat(self.series_weights_[serie], sum(X_train[serie])) \n                          for serie in series.columns]\n        weights_series = np.concatenate(weights_series)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {col: copy(self.weight_func) \n                                 for col in series.columns}\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = set(series.columns) - set(self.weight_func.keys())\n            if series_not_in_weight_func:\n                warnings.warn(\n                    (f\"{series_not_in_weight_func} not present in `weight_func`. \"\n                     f\"A weight of 1 is given to all their samples.\"),\n                     IgnoredArgumentWarning\n                )\n            self.weight_func_ = {col: lambda x: np.ones_like(x, dtype=float) \n                                 for col in series.columns}\n            self.weight_func_.update((k, v) for k, v in self.weight_func.items() if k in self.weight_func_)\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            idx = y_train_index[X_train[X_train[key] == 1.0].index]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if weights_series is not None:\n        weights = weights_series\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                (\"The resulting `weights` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.index_values        = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)        \n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train, y_index, y_train_index = self.create_train_X_y(series=series, exog=exog)\n    sample_weight = self.create_sample_weights(\n                        series        = series,\n                        X_train       = X_train,\n                        y_train_index = y_train_index,\n                    )\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y_index[[0, -1]]\n    self.index_type = type(y_index)\n    if isinstance(y_index, pd.DatetimeIndex):\n        self.index_freq = y_index.freqstr\n    else: \n        self.index_freq = y_index.step\n    self.index_values = y_index\n\n    in_sample_residuals = {}\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = y_train - self.regressor.predict(X_train)\n\n        for serie in series.columns:\n            in_sample_residuals[serie] = residuals.loc[X_train[serie] == 1.].to_numpy()\n            if len(in_sample_residuals[serie]) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                in_sample_residuals[serie] = rng.choice(\n                                                 a       = in_sample_residuals[serie], \n                                                 size    = 1000, \n                                                 replace = False\n                                             )\n    else:\n        for serie in series.columns:\n            in_sample_residuals[serie] = None\n\n    self.in_sample_residuals = in_sample_residuals\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated.\n    self.last_window = series.iloc[-self.max_lag:, ].copy()\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries._recursive_predict","title":"<code>_recursive_predict(steps, level, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>level</code> <code>str</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    level: str,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    level : str\n        Time series to be predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = last_window[-self.lags].reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        levels_dummies = np.zeros(shape=(1, len(self.series_col_names)), dtype=float)\n        levels_dummies[0][self.series_col_names.index(level)] = 1.\n\n        X = np.column_stack((X, levels_dummies.reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict","title":"<code>predict(steps, levels=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values, one column for each level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values, one column for each level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    predictions = []\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        preds_level = self._recursive_predict(\n                          steps       = steps,\n                          level       = level,\n                          last_window = copy(last_window_values),\n                          exog        = copy(exog_values)\n                      )\n\n        preds_level = pd.Series(\n                          data  = preds_level,\n                          index = expand_index(\n                                      index = last_window_index,\n                                      steps = steps\n                                  ),\n                          name = level\n                      )\n\n        preds_level = transform_series(\n                          series            = preds_level,\n                          transformer       = self.transformer_series_[level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions.append(preds_level)    \n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>dict</code> <p>Predictions generated by bootstrapping for each level. {level: pandas DataFrame, shape (steps, n_boot)}</p>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; dict:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.      \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : dict\n        Predictions generated by bootstrapping for each level.\n        {level: pandas DataFrame, shape (steps, n_boot)}\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if in_sample_residuals:\n        if not set(levels).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for levels: \"\n                 f\"{set(levels) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals_levels = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method \"\n                 \"`set_out_sample_residuals()` before `predict_interval()`, \"\n                 \"`predict_bootstrapping()` or `predict_dist()`.\")\n            )\n        else:\n            if not set(levels).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for levels: \"\n                     f\"{set(levels) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals_levels = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for level in levels:\n        if residuals_levels[level] is None:\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' are `None`. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (residuals_levels[level] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"values. Check `{check_residuals}`.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    boot_predictions = {}\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        level_boot_predictions = np.full(\n                                     shape      = (steps, n_boot),\n                                     fill_value = np.nan,\n                                     dtype      = float\n                                 )\n        rng = np.random.default_rng(seed=random_state)\n        seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n        residuals = residuals_levels[level]\n\n        for i in range(n_boot):\n            # In each bootstraping iteration the initial last_window and exog \n            # need to be restored.\n            last_window_boot = last_window_values.copy()\n            exog_boot = exog_values.copy() if exog is not None else None\n\n            rng = np.random.default_rng(seed=seeds[i])\n            sample_residuals = rng.choice(\n                                   a       = residuals,\n                                   size    = steps,\n                                   replace = True\n                               )\n\n            for step in range(steps):\n\n                prediction = self._recursive_predict(\n                                 steps       = 1,\n                                 level       = level,\n                                 last_window = last_window_boot,\n                                 exog        = exog_boot \n                             )\n\n                prediction_with_residual = prediction + sample_residuals[step]\n                level_boot_predictions[step, i] = prediction_with_residual\n\n                last_window_boot = np.append(\n                                       last_window_boot[1:],\n                                       prediction_with_residual\n                                   )\n                if exog is not None:\n                    exog_boot = exog_boot[1:]\n\n        level_boot_predictions = pd.DataFrame(\n                                     data    = level_boot_predictions,\n                                     index   = expand_index(last_window_index, steps=steps),\n                                     columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                                 )\n\n        if self.transformer_series_[level]:\n            for col in level_boot_predictions.columns:\n                level_boot_predictions[col] = transform_series(\n                                                  series            = level_boot_predictions[col],\n                                                  transformer       = self.transformer_series_[level],\n                                                  fit               = False,\n                                                  inverse_transform = True\n                                              )\n\n        boot_predictions[level] = level_boot_predictions\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_interval","title":"<code>predict_interval(steps, levels=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction  intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>level: predictions.</li> <li>level_lower_bound: lower bound of the interval.</li> <li>level_upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which, each prediction, is used as a predictor\n    for the next step and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.  \n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction \n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - level: predictions.\n            - level_lower_bound: lower bound of the interval.\n            - level_upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    check_interval(interval=interval)\n\n    preds = self.predict(\n                steps       = steps,\n                levels      = levels,\n                last_window = last_window,\n                exog        = exog\n            )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           levels              = levels,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions = []\n\n    for level in levels:\n        preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n        preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n        predictions.append(preds[level])\n        predictions.append(preds_interval)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.predict_dist","title":"<code>predict_dist(steps, distribution, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step and level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step and level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       levels              = levels,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p=='x'] + [\"loc\",\"scale\"]\n    predictions = []\n\n    for level in levels:\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples[level])\n        level_param_names = [f'{level}_{p}' for p in param_names]\n\n        pred_level = pd.DataFrame(\n                         data    = param_values,\n                         columns = level_param_names,\n                         index   = boot_samples[level].index\n                     )\n\n        predictions.append(pred_level)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, list, np.ndarray, range]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.lags = initialize_lags(type(self).__name__, lags)            \n    self.max_lag  = max(self.lags)\n    self.window_size = max(self.lags)\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each level in the form {level: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored. Keys must be the same as <code>levels</code>.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_series.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict,\n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each level in the\n        form {level: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored. Keys must be the same as `levels`.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_series.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{level: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {level: None for level in self.series_col_names}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of levels \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    for level, value in residuals.items():\n\n        residuals_level = value\n\n        if not transform and self.transformer_series_[level] is not None:\n            warnings.warn(\n                (\"Argument `transform` is set to `False` but forecaster was \"\n                f\"trained using a transformer {self.transformer_series_[level]} \"\n                f\"for level {level}. Ensure that the new residuals are \"\n                 \"already transformed or set `transform=True`.\")\n            )\n\n        if transform and self.transformer_series_ and self.transformer_series_[level]:\n            warnings.warn(\n                (\"Residuals will be transformed using the same transformer used \"\n                f\"when training the forecaster for level {level} : \"\n                f\"({self.transformer_series_[level]}). Ensure that the new \"\n                 \"residuals are on the same scale as the original time series.\")\n            )\n            residuals_level = transform_series(\n                                  series            = pd.Series(residuals_level, name='residuals'),\n                                  transformer       = self.transformer_series_[level],\n                                  fit               = False,\n                                  inverse_transform = False\n                              ).to_numpy()\n\n        if len(residuals_level) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[level] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[level]))\n            if len(residuals_level) &lt; free_space:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level\n                                    ))\n            else:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level[:free_space]\n                                    ))\n\n        self.out_sample_residuals[level] = residuals_level\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiSeries.html#skforecast.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.ForecasterAutoregMultiSeries.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py</code> <pre><code>def get_feature_importance(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances().\"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances()\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html","title":"<code>ForecasterAutoregMultiSeriesCustom</code>","text":""},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom","title":"<code>ForecasterAutoregMultiSeriesCustom(regressor, fun_predictors, window_size, name_predictors=None, transformer_series=None, transformer_exog=None, weight_func=None, series_weights=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series with a custom function to create predictors. New in version 0.7.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. The same function is applied  to all series.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> required <code>name_predictors</code> <code>list, default</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the  position the predictor has in the returned array of <code>fun_predictors</code>. <code>None</code> <code>transformer_series</code> <code>transformer(preprocessor), dict, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>None</code> <code>transformer_exog</code> <code>transformer, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, dict, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>None</code> <code>series_weights</code> <code>dict, default</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>fun_predictors</code> <code>Callable</code> <p>Function that receives a time series as input (numpy ndarray) and returns another numpy ndarray with the predictors. The same function is applied  to all series.</p> <code>source_code_fun_predictors</code> <code>str</code> <p>Source code of the custom function used to create the predictors.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> <code>name_predictors</code> <code>list</code> <p>Name of the predictors returned by <code>fun_predictors</code>. If <code>None</code>, predictors are named using the prefix 'custom_predictor_' where <code>i</code> is the index of the  position the predictor has in the returned array of <code>fun_predictors</code>. <code>transformer_series</code> <code>transformer(preprocessor), dict</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable, dict</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series.It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed by <code>fun_predictors</code> to create the predictors.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>index_values</code> <code>pandas Index</code> <p>Values of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series (levels) used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up to 1000 values in the form <code>{level: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom--notes","title":"Notes","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <pre><code>- `series_weights` : controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.\n- `weight_func` : controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.\n</code></pre> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    fun_predictors: Callable, \n    window_size: int,\n    name_predictors: Optional[list]=None,\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Union[Callable, dict]]=None,\n    series_weights: Optional[dict]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor                  = regressor\n    self.fun_predictors             = fun_predictors\n    self.source_code_fun_predictors = None\n    self.window_size                = window_size\n    self.name_predictors            = name_predictors\n    self.transformer_series         = transformer_series\n    self.transformer_series_        = None\n    self.transformer_exog           = transformer_exog\n    self.weight_func                = weight_func\n    self.weight_func_               = None\n    self.source_code_weight_func    = None\n    self.series_weights             = series_weights\n    self.series_weights_            = None\n    self.index_type                 = None\n    self.index_freq                 = None\n    self.index_values               = None\n    self.training_range             = None\n    self.last_window                = None\n    self.included_exog              = False\n    self.exog_type                  = None\n    self.exog_dtypes                = None\n    self.exog_col_names             = None\n    self.series_col_names           = None\n    self.X_train_col_names          = None\n    self.in_sample_residuals        = None\n    self.out_sample_residuals       = None\n    self.fitted                     = False\n    self.creation_date              = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                   = None\n    self.skforcast_version          = skforecast.__version__\n    self.python_version             = sys.version.split(\" \")[0]\n    self.forecaster_id              = forecaster_id\n\n    if not isinstance(window_size, int):\n        raise TypeError(\n            f\"Argument `window_size` must be an int. Got {type(window_size)}.\"\n        )\n\n    if not isinstance(fun_predictors, Callable):\n        raise TypeError(\n            f\"Argument `fun_predictors` must be a Callable. Got {type(fun_predictors)}.\"\n        )\n\n    self.source_code_fun_predictors = inspect.getsource(fun_predictors)\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = series_weights\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(series) - self.max_lag, )</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>series</code>.</p> <code>y_train_index</code> <code>pandas Index</code> <p>Index of <code>y_train</code>.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(series) - self.max_lag, )\n    y_index : pandas Index\n        Index of `series`.\n    y_train_index: pandas Index\n        Index of `y_train`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    if len(series) &lt; self.window_size + 1:\n        raise ValueError(\n            (f\"`series` must have as many values as the windows_size needed by \"\n             f\"{self.fun_predictors.__name__}. For this Forecaster the \"\n             f\"minimum length is {self.window_size + 1}\")\n        )\n\n    series_col_names = list(series.columns)\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items() if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n        if series_not_in_transformer_series:\n                warnings.warn(\n                    (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                     f\" No transformation is applied to these series.\"),\n                     IgnoredArgumentWarning\n                )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = True,\n                       inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n    X_levels = []\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        check_y(y=y)\n        y = transform_series(\n                series            = y,\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values, y_index = preprocess_y(y=y)\n\n        temp_X_train  = []\n        temp_y_train  = []\n\n        for j in range(len(y) - self.window_size):\n\n            train_index = np.arange(j, self.window_size + j)\n            test_index  = self.window_size + j\n\n            temp_X_train.append(self.fun_predictors(y=y_values[train_index]))\n            temp_y_train.append(y_values[test_index])\n\n        X_train_values = np.vstack(temp_X_train)\n        y_train_values = np.array(temp_y_train)\n\n        if np.isnan(X_train_values).any():\n            raise ValueError(\n                f\"`fun_predictors()` is returning `NaN` values for series {serie}.\"\n            )\n\n        if i == 0:\n            X_train = X_train_values\n            y_train = y_train_values\n        else:\n            X_train = np.vstack((X_train, X_train_values))\n            y_train = np.append(y_train, y_train_values)\n\n        X_level = [serie]*len(X_train_values)\n        X_levels.extend(X_level)\n\n    if self.name_predictors is None:\n        X_train_col_names = [f\"custom_predictor_{i}\" \n                             for i in range(X_train.shape[1])]\n    else:\n        if len(self.name_predictors) != X_train.shape[1]:\n            raise ValueError(\n                (\"The length of provided predictors names (`name_predictors`) do \"\n                 \"not match the number of columns created by `fun_predictors()`.\")\n            )\n        X_train_col_names = self.name_predictors.copy()\n\n    # y_values correspond only to the last series of `series`. Since the columns\n    # of X_train are the same for all series, the check is the same.\n    expected = self.fun_predictors(y_values[:-1])\n    observed = X_train[-1, :]\n\n    if expected.shape != observed.shape or not (expected == observed).all():\n        raise ValueError(\n            (f\"The `window_size` argument ({self.window_size}), declared when \"\n             f\"initializing the forecaster, does not correspond to the window \"\n             f\"used by `fun_predictors()`.\")\n        )\n\n    X_levels = pd.Series(X_levels)\n    X_levels = pd.get_dummies(X_levels, dtype=float)\n\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names\n              )\n\n    if exog is not None:\n        # The first `self.window_size` positions have to be removed from exog\n        # since they are not in X_train. Then exog is cloned as many times\n        # as series.\n        exog_to_train = exog.iloc[self.window_size:, ]\n        exog_to_train = pd.concat([exog_to_train]*len(series_col_names)).reset_index(drop=True)\n    else:\n        exog_to_train = None\n\n    X_train = pd.concat([X_train, exog_to_train, X_levels], axis=1)\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train = pd.Series(\n                  data = y_train,\n                  name = 'y'\n              )\n\n    y_train_index = pd.Index(\n                        np.tile(\n                            y_index[self.window_size: ].values,\n                            reps = len(series_col_names)\n                        )\n                    )\n\n    return X_train, y_train, y_index, y_train_index\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.create_sample_weights","title":"<code>create_sample_weights(series, X_train, y_train_index)</code>","text":"<p>Crate weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Time series used to create <code>X_train</code> with the method <code>create_train_X_y</code>.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <code>y_train_index</code> <code>pandas Index</code> <p>Index generated with the method <code>create_train_X_y</code>, fourth return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def create_sample_weights(\n    self,\n    series: pd.DataFrame,\n    X_train: pd.DataFrame,\n    y_train_index: pd.Index,\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Time series used to create `X_train` with the method `create_train_X_y`.\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n    y_train_index : pandas Index\n        Index generated with the method `create_train_X_y`, fourth return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    weights_series = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = set(series.columns) - set(self.series_weights.keys())\n        if series_not_in_series_weights:\n            warnings.warn(\n                (f\"{series_not_in_series_weights} not present in `series_weights`.\"\n                 f\" A weight of 1 is given to all their samples.\"),\n                IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series.columns}\n        self.series_weights_.update((k, v) for k, v in self.series_weights.items() if k in self.series_weights_)\n        weights_series = [np.repeat(self.series_weights_[serie], sum(X_train[serie])) \n                          for serie in series.columns]\n        weights_series = np.concatenate(weights_series)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {col: copy(self.weight_func) \n                                 for col in series.columns}\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = set(series.columns) - set(self.weight_func.keys())\n            if series_not_in_weight_func:\n                warnings.warn(\n                    (f\"{series_not_in_weight_func} not present in `weight_func`.\"\n                     f\" A weight of 1 is given to all their samples.\"),\n                    IgnoredArgumentWarning\n                )\n            self.weight_func_ = {col: lambda x: np.ones_like(x, dtype=float) \n                                 for col in series.columns}\n            self.weight_func_.update((k, v) for k, v in self.weight_func.items() if k in self.weight_func_)\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            idx = y_train_index[X_train[X_train[key] == 1.0].index]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if weights_series is not None:\n        weights = weights_series\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                (\"The resulting `weights` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.index_values        = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train, y_index, y_train_index = self.create_train_X_y(series=series, exog=exog)\n    sample_weight = self.create_sample_weights(\n                        series        = series,\n                        X_train       = X_train,\n                        y_train_index = y_train_index,\n                    )\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y_index[[0, -1]]\n    self.index_type = type(y_index)\n    if isinstance(y_index, pd.DatetimeIndex):\n        self.index_freq = y_index.freqstr\n    else: \n        self.index_freq = y_index.step\n    self.index_values = y_index\n\n    in_sample_residuals = {}\n\n    # This is done to save time during fit in functions such as backtesting()\n    if store_in_sample_residuals:\n\n        residuals = y_train - self.regressor.predict(X_train)\n\n        for serie in series.columns:\n            in_sample_residuals[serie] = residuals.loc[X_train[serie] == 1.].to_numpy()\n            if len(in_sample_residuals[serie]) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                in_sample_residuals[serie] = rng.choice(\n                                                 a       = in_sample_residuals[serie], \n                                                 size    = 1000, \n                                                 replace = False\n                                             )\n    else:\n        for serie in series.columns:\n            in_sample_residuals[serie] = np.array([None])\n\n    self.in_sample_residuals = in_sample_residuals\n\n    # The last time window of training data is stored so that predictors in\n    # the first iteration of `predict()` can be calculated.\n    self.last_window = series.iloc[-self.window_size:].copy()\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom._recursive_predict","title":"<code>_recursive_predict(steps, level, last_window, exog=None)</code>","text":"<p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>level</code> <code>str</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>numpy ndarray</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> required <code>exog</code> <code>numpy ndarray, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    level: str,\n    last_window: np.ndarray,\n    exog: Optional[np.ndarray]=None\n) -&gt; np.ndarray:\n\"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    level : str\n        Time series to be predicted.\n    last_window : numpy ndarray\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n    exog : numpy ndarray, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    predictions = np.full(shape=steps, fill_value=np.nan)\n\n    for i in range(steps):\n        X = self.fun_predictors(y=last_window).reshape(1, -1)\n        if exog is not None:\n            X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n\n        levels_dummies = np.zeros(shape=(1, len(self.series_col_names)), dtype=float)\n        levels_dummies[0][self.series_col_names.index(level)] = 1.\n\n        X = np.column_stack((X, levels_dummies.reshape(1, -1)))\n\n        with warnings.catch_warnings():\n            # Suppress scikit-learn warning: \"X does not have valid feature names,\n            # but NoOpTransformer was fitted with feature names\".\n            warnings.simplefilter(\"ignore\")\n            prediction = self.regressor.predict(X)\n            predictions[i] = prediction.ravel()[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window = np.append(last_window[1:], prediction)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict","title":"<code>predict(steps, levels=None, last_window=None, exog=None)</code>","text":"<p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values, one column for each level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values, one column for each level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:, ]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    predictions = []\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        preds_level = self._recursive_predict(\n                          steps       = steps,\n                          level       = level,\n                          last_window = copy(last_window_values),\n                          exog        = copy(exog_values)\n                      )\n\n        preds_level = pd.Series(\n                          data  = preds_level,\n                          index = expand_index(\n                                      index = last_window_index,\n                                      steps = steps\n                                  ),\n                          name = level\n                      )\n\n        preds_level = transform_series(\n                          series            = preds_level,\n                          transformer       = self.transformer_series_[level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions.append(preds_level)    \n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_bootstrapping","title":"<code>predict_bootstrapping(steps, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>dict</code> <p>Predictions generated by bootstrapping for each level.  {level: pandas DataFrame, shape (steps, n_boot)}</p>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; dict:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.        \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    boot_predictions : dict\n        Predictions generated by bootstrapping for each level. \n        {level: pandas DataFrame, shape (steps, n_boot)}\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    if in_sample_residuals:\n        if not set(levels).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for levels: \"\n                 f\"{set(levels) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals_levels = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method \"\n                 \"`set_out_sample_residuals()` before `predict_interval()`, \"\n                 \"`predict_bootstrapping()` or `predict_dist()`.\")\n            )\n        else:\n            if not set(levels).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for levels: \"\n                     f\"{set(levels) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals_levels = self.out_sample_residuals\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals\" if in_sample_residuals\n         else \"forecaster.out_sample_residuals\"\n    )\n    for level in levels:\n        if residuals_levels[level] is None:\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' are `None`. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (residuals_levels[level] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"values. Check `{check_residuals}`.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    last_window = last_window.iloc[-self.window_size:, ]\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = levels,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    boot_predictions = {}\n\n    for level in levels:\n\n        last_window_level = transform_series(\n                                series            = last_window[level],\n                                transformer       = self.transformer_series_[level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_level\n                                                )\n\n        level_boot_predictions = np.full(\n                                     shape      = (steps, n_boot),\n                                     fill_value = np.nan,\n                                     dtype      = float\n                                 )\n        rng = np.random.default_rng(seed=random_state)\n        seeds = rng.integers(low=0, high=10000, size=n_boot)\n\n        residuals = residuals_levels[level]\n\n        for i in range(n_boot):\n            # In each bootstraping iteration the initial last_window and exog \n            # need to be restored.\n            last_window_boot = last_window_values.copy()\n            exog_boot = exog_values.copy() if exog is not None else None\n\n            rng = np.random.default_rng(seed=seeds[i])\n            sample_residuals = rng.choice(\n                                   a       = residuals,\n                                   size    = steps,\n                                   replace = True\n                               )\n\n            for step in range(steps):\n\n                prediction = self._recursive_predict(\n                                 steps       = 1,\n                                 level       = level,\n                                 last_window = last_window_boot,\n                                 exog        = exog_boot \n                             )\n\n                prediction_with_residual = prediction + sample_residuals[step]\n                level_boot_predictions[step, i] = prediction_with_residual\n\n                last_window_boot = np.append(\n                                       last_window_boot[1:],\n                                       prediction_with_residual\n                                   )\n                if exog is not None:\n                    exog_boot = exog_boot[1:]\n\n        level_boot_predictions = pd.DataFrame(\n                                     data    = level_boot_predictions,\n                                     index   = expand_index(last_window_index, steps=steps),\n                                     columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n                                 )\n\n        if self.transformer_series_[level]:\n            for col in level_boot_predictions.columns:\n                level_boot_predictions[col] = transform_series(\n                                                  series            = level_boot_predictions[col],\n                                                  transformer       = self.transformer_series_[level],\n                                                  fit               = False,\n                                                  inverse_transform = True\n                                              )\n\n        boot_predictions[level] = level_boot_predictions\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_interval","title":"<code>predict_interval(steps, levels=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>level: predictions.</li> <li>level_lower_bound: lower bound of the interval.</li> <li>level_upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Iterative process in which, each prediction, is used as a predictor\n    for the next step and bootstrapping is used to estimate prediction\n    intervals. Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.  \n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - level: predictions.\n            - level_lower_bound: lower bound of the interval.\n            - level_upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    check_interval(interval=interval)\n\n    preds = self.predict(\n                steps       = steps,\n                levels      = levels,\n                last_window = last_window,\n                exog        = exog\n            )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           levels              = levels,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions = []\n\n    for level in levels:\n        preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n        preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n        predictions.append(preds[level])\n        predictions.append(preds_interval)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.predict_dist","title":"<code>predict_dist(steps, distribution, levels=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats. For example scipy.stats.norm.</p> required <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Values of the series used to create the predictors needed in the first re of prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step and level.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: Optional[Union[str, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    distribution : Object\n        A distribution object from scipy.stats. For example scipy.stats.norm.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.  \n    last_window : pandas DataFrame, default `None`\n        Values of the series used to create the predictors needed in the first\n        re of prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step and level.\n\n    \"\"\"\n\n    if levels is None:\n        levels = self.series_col_names\n    elif isinstance(levels, str):\n        levels = [levels]\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       levels              = levels,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )\n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p=='x'] + [\"loc\",\"scale\"]\n    predictions = []\n\n    for level in levels:\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples[level])\n        level_param_names = [f'{level}_{p}' for p in param_names]\n\n        pred_level = pd.DataFrame(\n                         data    = param_values,\n                         columns = level_param_names,\n                         index   = boot_samples[level].index\n                     )\n\n        predictions.append(pred_level)\n\n    predictions = pd.concat(predictions, axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each level in the form {level: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored. Keys must be the same as <code>levels</code>.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_series.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict,\n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each level in the\n        form {level: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored. Keys must be the same as `levels`.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_series.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{level: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {level: None for level in self.series_col_names}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of levels \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value \n                 for key, value in residuals.items() \n                 if key in self.out_sample_residuals.keys()}\n\n    for level, value in residuals.items():\n\n        residuals_level = value\n\n        if not transform and self.transformer_series_[level] is not None:\n            warnings.warn(\n                (\"Argument `transform` is set to `False` but forecaster was \"\n                f\"trained using a transformer {self.transformer_series_[level]} \"\n                f\"for level {level}. Ensure that the new residuals are \"\n                 \"already transformed or set `transform=True`.\")\n            )\n\n        if transform and self.transformer_series_ and self.transformer_series_[level]:\n            warnings.warn(\n                (\"Residuals will be transformed using the same transformer used \"\n                f\"when training the forecaster for level {level} : \"\n                f\"({self.transformer_series_[level]}). Ensure that the new \"\n                 \"residuals are on the same scale as the original time series.\")\n            )\n            residuals_level = transform_series(\n                                  series            = pd.Series(residuals_level, name='residuals'),\n                                  transformer       = self.transformer_series_[level],\n                                  fit               = False,\n                                  inverse_transform = False\n                              ).to_numpy()\n\n        if len(residuals_level) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[level] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[level]))\n            if len(residuals_level) &lt; free_space:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level\n                                    ))\n            else:\n                residuals_level = np.hstack((\n                                        self.out_sample_residuals[level],\n                                        residuals_level[:free_space]\n                                    ))\n\n        self.out_sample_residuals[level] = residuals_level\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_col_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiSeriesCustom.html#skforecast.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.ForecasterAutoregMultiSeriesCustom.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiSeriesCustom/ForecasterAutoregMultiSeriesCustom.py</code> <pre><code>def get_feature_importance(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances().\"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances()\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html","title":"<code>ForecasterAutoregMultiVariate</code>","text":""},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate","title":"<code>ForecasterAutoregMultiVariate(regressor, level, steps, lags, transformer_series=None, transformer_exog=None, weight_func=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>         Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive multivariate direct multi-step forecaster. A separate model  is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>level</code> <code>str</code> <p>Name of the time series to be predicted.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value must be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series.  {'series_column_name': lags}.</li> </ul> required <code>transformer_series</code> <code>transformer(preprocessor), dict, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>None</code> <code>transformer_exog</code> <code>transformer, default</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable, default</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int, default</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>int</code> <p>Number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> <code>lags</code> <code>numpy ndarray, dict</code> <p>Lags used as predictors.</p> <code>lags_</code> <code>dict</code> <p>Dictionary containing the lags of each series. Created from <code>lags</code> and  used internally.</p> <code>transformer_series</code> <code>transformer (preprocessor), dict, default `None`</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each  series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>max_lag</code> <code>int</code> <p>Maximum value of lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to <code>max_lag</code>.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window seen by the forecaster during training. It stores the values  needed to predict the next <code>step</code> immediately after the training data.   </p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes</code> <code>dict</code> <p>Type of each exogenous variable/s used in training. If <code>transformer_exog</code>  is used, the dtypes are calculated after the transformation.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>series_col_names</code> <code>list</code> <p>Names of the series used during training.</p> <code>X_train_col_names</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>in_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>out_sample_residuals</code> <code>dict</code> <p>Residuals of the models when predicting non training data. Only stored up to 1000 values per model in the form <code>{step: residuals}</code>. If <code>transformer_series</code>  is not <code>None</code>, residuals are assumed to be in the transformed scale. Use  <code>set_out_sample_residuals()</code> method to set values.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate--notes","title":"Notes","text":"<p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    level: str,\n    steps: int,\n    lags: Union[int, np.ndarray, list, dict],\n    transformer_series: Optional[Union[object, dict]]=None,\n    transformer_exog: Optional[object]=None,\n    weight_func: Optional[Callable]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor               = regressor\n    self.level                   = level\n    self.steps                   = steps\n    self.transformer_series      = transformer_series\n    self.transformer_series_     = None\n    self.transformer_exog        = transformer_exog\n    self.weight_func             = weight_func\n    self.source_code_weight_func = None\n    self.max_lag                 = None\n    self.window_size             = None\n    self.last_window             = None\n    self.index_type              = None\n    self.index_freq              = None\n    self.training_range          = None\n    self.included_exog           = False\n    self.exog_type               = None\n    self.exog_dtypes             = None\n    self.exog_col_names          = None\n    self.series_col_names        = None\n    self.X_train_col_names       = None\n    self.fitted                  = False\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date                = None\n    self.skforcast_version       = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(level, str):\n        raise TypeError(\n            f\"`level` argument must be a str. Got {type(level)}.\"\n        )\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            (f\"`steps` argument must be an int greater than or equal to 1. \"\n             f\"Got {type(steps)}.\")\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        for key in lags:\n            self.lags[key] = initialize_lags(\n                                 forecaster_name = type(self).__name__,\n                                 lags            = lags[key]\n                             )\n    else:\n        self.lags = initialize_lags(\n                        forecaster_name = type(self).__name__, \n                        lags            = lags\n                    )\n\n    self.lags_ = self.lags\n    self.max_lag = (\n        max(list(chain(*self.lags.values())))\n        if isinstance(self.lags, dict)\n        else max(self.lags)\n    )\n    self.window_size = self.max_lag\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.in_sample_residuals = {step: None for step in range(1, steps + 1)}\n    self.out_sample_residuals = None\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate._create_lags","title":"<code>_create_lags(y, lags)</code>","text":"<p>Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <code>lags</code> <code>numpy ndarray</code> <p>lags to create.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>2d numpy ndarray with the lagged values (predictors).  Shape: (samples - max(self.lags), len(self.lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>1d numpy ndarray with the values of the time series related to each  row of <code>X_data</code>.  Shape: (samples - max(self.lags), )</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    lags: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"       \n    Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n    lags : numpy ndarray\n        lags to create.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        2d numpy ndarray with the lagged values (predictors). \n        Shape: (samples - max(self.lags), len(self.lags))\n    y_data : numpy ndarray\n        1d numpy ndarray with the values of the time series related to each \n        row of `X_data`. \n        Shape: (samples - max(self.lags), )\n\n    \"\"\"\n\n    n_splits = len(y) - self.max_lag - (self.steps - 1) # rows of y_data\n    if n_splits &lt;= 0:\n        raise ValueError(\n            (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n             f\"of the series minus the number of steps ({len(y)-(self.steps-1)}).\")\n        )\n\n    X_data = np.full(shape=(n_splits, len(lags)), fill_value=np.nan, dtype=float)\n    for i, lag in enumerate(lags):\n        X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n\n    y_data = np.full(shape=(n_splits, self.steps), fill_value=np.nan, dtype=float)\n    for step in range(self.steps):\n        y_data[:, step] = y[self.max_lag + step : self.max_lag + step + n_splits]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_train_X_y","title":"<code>create_train_X_y(series, exog=None)</code>","text":"<p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Shape: (len(series) - self.max_lag, len(self.lags)len(series.columns) + exog.shape[1]steps)</p> <code>y_train</code> <code>pandas DataFrame</code> <p>Values (target) of the time series related to each row of <code>X_train</code>  for each step. Shape: (len(series) - self.max_lag, )</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step.\n        Shape: (len(series) - self.max_lag, len(self.lags)*len(series.columns) + exog.shape[1]*steps)\n    y_train : pandas DataFrame\n        Values (target) of the time series related to each row of `X_train` \n        for each step.\n        Shape: (len(series) - self.max_lag, )\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    series_col_names = list(series.columns)\n\n    if self.level not in series_col_names:\n        raise ValueError(\n            (f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n             f\"    forecaster `level` : {self.level}.\\n\"\n             f\"    `series` columns   : {series_col_names}.\")\n        )\n\n    self.lags_ = self.lags\n    if isinstance(self.lags_, dict):\n        if list(self.lags_.keys()) != series_col_names:\n            raise ValueError(\n                (f\"When `lags` parameter is a `dict`, its keys must be the \"\n                 f\"same as `series` column names.\\n\"\n                 f\"    Lags keys        : {list(self.lags_.keys())}.\\n\"\n                 f\"    `series` columns : {series_col_names}.\")\n            )\n    else:\n        self.lags_ = {serie: self.lags_ for serie in series_col_names}\n\n    if len(series) &lt; self.max_lag + self.steps:\n        raise ValueError(\n            (f\"Minimum length of `series` for training this forecaster is \"\n             f\"{self.max_lag + self.steps}. Got {len(series)}. Reduce the \"\n             f\"number of predicted steps, {self.steps}, or the maximum \"\n             f\"lag, {self.max_lag}, if no more data is available.\")\n        )\n\n    if self.transformer_series is None:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n    elif not isinstance(self.transformer_series, dict):\n        self.transformer_series_ = {serie: clone(self.transformer_series) \n                                    for serie in series_col_names}\n    else:\n        self.transformer_series_ = {serie: None for serie in series_col_names}\n        # Only elements already present in transformer_series_ are updated\n        self.transformer_series_.update(\n            (k, v) for k, v in deepcopy(self.transformer_series).items()\n            if k in self.transformer_series_\n        )\n        series_not_in_transformer_series = (\n            set(series.columns) - set(self.transformer_series.keys())\n        )\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                 f\" No transformation is applied to these series.\"),\n                 IgnoredArgumentWarning\n            )\n\n    if exog is not None:\n        if len(exog) != len(series):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series)})\")\n            )\n        check_exog(exog=exog, allow_nan=True)\n        # Need here for filter_train_X_y_for_step to work without fitting\n        self.included_exog = True \n        if isinstance(exog, pd.Series):\n            exog = transform_series(\n                        series            = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n        else:\n            exog = transform_dataframe(\n                        df                = exog,\n                        transformer       = self.transformer_exog,\n                        fit               = True,\n                        inverse_transform = False\n                   )\n\n        check_exog(exog=exog, allow_nan=False)\n        check_exog_dtypes(exog)\n        self.exog_dtypes = get_exog_dtypes(exog=exog)\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        if not (exog_index[:len(series.index)] == series.index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\") \n            )\n\n    for i, serie in enumerate(series.columns):\n\n        y = series[serie]\n        check_y(y=y)\n        y = transform_series(\n                series            = y,\n                transformer       = self.transformer_series_[serie],\n                fit               = True,\n                inverse_transform = False\n            )\n\n        y_values, y_index = preprocess_y(y=y)\n        X_train_values, y_train_values = self._create_lags(\n                                            y    = y_values,\n                                            lags = self.lags_[serie]\n                                        )\n        if i == 0:\n            X_train = X_train_values\n        else:\n            X_train = np.hstack((X_train, X_train_values))\n\n        if serie == self.level:\n            y_train = y_train_values\n\n    X_train_col_names = [f\"{key}_lag_{lag}\" for key in self.lags_\n                         for lag in self.lags_[key]]\n    X_train = pd.DataFrame(\n                  data    = X_train,\n                  columns = X_train_col_names,\n                  index   = y_index[self.max_lag + (self.steps -1): ]\n              )\n\n    if exog is not None:\n        # Transform exog to match direct format\n        # The first `self.max_lag` positions have to be removed from X_exog\n        # since they are not in X_lags.\n        exog_to_train = exog_to_direct(\n                            exog  = exog,\n                            steps = self.steps\n                        ).iloc[-X_train.shape[0]:, :]\n        X_train = pd.concat((X_train, exog_to_train), axis=1)\n\n    self.X_train_col_names = X_train.columns.to_list()\n\n    y_train_col_names = [f\"{self.level}_step_{i+1}\" for i in range(self.steps)]\n    y_train = pd.DataFrame(\n                  data    = y_train,\n                  index   = y_index[self.max_lag + (self.steps -1): ],\n                  columns = y_train_col_names,\n              )\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.filter_train_X_y_for_step","title":"<code>filter_train_X_y_for_step(step, X_train, y_train, remove_suffix=False)</code>","text":"<p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y()</code>. If  <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the  column names.      </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>step for which columns must be selected selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, first return.</p> required <code>y_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the method <code>create_train_X_y</code>, second return.</p> required <code>remove_suffix</code> <code>bool, default</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag)</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    remove_suffix: bool=False\n) -&gt; Tuple[pd.DataFrame, pd.Series]:\n\"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y()`. If \n    `remove_suffix=True` the suffix \"_step_i\" will be removed from the \n    column names.      \n\n    Parameters\n    ----------\n    step : int\n        step for which columns must be selected selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, first return.\n    y_train : pandas DataFrame\n        Dataframe generated with the method `create_train_X_y`, second return.\n    remove_suffix : bool, default `False`\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag)\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n             f\"and the maximum step is {self.steps}.\")\n        )\n\n    # Matrices X_train and y_train start at index 0.\n    y_train_step = y_train.iloc[:, step - 1]\n\n    if not self.included_exog:\n        X_train_step = X_train\n    else:\n        len_columns_lags = len(list(chain(*self.lags_.values())))\n        idx_columns_lags = np.arange(len_columns_lags)\n        n_exog = (len(self.X_train_col_names) - len_columns_lags) / self.steps\n        idx_columns_exog = (\n            np.arange((step-1)*n_exog, (step)*n_exog) + idx_columns_lags[-1] + 1 \n        )\n        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    if remove_suffix:\n        X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n                                for col_name in X_train_step.columns]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return  X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.create_sample_weights","title":"<code>create_sample_weights(X_train)</code>","text":"<p>Crate weights for each observation according to the forecaster's attribute <code>weight_func</code>. </p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe generated with the methods <code>create_train_X_y</code> and  <code>filter_train_X_y_for_step</code>, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame\n)-&gt; np.ndarray:\n\"\"\"\n    Crate weights for each observation according to the forecaster's attribute\n    `weight_func`. \n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe generated with the methods `create_train_X_y` and \n        `filter_train_X_y_for_step`, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.fit","title":"<code>fit(series, exog=None, store_in_sample_residuals=True)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>None</code> <code>store_in_sample_residuals</code> <code>bool, default</code> <p>If True, in-sample residuals will be stored in the forecaster object after fitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    store_in_sample_residuals: bool=True\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_in_sample_residuals : bool, default `True`\n        If True, in-sample residuals will be stored in the forecaster object\n        after fitting.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_dtypes         = None\n    self.exog_col_names      = None\n    self.series_col_names    = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = {step: None for step in range(1, self.steps + 1)}\n    self.fitted              = False\n    self.training_range      = None\n\n    self.series_col_names = list(series.columns)\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n            exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n        if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n            raise ValueError(\n                (f\"`exog` cannot contain a column named the same as one of the \"\n                 f\"series (column names of series).\\n\"\n                 f\"    `series` columns : {self.series_col_names}.\\n\"\n                 f\"    `exog`   columns : {self.exog_col_names}.\")\n            )\n\n    X_train, y_train = self.create_train_X_y(series=series, exog=exog)\n\n    # Train one regressor for each step\n    for step in range(1, self.steps + 1):\n        # self.regressors_ and self.filter_train_X_y_for_step expect\n        # first step to start at value 1\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            self.regressors_[step].fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressors_[step].fit(\n                X = X_train_step, \n                y = y_train_step, \n                **self.fit_kwargs\n            )\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            residuals = (\n                (y_train_step - self.regressors_[step].predict(X_train_step))\n            ).to_numpy()\n\n            if len(residuals) &gt; 1000:\n                # Only up to 1000 residuals are stored\n                rng = np.random.default_rng(seed=123)\n                residuals = rng.choice(\n                                a       = residuals, \n                                size    = 1000, \n                                replace = False\n                            )\n\n            self.in_sample_residuals[step] = residuals\n\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = preprocess_y(\n                            y = series[self.level],\n                            return_values = False\n                          )[1][[0, -1]]\n    self.index_type = type(X_train.index)\n    if isinstance(X_train.index, pd.DatetimeIndex):\n        self.index_freq = X_train.index.freqstr\n    else: \n        self.index_freq = X_train.index.step\n\n    self.last_window = series.iloc[-self.max_lag:].copy()\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict","title":"<code>predict(steps=None, last_window=None, exog=None, levels=None)</code>","text":"<p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n\n    if last_window is None:\n        last_window = deepcopy(self.last_window)\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window,\n        last_window_exog = None,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = self.steps,\n        levels           = None,\n        series_col_names = self.series_col_names\n    )\n\n    if exog is not None:\n        if isinstance(exog, pd.DataFrame):\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        else:\n            exog = transform_series(\n                       series            = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n        check_exog_dtypes(exog=exog)\n        exog_values = exog_to_direct_numpy(\n                          exog  = exog.to_numpy()[:max(steps)],\n                          steps = max(steps)\n                      )[0]\n    else:\n        exog_values = None\n\n    X_lags = np.array([[]], dtype=float)\n\n    for serie in self.series_col_names:\n\n        last_window_serie = transform_series(\n                                series            = last_window[serie],\n                                transformer       = self.transformer_series_[serie],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window_serie\n                                                )\n\n        X_lags = np.hstack(\n                    [X_lags, last_window_values[-self.lags_[serie]].reshape(1, -1)]\n                 )\n\n    predictions = np.full(shape=len(steps), fill_value=np.nan)\n\n    if exog is None:\n        Xs = [X_lags] * len(steps)\n    else:\n        n_exog = exog.shape[1] if isinstance(exog, pd.DataFrame) else 1\n        Xs = [\n            np.hstack(\n                [X_lags, exog_values[(step-1)*n_exog:(step)*n_exog].reshape(1, -1)]\n            )\n            for step in steps\n        ]\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        # Suppress scikit-learn warning: \"X does not have valid feature names,\n        # but NoOpTransformer was fitted with feature names\".\n        warnings.simplefilter(\"ignore\")\n        predictions = [\n            regressor.predict(X)[0] for regressor, X in zip(regressors, Xs)\n        ]\n\n    idx = expand_index(index=last_window_index, steps=max(steps))\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      columns = [self.level],\n                      index   = idx[np.array(steps)-1]\n                  )\n\n    predictions = transform_dataframe(\n                      df                = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_bootstrapping","title":"<code>predict_bootstrapping(steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the Notes section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_bootstrapping--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the Notes section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.     \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.               \n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n    Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(self.steps) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    if in_sample_residuals:\n        if not set(steps).issubset(set(self.in_sample_residuals.keys())):\n            raise ValueError(\n                (f\"Not `forecaster.in_sample_residuals` for steps: \"\n                 f\"{set(steps) - set(self.in_sample_residuals.keys())}.\")\n            )\n        residuals = self.in_sample_residuals\n    else:\n        if self.out_sample_residuals is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals` is `None`. Use \"\n                 \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n                 \"before `predict_interval()`, `predict_bootstrapping()` or \"\n                 \"`predict_dist()`.\")\n            )\n        else:\n            if not set(steps).issubset(set(self.out_sample_residuals.keys())):\n                raise ValueError(\n                    (f\"Not `forecaster.out_sample_residuals` for steps: \"\n                     f\"{set(steps) - set(self.out_sample_residuals.keys())}. \"\n                     f\"Use method `set_out_sample_residuals()`.\")\n                )\n        residuals = self.out_sample_residuals\n\n    check_residuals = (\n        'forecaster.in_sample_residuals' if in_sample_residuals\n        else 'forecaster.out_sample_residuals'\n    )\n    for step in steps:\n        if residuals[step] is None:\n            raise ValueError(\n                (f\"forecaster residuals for step {step} are `None`. \"\n                 f\"Check {check_residuals}.\")\n            )\n        elif (residuals[step] == None).any():\n            raise ValueError(\n                (f\"forecaster residuals for step {step} contains `None` values. \"\n                 f\"Check {check_residuals}.\")\n            )\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog \n                  )\n\n    # Predictions must be in the transformed scale before adding residuals\n    predictions = transform_dataframe(\n                      df                = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = False\n                  )\n    boot_predictions = pd.concat([predictions] * n_boot, axis=1)\n    boot_predictions.columns= [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n    rng = np.random.default_rng(seed=random_state)\n    for i, step in enumerate(steps):\n        sample_residuals = rng.choice(\n                               a       = residuals[step],\n                               size    = n_boot,\n                               replace = True\n                           )\n        boot_predictions.iloc[i, :] = boot_predictions.iloc[i, :] + sample_residuals\n\n    if self.transformer_series_[self.level]:\n        for col in boot_predictions.columns:\n            boot_predictions[col] = transform_series(\n                                        series            = boot_predictions[col],\n                                        transformer       = self.transformer_series_[self.level],\n                                        fit               = False,\n                                        inverse_transform = True\n                                    )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_interval","title":"<code>predict_interval(steps=None, last_window=None, exog=None, interval=[5, 95], n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Bootstrapping based prediction intervals. Both predictions and intervals are returned.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of  percentiles to compute, which must be between 0 and 100 inclusive.  For example, interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>[5, 95]</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_interval--notes","title":"Notes","text":"<p>More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_interval(\n    self,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    interval: list=[5, 95],\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Bootstrapping based prediction intervals.\n    Both predictions and intervals are returned.\n\n    Parameters\n    ----------\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    interval : list, default `[5, 95]`\n        Confidence of the prediction interval estimated. Sequence of \n        percentiles to compute, which must be between 0 and 100 inclusive. \n        For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    Notes\n    -----\n    More information about prediction intervals in forecasting:\n    https://otexts.com/fpp2/prediction-intervals.html\n    Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n    George Athanasopoulos.\n\n    \"\"\"\n\n    check_interval(interval=interval)\n\n    predictions = self.predict(\n                      steps       = steps,\n                      last_window = last_window,\n                      exog        = exog\n                  )\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps               = steps,\n                           last_window         = last_window,\n                           exog                = exog,\n                           n_boot              = n_boot,\n                           random_state        = random_state,\n                           in_sample_residuals = in_sample_residuals\n                       )\n\n    interval = np.array(interval)/100\n    predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n    predictions_interval.columns = ['lower_bound', 'upper_bound']\n    predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.predict_dist","title":"<code>predict_dist(distribution, steps=None, last_window=None, exog=None, n_boot=500, random_state=123, in_sample_residuals=True, levels=None)</code>","text":"<p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>Object</code> <p>A distribution object from scipy.stats.</p> required <code>steps</code> <code>int, list, None, default</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame, default </code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create prediction intervals. If <code>False</code>, out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see <code>set_out_sample_residuals()</code>).</p> <code>True</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: Optional[Union[int, list]]=None,\n    last_window: Optional[pd.DataFrame]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    levels: Any=None\n) -&gt; pd.DataFrame:\n\"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : Object\n        A distribution object from scipy.stats.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create prediction intervals. If `False`, out of\n        sample residuals are used. In the latter case, the user should have\n        calculated and stored the residuals within the forecaster (see\n        `set_out_sample_residuals()`).\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    \"\"\"\n\n    boot_samples = self.predict_bootstrapping(\n                       steps               = steps,\n                       last_window         = last_window,\n                       exog                = exog,\n                       n_boot              = n_boot,\n                       random_state        = random_state,\n                       in_sample_residuals = in_sample_residuals\n                   )       \n\n    param_names = [p for p in inspect.signature(distribution._pdf).parameters \n                   if not p=='x'] + [\"loc\",\"scale\"]\n    param_values = np.apply_along_axis(\n                        lambda x: distribution.fit(x),\n                        axis = 1,\n                        arr  = boot_samples\n                   )\n    predictions = pd.DataFrame(\n                      data    = param_values,\n                      columns = param_names,\n                      index   = boot_samples.index\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the scikit learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {step: clone(self.regressor)\n                        for step in range(1, self.steps + 1)}\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_lags","title":"<code>set_lags(lags)</code>","text":"<p>Set new value to the attribute <code>lags</code>. Attributes <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series.  {'series_column_name': lags}.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_lags(\n    self, \n    lags: Union[int, np.ndarray, list, dict]\n) -&gt; None:\n\"\"\"      \n    Set new value to the attribute `lags`.\n    Attributes `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, dict\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. \n            {'series_column_name': lags}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        for key in lags:\n            self.lags[key] = initialize_lags(\n                                 forecaster_name = type(self).__name__,\n                                 lags            = lags[key]\n                             )\n    else:\n        self.lags = initialize_lags(\n                        forecaster_name = type(self).__name__, \n                        lags            = lags\n                    )\n\n    self.lags_ = self.lags\n    self.max_lag = (\n        max(list(chain(*self.lags.values()))) if isinstance(self.lags, dict)\n        else max(self.lags)\n    )\n    self.window_size = self.max_lag\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.set_out_sample_residuals","title":"<code>set_out_sample_residuals(residuals, append=True, transform=True, random_state=123)</code>","text":"<p>Set new values to the attribute <code>out_sample_residuals</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process.</p> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>dict</code> <p>Dictionary of numpy ndarrays with the residuals of each model in the form {step: residuals}. If len(residuals) &gt; 1000, only a random  sample of 1000 values are stored.</p> required <code>append</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals</code>. Once the limit of 1000 values is reached, no more values are appended. If False, <code>out_sample_residuals</code> is overwritten with the new residuals.</p> <code>True</code> <code>transform</code> <code>bool, default</code> <p>If <code>True</code>, new residuals are transformed using self.transformer_y.</p> <code>True</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    residuals: dict, \n    append: bool=True,\n    transform: bool=True,\n    random_state: int=123\n)-&gt; None:\n\"\"\"\n    Set new values to the attribute `out_sample_residuals`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process.\n\n    Parameters\n    ----------\n    residuals : dict\n        Dictionary of numpy ndarrays with the residuals of each model in the\n        form {step: residuals}. If len(residuals) &gt; 1000, only a random \n        sample of 1000 values are stored.\n    append : bool, default `True`\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals`. Once the limit of 1000 values is\n        reached, no more values are appended. If False, `out_sample_residuals`\n        is overwritten with the new residuals.\n    transform : bool, default `True`\n        If `True`, new residuals are transformed using self.transformer_y.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n        raise TypeError(\n            (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n             \"`{step: residuals}`. \" \n             f\"Got {type(residuals)}.\")\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `set_out_sample_residuals()`.\")\n        )\n\n    if self.out_sample_residuals is None:\n        self.out_sample_residuals = {step: None \n                                     for step in range(1, self.steps + 1)}\n\n    if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n        warnings.warn(\n            (f\"\"\"\n            Only residuals of models (steps) \n{set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))}             are updated.\n            \"\"\"), IgnoredArgumentWarning\n        )\n\n    residuals = {key: value for key, value in residuals.items()\n                 if key in self.out_sample_residuals.keys()}\n\n    if not transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Argument `transform` is set to `False` but forecaster was trained \"\n             f\"using a transformer {self.transformer_series_[self.level]}. Ensure \"\n             f\"that the new residuals are already transformed or set `transform=True`.\")\n        )\n\n    if transform and self.transformer_series_[self.level] is not None:\n        warnings.warn(\n            (f\"Residuals will be transformed using the same transformer used when \"\n             f\"training the forecaster ({self.transformer_series_[self.level]}). Ensure \"\n             f\"the new residuals are on the same scale as the original time series.\")\n        )\n        for key, value in residuals.items():\n            residuals[key] = transform_series(\n                                 series            = pd.Series(value, name='residuals'),\n                                 transformer       = self.transformer_series_[self.level],\n                                 fit               = False,\n                                 inverse_transform = False\n                             ).to_numpy()\n\n    for key, value in residuals.items():\n        if len(value) &gt; 1000:\n            rng = np.random.default_rng(seed=random_state)\n            value = rng.choice(a=value, size=1000, replace=False)\n\n        if append and self.out_sample_residuals[key] is not None:\n            free_space = max(0, 1000 - len(self.out_sample_residuals[key]))\n            if len(value) &lt; free_space:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value\n                        ))\n            else:\n                value = np.hstack((\n                            self.out_sample_residuals[key],\n                            value[:free_space]\n                        ))\n\n        self.out_sample_residuals[key] = value\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.get_feature_importances","title":"<code>get_feature_importances(step)</code>","text":"<p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def get_feature_importances(\n    self,\n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f'`step` must be an integer. Got {type(step)}.'\n        )\n\n    if not self.fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    if (step &lt; 1) or (step &gt; self.steps):\n        raise ValueError(\n            (f\"The step must have a value from 1 to the maximum number of steps \"\n             f\"({self.steps}). Got {step}.\")\n        )\n\n    if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    len_columns_lags = len(list(chain(*self.lags_.values())))\n    idx_columns_lags = np.arange(len_columns_lags)\n    if self.included_exog:\n        idx_columns_exog = np.flatnonzero(\n                            [name.endswith(f\"step_{step}\")\n                             for name in self.X_train_col_names]\n                           )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n    feature_names = [self.X_train_col_names[i].replace(f\"_step_{step}\", \"\") \n                     for i in idx_columns]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            (f\"Impossible to access feature importances for regressor of type \"\n             f\"{type(estimator)}. This method is only valid when the \"\n             f\"regressor stores internally the feature importances in the \"\n             f\"attribute `feature_importances_` or `coef_`.\")\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterMultiVariate.html#skforecast.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.ForecasterAutoregMultiVariate.get_feature_importance","title":"<code>get_feature_importance(step)</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the  feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterAutoregMultiVariate/ForecasterAutoregMultiVariate.py</code> <pre><code>def get_feature_importance(\n    self,\n    step: int\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return impurity-based feature importance of the model stored in\n    the forecaster for a specific step. Since a separate model is created for\n    each forecast time step, it is necessary to select the model from which\n    retrieve information. Only valid when regressor stores internally the \n    feature importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances().\"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances(step=step)\n</code></pre>"},{"location":"api/ForecasterSarimax.html","title":"<code>ForecasterSarimax</code>","text":""},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax","title":"<code>ForecasterSarimax(regressor, transformer_y=None, transformer_exog=None, fit_kwargs=None, forecaster_id=None)</code>","text":"<p>This class turns ARIMA model from pmdarima library into a Forecaster  compatible with the skforecast API. New in version 0.7.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>pmdarima.arima.ARIMA</code> <p>An instance of an ARIMA from pmdarima library. This model internally wraps the statsmodels SARIMAX class.</p> required <code>transformer_y</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor), default </code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>fit_kwargs</code> <code>dict, default</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>None</code> <code>forecaster_id</code> <code>str, int default </code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>pmdarima.arima.ARIMA</code> <p>An instance of an ARIMA from pmdarima library. The model internally wraps the statsmodels SARIMAX class</p> <code>params</code> <code>dict</code> <p>Parameters of the sarimax model.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>window_size</code> <code>int</code> <p>Not used, present here for API consistency by convention.</p> <code>last_window</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>extended_index</code> <code>pandas Index</code> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index. Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_type</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_col_names</code> <code>list</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor. New in version 0.8.0</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforcast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>str, int default `None`</code> <p>Name used as an identifier of the forecaster. New in version 0.7.0</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def __init__(\n    self,\n    regressor: ARIMA,\n    transformer_y: Optional[object]=None,\n    transformer_exog: Optional[object]=None,\n    fit_kwargs: Optional[dict]=None,\n    forecaster_id: Optional[Union[str, int]]=None\n) -&gt; None:\n\n    self.regressor         = regressor\n    self.transformer_y     = transformer_y\n    self.transformer_exog  = transformer_exog\n    self.window_size       = 1\n    self.last_window       = None\n    self.extended_index    = None\n    self.fitted            = False\n    self.index_type        = None\n    self.index_freq        = None\n    self.training_range    = None\n    self.included_exog     = False\n    self.exog_type         = None\n    self.exog_col_names    = None\n    self.creation_date     = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.fit_date          = None\n    self.skforcast_version = skforecast.__version__\n    self.python_version    = sys.version.split(\" \")[0]\n    self.forecaster_id     = forecaster_id\n\n    if not isinstance(self.regressor, pmdarima.arima.ARIMA):\n        raise TypeError(\n            (f\"`regressor` must be an instance of type pmdarima.arima.ARIMA. \"\n             f\"Got {type(regressor)}.\")\n        )\n\n    self.params = self.regressor.get_params(deep=True)\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.fit","title":"<code>fit(y, exog=None)</code>","text":"<p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; None:\n\"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_y(y=y)\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `y`. \"\n                 f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n            )\n        check_exog(exog=exog)\n\n    # Reset values in case the forecaster has already been fitted.\n    self.index_type          = None\n    self.index_freq          = None\n    self.last_window         = None\n    self.extended_index      = None\n    self.included_exog       = False\n    self.exog_type           = None\n    self.exog_col_names      = None\n    self.X_train_col_names   = None\n    self.in_sample_residuals = None\n    self.fitted              = False\n    self.training_range      = None\n\n    if exog is not None:\n        self.included_exog = True\n        self.exog_type = type(exog)\n        self.exog_col_names = \\\n             exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog   \n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = True,\n                   inverse_transform = False\n               )\n\n    self.regressor.fit(y=y, X=exog, **self.fit_kwargs)\n    self.fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range = y.index[[0, -1]]\n    self.index_type = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq = y.index.freqstr\n    else: \n        self.index_freq = y.index.step\n\n    self.last_window = y.copy()\n    self.extended_index = self.regressor.arima_res_.fittedvalues.index.copy()\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict","title":"<code>predict(steps, last_window=None, last_window_exog=None, exog=None)</code>","text":"<p>Forecast future values</p> <p>Generate predictions (forecasts) n steps in the future. Note that if  exogenous variables were used in the model fit, they will be expected  for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Value of the exogenous variable/s for the next steps.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n) -&gt; pd.Series:\n\"\"\"\n    Forecast future values\n\n    Generate predictions (forecasts) n steps in the future. Note that if \n    exogenous variables were used in the model fit, they will be expected \n    for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Value of the exogenous variable/s for the next steps.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append if not needed\n    last_window_check = last_window.copy() if last_window is not None else self.last_window.copy()\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = None,\n        alpha            = None,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    # If last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            (\"To make predictions unrelated to the original data, both \"\n             \"`last_window` and `last_window_exog` must be provided.\")\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. To make predictions \"\n             \"unrelated to the original data, same variable/s must be provided \"\n             \"using `last_window_exog`.\")\n        )\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # check index append values\n        expected_index = expand_index(index=self.extended_index, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                (f\"To make predictions unrelated to the original data, `last_window` \"\n                 f\"has to start at the end of the index seen by the forecaster.\\n\"\n                 f\"    Series last index         : {self.extended_index[-1]}.\\n\"\n                 f\"    Expected index            : {expected_index}.\\n\"\n                 f\"    `last_window` index start : {last_window.index[0]}.\")\n            )\n\n        last_window = transform_series(\n                          series            = last_window.copy(),\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # TODO -----------------------------------------------------------------------------------------------------\n        # This is done because pmdarima deletes the series name\n        # Check issue: https://github.com/alkaline-ml/pmdarima/issues/535\n        last_window.name = None\n        # ----------------------------------------------------------------------------------------------------------\n\n        # last_window_exog\n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    (f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                     f\"has to start at the end of the index seen by the forecaster.\\n\"\n                     f\"    Series last index              : {self.extended_index[-1]}.\\n\"\n                     f\"    Expected index                 : {expected_index}.\\n\"\n                     f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\")\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog \n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n        self.regressor.arima_res_ = self.regressor.arima_res_.append(\n                                        endog = last_window,\n                                        exog  = last_window_exog,\n                                        refit = False\n                                    )\n        self.extended_index = self.regressor.arima_res_.fittedvalues.index\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    # Get following n steps predictions\n    predictions = self.regressor.predict(\n                      n_periods = steps,\n                      X         = exog\n                  )\n\n    # Reverse the transformation if needed\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n    predictions.name = 'pred'\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.predict_interval","title":"<code>predict_interval(steps, last_window=None, last_window_exog=None, exog=None, alpha=0.05, interval=None)</code>","text":"<p>Forecast future values and their confidence intervals</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> required <code>last_window</code> <code>pandas Series, default </code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only need when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>alpha</code> <code>float, default</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>0.05</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: Optional[pd.Series]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    alpha: float=0.05,\n    interval: list=None,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Forecast future values and their confidence intervals\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of future steps predicted.\n    last_window : pandas Series, default `None`\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window`. Only\n        need when `last_window` is not None and the forecaster has been\n        trained including exogenous variables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append if not needed\n    last_window_check = last_window.copy() if last_window is not None else self.last_window.copy()\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        fitted           = self.fitted,\n        included_exog    = self.included_exog,\n        index_type       = self.index_type,\n        index_freq       = self.index_freq,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_type        = self.exog_type,\n        exog_col_names   = self.exog_col_names,\n        interval         = interval,\n        alpha            = alpha,\n        max_steps        = None,\n        levels           = None,\n        series_col_names = None\n    )\n\n    # If last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            (\"To make predictions unrelated to the original data, both \"\n             \"`last_window` and `last_window_exog` must be provided.\")\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. To make predictions \"\n             \"unrelated to the original data, same variable/s must be provided \"\n             \"using `last_window_exog`.\")\n        )  \n\n    # If interval and alpha take alpha, if interval transform to alpha\n    if alpha is None:\n        if 100 - interval[1] != interval[0]:\n            raise ValueError(\n                (f\"When using `interval` in ForecasterSarimax, it must be symmetrical. \"\n                 f\"For example, interval of 95% should be as `interval = [2.5, 97.5]`. \"\n                 f\"Got {interval}.\")\n            )\n        alpha = 2*(100 - interval[1])/100\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # check index append values\n        expected_index = expand_index(index=self.extended_index, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                (f\"To make predictions unrelated to the original data, `last_window` \"\n                 f\"has to start at the end of the index seen by the forecaster.\\n\"\n                 f\"    Series last index         : {self.extended_index[-1]}.\\n\"\n                 f\"    Expected index            : {expected_index}.\\n\"\n                 f\"    `last_window` index start : {last_window.index[0]}.\")\n            )\n\n        last_window = transform_series(\n                          series            = last_window,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # TODO -----------------------------------------------------------------------------------------------------\n        # This is done because pmdarima deletes the series name\n        # Check issue: https://github.com/alkaline-ml/pmdarima/issues/535\n        last_window.name = None\n        # ----------------------------------------------------------------------------------------------------------\n\n        # Transform last_window_exog    \n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    (f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                     f\"has to start at the end of the index seen by the forecaster.\\n\"\n                     f\"    Series last index              : {self.extended_index[-1]}.\\n\"\n                     f\"    Expected index                 : {expected_index}.\\n\"\n                     f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\")\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog \n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n        self.regressor.arima_res_ = self.regressor.arima_res_.append(\n                                        endog = last_window,\n                                        exog  = last_window_exog,\n                                        refit = False\n                                    )\n        self.extended_index = self.regressor.arima_res_.fittedvalues.index\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            # pmdarima.arima.ARIMA only accepts DataFrames or 2d-arrays as exog\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    # Get following n steps predictions with intervals\n    predicted_mean, conf_int = self.regressor.predict(\n                                   n_periods       = steps,\n                                   X               = exog,\n                                   alpha           = alpha,\n                                   return_conf_int = True\n                               )\n\n    predictions = predicted_mean.to_frame(name=\"pred\")\n    predictions['lower_bound'] = conf_int[:, 0]\n    predictions['upper_bound'] = conf_int[:, 1]\n\n    # Reverse the transformation if needed\n    if self.transformer_y:\n        for col in predictions.columns:\n            predictions[col] = transform_series(\n                                series            = predictions[col],\n                                transformer       = self.transformer_y,\n                                fit               = False,\n                                inverse_transform = True\n                           )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_params","title":"<code>set_params(params)</code>","text":"<p>Set new values to the parameters of the model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n\"\"\"\n    Set new values to the parameters of the model stored in the forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.set_fit_kwargs","title":"<code>set_fit_kwargs(fit_kwargs)</code>","text":"<p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict\n) -&gt; None:\n\"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_feature_importances","title":"<code>get_feature_importances()</code>","text":"<p>Return feature importances of the regressor stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def get_feature_importances(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.fitted:\n        raise NotFittedError(\n            (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n             \"arguments before using `get_feature_importances()`.\")\n        )\n\n    feature_importances = self.regressor.params().to_frame().reset_index()\n    feature_importances.columns = ['feature', 'importance']\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.ForecasterSarimax.ForecasterSarimax.ForecasterSarimax.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>This method has been replaced by <code>get_feature_importances()</code>.</p> <p>Return feature importance of the regressor stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast/ForecasterSarimax/ForecasterSarimax.py</code> <pre><code>def get_feature_importance(\n    self\n) -&gt; pd.DataFrame:\n\"\"\"\n    This method has been replaced by `get_feature_importances()`.\n\n    Return feature importance of the regressor stored in the forecaster.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    warnings.warn(\n        (\"get_feature_importance() method has been renamed to get_feature_importances().\"\n         \"This method will be removed in skforecast 0.9.0.\")\n    )\n\n    return self.get_feature_importances()\n</code></pre>"},{"location":"api/exceptions.html","title":"<code>exceptions</code>","text":""},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingValuesExogWarning","title":"<code>MissingValuesExogWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify there are missing values in the exogenous data. This warning occurs when the input data passed to the <code>exog</code> arguments contains missing values. Most machine learning models do not accept missing values, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTypeWarning","title":"<code>DataTypeWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.LongTrainingWarning","title":"<code>LongTrainingWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify that a large number of models will be trained and the the process may take a while to run.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IgnoredArgumentWarning","title":"<code>IgnoredArgumentWarning(message)</code>","text":"<p>         Bases: <code>UserWarning</code></p> <p>Warning used to notify that an argument is ignored when using a method  or a function.</p> Source code in <code>skforecast/exceptions/exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/model_selection.html","title":"<code>model_selection</code>","text":""},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.backtesting_forecaster","title":"<code>backtesting_forecaster(forecaster, y, steps, metric, initial_train_size=None, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, verbose=False, show_progress=True)</code>","text":"<p>Backtesting of forecaster model.</p> <p>If <code>refit</code> is False, the model is trained only once using the <code>initial_train_size</code> first observations. If <code>refit</code> is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so  it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int, default</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> <code>None</code> <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration.</p> <code>False</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>. If <code>None</code>, no intervals are estimated.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_value</code> <code>float, list</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection/model_selection.py</code> <pre><code>def backtesting_forecaster(\n    forecaster,\n    y: pd.Series,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int]=None,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[Union[float, list], pd.DataFrame]:\n\"\"\"\n    Backtesting of forecaster model.\n\n    If `refit` is False, the model is trained only once using the `initial_train_size`\n    first observations. If `refit` is True, the model is trained in each iteration\n    increasing the training set. A copy of the original forecaster is created so \n    it is not modified during the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar. Defaults to True.\n\n    Returns\n    -------\n    metrics_value : float, list\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterAutoreg', \n                                         'ForecasterAutoregCustom', \n                                         'ForecasterAutoregDirect']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterAutoreg`, `ForecasterAutoregCustom` \"\n             \"or `ForecasterAutoregDirect`, for all other types of forecasters \"\n             \"use the functions available in the other `model_selection` modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        y                     = y,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    if type(forecaster).__name__ == 'ForecasterAutoregDirect' and \\\n       forecaster.steps &lt; steps + gap:\n        raise ValueError(\n            (\"When using a ForecasterAutoregDirect, the combination of steps \"\n             f\"+ gap ({steps+gap}) cannot be greater than the `steps` parameter \"\n             f\"declared when the forecaster is initialized ({forecaster.steps}).\")\n        )\n\n    if refit:\n        metrics_values, backtest_predictions = _backtesting_forecaster_refit(\n            forecaster            = forecaster,\n            y                     = y,\n            steps                 = steps,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            allow_incomplete_fold = allow_incomplete_fold,\n            exog                  = exog,\n            interval              = interval,\n            n_boot                = n_boot,\n            random_state          = random_state,\n            in_sample_residuals   = in_sample_residuals,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )\n    else:\n        metrics_values, backtest_predictions = _backtesting_forecaster_no_refit(\n            forecaster            = forecaster,\n            y                     = y,\n            steps                 = steps,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            gap                   = gap,\n            allow_incomplete_fold = allow_incomplete_fold,\n            exog                  = exog,\n            interval              = interval,\n            n_boot                = n_boot,\n            random_state          = random_state,\n            in_sample_residuals   = in_sample_residuals,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )  \n\n    return metrics_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.grid_search_forecaster","title":"<code>grid_search_forecaster(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, return_best=True, verbose=True)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forcaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series values.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection/model_selection.py</code> <pre><code>def grid_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forcaster model.\n    y : pandas Series\n        Training time series values. \n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        verbose               = verbose\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.random_search_forecaster","title":"<code>random_search_forecaster(forecaster, y, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, verbose=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forcaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>n_iter</code> <code>int, default</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection/model_selection.py</code> <pre><code>def random_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forcaster model.\n    y : pandas Series\n        Training time series. \n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        verbose               = verbose\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection.model_selection.bayesian_search_forecaster","title":"<code>bayesian_search_forecaster(forecaster, y, search_space, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, lags_grid=None, refit=False, n_trials=10, random_state=123, return_best=True, verbose=True, engine='optuna', kwargs_create_study={}, kwargs_study_optimize={}, kwargs_gp_minimize='deprecated')</code>","text":"<p>Bayesian optimization for a Forecaster object using time series backtesting and  optuna library.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect</code> <p>Forcaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>search_space</code> <code>Callable(optuna)</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, numpy ndarray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoreg</code> or <code>ForecasterAutoregDirect</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>n_trials</code> <code>int, default</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>10</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <code>engine</code> <code>str, default</code> <p>Bayesian optimization runs through the optuna library.</p> <code>'optuna'</code> <code>kwargs_create_study</code> <code>dict, default</code> <p>Only applies to engine='optuna'. Keyword arguments (key, value mappings)  to pass to optuna.create_study.</p> <code>{}</code> <code>kwargs_study_optimize</code> <code>dict, default</code> <p>Only applies to engine='optuna'. Other keyword arguments (key, value mappings)  to pass to study.optimize().</p> <code>{}</code> <code>kwargs_gp_minimize</code> <code>dict, default</code> <p>Only applies to engine='skopt'. Other keyword arguments (key, value mappings)  to pass to skopt.gp_minimize(). Deprecated in version 0.7.0</p> <code>'deprecated'</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> <code>results_opt_best</code> <code>optuna object (optuna)  </code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast/model_selection/model_selection.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster,\n    y: pd.Series,\n    search_space: Callable,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    n_trials: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    verbose: bool=True,\n    engine: str='optuna',\n    kwargs_create_study: dict={},\n    kwargs_study_optimize: dict={},\n    kwargs_gp_minimize: Any='deprecated'\n) -&gt; Tuple[pd.DataFrame, object]:\n\"\"\"\n    Bayesian optimization for a Forecaster object using time series backtesting and \n    optuna library.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregDirect\n        Forcaster model.\n    y : pandas Series\n        Training time series. \n    search_space : Callable (optuna)\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list of int, lists, numpy ndarray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoreg` or `ForecasterAutoregDirect`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    n_trials : int, default `10`\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default `123`\n        Sets a seed to the sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n    engine : str, default `'optuna'`\n        Bayesian optimization runs through the optuna library.\n    kwargs_create_study : dict, default `{'direction':'minimize', 'sampler':TPESampler(seed=123)}`\n        Only applies to engine='optuna'. Keyword arguments (key, value mappings) \n        to pass to optuna.create_study.\n    kwargs_study_optimize : dict, default `{}`\n        Only applies to engine='optuna'. Other keyword arguments (key, value mappings) \n        to pass to study.optimize().\n    kwargs_gp_minimize : dict, default `{}`\n        Only applies to engine='skopt'. Other keyword arguments (key, value mappings) \n        to pass to skopt.gp_minimize().\n        **Deprecated in version 0.7.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n    results_opt_best : optuna object (optuna)  \n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f'`exog` must have same number of samples as `y`. '\n            f'length `exog`: ({len(exog)}), length `y`: ({len(y)})'\n        )\n\n    if engine == 'skopt':\n        warnings.warn(\n            (\"The engine 'skopt' for `bayesian_search_forecaster` is deprecated \"\n             \"in favor of 'optuna' engine. To continue using it, use skforecast \"\n             \"0.6.0. The optimization will be performed using the 'optuna' engine.\")\n        )\n        engine = 'optuna'\n\n    if engine not in ['optuna']:\n        raise ValueError(\n            f\"\"\"`engine` only allows 'optuna', got {engine}.\"\"\"\n        )\n\n    results, results_opt_best = _bayesian_search_optuna(\n                                    forecaster            = forecaster,\n                                    y                     = y,\n                                    exog                  = exog,\n                                    lags_grid             = lags_grid,\n                                    search_space          = search_space,\n                                    steps                 = steps,\n                                    metric                = metric,\n                                    refit                 = refit,\n                                    initial_train_size    = initial_train_size,\n                                    fixed_train_size      = fixed_train_size,\n                                    gap                   = gap,\n                                    allow_incomplete_fold = allow_incomplete_fold,\n                                    n_trials              = n_trials,\n                                    random_state          = random_state,\n                                    return_best           = return_best,\n                                    verbose               = verbose,\n                                    kwargs_create_study   = kwargs_create_study,\n                                    kwargs_study_optimize = kwargs_study_optimize\n                                )\n\n    return results, results_opt_best\n</code></pre>"},{"location":"api/model_selection_multiseries.html","title":"<code>model_selection_multiseries</code>","text":""},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.backtesting_forecaster_multiseries","title":"<code>backtesting_forecaster_multiseries(forecaster, series, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, verbose=False, show_progress=True)</code>","text":"<p>Backtesting for multi-series and multivariate forecasters.</p> <p>If <code>refit</code> is False, the model is trained only once using the <code>initial_train_size</code> first observations. If <code>refit</code> is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so  it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int, default</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration.</p> <code>False</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If <code>None</code>, no intervals are estimated.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>. If there is more than one level, this structure will be repeated for each of them.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def backtesting_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int],\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Backtesting for multi-series and multivariate forecasters.\n\n    If `refit` is False, the model is trained only once using the `initial_train_size`\n    first observations. If `refit` is True, the model is trained in each iteration\n    increasing the training set. A copy of the original forecaster is created so \n    it is not modified during the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.   \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar. Defaults to True.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterAutoregMultiSeries', \n                                         'ForecasterAutoregMultiSeriesCustom', \n                                         'ForecasterAutoregMultiVariate']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterAutoregMultiSeries`, \"\n             \"`ForecasterAutoregMultiSeriesCustom` or `ForecasterAutoregMultiVariate`, \"\n             \"for all other types of forecasters use the functions available in \"\n             f\"the `model_selection` module. Got {type(forecaster).__name__}\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        series                = series,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    if type(forecaster).__name__ in ['ForecasterAutoregMultiSeries', \n                                     'ForecasterAutoregMultiSeriesCustom'] \\\n        and levels is not None and not isinstance(levels, (str, list)):\n        raise TypeError(\n            (\"`levels` must be a `list` of column names, a `str` of a column name \"\n             \"or `None` when using a `ForecasterAutoregMultiSeries` or \"\n             \"`ForecasterAutoregMultiSeriesCustom`. If the forecaster is of type \"\n             \"`ForecasterAutoregMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterAutoregMultiVariate' \\\n        and levels and levels != forecaster.level and levels != [forecaster.level]:\n        warnings.warn(\n            (f\"`levels` argument have no use when the forecaster is of type \"\n             f\"`ForecasterAutoregMultiVariate`. The level of this forecaster is \"\n             f\"{forecaster.level}, to predict another level, change the `level` \"\n             f\"argument when initializing the forecaster.\"),\n             IgnoredArgumentWarning\n        )\n\n    if refit:\n        metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries_refit(\n            forecaster            = forecaster,\n            series                = series,\n            steps                 = steps,\n            levels                = levels,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            allow_incomplete_fold = allow_incomplete_fold,\n            exog                  = exog,\n            interval              = interval,\n            n_boot                = n_boot,\n            random_state          = random_state,\n            in_sample_residuals   = in_sample_residuals,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )\n    else:\n        metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries_no_refit(\n            forecaster            = forecaster,\n            series                = series,\n            steps                 = steps,\n            levels                = levels,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            gap                   = gap,\n            allow_incomplete_fold = allow_incomplete_fold,\n            exog                  = exog,\n            interval              = interval,\n            n_boot                = n_boot,\n            random_state          = random_state,\n            in_sample_residuals   = in_sample_residuals,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.grid_search_forecaster_multiseries","title":"<code>grid_search_forecaster_multiseries(forecaster, series, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, return_best=True, verbose=True)</code>","text":"<p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forcaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def grid_search_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forcaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster            = forecaster,\n                  series                = series,\n                  param_grid            = param_grid,\n                  steps                 = steps,\n                  metric                = metric,\n                  initial_train_size    = initial_train_size,\n                  fixed_train_size      = fixed_train_size,\n                  gap                   = gap,\n                  allow_incomplete_fold = allow_incomplete_fold,\n                  levels                = levels,\n                  exog                  = exog,\n                  lags_grid             = lags_grid,\n                  refit                 = refit,\n                  return_best           = return_best,\n                  verbose               = verbose\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.random_search_forecaster_multiseries","title":"<code>random_search_forecaster_multiseries(forecaster, series, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, verbose=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forcaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>n_iter</code> <code>int, default</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def random_search_forecaster_multiseries(\n    forecaster,\n    series: pd.DataFrame,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forcaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, \n                                       random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster            = forecaster,\n                  series                = series,\n                  param_grid            = param_grid,\n                  steps                 = steps,\n                  metric                = metric,\n                  initial_train_size    = initial_train_size,\n                  fixed_train_size      = fixed_train_size,\n                  gap                   = gap,\n                  allow_incomplete_fold = allow_incomplete_fold,\n                  levels                = levels,\n                  exog                  = exog,\n                  lags_grid             = lags_grid,\n                  refit                 = refit,\n                  return_best           = return_best,\n                  verbose               = verbose\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.backtesting_forecaster_multivariate","title":"<code>backtesting_forecaster_multivariate(forecaster, series, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, refit=False, interval=None, n_boot=500, random_state=123, in_sample_residuals=True, verbose=False, show_progress=True)</code>","text":"<p>This function is an alias of backtesting_forecaster_multiseries.</p> <p>Backtesting for multi-series and multivariate forecasters.</p> <p>If <code>refit</code> is False, the model is trained only once using the <code>initial_train_size</code> first observations. If <code>refit</code> is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so  it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int, default</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code> is  already trained, no initial train is done and all data is used to evaluate the  model. However, the first <code>len(forecaster.last_window)</code> observations are needed  to create the initial predictors, so no predictions are calculated for them.  This useful to backtest the model on the same data used to train it. <code>None</code> is only allowed when <code>refit</code> is <code>False</code> and <code>forecaster</code> is already trained.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration.</p> <code>False</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If <code>None</code>, no intervals are estimated.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>. If there is more than one level, this structure will be repeated for each of them.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def backtesting_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: Optional[int],\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    interval: Optional[list]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    This function is an alias of backtesting_forecaster_multiseries.\n\n    Backtesting for multi-series and multivariate forecasters.\n\n    If `refit` is False, the model is trained only once using the `initial_train_size`\n    first observations. If `refit` is True, the model is trained in each iteration\n    increasing the training set. A copy of the original forecaster is created so \n    it is not modified during the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forecaster model.\n    series : pandas DataFrame\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` is \n        already trained, no initial train is done and all data is used to evaluate the \n        model. However, the first `len(forecaster.last_window)` observations are needed \n        to create the initial predictors, so no predictions are calculated for them. \n        This useful to backtest the model on the same data used to train it.\n        `None` is only allowed when `refit` is `False` and `forecaster` is already\n        trained.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.   \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar. Defaults to True.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        refit                 = refit,\n        interval              = interval,\n        n_boot                = n_boot,\n        random_state          = random_state,\n        in_sample_residuals   = in_sample_residuals,\n        verbose               = verbose,\n        show_progress         = show_progress\n\n    )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.grid_search_forecaster_multivariate","title":"<code>grid_search_forecaster_multivariate(forecaster, series, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, return_best=True, verbose=True)</code>","text":"<p>This function is an alias of grid_search_forecaster_multiseries.</p> <p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forcaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def grid_search_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    This function is an alias of grid_search_forecaster_multiseries.\n\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forcaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    results = grid_search_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        return_best           = return_best,\n        verbose               = verbose\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_multiseries.html#skforecast.model_selection_multiseries.model_selection_multiseries.random_search_forecaster_multivariate","title":"<code>random_search_forecaster_multivariate(forecaster, series, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, levels=None, exog=None, lags_grid=None, refit=False, n_iter=10, random_state=123, return_best=True, verbose=True)</code>","text":"<p>This function is an alias of random_search_forecaster_multiseries.</p> <p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate</code> <p>Forcaster model.</p> required <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>levels</code> <code>str, list, default</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account. The resulting metric will be the average of the optimization of all levels.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>list of int, lists, np.narray or range, default </code> <p>Lists of <code>lags</code> to try. Only used if forecaster is an instance of  <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiVariate</code>.</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>n_iter</code> <code>int, default</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_multiseries/model_selection_multiseries.py</code> <pre><code>def random_search_forecaster_multivariate(\n    forecaster,\n    series: pd.DataFrame,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    levels: Optional[Union[str, list]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    lags_grid: Optional[list]=None,\n    refit: bool=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    This function is an alias of random_search_forecaster_multiseries.\n\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterAutoregMultiSeries, ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate\n        Forcaster model.\n    series : pandas DataFrame\n        Training time series.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list of int, lists, np.narray or range, default `None`\n        Lists of `lags` to try. Only used if forecaster is an instance of \n        `ForecasterAutoregMultiSeries` or `ForecasterAutoregMultiVariate`.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column levels: levels configuration for each iteration.\n            - column lags: lags configuration for each iteration.\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration. The resulting \n            metric will be the average of the optimization of all levels.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    results = random_search_forecaster_multiseries(\n        forecaster            = forecaster,\n        series                = series,\n        param_distributions   = param_distributions,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        levels                = levels,\n        exog                  = exog,\n        lags_grid             = lags_grid,\n        refit                 = refit,\n        n_iter                = n_iter,\n        random_state          = random_state,\n        return_best           = return_best,\n        verbose               = verbose\n    ) \n\n    return results\n</code></pre>"},{"location":"api/model_selection_sarimax.html","title":"<code>model_selection_sarimax</code>","text":""},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.backtesting_sarimax","title":"<code>backtesting_sarimax(forecaster, y, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, alpha=None, interval=None, verbose=False, show_progress=True)</code>","text":"<p>Backtesting of ForecasterSarimax.</p> <p>If <code>refit</code> is False, the model is trained only once using the <code>initial_train_size</code> first observations. If <code>refit</code> is True, the model is trained in each iteration increasing the training set. A copy of the original forecaster is created so  it is not modified during the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration.</p> <code>False</code> <code>alpha</code> <code>float, default</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>None</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metrics_value</code> <code>float, list</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def backtesting_sarimax(\n    forecaster,\n    y: pd.Series,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    alpha: Optional[float]=None,\n    interval: Optional[list]=None,\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; Tuple[Union[float, list], pd.DataFrame]:\n\"\"\"\n    Backtesting of ForecasterSarimax.\n\n    If `refit` is False, the model is trained only once using the `initial_train_size`\n    first observations. If `refit` is True, the model is trained in each iteration\n    increasing the training set. A copy of the original forecaster is created so \n    it is not modified during the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int\n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration.\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.     \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar. Defaults to True.\n\n    Returns\n    -------\n    metrics_value : float, list\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n            - column pred: predictions.\n            - column lower_bound: lower bound of the interval.\n            - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterSarimax`, for all other \"\n             \"types of forecasters use the functions available in the other \"\n             \"`model_selection` modules.\")\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        steps                 = steps,\n        metric                = metric,\n        y                     = y,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        refit                 = refit,\n        interval              = interval,\n        alpha                 = alpha,\n        verbose               = verbose,\n        show_progress         = show_progress\n    )\n\n    if refit:\n        metrics_values, backtest_predictions = _backtesting_sarimax_refit(\n            forecaster            = forecaster,\n            y                     = y,\n            steps                 = steps,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            allow_incomplete_fold = allow_incomplete_fold,\n            exog                  = exog,\n            alpha                 = alpha,\n            interval              = interval,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )\n    else:\n        if gap != 0 or allow_incomplete_fold is not True:\n            warnings.warn(\n                (\"When using `refit=False`, the `gap` and `allow_incomplete_fold`\"\n                 \"arguments are ignored. Set `refit=True` to used them.\"), \n                 IgnoredArgumentWarning\n            )\n\n        metrics_values, backtest_predictions = _backtesting_sarimax_no_refit(\n            forecaster            = forecaster,\n            y                     = y,\n            steps                 = steps,\n            metric                = metric,\n            initial_train_size    = initial_train_size,\n            exog                  = exog,\n            alpha                 = alpha,\n            interval              = interval,\n            verbose               = verbose,\n            show_progress         = show_progress\n        )\n\n    return metrics_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.grid_search_sarimax","title":"<code>grid_search_sarimax(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, return_best=True, verbose=True)</code>","text":"<p>Exhaustive search over specified parameter values for a ForecasterSarimax object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forcaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series values.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def grid_search_sarimax(\n    forecaster,\n    y: pd.Series,\n    param_grid: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Exhaustive search over specified parameter values for a ForecasterSarimax object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forcaster model.\n    y : pandas Series\n        Training time series values. \n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        verbose               = verbose\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection_sarimax.html#skforecast.model_selection_sarimax.model_selection_sarimax.random_search_sarimax","title":"<code>random_search_sarimax(forecaster, y, param_distributions, steps, metric, initial_train_size, fixed_train_size=True, gap=0, allow_incomplete_fold=True, exog=None, refit=False, n_iter=10, random_state=123, return_best=True, verbose=True)</code>","text":"<p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forcaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error'}</li> <li>If <code>Callable</code>: Function with arguments y_true, y_pred that returns  a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>initial_train_size</code> <code>int</code> <p>Number of samples in the initial train split. The backtest forecaster is trained using the first <code>initial_train_size</code> observations.</p> required <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration of backtesting.</p> <code>False</code> <code>n_iter</code> <code>int, default</code> <p>Number of parameter settings that are sampled.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool, default</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds used for cv or backtesting.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast/model_selection_sarimax/model_selection_sarimax.py</code> <pre><code>def random_search_sarimax(\n    forecaster,\n    y: pd.Series,\n    param_distributions: dict,\n    steps: int,\n    metric: Union[str, Callable, list],\n    initial_train_size: int,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    refit: bool=False,\n    n_iter: int=10,\n    random_state: int=123,\n    return_best: bool=True,\n    verbose: bool=True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forcaster model.\n    y : pandas Series\n        Training time series. \n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    steps : int\n        Number of steps to predict.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n            - If `string`: {'mean_squared_error', 'mean_absolute_error',\n             'mean_absolute_percentage_error', 'mean_squared_log_error'}\n            - If `Callable`: Function with arguments y_true, y_pred that returns \n            a float.\n            - If `list`: List containing multiple strings and/or Callables.\n    initial_train_size : int \n        Number of samples in the initial train split. The backtest forecaster is\n        trained using the first `initial_train_size` observations.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration of backtesting.\n    n_iter : int, default `10`\n        Number of parameter settings that are sampled. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default `123`\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default `True`\n        Refit the `forecaster` using the best found parameters on the whole data.\n    verbose : bool, default `True`\n        Print number of folds used for cv or backtesting.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n            - column params: parameters configuration for each iteration.\n            - column metric: metric value estimated for each iteration.\n            - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        param_grid            = param_grid,\n        steps                 = steps,\n        metric                = metric,\n        initial_train_size    = initial_train_size,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        allow_incomplete_fold = allow_incomplete_fold,\n        exog                  = exog,\n        refit                 = refit,\n        return_best           = return_best,\n        verbose               = verbose\n    )\n\n    return results\n</code></pre>"},{"location":"api/plot.html","title":"<code>plot</code>","text":""},{"location":"api/plot.html#skforecast.plot.plot.plot_residuals","title":"<code>plot_residuals(residuals=None, y_true=None, y_pred=None, fig=None, **fig_kw)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>pandas Series, numpy ndarray, default </code> <p>Values of residuals. If <code>None</code>, residuals are calculated internally using <code>y_true</code> and <code>y_true</code>.</p> <code>None</code> <code>y_true</code> <code>pandas Series, numpy ndarray, default </code> <p>Ground truth (correct) values. Ignored if residuals is not <code>None</code>.</p> <code>None</code> <code>y_pred</code> <code>pandas Series, numpy ndarray, default </code> <p>Values of predictions. Ignored if residuals is not <code>None</code>.</p> <code>None</code> <code>fig</code> <code>matplotlib.figure.Figure, default</code> <p>Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure() internally.</p> <code>None</code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.figure()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_residuals(\n    residuals: Union[np.ndarray, pd.Series]=None,\n    y_true: Union[np.ndarray, pd.Series]=None,\n    y_pred: Union[np.ndarray, pd.Series]=None,\n    fig: matplotlib.figure.Figure=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Parameters\n    ----------\n    residuals : pandas Series, numpy ndarray, default `None`.\n        Values of residuals. If `None`, residuals are calculated internally using\n        `y_true` and `y_true`.\n    y_true : pandas Series, numpy ndarray, default `None`.\n        Ground truth (correct) values. Ignored if residuals is not `None`.\n    y_pred : pandas Series, numpy ndarray, default `None`. \n        Values of predictions. Ignored if residuals is not `None`.\n    fig : matplotlib.figure.Figure, default `None`. \n        Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure()\n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.figure()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if residuals is None and (y_true is None or y_pred is None):\n        raise ValueError(\n            \"If `residuals` argument is None then, `y_true` and `y_pred` must be provided.\"\n        )\n\n    if residuals is None:\n        residuals = y_pred - y_true\n\n    if fig is None:\n        fig = plt.figure(constrained_layout=True, **fig_kw)\n\n    gs  = matplotlib.gridspec.GridSpec(2, 2, figure=fig)\n    ax1 = plt.subplot(gs[0, :])\n    ax2 = plt.subplot(gs[1, 0])\n    ax3 = plt.subplot(gs[1, 1])\n\n    ax1.plot(residuals)\n    sns.histplot(residuals, kde=True, bins=30, ax=ax2)\n    plot_acf(residuals, ax=ax3, lags=60)\n\n    ax1.set_title(\"Residuals\")\n    ax2.set_title(\"Distribution\")\n    ax3.set_title(\"Autocorrelation\")\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_multivariate_time_series_corr","title":"<code>plot_multivariate_time_series_corr(corr, ax=None, **fig_kw)</code>","text":"<p>Heatmap plot of a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>pandas DataFrame</code> <p>correlation matrix</p> required <code>ax</code> <code>matplotlib.axes.Axes, default</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()  internally.</p> <code>None</code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_multivariate_time_series_corr(\n    corr: pd.DataFrame,\n    ax: matplotlib.axes.Axes=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Heatmap plot of a correlation matrix.\n\n    Parameters\n    ----------\n    corr : pandas DataFrame\n        correlation matrix\n    ax : matplotlib.axes.Axes, default `None`. \n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() \n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n\n    sns.heatmap(\n        corr,\n        annot=True,\n        linewidths=.5,\n        ax=ax,\n        cmap=sns.color_palette(\"viridis\", as_cmap=True)\n    )\n\n    ax.set_xlabel('Time series')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_distribution","title":"<code>plot_prediction_distribution(bootstrapping_predictions, bw_method=None, **fig_kw)</code>","text":"<p>Ridge plot of bootstrapping predictions. This plot is very useful to understand  the uncertainty of forecasting predictions.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrapping_predictions</code> <code>pandas DataFrame</code> <p>Bootstrapping predictions created with <code>Forecaster.predict_bootstrapping</code>.</p> required <code>bw_method</code> <code>str, scalar, Callable, default</code> <p>The method used to calculate the estimator bandwidth. This can be 'scott',  'silverman', a scalar constant or a Callable. If None (default), 'scott'  is used. See scipy.stats.gaussian_kde for more information.</p> <code>None</code> <code>fig_kw</code> <code>dict</code> <p>All additional keyword arguments are passed to the <code>pyplot.figure</code> call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast/plot/plot.py</code> <pre><code>def plot_prediction_distribution(\n    bootstrapping_predictions: pd.DataFrame,\n    bw_method: Optional[Any]=None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Ridge plot of bootstrapping predictions. This plot is very useful to understand \n    the uncertainty of forecasting predictions.\n\n    Parameters\n    ----------\n    bootstrapping_predictions : pandas DataFrame\n        Bootstrapping predictions created with `Forecaster.predict_bootstrapping`.\n    bw_method : str, scalar, Callable, default `None`\n        The method used to calculate the estimator bandwidth. This can be 'scott', \n        'silverman', a scalar constant or a Callable. If None (default), 'scott' \n        is used. See scipy.stats.gaussian_kde for more information.\n    fig_kw : dict\n        All additional keyword arguments are passed to the `pyplot.figure` call.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    index = bootstrapping_predictions.index.astype(str).to_list()[::-1]\n    palette = sns.cubehelix_palette(len(index), rot=-.25, light=.7, reverse=False)\n    fig, axs = plt.subplots(len(index), 1, sharex=True, **fig_kw)\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    for i, step in enumerate(index):\n        plot = (\n            bootstrapping_predictions.loc[step, :]\n            .plot.kde(ax=axs[i], bw_method=bw_method, lw=0.5)\n        )\n\n        # Fill density area\n        x = plot.get_children()[0]._x\n        y = plot.get_children()[0]._y\n        axs[i].fill_between(x, y, color=palette[i])\n        prediction_mean = bootstrapping_predictions.loc[step, :].mean()\n\n        # Closest point on x to the prediction mean\n        idx = np.abs(x - prediction_mean).argmin()\n        axs[i].vlines(x[idx], ymin=0, ymax=y[idx], linestyle=\"dashed\", color='w')\n\n        axs[i].spines['top'].set_visible(False)\n        axs[i].spines['right'].set_visible(False)\n        axs[i].spines['bottom'].set_visible(False)\n        axs[i].spines['left'].set_visible(False)\n        axs[i].set_yticklabels([])\n        axs[i].set_yticks([])\n        axs[i].set_ylabel(step, rotation='horizontal')\n        axs[i].set_xlabel('prediction')\n\n    fig.subplots_adjust(hspace=-0)\n    fig.suptitle('Forecasting distribution per step')\n\n    return fig\n</code></pre>"},{"location":"api/utils.html","title":"<code>utils</code>","text":""},{"location":"api/utils.html#skforecast.utils.utils.save_forecaster","title":"<code>save_forecaster(forecaster, file_name, verbose=True)</code>","text":"<p>Save forecaster model using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <p>Forecaster created with skforecast library.</p> required <code>file_name</code> <code>str</code> <p>File name given to the object.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster saved.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def save_forecaster(\n    forecaster, \n    file_name: str, \n    verbose: bool=True\n) -&gt; None:\n\"\"\"\n    Save forecaster model using joblib.\n\n    Parameters\n    ----------\n    forecaster: forecaster\n        Forecaster created with skforecast library.\n    file_name: str\n        File name given to the object.\n    verbose: bool, default `True`\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    joblib.dump(forecaster, filename=file_name)\n\n    if verbose:\n        forecaster.summary()\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.load_forecaster","title":"<code>load_forecaster(file_name, verbose=True)</code>","text":"<p>Load forecaster model using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Object file name.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster loaded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>forecaster</code> <code>forecaster</code> <p>Forecaster created with skforecast library.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def load_forecaster(\n    file_name: str,\n    verbose: bool=True\n) -&gt; object:\n\"\"\"\n    Load forecaster model using joblib.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: forecaster\n        Forecaster created with skforecast library.\n\n    \"\"\"\n\n    forecaster = joblib.load(filename=file_name)\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_lags","title":"<code>initialize_lags(forecaster_name, lags)</code>","text":"<p>Check lags argument input and generate the corresponding numpy ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>lags</code> <code>Any</code> <p>Lags used as predictors.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -&gt; np.ndarray:\n\"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray\n        Lags used as predictors.\n\n    \"\"\"\n\n    if isinstance(lags, int) and lags &lt; 1:\n        raise ValueError(\"Minimum value of lags allowed is 1.\")\n\n    if isinstance(lags, (list, np.ndarray)):\n        for lag in lags:\n            if not isinstance(lag, (int, np.int64, np.int32)):\n                raise TypeError(\"All values in `lags` must be int.\")\n\n    if isinstance(lags, (list, range, np.ndarray)) and min(lags) &lt; 1:\n        raise ValueError(\"Minimum value of lags allowed is 1.\")\n\n    if isinstance(lags, int):\n        lags = np.arange(lags) + 1\n    elif isinstance(lags, (list, range)):\n        lags = np.array(lags)\n    elif isinstance(lags, np.ndarray):\n        lags = lags\n    else:\n        if not forecaster_name == 'ForecasterAutoregMultiVariate':\n            raise TypeError(\n                (\"`lags` argument must be an int, 1d numpy ndarray, range or list. \"\n                 f\"Got {type(lags)}.\")\n            )\n        else:\n            raise TypeError(\n                (\"`lags` argument must be a dict, int, 1d numpy ndarray, range or list. \"\n                 f\"Got {type(lags)}.\")\n            )\n\n    return lags\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_weights","title":"<code>initialize_weights(forecaster_name, regressor, weight_func, series_weights)</code>","text":"<p>Check weights arguments, <code>weight_func</code> and <code>series_weights</code> for the different  forecasters. Create <code>source_code_weight_func</code>, source code of the custom  function(s) used to create weights.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>Regressor of the forecaster.</p> required <code>weight_func</code> <code>Callable, dict</code> <p>Argument <code>weight_func</code> of the forecaster.</p> required <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> required <p>Returns:</p> Name Type Description <code>weight_func</code> <code>Callable, dict</code> <p>Argument <code>weight_func</code> of the forecaster.</p> <code>source_code_weight_func</code> <code>str, dict</code> <p>Argument <code>source_code_weight_func</code> of the forecaster.</p> <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -&gt; Tuple[Union[Callable, dict], Union[str, dict], dict]:\n\"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries', 'ForecasterAutoregMultiSeriesCustom']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_select_fit_kwargs","title":"<code>check_select_fit_kwargs(regressor, fit_kwargs=None)</code>","text":"<p>Check if <code>fit_kwargs</code> is a dict and select only the keys that are used by the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>object</code> <p>Regressor object.</p> required <code>fit_kwargs</code> <code>dict, default</code> <p>Dictionary with the arguments to pass to the `fit' method of the forecaster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to be passed to the <code>fit</code> method of the  regressor after removing the unused keys.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict]=None\n) -&gt; dict:\n\"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n\n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k:v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_y","title":"<code>check_y(y)</code>","text":"<p>Raise Exception if <code>y</code> is not pandas Series or if it has missing values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_y(\n    y: Any\n) -&gt; None:\n\"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n\n    Parameters\n    ----------\n    y : Any\n        Time series values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\"`y` must be a pandas Series.\")\n\n    if y.isnull().any():\n        raise ValueError(\"`y` has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog","title":"<code>check_exog(exog, allow_nan=True)</code>","text":"<p>Raise Exception if <code>exog</code> is not pandas Series or pandas DataFrame. If <code>allow_nan = True</code>, issue a warning if <code>exog</code> contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>Any</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool, default</code> <p>If True, allows the presence of NaN values in <code>exog</code>. If False (default), issue a warning if <code>exog</code> contains NaN values.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_exog(\n    exog: Any,\n    allow_nan: bool=True\n) -&gt; None:\n\"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n\n    Parameters\n    ----------\n    exog : Any\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\"`exog` must be a pandas Series or DataFrame.\")\n\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(\n                (\"`exog` has missing values. Most machine learning models do not allow \"\n                 \"missing values. Fitting the forecaster may fail.\"), \n                 MissingValuesExogWarning\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.get_exog_dtypes","title":"<code>get_exog_dtypes(exog)</code>","text":"<p>Store dtypes of <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Name Type Description <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with the dtypes in <code>exog</code>.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def get_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -&gt; dict:\n\"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n\n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog_dtypes","title":"<code>check_exog_dtypes(exog)</code>","text":"<p>Raise Exception if <code>exog</code> has categorical columns with non integer values. This is needed when using machine learning regressors that allow categorical features. Issue a Warning if <code>exog</code> has columns that are not <code>init</code>, <code>float</code>, or <code>category</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas DataFrame, pandas Series</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -&gt; None:\n\"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_exog(exog=exog, allow_nan=False)\n\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(\n                (\"`exog` may contain only `int`, `float` or `category` dtypes. Most \"\n                 \"machine learning models do not allow other types of values. \"\n                 \"Fitting the forecaster may fail.\"), DataTypeWarning\n            )\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError(\n                    (\"Categorical columns in exog must contain only integer values. \"\n                     \"See skforecast docs for more info about how to include \"\n                     \"categorical features https://skforecast.org/\"\n                     \"latest/user_guides/categorical-features.html\")\n                )\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', \n        'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(\n                (\"`exog` may contain only `int`, `float` or `category` dtypes. Most \"\n                 \"machine learning models do not allow other types of values. \"\n                 \"Fitting the forecaster may fail.\"), DataTypeWarning\n            )\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int,\n        np.int32, np.int64]:\n            raise TypeError(\n                (\"If exog is of type category, it must contain only integer values. \"\n                 \"See skforecast docs for more info about how to include \"\n                 \"categorical features https://skforecast.org/\"\n                 \"latest/user_guides/categorical-features.html\")\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_interval","title":"<code>check_interval(interval=None, alpha=None)</code>","text":"<p>Check provided confidence interval sequence is valid.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>float, default</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_interval(\n    interval: list=None,\n    alpha: float=None\n) -&gt; None:\n\"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError(\n                (\"`interval` must be a `list`. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                (\"`interval` must contain exactly 2 values, respectively the \"\n                 \"lower and upper interval bounds. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if (interval[0] &lt; 0.) or (interval[0] &gt;= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.) or (interval[1] &gt; 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                (\"`alpha` must be a `float`. For example, interval of 95% \"\n                 \"should be as `alpha = 0.05`.\")\n            )\n\n        if (alpha &lt;= 0.) or (alpha &gt;= 1):\n            raise ValueError(\n                f\"`alpha` must have a value between 0 and 1. Got {alpha}.\"\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_predict_input","title":"<code>check_predict_input(forecaster_name, steps, fitted, included_exog, index_type, index_freq, window_size, last_window=None, last_window_exog=None, exog=None, exog_type=None, exog_col_names=None, interval=None, alpha=None, max_steps=None, levels=None, series_col_names=None)</code>","text":"<p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom,  ForecasterAutoregDirect, ForecasterAutoregMultiSeries,  ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.</p> required <code>steps</code> <code>int, list</code> <p>Number of future steps predicted.</p> required <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> required <code>included_exog</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type</code> <code>type</code> <p>Type of index of the input used in training.</p> required <code>index_freq</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to  <code>max_lag</code>.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame, default </code> <p>Values of the series used to create the predictors (lags) need in the  first iteration of prediction (t + 1).</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Values of the exogenous variables aligned with <code>last_window</code> in  ForecasterSarimax predictions.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, default </code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_type</code> <code>type, default</code> <p>Type of exogenous variable/s used in training.</p> <code>None</code> <code>exog_col_names</code> <code>list, default</code> <p>Names of columns of <code>exog</code> if <code>exog</code> used in training was a pandas DataFrame.</p> <code>None</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>float, default</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <code>max_steps</code> <code>Optional[int]</code> <p>Maximum number of steps allowed (<code>ForecasterAutoregDirect</code> and  <code>ForecasterAutoregMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>str, list, default</code> <p>Time series to be predicted (<code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>).</p> <code>None</code> <code>series_col_names</code> <code>list, default</code> <p>Names of the columns used during fit (<code>ForecasterAutoregMultiSeries</code>,  <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, list],\n    fitted: bool,\n    included_exog: bool,\n    index_type: type,\n    index_freq: str,\n    window_size: int,\n    last_window: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n    exog_type: Optional[Union[type, None]]=None,\n    exog_col_names: Optional[Union[list, None]]=None,\n    interval: Optional[list]=None,\n    alpha: Optional[float]=None,\n    max_steps: Optional[int]=None,\n    levels: Optional[Union[str, list]]=None,\n    series_col_names: Optional[list]=None\n) -&gt; None:\n\"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name. ForecasterAutoreg, ForecasterAutoregCustom, \n        ForecasterAutoregDirect, ForecasterAutoregMultiSeries, \n        ForecasterAutoregMultiSeriesCustom, ForecasterAutoregMultiVariate.\n    steps : int, list\n        Number of future steps predicted.\n    fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    included_exog : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type : type\n        Type of index of the input used in training.\n    index_freq : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, default `None`\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_col_names : list, default `None`\n        Names of columns of `exog` if `exog` used in training was a pandas\n        DataFrame.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterAutoregDirect` and \n        `ForecasterAutoregMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterAutoregMultiSeries` and\n        `ForecasterAutoregMultiSeriesCustom`).\n    series_col_names : list, default `None`\n        Names of the columns used during fit (`ForecasterAutoregMultiSeries`, \n        `ForecasterAutoregMultiSeriesCustom` and `ForecasterAutoregMultiVariate`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not fitted:\n        raise sklearn.exceptions.NotFittedError(\n            (\"This Forecaster instance is not fitted yet. Call `fit` with \"\n             \"appropriate arguments before using predict.\")\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n           (f\"The minimum value of `steps` must be equal to or greater than 1. \"\n            f\"Got {min(steps)}.\")\n        )\n\n    if max_steps is not None:\n        if max(steps) &gt; max_steps:\n            raise ValueError(\n                (f\"The maximum value of `steps` must be less than or equal to \"\n                 f\"the value of steps defined when initializing the forecaster. \"\n                 f\"Got {max(steps)}, but the maximum is {max_steps}.\")\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterAutoregMultiSeriesCustom']:\n        if levels is not None and not isinstance(levels, (str, list)):\n            raise TypeError(\n                (\"`levels` must be a `list` of column names, a `str` of a \"\n                 \"column name or `None`.\")\n            )\n        if len(set(levels) - set(series_col_names)) != 0:\n            raise ValueError(\n                f\"`levels` must be in `series_col_names` : {series_col_names}.\"\n            )\n\n    if exog is None and included_exog:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. \"\n             \"Same variable/s must be provided when predicting.\")\n        )\n\n    if exog is not None and not included_exog:\n        raise ValueError(\n            (\"Forecaster trained without exogenous variable/s. \"\n             \"`exog` must be `None` when predicting.\")\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if forecaster_name in ['ForecasterAutoregMultiSeries', \n                           'ForecasterAutoregMultiSeriesCustom',\n                           'ForecasterAutoregMultiVariate']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        if forecaster_name in ['ForecasterAutoregMultiSeries', \n                               'ForecasterAutoregMultiSeriesCustom'] and \\\n            len(set(levels) - set(last_window.columns)) != 0:\n            raise ValueError(\n                (f\"`last_window` must contain a column(s) named as the level(s) \"\n                 f\"to be predicted.\\n\"\n                 f\"    `levels` : {levels}.\\n\"\n                 f\"    `last_window` columns : {list(last_window.columns)}.\")\n            )\n\n        if forecaster_name == 'ForecasterAutoregMultiVariate' and \\\n            (series_col_names != list(last_window.columns)):\n            raise ValueError(\n                (f\"`last_window` columns must be the same as `series` column names.\\n\"\n                 f\"    `last_window` columns : {list(last_window.columns)}.\\n\"\n                 f\"    `series` columns      : {series_col_names}.\")\n            )    \n    else:    \n        if not isinstance(last_window, pd.Series):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series. Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) &lt; window_size:\n        raise ValueError(\n            (f\"`last_window` must have as many values as needed to \"\n             f\"generate the predictors. For this forecaster it is {window_size}.\")\n        )\n    if last_window.isnull().any().all():\n        raise ValueError(\n            (\"`last_window` has missing values.\")\n        )\n    _, last_window_index = preprocess_last_window(\n                               last_window  = last_window.iloc[:0],\n                               return_values = False\n                           ) \n    if not isinstance(last_window_index, index_type):\n        raise TypeError(\n            (f\"Expected index of type {index_type} for `last_window`. \"\n             f\"Got {type(last_window_index)}.\")\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq:\n            raise TypeError(\n                (f\"Expected frequency of type {index_freq} for `last_window`. \"\n                 f\"Got {last_window_index.freqstr}.\")\n            )\n\n    # Checks exog\n    if exog is not None:\n        # Check type, nulls and expected type\n        if not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(\"`exog` must be a pandas Series or DataFrame.\")\n        if exog.isnull().any().any():\n            warnings.warn(\n                (\"`exog` has missing values. Most of machine learning models do \"\n                 \"not allow missing values. `predict` method may fail.\"), \n                 MissingValuesExogWarning\n            )\n        if not isinstance(exog, exog_type):\n            raise TypeError(\n                f\"Expected type for `exog`: {exog_type}. Got {type(exog)}.\"    \n            )\n\n        # Check exog has many values as distance to max step predicted\n        last_step = max(steps) if isinstance(steps, list) else steps\n        if len(exog) &lt; last_step:\n            raise ValueError(\n                (f\"`exog` must have at least as many values as the distance to \"\n                 f\"the maximum step predicted, {last_step}.\")\n            )\n\n        # Check all columns are in the pandas DataFrame\n        if isinstance(exog, pd.DataFrame):\n            col_missing = set(exog_col_names).difference(set(exog.columns))\n            if col_missing:\n                raise ValueError(\n                    (f\"Missing columns in `exog`. Expected {exog_col_names}. \"\n                     f\"Got {exog.columns.to_list()}.\") \n                )\n\n        # Check index dtype and freq\n        _, exog_index = preprocess_exog(\n                            exog          = exog.iloc[:0, ],\n                            return_values = False\n                        )\n        if not isinstance(exog_index, index_type):\n            raise TypeError(\n                (f\"Expected index of type {index_type} for `exog`. \"\n                 f\"Got {type(exog_index)}.\")\n            )   \n        if isinstance(exog_index, pd.DatetimeIndex):\n            if not exog_index.freqstr == index_freq:\n                raise TypeError(\n                    (f\"Expected frequency of type {index_freq} for `exog`. \"\n                     f\"Got {exog_index.freqstr}.\")\n                )\n\n        # Check exog starts one step ahead of last_window end.\n        expected_index = expand_index(last_window.index, 1)[0]\n        if expected_index != exog.index[0]:\n            raise ValueError(\n                (f\"To make predictions `exog` must start one step ahead of `last_window`.\\n\"\n                 f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                 f\"    `exog` starts at      : {exog.index[0]}.\\n\"\n                 f\"     Expected index       : {expected_index}.\")\n            )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not included_exog:\n                raise ValueError(\n                    (\"Forecaster trained without exogenous variable/s. \"\n                     \"`last_window_exog` must be `None` when predicting.\")\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`last_window_exog` must be a pandas Series or a \"\n                     f\"pandas DataFrame. Got {type(last_window_exog)}.\")\n                )\n            if len(last_window_exog) &lt; window_size:\n                raise ValueError(\n                    (f\"`last_window_exog` must have as many values as needed to \"\n                     f\"generate the predictors. For this forecaster it is {window_size}.\")\n                )\n            if last_window_exog.isnull().any().all():\n                warnings.warn(\n                (\"`last_window_exog` has missing values. Most of machine learning \"\n                 \"models do not allow missing values. `predict` method may fail.\"),\n                MissingValuesExogWarning\n            )\n            _, last_window_exog_index = preprocess_last_window(\n                                            last_window   = last_window_exog.iloc[:0],\n                                            return_values = False\n                                        ) \n            if not isinstance(last_window_exog_index, index_type):\n                raise TypeError(\n                    (f\"Expected index of type {index_type} for `last_window_exog`. \"\n                     f\"Got {type(last_window_exog_index)}.\")\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq:\n                    raise TypeError(\n                        (f\"Expected frequency of type {index_freq} for \"\n                         f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\")\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_col_names).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        (f\"Missing columns in `exog`. Expected {exog_col_names}. \"\n                         f\"Got {last_window_exog.columns.to_list()}.\") \n                    )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_y","title":"<code>preprocess_y(y, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <pre><code>- If index is of type `DatetimeIndex` and has frequency, nothing is changed.\n- If index is of type `RangeIndex`, nothing is changed.\n- If index is of type `DatetimeIndex` but has no frequency, a `RangeIndex` is created.\n- If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Time series.</p> required <code>return_values</code> <code>bool, default</code> <p>If <code>True</code> return the values of <code>y</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>y_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>y</code>.</p> <code>y_index</code> <code>pandas Index</code> <p>Index of <code>y</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_y(\n    y: pd.Series,\n    return_values: bool=True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n\"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n        - If index is of type `DatetimeIndex` and has frequency, nothing is \n        changed.\n        - If index is of type `RangeIndex`, nothing is changed.\n        - If index is of type `DatetimeIndex` but has no frequency, a \n        `RangeIndex` is created.\n        - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"`y` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"`y` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy() if return_values else None\n\n    return y_values, y_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_last_window","title":"<code>preprocess_last_window(last_window, return_values=True)</code>","text":"<p>Return values and index of series separately. Index is overwritten  according to the next rules:</p> <pre><code>- If index is of type `DatetimeIndex` and has frequency, nothing is changed.\n- If index is of type `RangeIndex`, nothing is changed.\n- If index is of type `DatetimeIndex` but has no frequency, a `RangeIndex` is created.\n- If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Time series values.</p> required <code>return_values</code> <code>bool, default</code> <p>If <code>True</code> return the values of <code>last_window</code> as numpy ndarray. This option  is intended to avoid copying data when it is not necessary.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>last_window_values</code> <code>numpy ndarray</code> <p>Numpy array with values of <code>last_window</code>.</p> <code>last_window_index</code> <code>pandas Index</code> <p>Index of <code>last_window</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_last_window(\n    last_window: Union[pd.Series, pd.DataFrame],\n    return_values: bool=True\n ) -&gt; Tuple[np.ndarray, pd.Index]:\n\"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n\n        - If index is of type `DatetimeIndex` and has frequency, nothing is \n        changed.\n        - If index is of type `RangeIndex`, nothing is changed.\n        - If index is of type `DatetimeIndex` but has no frequency, a \n        `RangeIndex` is created.\n        - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn(\n            (\"`last_window` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n    else:\n        warnings.warn(\n            (\"`last_window` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n\n    last_window_values = last_window.to_numpy() if return_values else None\n\n    return last_window_values, last_window_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_exog","title":"<code>preprocess_exog(exog, return_values=True)</code>","text":"<p>Return values and index of series or data frame separately. Index is overwritten  according to the next rules:</p> <pre><code>- If index is of type `DatetimeIndex` and has frequency, nothing is changed.\n- If index is of type `RangeIndex`, nothing is changed.\n- If index is of type `DatetimeIndex` but has no frequency, a `RangeIndex` is created.\n- If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>return_values</code> <code>bool, default</code> <p>If <code>True</code> return the values of <code>exog</code> as numpy ndarray. This option is  intended to avoid copying data when it is not necessary.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>exog_values</code> <code>None, numpy ndarray</code> <p>Numpy array with values of <code>exog</code>.</p> <code>exog_index</code> <code>pandas Index</code> <p>Index of <code>exog</code> modified according to the rules.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def preprocess_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    return_values: bool=True\n) -&gt; Tuple[Union[None, np.ndarray], pd.Index]:\n\"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n\n        - If index is of type `DatetimeIndex` and has frequency, nothing is \n        changed.\n        - If index is of type `RangeIndex`, nothing is changed.\n        - If index is of type `DatetimeIndex` but has no frequency, a \n        `RangeIndex` is created.\n        - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n\n    \"\"\"\n\n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn(\n            (\"`exog` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    else:\n        warnings.warn(\n            (\"`exog` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    exog_values = exog.to_numpy() if return_values else None\n\n    return exog_values, exog_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.cast_exog_dtypes","title":"<code>cast_exog_dtypes(exog, exog_dtypes)</code>","text":"<p>Cast <code>exog</code> to a specified types. This is done because, for a forecaster to  accept a categorical exog, it must contain only integer values. Due to the  internal modifications of numpy, the values may be casted to <code>float</code>, so  they have to be re-converted to <code>int</code>.</p> <pre><code>- If `exog` is a pandas Series, `exog_dtypes` must be a dict with a single value.\n- If `exog_dtypes` is `category` but the current type of `exog` is `float`, then the type is cast to `int` and then to `category`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with name and type of the series or data frame columns.</p> required <p>Returns:</p> Name Type Description <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables casted to the indicated dtypes.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def cast_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    exog_dtypes: dict,\n) -&gt; Union[pd.Series, pd.DataFrame]: # pragma: no cover\n\"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n        - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n        single value.\n        - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n        then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k:v for k, v in exog_dtypes.items() if k in exog.columns}\n\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes==float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct","title":"<code>exog_to_direct(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>pandas DataFrame</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def exog_to_direct(\n    exog: Union[pd.Series, pd.DataFrame],\n    steps: int\n)-&gt; pd.DataFrame:\n\"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : pandas DataFrame\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_column_transformed.index = pd.RangeIndex(len(exog_column_transformed))\n        exog_column_transformed.columns = [f\"{col}_step_{i+1}\" \n                                           for col in exog_column_transformed.columns]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = pd.concat(exog_transformed, axis=1, copy=False)\n    else:\n        exog_transformed = exog_column_transformed\n\n    exog_transformed.index = exog_idx[-len(exog_transformed):]\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct_numpy","title":"<code>exog_to_direct_numpy(exog, steps)</code>","text":"<p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>numpy ndarray, shape(samples,)</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int.</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_transformed</code> <code>numpy ndarray</code> <p>Exogenous variables transformed.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray,\n    steps: int\n)-&gt; np.ndarray:\n\"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : numpy ndarray, shape(samples,)\n        Exogenous variables.\n    steps : int.\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_transformed : numpy ndarray\n        Exogenous variables transformed.\n\n    \"\"\"\n\n    if not isinstance(exog, np.ndarray):\n        raise TypeError(f\"`exog` must be a numpy ndarray. Got {type(exog)}.\")\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_transformed = []\n\n    for i in range(steps):\n        exog_column_transformed = exog[i : n_rows - (steps - 1 - i)]\n        exog_transformed.append(exog_column_transformed)\n\n    if len(exog_transformed) &gt; 1:\n        exog_transformed = np.concatenate(exog_transformed, axis=1)\n    else:\n        exog_transformed = exog_column_transformed\n\n    return exog_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.expand_index","title":"<code>expand_index(index, steps)</code>","text":"<p>Create a new index of length <code>steps</code> starting at the end of the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index, None</code> <p>Original index.</p> required <code>steps</code> <code>int</code> <p>Number of steps to expand.</p> required <p>Returns:</p> Name Type Description <code>new_index</code> <code>pandas Index</code> <p>New index.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def expand_index(\n    index: Union[pd.Index, None], \n    steps: int\n) -&gt; pd.Index:\n\"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n\n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if isinstance(index, pd.Index):\n\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n    else: \n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n\n    return new_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_series","title":"<code>transform_series(series, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas Series with a scikit-learn alike transformer (preprocessor). The transformer used must have the following methods: fit,  transform, fit_transform and inverse_transform. ColumnTransformers are not  allowed since they do not have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series</code> <p>Series to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer (preprocessor).</code> <p>scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed  since they do not have inverse_transform method.</p> required <code>fit</code> <code>bool, default</code> <p>Train the transformer before applying it.</p> <code>False</code> <code>inverse_transform</code> <code>bool, default</code> <p>Transform back the data to the original representation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>series_transformed</code> <code>pandas Series, pandas DataFrame</code> <p>Transformed Series. Depending on the transformer used, the output may  be a Series or a DataFrame.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def transform_series(\n    series: pd.Series,\n    transformer,\n    fit: bool=False,\n    inverse_transform: bool=False\n) -&gt; Union[pd.Series, pd.DataFrame]:\n\"\"\"      \n    Transform raw values of pandas Series with a scikit-learn alike transformer\n    (preprocessor). The transformer used must have the following methods: fit, \n    transform, fit_transform and inverse_transform. ColumnTransformers are not \n    allowed since they do not have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer (preprocessor).\n        scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform. ColumnTransformers are not allowed \n        since they do not have inverse_transform method.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n\n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            (f\"`series` argument must be a pandas Series. Got {type(series)}.\")\n        )\n\n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n\n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    if inverse_transform:\n        values_transformed = transformer.inverse_transform(data)\n    else:\n        values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.flatten(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_dataframe","title":"<code>transform_dataframe(df, transformer, fit=False, inverse_transform=False)</code>","text":"<p>Transform raw values of pandas DataFrame with a scikit-learn alike transformer, preprocessor or ColumnTransformer. <code>inverse_transform</code> is not  available when using ColumnTransformers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor or ColumnTransformer.</code> <p>scikit-learn alike transformer, preprocessor or ColumnTransformer.</p> required <code>fit</code> <code>bool, default</code> <p>Train the transformer before applying it.</p> <code>False</code> <code>inverse_transform</code> <code>bool, default</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df_transformed</code> <code>pandas DataFrame</code> <p>Transformed DataFrame.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer,\n    fit: bool=False,\n    inverse_transform: bool=False\n) -&gt; pd.DataFrame:\n\"\"\"      \n    Transform raw values of pandas DataFrame with a scikit-learn alike\n    transformer, preprocessor or ColumnTransformer. `inverse_transform` is not \n    available when using ColumnTransformers.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor or ColumnTransformer.\n        scikit-learn alike transformer, preprocessor or ColumnTransformer.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise Exception(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_optional_dependency","title":"<code>check_optional_dependency(package_name)</code>","text":"<p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_optional_dependency(\n    package_name: str\n) -&gt; None:\n\"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.multivariate_time_series_corr","title":"<code>multivariate_time_series_corr(time_series, other, lags, method='pearson')</code>","text":"<p>Compute correlation between a time_series and the lagged values of other  time series. </p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>pandas Series</code> <p>Target time series.</p> required <code>other</code> <code>pandas DataFrame</code> <p>Time series whose lagged values are correlated to <code>time_series</code>.</p> required <code>lags</code> <code>int, list, numpy ndarray</code> <p>Lags to be included in the correlation analysis.</p> required <code>method</code> <code>str, default 'pearson'</code> <ul> <li>'pearson': standard correlation coefficient.</li> <li>'kendall': Kendall Tau correlation coefficient.</li> <li>'spearman': Spearman rank correlation.</li> </ul> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>corr</code> <code>pandas DataFrame</code> <p>Correlation values.</p> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: Union[int, list, np.array],\n    method: str='pearson'\n)-&gt; pd.DataFrame:\n\"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n\n    return corr\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_backtesting_input","title":"<code>check_backtesting_input(forecaster, steps, metric, y=None, series=None, initial_train_size=None, fixed_train_size=True, gap=0, allow_incomplete_fold=True, refit=False, interval=None, alpha=None, n_boot=500, random_state=123, in_sample_residuals=True, verbose=False, show_progress=True)</code>","text":"<p>This is a helper function to check most inputs of backtesting functions in  modules <code>model_selection</code>, <code>model_selection_multiseries</code> and  <code>model_selection_sarimax</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>object</code> <p>Forecaster model.</p> required <code>steps</code> <code>int, list</code> <p>Number of future steps predicted.</p> required <code>metric</code> <code>str, Callable, list</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>pandas DataFrame</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>initial_train_size</code> <code>int, default</code> <p>Number of samples in the initial train split. If <code>None</code> and <code>forecaster</code>  is already trained, no initial train is done and all data is used to  evaluate the model.</p> <code>None</code> <code>fixed_train_size</code> <code>bool, default</code> <p>If True, train size doesn't increase but moves by <code>steps</code> in each iteration.</p> <code>True</code> <code>gap</code> <code>int, default</code> <p>Number of samples to be excluded after the end of each training set and  before the test set.</p> <code>0</code> <code>allow_incomplete_fold</code> <code>bool, default</code> <p>Last fold is allowed to have a smaller number of samples than the  <code>test_size</code>. If <code>False</code>, the last fold is excluded.</p> <code>True</code> <code>refit</code> <code>bool, default</code> <p>Whether to re-fit the forecaster in each iteration.</p> <code>False</code> <code>interval</code> <code>list, default</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive.</p> <code>None</code> <code>alpha</code> <code>float, default</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <code>n_boot</code> <code>int, default</code> <p>Number of bootstrapping iterations used to estimate prediction intervals.</p> <code>500</code> <code>random_state</code> <code>int, default</code> <p>Sets a seed to the random generator, so that boot intervals are always  deterministic.</p> <code>123</code> <code>in_sample_residuals</code> <code>bool, default</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>True</code> <code>verbose</code> <code>bool, default</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast/utils/utils.py</code> <pre><code>def check_backtesting_input(\n    forecaster: object,\n    steps: int,\n    metric: Union[str, Callable, list],\n    y: Optional[pd.Series]=None,\n    series: Optional[pd.DataFrame]=None,\n    initial_train_size: Optional[int]=None,\n    fixed_train_size: bool=True,\n    gap: int=0,\n    allow_incomplete_fold: bool=True,\n    refit: bool=False,\n    interval: Optional[list]=None,\n    alpha: Optional[float]=None,\n    n_boot: int=500,\n    random_state: int=123,\n    in_sample_residuals: bool=True,\n    verbose: bool=False,\n    show_progress: bool=True\n) -&gt; None:\n\"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`, `model_selection_multiseries` and \n    `model_selection_sarimax`.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    steps : int, list\n        Number of future steps predicted.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    y : pandas Series\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame\n        Training time series for multi-series forecasters.\n    initial_train_size : int, default `None`\n        Number of samples in the initial train split. If `None` and `forecaster` \n        is already trained, no initial train is done and all data is used to \n        evaluate the model.\n    fixed_train_size : bool, default `True`\n        If True, train size doesn't increase but moves by `steps` in each iteration.\n    gap : int, default `0`\n        Number of samples to be excluded after the end of each training set and \n        before the test set.\n    allow_incomplete_fold : bool, default `True`\n        Last fold is allowed to have a smaller number of samples than the \n        `test_size`. If `False`, the last fold is excluded.\n    refit : bool, default `False`\n        Whether to re-fit the forecaster in each iteration.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `500`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.  \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress: bool, default `True`\n        Whether to show a progress bar. Defaults to True.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecasters_uni = ['ForecasterAutoreg', 'ForecasterAutoregCustom', \n                       'ForecasterAutoregDirect', 'ForecasterSarimax']\n    forecasters_multi = ['ForecasterAutoregMultiSeries', \n                         'ForecasterAutoregMultiSeriesCustom', \n                         'ForecasterAutoregMultiVariate']\n\n    if type(forecaster).__name__ in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    if type(forecaster).__name__ in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n\n    if not isinstance(steps, (int, np.integer)) or steps &lt; 1:\n        raise TypeError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n    if not isinstance(gap, (int, np.integer)) or gap &lt; 0:\n        raise TypeError(\n            f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n        )\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            (f\"`metric` must be a string, a callable function, or a list containing \"\n             f\"multiple strings and/or callables. Got {type(metric)}.\")\n        )\n\n    if initial_train_size is not None:\n        if not isinstance(initial_train_size, (int, np.integer)):\n            raise TypeError(\n                (f\"If used, `initial_train_size` must be an integer greater than the \"\n                 f\"window_size of the forecaster. Got type {type(initial_train_size)}.\")\n            )\n        if initial_train_size &gt;= data_length:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer smaller \"\n                 f\"than the length of `{data_name}` ({data_length}).\")\n            )    \n        if initial_train_size &lt; forecaster.window_size:\n            raise ValueError(\n                (f\"If used, `initial_train_size` must be an integer greater than \"\n                 f\"the window_size of the forecaster ({forecaster.window_size}).\")\n            )\n        if initial_train_size + gap &gt;= data_length:\n            raise ValueError(\n                (f\"The combination of initial_train_size {initial_train_size} and \"\n                 f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                 f\"({data_length}).\")\n            )\n    else:\n        if type(forecaster).__name__ == 'ForecasterSarimax':\n            raise ValueError(\n                (f\"`initial_train_size` must be an integer smaller than the \"\n                 f\"length of `{data_name}` ({data_length}).\")\n            )    \n        else:\n            if not forecaster.fitted:\n                raise NotFittedError(\n                    (\"`forecaster` must be already trained if no `initial_train_size` \"\n                     \"is provided.\")\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if not isinstance(fixed_train_size, bool):\n        raise TypeError(\"`fixed_train_size` must be a boolean: `True`, `False`.\")\n    if not isinstance(allow_incomplete_fold, bool):\n        raise TypeError(\"`allow_incomplete_fold` must be a boolean: `True`, `False`.\")\n    if not isinstance(refit, bool):\n        raise TypeError(\"`refit` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot &lt; 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state &lt; 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(in_sample_residuals, bool):\n        raise TypeError(\"`in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(verbose, bool):\n        raise TypeError(\"`verbose` must be a boolean: `True`, `False`.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) &lt; steps:\n        raise ValueError(\n            (f\"There is not enough data to evaluate {steps} steps in a single \"\n             f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n             f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n             f\"    Steps                   : {steps}\")\n        )\n\n    return\n</code></pre>"},{"location":"examples/examples.html","title":"Examples and tutorials","text":""},{"location":"examples/examples.html#english","title":"English","text":"<p> Skforecast: time series forecasting with Python and Scikit-learn </p> <p> Forecasting electricity demand with Python </p> <p> Forecasting web traffic with machine learning and Python </p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost </p> <p> Bitcoin price prediction with Python</p> <p> Prediction intervals in forecasting models</p> <p> Multi-series forecasting</p> <p> Reducing the influence of Covid-19 on time series forecasting models</p> <p> Forecasting time series with missing values</p> <p> Intermittent demand forecasting</p> <p></p>"},{"location":"examples/examples.html#espanol","title":"Espa\u00f1ol","text":"<p> Skforecast: forecasting series temporales con Python y Scikit-learn </p> <p> Forecasting de la demanda el\u00e9ctrica</p> <p> Forecasting de las visitas a una p\u00e1gina web </p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost </p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p> <p> Intervalos de predicci\u00f3n en modelos de forecasting</p> <p> Multi-series forecasting</p> <p> Predicci\u00f3n demanda intermitente</p>"},{"location":"faq/cyclical-features-time-series.html","title":"Cyclical features in time series","text":"<p>Cyclical features play an important role in time series prediction because they capture recurring patterns or oscillations within a data set. These patterns repeat at fixed intervals, and effective incorporation of cyclical features into a machine learning model requires careful pre-processing and feature engineering.</p> <p>Due to the circular nature of cyclical features, it is not recommended to use them directly as numerical inputs in a machine learning model. Instead, they should be encoded in a format that captures their cyclical behaviour. There are several common encoding techniques:</p> <ul> <li><p>One-hot encoding: If the cyclical feature consists of distinct categories, such as seasons or months, one-hot encoding can be used. This approach creates binary variables for each category, allowing the model to understand the presence or absence of specific categories.</p> </li> <li><p>Trigonometric coding: For periodic features such as time of day or day of week, trigonometric functions such as sine and cosine can be used for coding. By mapping the cyclic feature onto a unit circle, these functions preserve the cyclic relationships. In addition, this method introduces only two additional features, making it an efficient coding technique.</p> </li> <li><p>Basis functions: Basis functions are mathematical functions that span a vector space and can be used to represent other functions within that space. When using basis functions, the cyclic feature is transformed into a new set of features based on the chosen basis functions. Some commonly used basis functions for encoding cyclic features include Fourier basis functions, B-spline basis functions, and Gaussian basis functions. B-splines are a way of approximating non-linear functions using a piecewise combination of polynomials.</p> </li> </ul> <p>By applying these encoding techniques, cyclic features can be effectively incorporated into a machine learning model, allowing it to capture and exploit the valuable recurring patterns present in time series data.</p> <p> \ud83d\udd89 Note </p> <p>The following examples are is inspired by Time-related feature engineering, scikit-lego\u2019s documentation and Three Approaches to Encoding Time Information as Features for ML Models By Eryk Lewinson.</p> In\u00a0[2]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklego.preprocessing import RepeatingBasisFunction\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n\n# Warnings configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt plt.style.use('seaborn-v0_8-darkgrid')  # Modelling and Forecasting # ============================================================================== from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import SplineTransformer from sklearn.compose import make_column_transformer from sklego.preprocessing import RepeatingBasisFunction from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster  # Warnings configuration # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[3]: Copied! <pre># Data simulation\n# ==============================================================================\nnp.random.seed(123)\ndates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\")\ndata = pd.DataFrame(index=dates)\ndata.index.name = \"date\"\ndata[\"day_idx\"] = range(len(data))\ndata['month'] = data.index.month\n\n# Create the components that will be combined to get the target series\nsignal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi)\nsignal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2)\nnoise = np.random.normal(0, 0.85, len(data))\ny = signal_1 + signal_2 + noise\n\ndata[\"y\"] = y\ndata = data[[\"y\", \"month\"]]\ndata.head(3)\n</pre> # Data simulation # ============================================================================== np.random.seed(123) dates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\") data = pd.DataFrame(index=dates) data.index.name = \"date\" data[\"day_idx\"] = range(len(data)) data['month'] = data.index.month  # Create the components that will be combined to get the target series signal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi) signal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365/2) noise = np.random.normal(0, 0.85, len(data)) y = signal_1 + signal_2 + noise  data[\"y\"] = y data = data[[\"y\", \"month\"]] data.head(3) Out[3]: y month date 2020-01-01 2.928244 1 2020-01-02 4.866145 1 2020-01-03 4.425159 1 In\u00a0[4]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2022-06-30 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2022-06-30 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-01-01 00:00:00 --- 2022-06-30 00:00:00  (n=912)\nDates test  : 2022-07-01 00:00:00 --- 2023-12-31 00:00:00  (n=549)\n</pre> In\u00a0[5]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(9, 3))\ndata_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax)\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax=plt.subplots(figsize=(9, 3)) data_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax) ax.legend(); In\u00a0[6]: Copied! <pre># One hot encoding of week_day and hour_day\n# ==============================================================================\none_hot_encoder = make_column_transformer(\n                        (\n                            OneHotEncoder(sparse_output=False, drop='if_binary'),\n                            ['month'],\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n\ndata_encoded_oh = one_hot_encoder.fit_transform(data)\ndata_encoded_oh.head(3)\n</pre> # One hot encoding of week_day and hour_day # ============================================================================== one_hot_encoder = make_column_transformer(                         (                             OneHotEncoder(sparse_output=False, drop='if_binary'),                             ['month'],                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\")  data_encoded_oh = one_hot_encoder.fit_transform(data) data_encoded_oh.head(3) Out[6]: month_1 month_2 month_3 month_4 month_5 month_6 month_7 month_8 month_9 month_10 month_11 month_12 y date 2020-01-01 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.928244 2020-01-02 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.866145 2020-01-03 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.425159 In\u00a0[7]: Copied! <pre># Cyclical encoding with sine/cosine transformation\n# ==============================================================================\ndef sin_transformer(period):\n\"\"\"\n\tReturns a transformer that applies sine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\ndef cos_transformer(period):\n\"\"\"\n\tReturns a transformer that applies cosine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n\ndata_encoded_sin_cos = data.copy()\ndata_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos.head()\n</pre> # Cyclical encoding with sine/cosine transformation # ============================================================================== def sin_transformer(period): \t\"\"\" \tReturns a transformer that applies sine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))  def cos_transformer(period): \t\"\"\" \tReturns a transformer that applies cosine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))  data_encoded_sin_cos = data.copy() data_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos.head() Out[7]: y month month_sin month_cos date 2020-01-01 2.928244 1 0.5 0.866025 2020-01-02 4.866145 1 0.5 0.866025 2020-01-03 4.425159 1 0.5 0.866025 2020-01-04 3.069222 1 0.5 0.866025 2020-01-05 4.021290 1 0.5 0.866025 In\u00a0[8]: Copied! <pre># Plot of the transformation\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(4., 3.5))\nsp = ax.scatter(\n        data_encoded_sin_cos[\"month_sin\"],\n        data_encoded_sin_cos[\"month_cos\"],\n        c=data_encoded_sin_cos[\"month\"],\n        cmap='viridis'\n     )\nax.set(\n    xlabel=\"sin(month)\",\n    ylabel=\"cos(month)\",\n)\n_ = fig.colorbar(sp)\ndata_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month')\n</pre> # Plot of the transformation # ============================================================================== fig, ax = plt.subplots(figsize=(4., 3.5)) sp = ax.scatter(         data_encoded_sin_cos[\"month_sin\"],         data_encoded_sin_cos[\"month_cos\"],         c=data_encoded_sin_cos[\"month\"],         cmap='viridis'      ) ax.set(     xlabel=\"sin(month)\",     ylabel=\"cos(month)\", ) _ = fig.colorbar(sp) data_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month') In\u00a0[9]: Copied! <pre># Create feature day of year\n# ==============================================================================\ndata['day_of_year'] = data.index.day_of_year\ndata.head(3)\n</pre> # Create feature day of year # ============================================================================== data['day_of_year'] = data.index.day_of_year data.head(3) Out[9]: y month day_of_year date 2020-01-01 2.928244 1 1 2020-01-02 4.866145 1 2 2020-01-03 4.425159 1 3 In\u00a0[10]: Copied! <pre># B-spline functions\n# ==============================================================================\ndef spline_transformer(period, degree=3, extrapolation=\"periodic\"):\n\"\"\"\n    Returns a transformer that applies B-spline transformation.\n    \"\"\"\n    return SplineTransformer(\n                degree=degree,\n                n_knots=period+1,\n                knots='uniform',\n                extrapolation=extrapolation,\n                include_bias=True\n            ).set_output(transform=\"pandas\")\n\nsplines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']])\nsplines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))]\n</pre> # B-spline functions # ============================================================================== def spline_transformer(period, degree=3, extrapolation=\"periodic\"):     \"\"\"     Returns a transformer that applies B-spline transformation.     \"\"\"     return SplineTransformer(                 degree=degree,                 n_knots=period+1,                 knots='uniform',                 extrapolation=extrapolation,                 include_bias=True             ).set_output(transform=\"pandas\")  splines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']]) splines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))] <p>The graph below shows the 12 spline functions generated using the day of the year as input. As 12 splines are created with knots evenly distributed along the range 1 to 365 (day_of_year), each curve represents the proximity to the beginning of a particular month.</p> In\u00a0[25]: Copied! <pre># Location of the maximum value of each spline\n# ==============================================================================\nsplines_month.idxmax()\n</pre> # Location of the maximum value of each spline # ============================================================================== splines_month.idxmax() Out[25]: <pre>0    2020-01-01\n1    2020-01-31\n2    2020-03-02\n3    2020-04-01\n4    2020-05-01\n5    2020-06-01\n6    2020-07-01\n7    2020-07-31\n8    2020-08-31\n9    2020-09-30\n10   2020-10-30\n11   2020-11-30\ndtype: datetime64[ns]</pre> In\u00a0[24]: Copied! <pre># Plot of the B-splines functions for the first 365 days\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 5))\nsplines_month.head(365).plot(\n    ax       = ax,\n    subplots = True,\n    sharex   = True,\n    legend   = False,\n    yticks   = [],\n    title    = 'Splines functions for the first 365 days'\n);\n</pre> # Plot of the B-splines functions for the first 365 days # ============================================================================== fig, ax = plt.subplots(figsize=(8, 5)) splines_month.head(365).plot(     ax       = ax,     subplots = True,     sharex   = True,     legend   = False,     yticks   = [],     title    = 'Splines functions for the first 365 days' ); In\u00a0[13]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_splines = pd.concat([data, splines_month], axis=1)\ndata_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month'])\ndata_encoded_splines.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_splines = pd.concat([data, splines_month], axis=1) data_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month']) data_encoded_splines.head(3) Out[13]: y spline0 spline1 spline2 spline3 spline4 spline5 spline6 spline7 spline8 spline9 spline10 spline11 date 2020-01-01 2.928244 0.166667 0.666667 0.166667 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 4.866145 0.150763 0.665604 0.183628 0.000006 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 4.425159 0.135904 0.662485 0.201563 0.000047 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[28]: Copied! <pre># Radial basis functions\n# ==============================================================================\nrbf = RepeatingBasisFunction(\n            n_periods   = 12,\n            remainder   = 'drop',\n            column      = 'day_of_year',\n            input_range = (1, 365)\n        )\nrbf.fit(data)\nrbf_month = rbf.transform(data)\nrbf_month = pd.DataFrame(\n                data    = rbf_month,\n                index   = data.index,\n                columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]\n            )\nrbf_month.head(3)\n</pre> # Radial basis functions # ============================================================================== rbf = RepeatingBasisFunction(             n_periods   = 12,             remainder   = 'drop',             column      = 'day_of_year',             input_range = (1, 365)         ) rbf.fit(data) rbf_month = rbf.transform(data) rbf_month = pd.DataFrame(                 data    = rbf_month,                 index   = data.index,                 columns = [f\"rbf_{i}\" for i in range(splines_month.shape[1])]             ) rbf_month.head(3) Out[28]: rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[29]: Copied! <pre># Location of the maximum value of each rbf\n# ==============================================================================\nrbf_month.idxmax()\n</pre> # Location of the maximum value of each rbf # ============================================================================== rbf_month.idxmax() Out[29]: <pre>rbf_0    2020-01-01\nrbf_1    2020-01-31\nrbf_2    2020-03-02\nrbf_3    2020-04-01\nrbf_4    2020-05-01\nrbf_5    2020-06-01\nrbf_6    2020-07-01\nrbf_7    2020-07-31\nrbf_8    2020-08-31\nrbf_9    2020-09-30\nrbf_10   2020-10-30\nrbf_11   2020-11-30\ndtype: datetime64[ns]</pre> In\u00a0[30]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_rbf = pd.concat([data, rbf_month], axis=1)\ndata_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month'])\ndata_encoded_rbf.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_rbf = pd.concat([data, rbf_month], axis=1) data_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month']) data_encoded_rbf.head(3) Out[30]: y rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 2.928244 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 4.866145 0.998914 0.392526 0.020875 0.000150 1.463375e-07 1.929034e-11 3.441402e-16 9.976816e-12 8.635293e-08 0.000101 0.016035 0.344032 2020-01-03 4.425159 0.995662 0.417914 0.023740 0.000183 1.898798e-07 2.673609e-11 5.094813e-16 7.151579e-12 6.611833e-08 0.000083 0.014009 0.321032 In\u00a0[32]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags = [70]\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags = [70]              ) In\u00a0[33]: Copied! <pre># Train and validate a forecaster using each encoding method\n# ==============================================================================\ndatasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,\n            data_encoded_rbf]\nencoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',\n                    'data_encoded_rbf']\nfig, ax = plt.subplots(figsize=(8, 4))\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\n\nfor i, data_encoded in enumerate(datasets):\n\n    metric, predictions = backtesting_forecaster(\n                              forecaster         = forecaster,\n                              y                  = data_encoded['y'],\n                              exog               = data_encoded.drop(columns='y'),\n                              initial_train_size = len(data_encoded.loc[:end_train]),\n                              fixed_train_size   = False,\n                              steps              = 365,\n                              refit              = False,\n                              metric             = 'mean_squared_error',\n                              verbose            = False, # Change to True to see detailed information,\n                              show_progress      = False\n                          )\n\n    print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")\n    predictions.plot(label=encoding_methods[i], ax=ax)\n    ax.legend(labels=['test'] + encoding_methods);\n</pre> # Train and validate a forecaster using each encoding method # ============================================================================== datasets = [data_encoded_oh, data_encoded_sin_cos, data_encoded_splines,             data_encoded_rbf] encoding_methods = ['one hot encoding', 'sine/cosine encoding', 'spline encoding',                     'data_encoded_rbf'] fig, ax = plt.subplots(figsize=(8, 4)) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)  for i, data_encoded in enumerate(datasets):      metric, predictions = backtesting_forecaster(                               forecaster         = forecaster,                               y                  = data_encoded['y'],                               exog               = data_encoded.drop(columns='y'),                               initial_train_size = len(data_encoded.loc[:end_train]),                               fixed_train_size   = False,                               steps              = 365,                               refit              = False,                               metric             = 'mean_squared_error',                               verbose            = False, # Change to True to see detailed information,                               show_progress      = False                           )      print(f\"Backtest error using {encoding_methods[i]}: {metric:.2f}\")     predictions.plot(label=encoding_methods[i], ax=ax)     ax.legend(labels=['test'] + encoding_methods); <pre>Backtest error using one hot encoding: 1.10\nBacktest error using sine/cosine encoding: 1.12\nBacktest error using spline encoding: 0.75\nBacktest error using data_encoded_rbf: 0.74\n</pre> <p>%%html</p>"},{"location":"faq/cyclical-features-time-series.html#cyclical-features-in-time-series-forecasting","title":"Cyclical features in time series forecasting\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#data","title":"Data\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#one-hot-encoding","title":"One hot encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#sine-cosine-encoding","title":"Sine-Cosine encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#b-splines-functions","title":"B-splines functions\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#radial-basis-functions-rbf","title":"Radial basis functions (RBF)\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#compare-forecasting-results","title":"Compare forecasting results\u00b6","text":"<p>A non-informative lag is included so that the impact of cyclical features can be assessed without being obscured by the autoregressive component.</p>"},{"location":"faq/faq.html","title":"Frequently Asked Questions and forecasting tips","text":"<p>Thank you for choosing skforecast and visiting our Frequently Asked Questions (FAQ) page. Here, we aim to provide solutions to commonly encountered issues on our Github repository. If your question is not answered on this page, we encourage you to create a new issue on our Github so that it can be addressed, and other users can also benefit from it. Additionally, we have included some forecasting tips to help you get the most out of skforecast.</p> <ul> <li> <p>Avoid negative predictions when forecasting</p> </li> <li> <p>Forecasting time series with missing values</p> </li> <li> <p>Cyclical features in time series</p> </li> <li> <p>Profiling skforecast</p> </li> </ul>"},{"location":"faq/forecasting-time-series-with-missing-values.html","title":"Forecasting time series with missing values","text":"<p>In many real use cases of forecasting, although historical data are available, it is common for the time series to be incomplete. The presence of missing values in the data is a major problem since most forecasting algorithms require the time series to be complete in order to train a model.</p> <p>A commonly employed strategy to overcome this problem is to impute missing values before training the model, for example, using a moving average. However, the quality of the imputations may not be good, impairing the training of the model. One way to improve the imputation strategy is to combine it with weighted time series forecasting. The latter consists of reducing the weight of the imputed observations and thus their influence during model training.</p> <p>This document shows two examples of how skforecast makes it easy to apply this strategy.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\ndark_style = {\n    'figure.facecolor': '#212946',\n    'axes.facecolor': '#212946',\n    'savefig.facecolor':'#212946',\n    'axes.grid': True,\n    'axes.grid.which': 'both',\n    'axes.spines.left': False,\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'axes.spines.bottom': False,\n    'grid.color': '#2A3459',\n    'grid.linewidth': '1',\n    'text.color': '0.9',\n    'axes.labelcolor': '0.9',\n    'xtick.color': '0.9',\n    'ytick.color': '0.9',\n    'font.size': 12,\n    'lines.linewidth': 1.5\n}\nplt.rcParams.update(dark_style)\n\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt  plt.style.use('fivethirtyeight') dark_style = {     'figure.facecolor': '#212946',     'axes.facecolor': '#212946',     'savefig.facecolor':'#212946',     'axes.grid': True,     'axes.grid.which': 'both',     'axes.spines.left': False,     'axes.spines.right': False,     'axes.spines.top': False,     'axes.spines.bottom': False,     'grid.color': '#2A3459',     'grid.linewidth': '1',     'text.color': '0.9',     'axes.labelcolor': '0.9',     'xtick.color': '0.9',     'ytick.color': '0.9',     'font.size': 12,     'lines.linewidth': 1.5 } plt.rcParams.update(dark_style)  from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster <p>  \u00a0 Note </p> <p>In this document, a forecaster of type <code>ForecasterAutoreg</code> is used. The same strategy can be applied with any forecaster from skforecast.</p> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n    'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d')\ndata = data[['fecha', 'Usos bicis total d\u00eda']]\ndata.columns = ['date', 'users']\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Data download # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'     'Estadistica-machine-learning-python/master/data/usuarios_diarios_bicimad.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d') data = data[['fecha', 'Usos bicis total d\u00eda']] data.columns = ['date', 'users'] data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head(3) Out[2]: users date 2014-06-23 99 2014-06-24 72 2014-06-25 119 In\u00a0[3]: Copied! <pre># Generating gaps with missing values\n# ==============================================================================\ngaps = [\n    ['2020-09-01', '2020-10-10'],\n    ['2020-11-08', '2020-12-15'],\n]\n\nfor gap in gaps:\n    data.loc[gap[0]:gap[1]] = np.nan\n</pre> # Generating gaps with missing values # ============================================================================== gaps = [     ['2020-09-01', '2020-10-10'],     ['2020-11-08', '2020-12-15'], ]  for gap in gaps:     data.loc[gap[0]:gap[1]] = np.nan In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2020-06-01': '2021-06-01']\nend_train = '2021-03-01'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2020-06-01': '2021-06-01'] end_train = '2021-03-01' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-06-01 00:00:00 --- 2021-03-01 00:00:00  (n=274)\nDates test  : 2021-03-01 00:00:00 --- 2021-06-01 00:00:00  (n=93)\n</pre> In\u00a0[5]: Copied! <pre># Time series plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(12, 4))\ndata_train.users.plot(ax=ax, label='train', linewidth=1)\ndata_test.users.plot(ax=ax, label='test', linewidth=1)\n\nfor gap in gaps:\n    ax.plot(\n        [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],\n        [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],\n         data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],\n        color = 'red',\n        linestyle = '--',\n        label = 'gap'\n        )\n\nax.set_title('Number of users BiciMAD')\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax.legend(by_label.values(), by_label.keys(), loc='lower right');\n</pre> # Time series plot # ============================================================================== fig, ax = plt.subplots(figsize=(12, 4)) data_train.users.plot(ax=ax, label='train', linewidth=1) data_test.users.plot(ax=ax, label='test', linewidth=1)  for gap in gaps:     ax.plot(         [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],         [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],          data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],         color = 'red',         linestyle = '--',         label = 'gap'         )  ax.set_title('Number of users BiciMAD') handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) ax.legend(by_label.values(), by_label.keys(), loc='lower right'); In\u00a0[6]: Copied! <pre># Value imputation using linear interpolation\n# ======================================================================================\ndata['users_imputed'] = data['users'].interpolate(method='linear')\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n</pre> # Value imputation using linear interpolation # ====================================================================================== data['users_imputed'] = data['users'].interpolate(method='linear') data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :] <p>To minimize the influence on the model of imputed values, a custom function is defined to create weights following the rules:</p> <ul> <li><p>Weight of 0 if the index date has been imputed or is within 14 days ahead of an imputed day.</p> </li> <li><p>Weight of 1 otherwise.</p> </li> </ul> <p>If an observation has a weight of 0, it has no influence at all during model training.</p> <p>  \u00a0 Note </p> <p>Imputed values should neither participate in the training process as a target nor as a predictor (lag). Therefore, values within a window size as large as the lags used should also be excluded.</p> In\u00a0[7]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is in any gap.\n    \"\"\"\n    gaps = [\n        ['2020-09-01', '2020-10-10'],\n        ['2020-11-08', '2020-12-15'],\n    ]\n    \n    missing_dates = [pd.date_range(\n                        start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),\n                        end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),\n                        freq  = 'D'\n                    ) for gap in gaps]\n    missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))   \n    weights = np.where(index.isin(missing_dates), 0, 1)\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is in any gap.     \"\"\"     gaps = [         ['2020-09-01', '2020-10-10'],         ['2020-11-08', '2020-12-15'],     ]          missing_dates = [pd.date_range(                         start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),                         end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),                         freq  = 'D'                     ) for gap in gaps]     missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))        weights = np.where(index.isin(missing_dates), 0, 1)      return weights <p><code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[8]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = LGBMRegressor(random_state=123),\n                 lags        = 14,\n                 weight_func = custom_weights\n             )\n\n# Backtesting: predict the next 7 days at a time.\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                            forecaster         = forecaster,\n                            y                  = data.users_imputed,\n                            initial_train_size = len(data.loc[:end_train]),\n                            fixed_train_size   = False,\n                            steps              = 7,\n                            metric             = 'mean_absolute_error',\n                            refit              = True,\n                            verbose            = False\n                        )\nprint(f\"Backtesting metric (mean_absolute_error): {metric:.2f}\")\npredictions.head(4)\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = LGBMRegressor(random_state=123),                  lags        = 14,                  weight_func = custom_weights              )  # Backtesting: predict the next 7 days at a time. # ============================================================================== metric, predictions = backtesting_forecaster(                             forecaster         = forecaster,                             y                  = data.users_imputed,                             initial_train_size = len(data.loc[:end_train]),                             fixed_train_size   = False,                             steps              = 7,                             metric             = 'mean_absolute_error',                             refit              = True,                             verbose            = False                         ) print(f\"Backtesting metric (mean_absolute_error): {metric:.2f}\") predictions.head(4) <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric (mean_absolute_error): 1904.83\n</pre> Out[8]: pred 2021-03-02 10524.159747 2021-03-03 10087.283682 2021-03-04 8882.926166 2021-03-05 9474.810215 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/forecasting-time-series-with-missing-values.html#forecasting-time-series-with-missing-values","title":"Forecasting time series with missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#data","title":"Data\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#impute-missing-values","title":"Impute missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#give-weight-of-zero-to-imputed-values","title":"Give weight of zero to imputed values\u00b6","text":""},{"location":"faq/non-negative-predictions.html","title":"Avoid negative predictions when forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 from sklearn.linear_model import Ridge from sklearn.preprocessing import FunctionTransformer from sklearn.metrics import mean_squared_error from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'\n       'learning-python/master/data/bike_sharing_dataset_clean.csv')\ndata = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000)\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('H')\ndata = data.sort_index()\n</pre> # Downloading data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'        'learning-python/master/data/bike_sharing_dataset_clean.csv') data = pd.read_csv(url, usecols=['date_time', 'users'], nrows=1000) data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('H') data = data.sort_index() In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2011-01-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2011-01-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train: 2011-01-01 00:00:00 --- 2011-01-31 23:00:00  (n=744)\nDates test : 2011-02-01 00:00:00 --- 2011-02-11 15:00:00  (n=256)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(11, 4))\ndata_train['users'].plot(ax=ax, label='train')\ndata_test['users'].plot(ax=ax, label='test')\nax.set_title('Number of users')\nax.legend();\n</pre> # Plot time series # ============================================================================== fig, ax = plt.subplots(figsize=(11, 4)) data_train['users'].plot(ax=ax, label='train') data_test['users'].plot(ax=ax, label='test') ax.set_title('Number of users') ax.legend(); In\u00a0[5]: Copied! <pre># Create forecaster and train\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24\n             )\n</pre> # Create forecaster and train # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags      = 24              ) In\u00a0[6]: Copied! <pre># Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(11, 4))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(11, 4)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>The graph above shows that some predictions are negative.</p> In\u00a0[8]: Copied! <pre># Negative predictions\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions predictions[predictions.pred &lt; 0] Out[8]: pred 2011-02-01 00:00:00 -23.337236 2011-02-01 01:00:00 -17.691405 2011-02-02 00:00:00 -5.243456 2011-02-02 01:00:00 -19.363139 2011-02-03 01:00:00 -6.441943 2011-02-04 01:00:00 -10.579940 2011-02-05 00:00:00 -6.026119 2011-02-05 01:00:00 -21.396841 2011-02-07 04:00:00 -3.412043 2011-02-07 05:00:00 -3.701964 2011-02-08 00:00:00 -17.045913 2011-02-08 01:00:00 -13.233004 2011-02-09 00:00:00 -25.315228 2011-02-09 01:00:00 -24.743686 2011-02-10 01:00:00 -5.704407 2011-02-11 01:00:00 -11.758940 In\u00a0[9]: Copied! <pre># Transform data into logarithmic scale\n# =============================================================================\ndata_log = np.log1p(data)\ndata_train_log = np.log1p(data_train)\ndata_test_log  = np.log1p(data_test)\n</pre> # Transform data into logarithmic scale # ============================================================================= data_log = np.log1p(data) data_train_log = np.log1p(data_train) data_test_log  = np.log1p(data_test) In\u00a0[10]: Copied! <pre># Create forecaster and train\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 lags          = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data_log['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_log.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n</pre> # Create forecaster and train # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  lags          = 24,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data_log['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_log.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Revert the transformation\n# ==============================================================================\npredictions = np.expm1(predictions)\npredictions.head(4)\n</pre> # Revert the transformation # ============================================================================== predictions = np.expm1(predictions) predictions.head(4) Out[11]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 In\u00a0[12]: Copied! <pre>metric = mean_squared_error(y_true=data_test['users'], y_pred=predictions)\nprint(f\"Backtesting metric: {metric}\")\n</pre> metric = mean_squared_error(y_true=data_test['users'], y_pred=predictions) print(f\"Backtesting metric: {metric}\") <pre>Backtesting metric: 1991.9332571759892\n</pre> In\u00a0[13]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(11, 4))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(11, 4)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation. If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[14]: Copied! <pre># Create custom transformer\n# =============================================================================\ntransformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n\n# Create forecaster and train\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 24,\n                 transformer_y    = transformer_y,\n                 transformer_exog = None\n             )\nforecaster.fit(data['users'])\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['users'],\n                          steps                 = 24,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = False,\n                          verbose               = False,\n                          show_progress         = True  \n                      )\n\nprint(f\"Backtesting metric: {metric}\")\npredictions.head()\n</pre> # Create custom transformer # ============================================================================= transformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)  # Create forecaster and train # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 24,                  transformer_y    = transformer_y,                  transformer_exog = None              ) forecaster.fit(data['users'])  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['users'],                           steps                 = 24,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = False,                           verbose               = False,                           show_progress         = True                         )  print(f\"Backtesting metric: {metric}\") predictions.head() <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting metric: 1991.9332571759892\n</pre> Out[14]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 2011-02-01 04:00:00 3.865169 In\u00a0[15]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/non-negative-predictions.html#avoid-negative-predictions-when-forecasting","title":"Avoid negative predictions when forecasting\u00b6","text":"<p>When training a forecasting model, even though none of the training observations is negative, some predictions may turn out to be negative. To avoid this, it is possible to model the series on a logarithmic scale.</p>"},{"location":"faq/non-negative-predictions.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/non-negative-predictions.html#data","title":"Data\u00b6","text":""},{"location":"faq/non-negative-predictions.html#forecaster","title":"Forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#modeling-time-series-in-logarithmic-scale","title":"Modeling time series in logarithmic scale\u00b6","text":""},{"location":"faq/non-negative-predictions.html#include-a-logarithmic-transformer-as-part-of-the-forecaster","title":"Include a logarithmic transformer as part of the forecaster\u00b6","text":""},{"location":"faq/profiling-skforecast.html","title":"Profiling skforecast","text":"<p>This document shows the profiling of the main classes, methods and functions available in skforecast. Understanding the bottlenecks will help to:</p> <ul> <li>Use it more efficiently</li> <li>Improve the code for future releases</li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\n%load_ext pyinstrument\n</pre> # Libraries # ============================================================================== import time import numpy as np import pandas as pd import matplotlib.pyplot as plt  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import backtesting_forecaster  from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import HistGradientBoostingRegressor from lightgbm import LGBMRegressor  %load_ext pyinstrument <p>A time series of length 1000 with random values is created.</p> In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\nnp.random.seed(123)\nn = 1_000\ndata = pd.Series(data = np.random.normal(size=n))\n</pre> # Data # ============================================================================== np.random.seed(123) n = 1_000 data = pd.Series(data = np.random.normal(size=n)) <p>To isolate the training process of the regressor from the other parts of the code, a dummy regressor class is created. This dummy regressor has a fit method that does nothing, and a predict method that returns a constant value.</p> In\u00a0[3]: Copied! <pre>class DummyRegressor(LinearRegression):\n\"\"\"\n    Dummy regressor with dummy fit and predict methods.\n    \"\"\"\n    \n    def fit(self, X, y):\n        pass\n\n    def predict(self, y):\n        predictions = np.ones(shape = len(y))\n        return predictions\n</pre> class DummyRegressor(LinearRegression):     \"\"\"     Dummy regressor with dummy fit and predict methods.     \"\"\"          def fit(self, X, y):         pass      def predict(self, y):         predictions = np.ones(shape = len(y))         return predictions          In\u00a0[4]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) <p>Almost all of the time spent by <code>fit</code> is required by the <code>create_train_X_y</code> method.</p> In\u00a0[5]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) <p>When training a forecaster with a real machine learning regressor, the time spent by <code>create_train_X_y</code> is negligible compared to the time needed by the <code>fit</code> method of the regressor. Therefore, improving the speed of <code>create_train_X_y</code> will not have much impact.</p> <p>Understand how the <code>create_train_X_y</code> method is influenced by the length of the series and the number of lags.</p> In\u00a0[6]: Copied! <pre># Profiling `create_train_X_y` for different length of series and number of lags\n# ======================================================================================\nseries_length = np.linspace(1000, 1000000, num=5, dtype=int)\nn_lags = [5, 10, 50, 100, 200]\nresults = {}\n\nfor lags in n_lags:\n    execution_time = []\n    forecaster = ForecasterAutoreg(\n                     regressor = DummyRegressor(),\n                     lags      = lags\n                 )\n\n    for n in series_length:\n        y = pd.Series(data = np.random.normal(size=n))\n        tic = time.perf_counter()\n        _ = forecaster.create_train_X_y(y=y)\n        toc = time.perf_counter()\n        execution_time.append(toc-tic)\n\n    results[lags] = execution_time\n\nresults = pd.DataFrame(\n              data =  results,\n              index = series_length\n          )\n\nresults\n</pre> # Profiling `create_train_X_y` for different length of series and number of lags # ====================================================================================== series_length = np.linspace(1000, 1000000, num=5, dtype=int) n_lags = [5, 10, 50, 100, 200] results = {}  for lags in n_lags:     execution_time = []     forecaster = ForecasterAutoreg(                      regressor = DummyRegressor(),                      lags      = lags                  )      for n in series_length:         y = pd.Series(data = np.random.normal(size=n))         tic = time.perf_counter()         _ = forecaster.create_train_X_y(y=y)         toc = time.perf_counter()         execution_time.append(toc-tic)      results[lags] = execution_time  results = pd.DataFrame(               data =  results,               index = series_length           )  results Out[6]: 5 10 50 100 200 1000 0.000486 0.003027 0.018650 0.029583 0.050531 250750 0.005871 0.008976 0.167111 0.300689 0.295633 500500 0.017531 0.034498 0.307830 0.587562 1.058620 750250 0.023754 0.064929 0.532906 0.953699 1.715181 1000000 0.023783 0.133474 0.691882 1.385749 2.270863 In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 4))\nresults.plot(ax=ax, marker='.')\nax.set_xlabel('length of series')\nax.set_ylabel('time (seconds)')\nax.set_title('Profiling create_train_X_y()')\nax.legend(title='number of lags');\n</pre> fig, ax = plt.subplots(figsize=(7, 4)) results.plot(ax=ax, marker='.') ax.set_xlabel('length of series') ax.set_ylabel('time (seconds)') ax.set_title('Profiling create_train_X_y()') ax.legend(title='number of lags'); In\u00a0[8]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[9]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) In\u00a0[10]: Copied! <pre>forecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[11]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) <p>Inside the <code>predict</code> method, the <code>append</code> action is the most expensive but, similar to what happen with <code>fit</code>, it is negligible compared to the time need by the <code>predict</code> method of the regressor.</p> In\u00a0[12]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/profiling-skforecast.html#profiling-skforecast","title":"Profiling skforecast\u00b6","text":""},{"location":"faq/profiling-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/profiling-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"faq/profiling-skforecast.html#dummy-regressor","title":"Dummy regressor\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-fit","title":"Profiling fit\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-create_train_x_y","title":"Profiling create_train_X_y\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-predict","title":"Profiling predict\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html","title":"Single vs multi series forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from statsmodels.graphics.tsaplots import plot_acf  from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries In\u00a0[3]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' +\n       'data/vic_elec.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation (aggregation at daily level)\n# ==============================================================================\ndata['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ')\ndata = data.set_index('Time')\ndata = data.asfreq('30min')\ndata = data.sort_index()\ndata = data.drop(columns='Date')\ndata = data.resample(rule='H', closed='left', label ='right')\\\n       .agg({'Demand': 'sum', 'Temperature': 'mean'})\n\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' +        'data/vic_elec.csv') data = pd.read_csv(url, sep=',')  # Data preparation (aggregation at daily level) # ============================================================================== data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ') data = data.set_index('Time') data = data.asfreq('30min') data = data.sort_index() data = data.drop(columns='Date') data = data.resample(rule='H', closed='left', label ='right')\\        .agg({'Demand': 'sum', 'Temperature': 'mean'})  data.head() Out[3]: Demand Temperature Time 2011-12-31 14:00:00 8646.190700 21.225 2011-12-31 15:00:00 7926.529376 20.625 2011-12-31 16:00:00 7901.826990 20.325 2011-12-31 17:00:00 7255.721350 19.850 2011-12-31 18:00:00 6792.503352 19.025 In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2014-06-01 00:00:00': '2014-07-31 23:59:00']\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2014-06-01 00:00:00': '2014-07-31 23:59:00'] end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Train dates : 2014-06-01 00:00:00 --- 2014-07-15 23:00:00  (n=1080)\nTest dates  : 2014-07-16 00:00:00 --- 2014-07-31 23:00:00  (n=384)\n</pre> In\u00a0[5]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['Demand'].plot(label='train', ax=axes[0])\ndata_test['Demand'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_train['Temperature'].plot(label='train', ax=axes[1])\ndata_test['Temperature'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('\u00baC')\naxes[1].set_title('Temperature')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)  data_train['Demand'].plot(label='train', ax=axes[0]) data_test['Demand'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_train['Temperature'].plot(label='train', ax=axes[1]) data_test['Temperature'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('\u00baC') axes[1].set_title('Temperature')  fig.tight_layout() plt.show(); In\u00a0[6]: Copied! <pre># Autocorrelation plot\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\nplot_acf(data['Demand'], ax=axes[0], lags=72)\naxes[0].set_title('Demand')\nplot_acf(data['Temperature'], ax=axes[1], lags=72)\naxes[1].set_title('Temperature')\nplt.show()\n</pre> # Autocorrelation plot # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True) plot_acf(data['Demand'], ax=axes[0], lags=72) axes[0].set_title('Demand') plot_acf(data['Temperature'], ax=axes[1], lags=72) axes[1].set_title('Temperature') plt.show() <p>For this first study, the forecasting performance of a multi-series forecaster (non-multivariate) will be compared with that of single forecasters. For all of them, a linear model will be used as regressor.</p> <p>First, an individual autoregressive forecaster is trained for each series and their forecasting capability is measured with backtesting.</p> In\u00a0[7]: Copied! <pre># ForecasterAutoreg with linear models\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = LinearRegression(),\n                 lags             = 24,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nmetric_demand, predictions_demand = backtesting_forecaster(\n                                        forecaster         = forecaster,\n                                        y                  = data['Demand'],\n                                        initial_train_size = len(data_train),\n                                        fixed_train_size   = True,\n                                        steps              = 24,\n                                        metric             = 'mean_absolute_error',\n                                        refit              = True,\n                                        verbose            = False\n                                )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster(\n                                                    forecaster         = forecaster,\n                                                    y                  = data['Temperature'],\n                                                    initial_train_size = len(data_train),\n                                                    fixed_train_size   = True,\n                                                    steps              = 24,\n                                                    metric             = 'mean_absolute_error',\n                                                    refit              = True,\n                                                    verbose            = False\n                                              )\n</pre> # ForecasterAutoreg with linear models # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = LinearRegression(),                  lags             = 24,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  metric_demand, predictions_demand = backtesting_forecaster(                                         forecaster         = forecaster,                                         y                  = data['Demand'],                                         initial_train_size = len(data_train),                                         fixed_train_size   = True,                                         steps              = 24,                                         metric             = 'mean_absolute_error',                                         refit              = True,                                         verbose            = False                                 )  metric_temperature, predictions_temperature = backtesting_forecaster(                                                     forecaster         = forecaster,                                                     y                  = data['Temperature'],                                                     initial_train_size = len(data_train),                                                     fixed_train_size   = True,                                                     steps              = 24,                                                     metric             = 'mean_absolute_error',                                                     refit              = True,                                                     verbose            = False                                               )  In\u00a0[8]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_test['Demand'].plot(label='test', ax=axes[0])\npredictions_demand.plot(label='pred', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_test['Temperature'].plot(label='test', ax=axes[1])\npredictions_temperature.plot(label='pred', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('\u00baC')\naxes[1].set_title('Temperature')\naxes[0].legend()\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)  data_test['Demand'].plot(label='test', ax=axes[0]) predictions_demand.plot(label='pred', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_test['Temperature'].plot(label='test', ax=axes[1]) predictions_temperature.plot(label='pred', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('\u00baC') axes[1].set_title('Temperature') axes[0].legend()  fig.tight_layout() plt.show(); In\u00a0[9]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics individual forecasters')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\nprint(f\"Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics individual forecasters') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") print(f\"Temperature: {metric_temperature:.2f}\") <pre>Backtesting metrics individual forecasters\n===================================================\nDemand: 571.22\nTemperature: 1.88\n</pre> <p>Now, a multi-series (non-multivariate) forecaster is trained to model both series at the same time. As for the single forecasters, its performance is measured with backtesting.</p> In\u00a0[10]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LinearRegression(),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster_multiseries(\n                                                 forecaster         = forecaster,\n                                                 series             = data[['Demand', 'Temperature']],\n                                                 level              = 'Temperature',\n                                                 initial_train_size = len(data_train),\n                                                 fixed_train_size   = True,\n                                                 steps              = 24,\n                                                 metric             = 'mean_absolute_error',\n                                                 refit              = True,\n                                                 verbose            = False\n                                              )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LinearRegression(),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     )  metric_temperature, predictions_temperature = backtesting_forecaster_multiseries(                                                  forecaster         = forecaster,                                                  series             = data[['Demand', 'Temperature']],                                                  level              = 'Temperature',                                                  initial_train_size = len(data_train),                                                  fixed_train_size   = True,                                                  steps              = 24,                                                  metric             = 'mean_absolute_error',                                                  refit              = True,                                                  verbose            = False                                               ) In\u00a0[11]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics using a multi-series forecaster')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\nprint(f\"Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics using a multi-series forecaster') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") print(f\"Temperature: {metric_temperature:.2f}\") <pre>Backtesting metrics using a multi-series forecaster\n===================================================\nDemand: 571.41\nTemperature: 1.78\n</pre> <p>The performance of both approaches is very similar, then, using a multi-series model has the advantage of maintaining onl one model instead of two.</p> <p>In the previous example, both time series had a similar dynamic: the current value is highly correlated with the lags 1, 2, 3, 24, 48. What would happen if one the series do not share the same internal dynamics?</p> <p>To better understand the behavior of the multi-series forecaster in this scenario, the variable temperature is replaced with random noise and then, the forecaster is fitted and evaluated.</p> In\u00a0[12]: Copied! <pre># Copy data and modify Temperature column as a normal distribution\n# ==============================================================================\nnp.random.seed(123)\ndata_modified = data.copy()\ndata_modified['Temperature'] = np.random.normal(size=len(data_modified), loc=10, scale=1)\n\n# Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train_modified = data_modified.loc[:end_train, :].copy()\ndata_test_modified  = data_modified.loc[end_train:, :].copy()\n\nprint(f\"Train dates : {data_train_modified.index.min()} --- {data_train_modified.index.max()}  (n={len(data_train_modified)})\")\nprint(f\"Test dates  : {data_test_modified.index.min()} --- {data_test_modified.index.max()}  (n={len(data_test_modified)})\")\n\n# Plot Temperature\n# ==============================================================================\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True)\ndata_train_modified['Demand'].plot(label='train', ax=axes[0])\ndata_test_modified['Demand'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('MW')\naxes[0].set_title('Energy demand')\naxes[0].legend()\n\ndata_train_modified['Temperature'].plot(label='train', ax=axes[1])\ndata_test_modified['Temperature'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('MW')\naxes[1].set_title('Energy demand')\naxes[1].legend()\naxes[1].legend()\nplt.show();\n</pre> # Copy data and modify Temperature column as a normal distribution # ============================================================================== np.random.seed(123) data_modified = data.copy() data_modified['Temperature'] = np.random.normal(size=len(data_modified), loc=10, scale=1)  # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train_modified = data_modified.loc[:end_train, :].copy() data_test_modified  = data_modified.loc[end_train:, :].copy()  print(f\"Train dates : {data_train_modified.index.min()} --- {data_train_modified.index.max()}  (n={len(data_train_modified)})\") print(f\"Test dates  : {data_test_modified.index.min()} --- {data_test_modified.index.max()}  (n={len(data_test_modified)})\")  # Plot Temperature # ============================================================================== fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 5), sharex=True) data_train_modified['Demand'].plot(label='train', ax=axes[0]) data_test_modified['Demand'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('MW') axes[0].set_title('Energy demand') axes[0].legend()  data_train_modified['Temperature'].plot(label='train', ax=axes[1]) data_test_modified['Temperature'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('MW') axes[1].set_title('Energy demand') axes[1].legend() axes[1].legend() plt.show(); <pre>Train dates : 2014-06-01 00:00:00 --- 2014-07-15 23:00:00  (n=1080)\nTest dates  : 2014-07-16 00:00:00 --- 2014-07-31 23:00:00  (n=384)\n</pre> In\u00a0[13]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = LinearRegression(),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data_modified[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train_modified),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = LinearRegression(),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data_modified[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train_modified),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     ) In\u00a0[14]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint('Backtesting metrics using a multi-series forecaster')\nprint('===================================================')\nprint(f\"Demand: {metric_demand:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print('Backtesting metrics using a multi-series forecaster') print('===================================================') print(f\"Demand: {metric_demand:.2f}\") <pre>Backtesting metrics using a multi-series forecaster\n===================================================\nDemand: 793.37\n</pre> <p>The error of the model has increased remarkably. This is due two the fact that the model is constrained to learn a unique global pattern for both series. Since the time series have very different internal dynamics (relation between their current and past values) the model is no capable of learning a pattern that suits both series.</p> <p>The same experiment is repeated, but this time using a non lineal model.</p> In\u00a0[15]: Copied! <pre># ForecasterAutoreg with non linear models\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags             = 24,\n                 #transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nmetric_demand, predictions_demand = backtesting_forecaster(\n                                        forecaster         = forecaster,\n                                        y                  = data['Demand'],\n                                        initial_train_size = len(data_train),\n                                        fixed_train_size   = True,\n                                        steps              = 24,\n                                        metric             = 'mean_absolute_error',\n                                        refit              = True,\n                                        verbose            = False\n                                )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster(\n                                                    forecaster         = forecaster,\n                                                    y                  = data['Temperature'],\n                                                    initial_train_size = len(data_train),\n                                                    fixed_train_size   = True,\n                                                    steps              = 24,\n                                                    metric             = 'mean_absolute_error',\n                                                    refit              = True,\n                                                    verbose            = False\n                                              )\n</pre> # ForecasterAutoreg with non linear models # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags             = 24,                  #transformer_y    = StandardScaler(),                  transformer_exog = None              )  metric_demand, predictions_demand = backtesting_forecaster(                                         forecaster         = forecaster,                                         y                  = data['Demand'],                                         initial_train_size = len(data_train),                                         fixed_train_size   = True,                                         steps              = 24,                                         metric             = 'mean_absolute_error',                                         refit              = True,                                         verbose            = False                                 )  metric_temperature, predictions_temperature = backtesting_forecaster(                                                     forecaster         = forecaster,                                                     y                  = data['Temperature'],                                                     initial_train_size = len(data_train),                                                     fixed_train_size   = True,                                                     steps              = 24,                                                     metric             = 'mean_absolute_error',                                                     refit              = True,                                                     verbose            = False                                               )  In\u00a0[16]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\nprint(f\"Backtest error Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") print(f\"Backtest error Temperature: {metric_temperature:.2f}\") <pre>Backtest error Demand: 632.25\nBacktest error Temperature: 2.16\n</pre> In\u00a0[17]: Copied! <pre># ForecasterAutoregMultiSeries with non linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 #transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n\nmetric_temperature, predictions_temperature = backtesting_forecaster_multiseries(\n                                                 forecaster         = forecaster,\n                                                 series             = data[['Demand', 'Temperature']],\n                                                 level              = 'Temperature',\n                                                 initial_train_size = len(data_train),\n                                                 fixed_train_size   = True,\n                                                 steps              = 24,\n                                                 metric             = 'mean_absolute_error',\n                                                 refit              = True,\n                                                 verbose            = False\n                                              )\n</pre> # ForecasterAutoregMultiSeries with non linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  #transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     )  metric_temperature, predictions_temperature = backtesting_forecaster_multiseries(                                                  forecaster         = forecaster,                                                  series             = data[['Demand', 'Temperature']],                                                  level              = 'Temperature',                                                  initial_train_size = len(data_train),                                                  fixed_train_size   = True,                                                  steps              = 24,                                                  metric             = 'mean_absolute_error',                                                  refit              = True,                                                  verbose            = False                                               ) In\u00a0[18]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\nprint(f\"Backtest error Temperature: {metric_temperature:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") print(f\"Backtest error Temperature: {metric_temperature:.2f}\") <pre>Backtest error Demand: 664.99\nBacktest error Temperature: 2.01\n</pre> <p>Again, the performance of moth approaches are very similar. Let's see what happen if the resies do not share the same dinamycs.</p> In\u00a0[24]: Copied! <pre># ForecasterAutoregMultiSeries with linear models\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\n# Backtesting\n# ==============================================================================\nmetric_demand, predictions_demand = backtesting_forecaster_multiseries(\n                                          forecaster         = forecaster,\n                                          series             = data_modified[['Demand', 'Temperature']],\n                                          level              = 'Demand',\n                                          initial_train_size = len(data_train_modified),\n                                          fixed_train_size   = True,\n                                          steps              = 24,\n                                          metric             = 'mean_absolute_error',\n                                          refit              = True,\n                                          verbose            = False\n                                    )\n</pre> # ForecasterAutoregMultiSeries with linear models # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  # Backtesting # ============================================================================== metric_demand, predictions_demand = backtesting_forecaster_multiseries(                                           forecaster         = forecaster,                                           series             = data_modified[['Demand', 'Temperature']],                                           level              = 'Demand',                                           initial_train_size = len(data_train_modified),                                           fixed_train_size   = True,                                           steps              = 24,                                           metric             = 'mean_absolute_error',                                           refit              = True,                                           verbose            = False                                     ) In\u00a0[25]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nprint(f\"Backtest error Demand: {metric_demand:.2f}\")\n</pre> # Backtesting metrics # ============================================================================== print(f\"Backtest error Demand: {metric_demand:.2f}\") <pre>Backtest error Demand: 684.26\n</pre> <p>When using a non linear model, the forecasting performance is not harmed even though the time series do not follow the same dynamics. This is because the model is being able to identify that the series have more differences than similarities, and has flexibility enough to learn a different pattern for each one of the series.</p> <p>One way to verify this hypothesis is to analyze how the threes of the model are created.</p> In\u00a0[22]: Copied! <pre>from sklearn.tree import export_text\n\nforecaster.fit(series=data_modified[['Demand', 'Temperature']])\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n\nfor tree in forecaster.regressor.estimators_[:3]:\n    \n    print(export_text(\n        decision_tree = tree,\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> from sklearn.tree import export_text  forecaster.fit(series=data_modified[['Demand', 'Temperature']]) feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)  for tree in forecaster.regressor.estimators_[:3]:          print(export_text(         decision_tree = tree,         feature_names = feature_names,         max_depth = 2     ) )  <pre>|--- lag_1 &lt;= -0.01\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_24 &lt;= -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_3 &lt;= 1.66\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_3 &gt;  1.66\n|   |   |   |--- truncated branch of depth 9\n|--- lag_1 &gt;  -0.01\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_22 &lt;= 1.39\n|   |   |   |--- truncated branch of depth 26\n|   |   |--- lag_22 &gt;  1.39\n|   |   |   |--- truncated branch of depth 9\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_1 &lt;= 0.65\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.65\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_1 &lt;= -0.24\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_24 &lt;= -0.61\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  -0.61\n|   |   |   |--- truncated branch of depth 16\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_1 &lt;= -0.25\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_1 &gt;  -0.25\n|   |   |   |--- truncated branch of depth 3\n|--- lag_1 &gt;  -0.24\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_21 &lt;= 2.56\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_21 &gt;  2.56\n|   |   |   |--- truncated branch of depth 3\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_1 &lt;= 0.53\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.53\n|   |   |   |--- truncated branch of depth 19\n\n|--- lag_1 &lt;= 0.29\n|   |--- lag_24 &lt;= -0.76\n|   |   |--- Temperature &lt;= 0.50\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- Temperature &gt;  0.50\n|   |   |   |--- truncated branch of depth 15\n|   |--- lag_24 &gt;  -0.76\n|   |   |--- lag_24 &lt;= 0.18\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_24 &gt;  0.18\n|   |   |   |--- truncated branch of depth 22\n|--- lag_1 &gt;  0.29\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_1 &lt;= 1.05\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  1.05\n|   |   |   |--- truncated branch of depth 17\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_3 &lt;= 2.04\n|   |   |   |--- truncated branch of depth 24\n|   |   |--- lag_3 &gt;  2.04\n|   |   |   |--- truncated branch of depth 4\n\n</pre> In\u00a0[26]: Copied! <pre>forecaster = ForecasterAutoregMultiSeries(\n                 regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),\n                 lags               = 24,\n                 transformer_exog   = None\n             )\n\n\nforecaster.fit(series=data_modified[['Demand', 'Temperature']])\n\nfor tree in forecaster.regressor.estimators_[:10]:\n    \n    print(export_text(\n        decision_tree = tree,\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> forecaster = ForecasterAutoregMultiSeries(                  regressor          = RandomForestRegressor(random_state=123, n_jobs=-1),                  lags               = 24,                  transformer_exog   = None              )   forecaster.fit(series=data_modified[['Demand', 'Temperature']])  for tree in forecaster.regressor.estimators_[:10]:          print(export_text(         decision_tree = tree,         feature_names = feature_names,         max_depth = 2     ) ) <pre>|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9965.84\n|   |   |--- lag_24 &lt;= 8731.67\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  8731.67\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9965.84\n|   |   |--- lag_1 &lt;= 10980.40\n|   |   |   |--- truncated branch of depth 14\n|   |   |--- lag_1 &gt;  10980.40\n|   |   |   |--- truncated branch of depth 16\n|--- Temperature &gt;  0.50\n|   |--- lag_17 &lt;= 10.41\n|   |   |--- lag_19 &lt;= 8.65\n|   |   |   |--- truncated branch of depth 11\n|   |   |--- lag_19 &gt;  8.65\n|   |   |   |--- truncated branch of depth 28\n|   |--- lag_17 &gt;  10.41\n|   |   |--- lag_3 &lt;= 10.28\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_3 &gt;  10.28\n|   |   |   |--- truncated branch of depth 19\n\n|--- Demand &lt;= 0.50\n|   |--- lag_16 &lt;= 10.74\n|   |   |--- lag_19 &lt;= 11.09\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_19 &gt;  11.09\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_16 &gt;  10.74\n|   |   |--- lag_11 &lt;= 10.99\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_11 &gt;  10.99\n|   |   |   |--- truncated branch of depth 10\n|--- Demand &gt;  0.50\n|   |--- lag_1 &lt;= 10028.92\n|   |   |--- lag_24 &lt;= 8855.49\n|   |   |   |--- truncated branch of depth 19\n|   |   |--- lag_24 &gt;  8855.49\n|   |   |   |--- truncated branch of depth 19\n|   |--- lag_1 &gt;  10028.92\n|   |   |--- lag_1 &lt;= 11282.66\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11282.66\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_5 &lt;= 3236.52\n|   |--- lag_15 &lt;= 12.22\n|   |   |--- lag_21 &lt;= 12.50\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_21 &gt;  12.50\n|   |   |   |--- truncated branch of depth 3\n|   |--- lag_15 &gt;  12.22\n|   |   |--- lag_11 &lt;= 9.68\n|   |   |   |--- truncated branch of depth 3\n|   |   |--- lag_11 &gt;  9.68\n|   |   |   |--- truncated branch of depth 3\n|--- lag_5 &gt;  3236.52\n|   |--- lag_1 &lt;= 9714.65\n|   |   |--- lag_24 &lt;= 8784.49\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8784.49\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9714.65\n|   |   |--- lag_1 &lt;= 11270.14\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11270.14\n|   |   |   |--- truncated branch of depth 16\n\n|--- lag_11 &lt;= 3236.52\n|   |--- lag_4 &lt;= 12.22\n|   |   |--- lag_24 &lt;= 9.37\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  9.37\n|   |   |   |--- truncated branch of depth 29\n|   |--- lag_4 &gt;  12.22\n|   |   |--- lag_24 &lt;= 10.36\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_24 &gt;  10.36\n|   |   |   |--- truncated branch of depth 3\n|--- lag_11 &gt;  3236.52\n|   |--- lag_1 &lt;= 9797.43\n|   |   |--- lag_24 &lt;= 8874.05\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_24 &gt;  8874.05\n|   |   |   |--- truncated branch of depth 19\n|   |--- lag_1 &gt;  9797.43\n|   |   |--- lag_1 &lt;= 11559.09\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_1 &gt;  11559.09\n|   |   |   |--- truncated branch of depth 14\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9798.85\n|   |   |--- lag_24 &lt;= 8788.75\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8788.75\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9798.85\n|   |   |--- lag_1 &lt;= 11236.58\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11236.58\n|   |   |   |--- truncated branch of depth 20\n|--- Temperature &gt;  0.50\n|   |--- lag_1 &lt;= 7.91\n|   |   |--- lag_5 &lt;= 9.75\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_5 &gt;  9.75\n|   |   |   |--- truncated branch of depth 5\n|   |--- lag_1 &gt;  7.91\n|   |   |--- lag_3 &lt;= 13.27\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_3 &gt;  13.27\n|   |   |   |--- value: [7.58]\n\n|--- lag_2 &lt;= 3236.21\n|   |--- lag_21 &lt;= 11.27\n|   |   |--- lag_3 &lt;= 11.13\n|   |   |   |--- truncated branch of depth 29\n|   |   |--- lag_3 &gt;  11.13\n|   |   |   |--- truncated branch of depth 14\n|   |--- lag_21 &gt;  11.27\n|   |   |--- lag_20 &lt;= 8.48\n|   |   |   |--- truncated branch of depth 4\n|   |   |--- lag_20 &gt;  8.48\n|   |   |   |--- truncated branch of depth 13\n|--- lag_2 &gt;  3236.21\n|   |--- lag_1 &lt;= 9764.90\n|   |   |--- lag_24 &lt;= 8486.44\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  8486.44\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9764.90\n|   |   |--- lag_1 &lt;= 11236.58\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11236.58\n|   |   |   |--- truncated branch of depth 17\n\n|--- lag_13 &lt;= 3236.21\n|   |--- lag_17 &lt;= 11.41\n|   |   |--- lag_18 &lt;= 11.56\n|   |   |   |--- truncated branch of depth 23\n|   |   |--- lag_18 &gt;  11.56\n|   |   |   |--- truncated branch of depth 12\n|   |--- lag_17 &gt;  11.41\n|   |   |--- lag_10 &lt;= 11.20\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_10 &gt;  11.20\n|   |   |   |--- truncated branch of depth 5\n|--- lag_13 &gt;  3236.21\n|   |--- lag_1 &lt;= 9384.82\n|   |   |--- lag_24 &lt;= 8669.37\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_24 &gt;  8669.37\n|   |   |   |--- truncated branch of depth 13\n|   |--- lag_1 &gt;  9384.82\n|   |   |--- lag_1 &lt;= 11270.14\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_1 &gt;  11270.14\n|   |   |   |--- truncated branch of depth 14\n\n|--- lag_10 &lt;= 3236.52\n|   |--- lag_7 &lt;= 11.31\n|   |   |--- lag_17 &lt;= 8.35\n|   |   |   |--- truncated branch of depth 11\n|   |   |--- lag_17 &gt;  8.35\n|   |   |   |--- truncated branch of depth 24\n|   |--- lag_7 &gt;  11.31\n|   |   |--- lag_2 &lt;= 9.73\n|   |   |   |--- truncated branch of depth 13\n|   |   |--- lag_2 &gt;  9.73\n|   |   |   |--- truncated branch of depth 11\n|--- lag_10 &gt;  3236.52\n|   |--- lag_1 &lt;= 9797.43\n|   |   |--- lag_24 &lt;= 8673.36\n|   |   |   |--- truncated branch of depth 20\n|   |   |--- lag_24 &gt;  8673.36\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9797.43\n|   |   |--- lag_1 &lt;= 11297.26\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_1 &gt;  11297.26\n|   |   |   |--- truncated branch of depth 17\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9963.52\n|   |   |--- lag_24 &lt;= 8802.79\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8802.79\n|   |   |   |--- truncated branch of depth 17\n|   |--- lag_1 &gt;  9963.52\n|   |   |--- lag_1 &lt;= 11270.25\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_1 &gt;  11270.25\n|   |   |   |--- truncated branch of depth 16\n|--- Temperature &gt;  0.50\n|   |--- lag_21 &lt;= 11.23\n|   |   |--- lag_22 &lt;= 12.26\n|   |   |   |--- truncated branch of depth 34\n|   |   |--- lag_22 &gt;  12.26\n|   |   |   |--- truncated branch of depth 6\n|   |--- lag_21 &gt;  11.23\n|   |   |--- lag_9 &lt;= 10.55\n|   |   |   |--- truncated branch of depth 14\n|   |   |--- lag_9 &gt;  10.55\n|   |   |   |--- truncated branch of depth 9\n\n|--- Temperature &lt;= 0.50\n|   |--- lag_1 &lt;= 9170.38\n|   |   |--- lag_24 &lt;= 8648.16\n|   |   |   |--- truncated branch of depth 16\n|   |   |--- lag_24 &gt;  8648.16\n|   |   |   |--- truncated branch of depth 16\n|   |--- lag_1 &gt;  9170.38\n|   |   |--- lag_1 &lt;= 10517.94\n|   |   |   |--- truncated branch of depth 18\n|   |   |--- lag_1 &gt;  10517.94\n|   |   |   |--- truncated branch of depth 17\n|--- Temperature &gt;  0.50\n|   |--- lag_9 &lt;= 11.42\n|   |   |--- lag_9 &lt;= 11.35\n|   |   |   |--- truncated branch of depth 27\n|   |   |--- lag_9 &gt;  11.35\n|   |   |   |--- truncated branch of depth 6\n|   |--- lag_9 &gt;  11.42\n|   |   |--- lag_22 &lt;= 8.38\n|   |   |   |--- truncated branch of depth 3\n|   |   |--- lag_22 &gt;  8.38\n|   |   |   |--- truncated branch of depth 12\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(export_text(\n        decision_tree = forecaster.regressor.estimators_[0],\n        feature_names = feature_names,\n        max_depth = 2\n    )\n)\n</pre> print(export_text(         decision_tree = forecaster.regressor.estimators_[0],         feature_names = feature_names,         max_depth = 2     ) ) <pre>|--- lag_1 &lt;= -0.01\n|   |--- Demand &lt;= 0.50\n|   |   |--- lag_3 &lt;= 1.66\n|   |   |   |--- truncated branch of depth 25\n|   |   |--- lag_3 &gt;  1.66\n|   |   |   |--- truncated branch of depth 9\n|   |--- Demand &gt;  0.50\n|   |   |--- lag_24 &lt;= -0.72\n|   |   |   |--- truncated branch of depth 17\n|   |   |--- lag_24 &gt;  -0.72\n|   |   |   |--- truncated branch of depth 17\n|--- lag_1 &gt;  -0.01\n|   |--- Temperature &lt;= 0.50\n|   |   |--- lag_1 &lt;= 0.65\n|   |   |   |--- truncated branch of depth 15\n|   |   |--- lag_1 &gt;  0.65\n|   |   |   |--- truncated branch of depth 16\n|   |--- Temperature &gt;  0.50\n|   |   |--- lag_22 &lt;= 1.39\n|   |   |   |--- truncated branch of depth 26\n|   |   |--- lag_22 &gt;  1.39\n|   |   |   |--- truncated branch of depth 9\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>r = export_text(\n        decision_tree = forecaster.regressor.estimators_[0],\n        feature_names = feature_names\n    )\n\nre.split(r\"|--- [a-zA-Z]\", r)\n</pre> r = export_text(         decision_tree = forecaster.regressor.estimators_[0],         feature_names = feature_names     )  re.split(r\"|--- [a-zA-Z]\", r) In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = []\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n</pre> top_3_nodes = [] feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns) In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = []\nfeature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n\nfor tree in forecaster.regressor.estimators_:\n    r = export_text(\n        decision_tree = tree,\n        feature_names = feature_names\n    )\n    r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')]\n    r = [re.sub('--- ', '', node) for node in r]\n    r = [re.sub('  +', '', node) for node in r]\n    r = [node.split(' ')[0]  for node in r]\n    r = [word for word in r if word in feature_names]\n    # remove duplicates\n    #r = list(dict.fromkeys(r))\n    top_3_nodes.append(r)\n</pre> top_3_nodes = [] feature_names = list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)  for tree in forecaster.regressor.estimators_:     r = export_text(         decision_tree = tree,         feature_names = feature_names     )     r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')]     r = [re.sub('--- ', '', node) for node in r]     r = [re.sub('  +', '', node) for node in r]     r = [node.split(' ')[0]  for node in r]     r = [word for word in r if word in feature_names]     # remove duplicates     #r = list(dict.fromkeys(r))     top_3_nodes.append(r)  In\u00a0[\u00a0]: Copied! <pre>top_3_nodes[0]\n</pre> top_3_nodes[0] Out[\u00a0]: <pre>['lag_1',\n 'Demand',\n 'lag_3',\n 'lag_12',\n 'lag_4',\n 'lag_24',\n 'lag_24',\n 'lag_21',\n 'lag_21',\n 'lag_4',\n 'lag_9',\n 'lag_11',\n 'lag_10',\n 'lag_9',\n 'lag_9',\n 'lag_4',\n 'lag_4',\n 'lag_10',\n 'lag_19',\n 'lag_20',\n 'lag_15',\n 'lag_15',\n 'lag_20',\n 'lag_18',\n 'lag_18',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_11',\n 'lag_9',\n 'lag_14',\n 'lag_14',\n 'lag_7',\n 'lag_12',\n 'lag_12',\n 'lag_7',\n 'lag_17',\n 'lag_17',\n 'lag_9',\n 'lag_17',\n 'lag_7',\n 'lag_7',\n 'lag_17',\n 'lag_12',\n 'lag_9',\n 'lag_9',\n 'lag_12',\n 'lag_11',\n 'lag_11',\n 'lag_9',\n 'lag_19',\n 'lag_21',\n 'lag_18',\n 'lag_8',\n 'lag_4',\n 'lag_4',\n 'lag_8',\n 'lag_8',\n 'lag_8',\n 'lag_18',\n 'lag_15',\n 'lag_7',\n 'lag_7',\n 'lag_15',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_23',\n 'lag_12',\n 'lag_9',\n 'lag_9',\n 'lag_12',\n 'lag_16',\n 'lag_16',\n 'lag_23',\n 'lag_16',\n 'lag_4',\n 'lag_4',\n 'lag_16',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_9',\n 'lag_1',\n 'lag_17',\n 'lag_17',\n 'lag_1',\n 'lag_9',\n 'lag_20',\n 'lag_20',\n 'lag_8',\n 'lag_8',\n 'lag_4',\n 'lag_5',\n 'lag_3',\n 'lag_13',\n 'lag_13',\n 'lag_3',\n 'lag_5',\n 'lag_4',\n 'lag_14',\n 'lag_14',\n 'lag_4',\n 'lag_3',\n 'lag_3',\n 'lag_12',\n 'lag_10',\n 'lag_14',\n 'lag_15',\n 'lag_15',\n 'lag_14',\n 'lag_10',\n 'lag_2',\n 'lag_21',\n 'lag_10',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_22',\n 'lag_22',\n 'lag_20',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_22',\n 'lag_12',\n 'lag_12',\n 'lag_10',\n 'lag_12',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_12',\n 'lag_21',\n 'lag_12',\n 'lag_8',\n 'lag_8',\n 'lag_10',\n 'lag_10',\n 'lag_12',\n 'lag_8',\n 'lag_3',\n 'lag_3',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_2',\n 'lag_11',\n 'lag_24',\n 'lag_24',\n 'lag_19',\n 'lag_18',\n 'lag_2',\n 'lag_2',\n 'lag_18',\n 'lag_17',\n 'lag_17',\n 'lag_19',\n 'lag_7',\n 'lag_21',\n 'lag_21',\n 'lag_7',\n 'lag_11',\n 'lag_19',\n 'lag_21',\n 'lag_3',\n 'lag_19',\n 'lag_19',\n 'lag_3',\n 'lag_21',\n 'lag_4',\n 'lag_4',\n 'lag_23',\n 'lag_23',\n 'lag_19',\n 'lag_3',\n 'lag_18',\n 'lag_22',\n 'lag_15',\n 'lag_19',\n 'lag_23',\n 'lag_23',\n 'lag_19',\n 'lag_10',\n 'lag_10',\n 'lag_15',\n 'lag_18',\n 'lag_12',\n 'lag_12',\n 'lag_18',\n 'lag_22',\n 'lag_14',\n 'lag_18',\n 'lag_18',\n 'lag_3',\n 'lag_3',\n 'lag_14',\n 'lag_3',\n 'lag_3',\n 'lag_18',\n 'lag_15',\n 'lag_11',\n 'lag_14',\n 'lag_14',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_21',\n 'lag_2',\n 'lag_21',\n 'lag_21',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_9',\n 'lag_8',\n 'lag_8',\n 'lag_9',\n 'lag_21',\n 'lag_15',\n 'Demand',\n 'lag_24',\n 'lag_1',\n 'lag_8',\n 'lag_17',\n 'lag_15',\n 'lag_13',\n 'lag_13',\n 'lag_15',\n 'lag_17',\n 'lag_8',\n 'lag_12',\n 'lag_1',\n 'lag_5',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n 'lag_21',\n 'lag_21',\n 'lag_12',\n 'lag_5',\n 'lag_11',\n 'lag_1',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_11',\n 'lag_14',\n 'lag_14',\n 'lag_14',\n 'lag_14',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_23',\n 'lag_23',\n 'lag_5',\n 'lag_20',\n 'lag_20',\n 'lag_5',\n 'lag_11',\n 'lag_11',\n 'lag_23',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_12',\n 'lag_1',\n 'lag_7',\n 'lag_7',\n 'lag_7',\n 'lag_7',\n 'lag_1',\n 'lag_4',\n 'lag_8',\n 'lag_8',\n 'lag_4',\n 'lag_1',\n 'lag_11',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_1',\n 'lag_4',\n 'lag_1',\n 'lag_7',\n 'lag_2',\n 'lag_2',\n 'lag_7',\n 'lag_8',\n 'lag_8',\n 'lag_1',\n 'lag_3',\n 'lag_5',\n 'lag_5',\n 'lag_3',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_21',\n 'lag_1',\n 'lag_6',\n 'lag_6',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_21',\n 'lag_1',\n 'lag_3',\n 'lag_22',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_5',\n 'lag_22',\n 'lag_10',\n 'lag_24',\n 'lag_24',\n 'lag_11',\n 'lag_19',\n 'lag_19',\n 'lag_11',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_3',\n 'lag_24',\n 'lag_1',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_8',\n 'lag_16',\n 'lag_16',\n 'lag_1',\n 'lag_3',\n 'lag_24',\n 'lag_24',\n 'lag_3',\n 'lag_9',\n 'lag_23',\n 'lag_5',\n 'lag_8',\n 'lag_8',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_15',\n 'lag_15',\n 'lag_23',\n 'lag_1',\n 'lag_5',\n 'lag_1',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_2',\n 'lag_20',\n 'lag_20',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_4',\n 'lag_4',\n 'lag_24',\n 'lag_1',\n 'lag_13',\n 'lag_1',\n 'lag_23',\n 'lag_3',\n 'lag_3',\n 'lag_23',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_5',\n 'lag_18',\n 'lag_18',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_13',\n 'lag_21',\n 'lag_6',\n 'lag_6',\n 'lag_21',\n 'lag_1',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_24',\n 'lag_9',\n 'lag_12',\n 'lag_12',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_24',\n 'lag_3',\n 'lag_3',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_8',\n 'lag_11',\n 'lag_11',\n 'lag_8',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_5',\n 'lag_19',\n 'lag_19',\n 'lag_5',\n 'lag_1',\n 'lag_17',\n 'lag_16',\n 'lag_18',\n 'lag_18',\n 'lag_16',\n 'lag_19',\n 'lag_19',\n 'lag_17',\n 'lag_1',\n 'lag_15',\n 'lag_1',\n 'lag_24',\n 'lag_14',\n 'lag_24',\n 'lag_24',\n 'lag_14',\n 'lag_6',\n 'lag_9',\n 'lag_9',\n 'lag_6',\n 'lag_21',\n 'lag_21',\n 'lag_24',\n 'lag_19',\n 'lag_22',\n 'lag_9',\n 'lag_9',\n 'lag_22',\n 'lag_19',\n 'lag_19',\n 'lag_19',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_23',\n 'lag_14',\n 'lag_14',\n 'lag_1',\n 'lag_10',\n 'lag_20',\n 'lag_20',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_4',\n 'lag_17',\n 'lag_17',\n 'lag_4',\n 'lag_15',\n 'lag_4',\n 'lag_23',\n 'lag_13',\n 'lag_23',\n 'lag_6',\n 'lag_6',\n 'lag_23',\n 'lag_24',\n 'lag_24',\n 'lag_13',\n 'lag_20',\n 'lag_15',\n 'lag_15',\n 'lag_20',\n 'lag_21',\n 'lag_21',\n 'lag_23',\n 'lag_11',\n 'lag_24',\n 'lag_5',\n 'lag_5',\n 'lag_24',\n 'lag_8',\n 'lag_8',\n 'lag_11',\n 'lag_10',\n 'lag_14',\n 'lag_14',\n 'lag_10',\n 'lag_4',\n 'lag_7',\n 'lag_6',\n 'lag_6',\n 'lag_7',\n 'lag_4',\n 'lag_6',\n 'lag_21',\n 'lag_21',\n 'lag_6',\n 'lag_23',\n 'lag_23',\n 'lag_4',\n 'lag_24',\n 'lag_1',\n 'lag_7',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_18',\n 'lag_18',\n 'lag_1',\n 'lag_20',\n 'lag_24',\n 'lag_20',\n 'lag_20',\n 'lag_24',\n 'lag_20',\n 'lag_9',\n 'lag_6',\n 'lag_6',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_12',\n 'lag_14',\n 'lag_14',\n 'lag_12',\n 'lag_14',\n 'lag_6',\n 'lag_6',\n 'lag_14',\n 'lag_21',\n 'lag_21',\n 'lag_8',\n 'lag_7',\n 'lag_15',\n 'lag_9',\n 'lag_9',\n 'lag_15',\n 'lag_22',\n 'lag_22',\n 'lag_7',\n 'lag_14',\n 'lag_8',\n 'lag_8',\n 'lag_14',\n 'lag_7',\n 'lag_8',\n 'lag_12',\n 'lag_18',\n 'lag_13',\n 'lag_13',\n 'lag_18',\n 'lag_8',\n 'lag_18',\n 'lag_18',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_12',\n 'lag_1',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_20',\n 'lag_20',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_2',\n 'lag_24',\n 'lag_14',\n 'lag_23',\n 'lag_10',\n 'lag_23',\n 'lag_23',\n 'lag_10',\n 'lag_6',\n 'lag_6',\n 'lag_23',\n 'lag_14',\n 'lag_23',\n 'lag_23',\n 'lag_11',\n 'lag_11',\n 'lag_13',\n 'lag_13',\n 'lag_24',\n 'lag_4',\n 'lag_23',\n 'lag_23',\n 'lag_4',\n 'lag_2',\n 'lag_13',\n 'lag_16',\n 'lag_10',\n 'lag_10',\n 'lag_9',\n 'lag_9',\n 'lag_16',\n 'lag_14',\n 'lag_14',\n 'lag_13',\n 'lag_7',\n 'lag_24',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_20',\n 'lag_1',\n 'lag_1',\n 'lag_24',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_22',\n 'lag_7',\n 'lag_1',\n 'lag_24',\n 'lag_15',\n 'lag_15',\n 'lag_24',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_7',\n 'lag_7',\n 'lag_6',\n 'lag_6',\n 'Temperature',\n 'lag_1',\n 'lag_23',\n 'lag_20',\n 'lag_5',\n 'lag_4',\n 'lag_1',\n 'lag_14',\n 'lag_19',\n 'lag_11',\n 'lag_11',\n 'lag_19',\n 'lag_21',\n 'lag_21',\n 'lag_14',\n 'lag_6',\n 'lag_15',\n 'lag_15',\n 'lag_6',\n 'lag_1',\n 'lag_22',\n 'lag_3',\n 'lag_3',\n 'lag_22',\n 'lag_11',\n 'lag_11',\n 'lag_12',\n 'lag_12',\n 'lag_4',\n 'lag_20',\n 'lag_1',\n 'lag_9',\n 'lag_23',\n 'lag_23',\n 'lag_9',\n 'lag_1',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_3',\n 'lag_3',\n 'lag_20',\n 'lag_24',\n 'lag_1',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_24',\n 'lag_5',\n 'lag_1',\n 'lag_4',\n 'lag_18',\n 'lag_10',\n 'lag_10',\n 'lag_18',\n 'lag_4',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_21',\n 'lag_18',\n 'lag_12',\n 'lag_12',\n 'lag_18',\n 'lag_21',\n 'lag_11',\n 'lag_11',\n 'lag_20',\n 'lag_3',\n 'lag_3',\n 'lag_14',\n 'lag_20',\n 'lag_8',\n 'lag_8',\n 'lag_20',\n 'lag_11',\n 'lag_11',\n 'lag_8',\n 'lag_8',\n 'lag_14',\n 'lag_3',\n 'lag_21',\n 'lag_21',\n 'lag_20',\n 'lag_9',\n 'lag_18',\n 'lag_18',\n 'lag_9',\n 'lag_20',\n 'lag_3',\n 'lag_1',\n 'lag_20',\n 'lag_2',\n 'lag_19',\n 'lag_1',\n 'lag_1',\n 'lag_19',\n 'lag_2',\n 'lag_21',\n 'lag_21',\n 'lag_7',\n 'lag_7',\n 'lag_20',\n 'lag_1',\n 'lag_10',\n 'lag_14',\n 'lag_14',\n 'lag_10',\n 'lag_19',\n 'lag_19',\n 'lag_1',\n 'lag_19',\n 'lag_19',\n 'lag_20',\n 'lag_20',\n 'lag_1',\n 'lag_18',\n 'lag_22',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_1',\n 'lag_16',\n 'lag_16',\n 'lag_1',\n 'lag_3',\n 'lag_3',\n 'lag_18',\n 'lag_12',\n 'lag_1',\n 'lag_19',\n 'lag_19',\n 'lag_1',\n 'lag_13',\n 'lag_13',\n 'lag_12',\n 'lag_2',\n 'lag_14',\n 'lag_14',\n 'lag_2',\n 'lag_9',\n 'lag_9',\n 'lag_23',\n 'lag_10',\n 'lag_12',\n 'lag_14',\n 'lag_13',\n 'lag_9',\n 'lag_10',\n 'lag_10',\n 'lag_9',\n 'lag_1',\n 'lag_1',\n 'lag_15',\n 'lag_15',\n 'lag_13',\n 'lag_13',\n 'lag_24',\n 'lag_4',\n 'lag_4',\n 'lag_24',\n 'lag_22',\n 'lag_22',\n 'lag_13',\n 'lag_12',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n 'lag_18',\n 'lag_18',\n 'lag_14',\n 'lag_11',\n 'lag_22',\n 'lag_23',\n 'lag_23',\n 'lag_16',\n 'lag_16',\n 'lag_22',\n 'lag_1',\n 'lag_3',\n 'lag_3',\n 'lag_1',\n 'lag_22',\n 'lag_22',\n 'lag_11',\n 'lag_8',\n 'lag_1',\n 'lag_1',\n 'lag_8',\n 'lag_12',\n 'lag_17',\n 'lag_17',\n 'lag_11',\n 'lag_22',\n 'lag_22',\n 'lag_11',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_21',\n 'lag_10',\n 'lag_11',\n 'lag_5',\n 'lag_8',\n 'lag_15',\n 'lag_15',\n 'lag_8',\n 'lag_13',\n 'lag_17',\n 'lag_17',\n 'lag_13',\n 'lag_7',\n 'lag_7',\n 'lag_5',\n 'lag_9',\n 'lag_6',\n 'lag_6',\n 'lag_4',\n 'lag_4',\n 'lag_9',\n 'lag_11',\n 'lag_1',\n 'lag_1',\n 'lag_18',\n 'lag_1',\n 'lag_16',\n 'lag_1',\n 'lag_12',\n 'lag_12',\n 'lag_1',\n 'lag_1',\n 'lag_22',\n 'lag_15',\n 'lag_15',\n 'lag_22',\n 'lag_9',\n 'lag_9',\n 'lag_1',\n 'lag_8',\n 'lag_8',\n 'lag_16',\n 'lag_1',\n 'lag_1',\n 'lag_21',\n 'lag_23',\n 'lag_23',\n 'lag_21',\n 'lag_24',\n 'lag_24',\n 'lag_1',\n 'lag_21',\n 'lag_5',\n 'lag_5',\n 'lag_21',\n 'lag_1',\n 'lag_11',\n 'lag_11',\n 'lag_1',\n 'lag_19',\n 'lag_18',\n 'lag_18',\n 'lag_21',\n 'lag_22',\n 'lag_22',\n 'lag_13',\n 'lag_13',\n 'lag_21',\n 'lag_19',\n 'lag_1',\n 'lag_20',\n 'lag_2',\n 'lag_2',\n 'lag_20',\n 'lag_1',\n 'lag_22',\n 'lag_1',\n 'lag_4',\n 'lag_4',\n 'lag_1',\n 'lag_22',\n 'lag_11',\n 'lag_11',\n 'lag_18',\n 'lag_18',\n 'lag_18',\n 'lag_10',\n 'lag_1',\n 'lag_9',\n 'lag_1',\n 'lag_13',\n 'lag_17',\n 'lag_17',\n 'lag_13',\n 'lag_2',\n 'lag_2',\n 'lag_1',\n 'lag_5',\n 'lag_22',\n 'lag_22',\n 'lag_5',\n 'lag_13',\n 'lag_13',\n 'lag_9',\n 'lag_22',\n 'lag_22',\n 'lag_21',\n 'lag_9',\n 'lag_9',\n 'lag_21',\n 'lag_23',\n 'lag_23',\n 'lag_1',\n 'lag_3',\n 'lag_6',\n 'lag_2',\n 'lag_2',\n 'lag_6',\n 'lag_22',\n 'lag_22',\n 'lag_3',\n 'lag_11',\n 'lag_4',\n 'lag_2',\n 'lag_2',\n 'lag_4',\n 'lag_5',\n 'lag_5',\n 'lag_11',\n 'lag_12',\n 'lag_7',\n 'lag_7',\n 'lag_12',\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>for feature in feature_names:\n    \n\ntop_3_nodes\n</pre> for feature in feature_names:       top_3_nodes <pre>\n  Cell In [118], line 4\n    top_3_nodes\n    ^\nIndentationError: expected an indented block\n</pre> In\u00a0[\u00a0]: Copied! <pre>top_3_nodes = pd.DataFrame(\n                data = top_3_nodes,\n                columns= ['first_node', 'second_node', 'third_node']\n              )\ntop_3_nodes\n</pre> top_3_nodes = pd.DataFrame(                 data = top_3_nodes,                 columns= ['first_node', 'second_node', 'third_node']               ) top_3_nodes  Out[\u00a0]: first_node second_node third_node 0 lag_1 Demand lag_3 1 lag_1 Temperature lag_24 2 lag_1 lag_24 Temperature 3 lag_1 Temperature lag_24 4 lag_1 Demand lag_19 ... ... ... ... 95 lag_1 Demand lag_12 96 lag_1 Temperature lag_24 97 lag_1 Temperature lag_24 98 lag_1 Temperature lag_24 99 lag_1 Demand lag_18 <p>100 rows \u00d7 3 columns</p> In\u00a0[\u00a0]: Copied! <pre>for col in top_3_nodes.columns:\n    print(col)\n    print(\"---\")\n    print(top_3_nodes[col].value_counts())\n</pre> for col in top_3_nodes.columns:     print(col)     print(\"---\")     print(top_3_nodes[col].value_counts()) <pre>first_node\n---\nlag_1    100\nName: first_node, dtype: int64\nsecond_node\n---\nDemand         35\nTemperature    35\nlag_24         20\nlag_23         10\nName: second_node, dtype: int64\nthird_node\n---\nlag_24         35\nTemperature    19\nDemand         11\nlag_18          9\nlag_17          7\nlag_1           4\nlag_12          4\nlag_20          2\nlag_4           2\nlag_14          2\nlag_3           1\nlag_19          1\nlag_8           1\nlag_9           1\nlag_6           1\nName: third_node, dtype: int64\n</pre> In\u00a0[\u00a0]: Copied! <pre>import re\nr = [re.sub('\\|--- ', '', node) for node in r.split('|   |')[:3]]\nr = [re.sub('--- ', '', node) for node in r]\nr = [re.sub('  +', '', node) for node in r]\nr = [node.split(' ')[0]  for node in r]\nprint(r)\n</pre> import re r = [re.sub('\\|--- ', '', node) for node in r.split('|   |')[:3]] r = [re.sub('--- ', '', node) for node in r] r = [re.sub('  +', '', node) for node in r] r = [node.split(' ')[0]  for node in r] print(r)  <pre>['lag_1', 'Demand', 'lag_19']\n</pre> In\u00a0[\u00a0]: Copied! <pre>r = [node.split(' ')[0]  for node in r]\n</pre> r = [node.split(' ')[0]  for node in r] Out[\u00a0]: <pre>['lag_1',\n 'Demand',\n 'lag_19',\n '',\n 'truncated',\n 'lag_19',\n '',\n 'truncated',\n 'Demand',\n 'lag_24',\n '',\n 'truncated',\n 'lag_24',\n '',\n 'truncated',\n 'Demand',\n 'lag_3',\n '',\n 'truncated',\n 'lag_3',\n '',\n 'truncated',\n 'Demand',\n 'lag_1',\n '',\n 'truncated',\n 'lag_1',\n '',\n 'truncated']</pre> In\u00a0[\u00a0]: Copied! <pre>[node.split(' ')[0]  for node in r]\n</pre> [node.split(' ')[0]  for node in r] Out[\u00a0]: <pre>['|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&lt;',\n '=',\n '',\n '-',\n '0',\n '.',\n '0',\n '1',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '9',\n '',\n '&lt;',\n '=',\n '',\n '1',\n '.',\n '1',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '2',\n '6',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '9',\n '',\n '&gt;',\n '',\n '',\n '1',\n '.',\n '1',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '2',\n '4',\n '',\n '&lt;',\n '=',\n '',\n '-',\n '0',\n '.',\n '6',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '6',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '2',\n '4',\n '',\n '&gt;',\n '',\n '',\n '-',\n '0',\n '.',\n '6',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '7',\n '\\n',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&gt;',\n '',\n '',\n '-',\n '0',\n '.',\n '0',\n '1',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '3',\n '',\n '&lt;',\n '=',\n '',\n '2',\n '.',\n '4',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '2',\n '5',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '3',\n '',\n '&gt;',\n '',\n '',\n '2',\n '.',\n '4',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '4',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'D',\n 'e',\n 'm',\n 'a',\n 'n',\n 'd',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '5',\n '0',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&lt;',\n '=',\n '',\n '0',\n '.',\n '8',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '5',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 'l',\n 'a',\n 'g',\n '_',\n '1',\n '',\n '&gt;',\n '',\n '',\n '0',\n '.',\n '8',\n '2',\n '\\n',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '',\n '',\n '',\n '|',\n '-',\n '-',\n '-',\n '',\n 't',\n 'r',\n 'u',\n 'n',\n 'c',\n 'a',\n 't',\n 'e',\n 'd',\n '',\n 'b',\n 'r',\n 'a',\n 'n',\n 'c',\n 'h',\n '',\n 'o',\n 'f',\n '',\n 'd',\n 'e',\n 'p',\n 't',\n 'h',\n '',\n '1',\n '8',\n '\\n']</pre> In\u00a0[\u00a0]: Copied! <pre>list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns)\n</pre> list(forecaster.create_train_X_y(data_modified[['Demand', 'Temperature']])[0].columns) Out[\u00a0]: <pre>['lag_1',\n 'lag_2',\n 'lag_3',\n 'lag_4',\n 'lag_5',\n 'lag_6',\n 'lag_7',\n 'lag_8',\n 'lag_9',\n 'lag_10',\n 'lag_11',\n 'lag_12',\n 'lag_13',\n 'lag_14',\n 'lag_15',\n 'lag_16',\n 'lag_17',\n 'lag_18',\n 'lag_19',\n 'lag_20',\n 'lag_21',\n 'lag_22',\n 'lag_23',\n 'lag_24',\n 'Demand',\n 'Temperature']</pre> In\u00a0[\u00a0]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"faq/single-vs-multi-series-forecasting.html#single-series-vs-multi-series-forecasting","title":"Single series vs multi series forecasting\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags. That is, the past values of the series are used to forecast its future. In multi-time series forecasting, two or more time series are modeled together using a single model.</p> <p>\u00bfIs it better to create a model for each series or a single one for all of them?</p>"},{"location":"faq/single-vs-multi-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#multi-series-forecaster-with-linear-models","title":"Multi series forecaster with linear models\u00b6","text":""},{"location":"faq/single-vs-multi-series-forecasting.html#multi-series-forecaster-with-non-linear-models","title":"Multi series forecaster with non linear models\u00b6","text":""},{"location":"introduction-forecasting/forecaster-attributes.html","title":"Forecaster Attributes","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n</pre> # Libraries # ============================================================================== import pandas as pd from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 5\n             )\n\nforecaster.fit(y=data['y'])\nforecaster\n</pre> # Download data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv') data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 5              )  forecaster.fit(y=data['y']) forecaster Out[2]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:22:31 \nLast fit date: 2023-05-29 13:22:31 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[3]: Copied! <pre># List of attributes\n# ==============================================================================\nfor attribute, value in forecaster.__dict__.items():\n    print(attribute)\n</pre> # List of attributes # ============================================================================== for attribute, value in forecaster.__dict__.items():     print(attribute) <pre>regressor\ntransformer_y\ntransformer_exog\nweight_func\nsource_code_weight_func\nlast_window\nindex_type\nindex_freq\ntraining_range\nincluded_exog\nexog_type\nexog_dtypes\nexog_col_names\nX_train_col_names\nin_sample_residuals\nout_sample_residuals\nfitted\ncreation_date\nfit_date\nskforcast_version\npython_version\nforecaster_id\nlags\nmax_lag\nwindow_size\nfit_kwargs\n</pre> In\u00a0[4]: Copied! <pre># Forecaster regressor\n# ==============================================================================\nforecaster.regressor\n</pre> # Forecaster regressor # ============================================================================== forecaster.regressor Out[4]: <pre>RandomForestRegressor(random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor<pre>RandomForestRegressor(random_state=123)</pre> <p>  \u00a0 Note </p> <p>In the forecasters that follows a Direct Strategy, one instance of the regressor is trained for each step. All of them are stored in <code>self.regressors_</code></p> In\u00a0[5]: Copied! <pre># Forecaster lags\n# ==============================================================================\nforecaster.lags\n</pre> # Forecaster lags # ============================================================================== forecaster.lags Out[5]: <pre>array([1, 2, 3, 4, 5])</pre> In\u00a0[6]: Copied! <pre># Forecaster last window\n# ==============================================================================\nforecaster.last_window\n</pre> # Forecaster last window # ============================================================================== forecaster.last_window Out[6]: <pre>datetime\n2008-02-01    0.761822\n2008-03-01    0.649435\n2008-04-01    0.827887\n2008-05-01    0.816255\n2008-06-01    0.762137\nFreq: MS, Name: y, dtype: float64</pre> <p>  \u00a0 Note </p> <p>learn how to get your forecasters into production and get the most out of them with <code>last_window</code>. Using forecaster models in production.</p> In\u00a0[7]: Copied! <pre># Forecaster window size\n# ==============================================================================\nforecaster.window_size\n</pre> # Forecaster window size # ============================================================================== forecaster.window_size Out[7]: <pre>5</pre> <p>  \u00a0 Note </p> <p>In the forecasters that follows a Direct Strategy and in the  Independent multi-series forecasting this parameter is a <code>dict</code> containing the residuals for each regressor/serie.</p> In\u00a0[8]: Copied! <pre># Forecaster in sample residuals\n# ==============================================================================\nprint(\"Length:\", len(forecaster.in_sample_residuals))\nforecaster.in_sample_residuals[:5]\n</pre> # Forecaster in sample residuals # ============================================================================== print(\"Length:\", len(forecaster.in_sample_residuals)) forecaster.in_sample_residuals[:5] <pre>Length: 199\n</pre> Out[8]: <pre>array([ 0.02232928,  0.0597808 , -0.10463712, -0.03318622, -0.00291908])</pre> In\u00a0[9]: Copied! <pre># Forecaster out sample residuals\n# ==============================================================================\nforecaster.out_sample_residuals\n</pre> # Forecaster out sample residuals # ============================================================================== forecaster.out_sample_residuals In\u00a0[10]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"introduction-forecasting/forecaster-attributes.html#understanding-the-forecaster-attributes","title":"Understanding the forecaster attributes\u00b6","text":"<p>During the process of creating and training a forecaster, the object stores a lot of information in its attributes that can be useful to the user. We will explore the main attributes included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#create-and-train-a-forecaster","title":"Create and train a forecaster\u00b6","text":"<p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#regressor","title":"Regressor\u00b6","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#lags","title":"Lags\u00b6","text":"<p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#last-window","title":"Last window\u00b6","text":"<p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#window-size","title":"Window size\u00b6","text":"<p>The size of the data window needed to create the predictors. It is equal to <code>forecaster.max_lag</code>.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#in-sample-residuals","title":"In sample residuals\u00b6","text":"<p>Residuals from models predicting training data. Only stored up to 1000 values. If <code>transformer_series</code> is not <code>None</code>, the residuals are stored in the transformed scale.</p>"},{"location":"introduction-forecasting/forecaster-attributes.html#out-sample-residuals","title":"Out sample residuals\u00b6","text":"<p>Residuals from models predicting non training data. Only stored up to 1000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <p>As no values have been added, the parameter is <code>None</code>.</p>"},{"location":"introduction-forecasting/forecaster-parameters.html","title":"Understanding the forecaster parameters","text":"<p>Understanding what can be done when initializing a forecaster with skforecast can have a significant impact on the accuracy and effectiveness of the model. This guide highlights key considerations to keep in mind when initializing a forecaster and how these functionalities can be used to create more powerful and accurate forecasting models in Python.</p> <p>We will explore the arguments that can be included in a <code>ForecasterAutoreg</code>, but this can be extrapolated to any of the skforecast forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = None,\n                 lags             = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = None\n             )\n</code></pre> <p>Tip</p> <p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> must be specified.</p>"},{"location":"introduction-forecasting/forecaster-parameters.html#regressor","title":"Regressor","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API. Therefore, any of these regressors can be used to create a forecaster:</p> <ul> <li> <p>HistGradientBoostingRegressor</p> </li> <li> <p>LGBMRegressor</p> </li> <li> <p>XGBoost</p> </li> <li> <p>CatBoost</p> </li> </ul> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = None\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#lags","title":"Lags","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model.</p> <p></p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(),\n                 lags      = 5\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#transformers","title":"Transformers","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <p>Both arguments expect an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform.</p> <p>More information: Scikit-learn transformers and pipelines.</p> <p>Example</p> <p>In this example, a scikit-learn <code>StandardScaler</code> preprocessor is used for both the time series and the exogenous variables.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = StandardScaler()\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#weighted-time-series-forecasting","title":"Weighted time series forecasting","text":"<p>The presence of unreliable or unrepresentative values in the data history poses a significant challenge, as it hinders model learning. However, most forecasting algorithms require complete time series data, making it impossible to remove these observations. An alternative solution is to reduce the weight of the affected observations during model training. Skforecast facilitates the control of data weights with the <code>weight_func</code> argument.</p> <p>More information: Weighted time series forecasting.</p> <p>Example</p> <p>The following example shows how a part of the time series can be excluded from the model training by assigning it a weight of zero using the <code>custom_weights</code> function, depending on the index.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = custom_weights,\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#inclusion-of-kwargs-in-the-regressor-fit-method","title":"Inclusion of kwargs in the regressor fit method","text":"<p>Some regressors include the possibility to add some additional configuration during the fitting method. The predictor parameter <code>fit_kwargs</code> allows these arguments to be set when the forecaster is declared.</p> <p>Danger</p> <p>To add weights to the forecaster, it must be done through the <code>weight_func</code> argument and not through a <code>fit_kwargs</code>.</p> <p>Example</p> <p>The following example demonstrates the inclusion of categorical features in an LGBM regressor. This must be done during the <code>LGBMRegressor</code> fit method. Fit parameters lightgbm</p> <p>More information: Categorical features.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = {'categorical_feature': ['exog_1', 'exog_2']}\n             )\n</code></pre>"},{"location":"introduction-forecasting/forecaster-parameters.html#forecaster-id","title":"Forecaster ID","text":"<p>Name used as an identifier of the forecaster. It may be used, for example to identify the time series being modeled.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\n\nforecaster = ForecasterAutoreg(\n                 regressor        = RandomForestRegressor(),\n                 lags             = 5,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"introduction-forecasting/introduction-forecasting.html","title":"Introduction to forecasting","text":""},{"location":"introduction-forecasting/introduction-forecasting.html#time-series-and-forecasting","title":"Time series and forecasting","text":"<p>A time series is a sequence of data arranged chronologically and spaced at equal or irregular intervals. The forecasting process consists of predicting the future value of a time series, either by modeling the series solely based on its past behavior (autoregressive) or by incorporating other external variables.</p> <p></p>"},{"location":"introduction-forecasting/introduction-forecasting.html#machine-learning-for-forecasting","title":"Machine learning for forecasting","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model.</p> <p></p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix.</p> <p>This type of transformation also allows to include additional variables.</p> <p></p> <p> Time series transformation including an exogenous variable.</p> <p>Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ... p are considered predictors for the target quantity of the time series at time step p+1. </p> <p></p> <p> Diagram of training a machine learning model with time series data.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#single-step-forecasting","title":"Single-step forecasting","text":"<p>Single-step prediction is used when the goal is to predict only the next value of the series.</p> <p></p> <p> Diagram of single-step forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-step-forecasting","title":"Multi-step forecasting","text":"<p>When working with time series, it is seldom needed to predict only the next element in the series (t+1). Instead, the most common goal is to predict a whole future interval (t+1, ..., t+n)  or a far point in time (t+n). Several strategies allow generating this type of prediction.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting","text":"<p>Since the value t(n-1) is required to predict t(n), and t(n-1) is unknown, a recursive process is applied in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting and can be easily generated with the <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> classes.</p> <p></p> <p> Diagram of recursive multi-step forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#direct-multi-step-forecasting","title":"Direct multi-step forecasting","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. This entire process is automated in the <code>ForecasterAutoregDirect</code> class. </p> <p></p> <p> Diagram of direct multi-step forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multiple-output-forecasting","title":"Multiple output forecasting","text":"<p>Some machine learning models, such as long short-term memory (LSTM) neural network, can predict simultaneously several values of a sequence (one-shot). This strategy is not currently implemented in skforecast library.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-time-series-forecasting","title":"Multi-time series forecasting","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model. There are two distinct strategies for multi-series forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#independent-multi-series-forecasting","title":"Independent Multi-Series Forecasting","text":"<p>A single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p></p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context.</p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied</p> <p></p> <p> Diagram of recursive forecasting with multiple independent time series.</p> <p>The <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes cover this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#dependent-multi-series-forecasting-multivariate-time-series","title":"Dependent Multi-Series Forecasting (multivariate time series)","text":"<p>All series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p></p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-variate-series context.</p> <p>The <code>ForecasterAutoregMultiVariate</code> class covers this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive prediction Probabilistic prediction Exogenous features Custom features ForecasterAutoreg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterAutoregDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiSeriesCustom \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-forecasting-models","title":"Backtesting forecasting models","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)","text":"<p>In this approach, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p></p> <p></p> <p> Backtesting with refit and increasing training size (fixed origin).</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p></p> <p></p> <p> Backtesting with refit and fixed training size (rolling origin).</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-without-refit","title":"Backtesting without refit","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p></p> <p></p> <p> Backtesting without refit.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-including-gap","title":"Backtesting including gap","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p><p></p> Backtesting with refit and gap.</p> <p>Check the Backtesting user guide for a code example.</p>"},{"location":"releases/releases.html","title":"Change Log","text":"<p>All significant changes to this project are documented in this release file.</p>"},{"location":"releases/releases.html#081-2023-05-27","title":"[0.8.1] - [2023-05-27]","text":"<p>Added</p> <ul> <li>Argument <code>store_in_sample_residuals=True</code> in <code>fit</code> method added to all forecasters to speed up functions such as backtesting.</li> </ul> <p>Changed</p> <ul> <li>Refactor <code>utils.exog_to_direct</code> and <code>utils.exog_to_direct_numpy</code> to increase performance.</li> </ul> <p>Fixed</p> <ul> <li><code>utils.check_exog_dtypes</code> now compares the <code>dtype.name</code> instead of the <code>dtype</code>. (suggested by Metaming https://github.com/Metaming)</li> </ul>"},{"location":"releases/releases.html#080-2023-05-16","title":"[0.8.0] - [2023-05-16]","text":"<p>Added</p> <ul> <li> <p>Added the <code>fit_kwargs</code> argument to all forecasters to allow the inclusion of additional keyword arguments passed to the regressor's <code>fit</code> method.</p> </li> <li> <p>Added the <code>set_fit_kwargs</code> method to set the <code>fit_kwargs</code> attribute.</p> </li> <li> <p>Support for <code>pandas 2.0.x</code>.</p> </li> <li> <p>Added <code>exceptions</code> module with custom warnings.</p> </li> <li> <p>Added function <code>utils.check_exog_dtypes</code> to issue a warning if exogenous variables are one of type <code>init</code>, <code>float</code>, or <code>category</code>. Raise Exception if <code>exog</code> has categorical columns with non integer values.</p> </li> <li> <p>Added function <code>utils.get_exog_dtypes</code> to get the data types of the exogenous variables included during the training of the forecaster model. </p> </li> <li> <p>Added function <code>utils.cast_exog_dtypes</code> to cast data types of the exogenous variables using a dictionary as a mapping.</p> </li> <li> <p>Added function <code>utils.check_select_fit_kwargs</code> to check if the argument <code>fit_kwargs</code> is a dictionary and select only the keys used by the <code>fit</code> method of the regressor.</p> </li> <li> <p>Added argument <code>gap</code> to functions in <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to omit observations between training and prediction.</p> </li> <li> <p>Added argument <code>show_progress</code> to functions <code>model_selection.backtesting_forecaster</code>, <code>model_selection_multiseries.backtesting_forecaster_multiseries</code> and <code>model_selection_sarimax.backtesting_forecaster_sarimax</code> to indicate weather to show a progress bar.</p> </li> <li> <p>Added argument <code>remove_suffix</code>, default <code>False</code>, to the method <code>filter_train_X_y_for_step()</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the column names of the training matrices.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename optional dependency package <code>statsmodels</code> to <code>sarimax</code>. Now only <code>pmdarima</code> will be installed, <code>statsmodels</code> is no longer needed.</p> </li> <li> <p>Rename <code>get_feature_importance()</code> to <code>get_feature_importances()</code> in all Forecasters. <code>get_feature_importance()</code> method will me removed in skforecast 0.9.0.</p> </li> <li> <p>Refactor <code>get_feature_importances()</code> in all Forecasters.</p> </li> <li> <p>Remove <code>model_selection_statsmodels</code> in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>, (deprecated since 0.7.0).</p> </li> <li> <p>Remove attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> in favor of <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>, (deprecated since 0.7.0).</p> </li> <li> <p>The <code>utils.check_exog</code> function now includes a new optional parameter, <code>allow_nan</code>, that controls whether a warning should be issued if the input <code>exog</code> contains NaN values. </p> </li> <li> <p><code>utils.check_exog</code> is applied before and after <code>exog</code> transformations.</p> </li> <li> <p>The <code>utils.preprocess_y</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>The <code>utils.preprocess_exog</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>Replaced <code>tqdm.tqdm</code> by <code>tqdm.auto.tqdm</code>.</p> </li> <li> <p>Refactor <code>utils.exog_to_direct</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>The dtypes of exogenous variables are maintained when generating the training matrices with the <code>create_train_X_y</code> method in all the Forecasters.</li> </ul>"},{"location":"releases/releases.html#070-2023-03-21","title":"[0.7.0] - [2023-03-21]","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultiSeriesCustom</code>.</p> </li> <li> <p>Class <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code> (wrapper of pmdarima).</p> </li> <li> <p>Method <code>predict_interval()</code> to <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li> <p>Method <code>predict_bootstrapping()</code> to all forecasters, generate multiple forecasting predictions using a bootstrapping process.</p> </li> <li> <p>Method <code>predict_dist()</code> to all forecasters, fit a given probability distribution for each step using a bootstrapping process.</p> </li> <li> <p>Function <code>plot_prediction_distribution</code> in module <code>plot</code>.</p> </li> <li> <p>Alias <code>backtesting_forecaster_multivariate</code> for <code>backtesting_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>grid_search_forecaster_multivariate</code> for <code>grid_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>random_search_forecaster_multivariate</code> for <code>random_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Attribute <code>forecaster_id</code> to all Forecasters.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.7</code> compatibility.</p> </li> <li> <p>Added <code>python 3.11</code> compatibility.</p> </li> <li> <p><code>model_selection_statsmodels</code> is deprecated in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>. It will be removed in version 0.8.0.</p> </li> <li> <p>Remove <code>levels_weights</code> argument in <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, deprecated since version 0.6.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> renamed to <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>. Old names will be removed in version 0.8.0.</p> </li> <li> <p>Remove engine <code>'skopt'</code> in <code>bayesian_search_forecaster</code> in favor of engine <code>'optuna'</code>. To continue using it, use skforecast 0.6.0.</p> </li> <li> <p><code>in_sample_residuals</code> and <code>out_sample_residuals</code> are stored as numpy ndarrays instead of pandas series.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>set_out_sample_residuals()</code> is now expecting a <code>dict</code> for the <code>residuals</code> argument instead of a <code>pandas DataFrame</code>.</p> </li> <li> <p>Remove the <code>scikit-optimize</code> dependency.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Remove operator <code>**</code> in <code>set_params()</code> method for all forecasters.</p> </li> <li> <p>Replace <code>getfullargspec</code> in favor of <code>inspect.signature</code> (contribution by @jordisilv).</p> </li> </ul>"},{"location":"releases/releases.html#060-2022-11-30","title":"[0.6.0] - [2022-11-30]","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultivariate</code>.</p> </li> <li> <p>Function <code>initialize_lags</code> in <code>utils</code> module  to create lags values in the initialization of forecasters (applies to all forecasters).</p> </li> <li> <p>Function <code>initialize_weights</code> in <code>utils</code> module to check and initialize arguments <code>series_weights</code>and <code>weight_func</code> (applies to all forecasters).</p> </li> <li> <p>Argument <code>weights_func</code> in all Forecasters to allow weighted time series forecasting. Individual time based weights can be assigned to each value of the series during the model training.</p> </li> <li> <p>Argument <code>series_weights</code> in <code>ForecasterAutoregMultiSeries</code> to define individual weights each series.</p> </li> <li> <p>Include argument <code>random_state</code> in all Forecasters <code>set_out_sample_residuals</code> methods for random sampling with reproducible output.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>predict</code> and <code>predict_interval</code> methods allow the simultaneous prediction of multiple levels.</p> </li> <li> <p><code>backtesting_forecaster_multiseries</code> allows backtesting multiple levels simultaneously.</p> </li> <li> <p><code>metric</code> argument can be a list in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Function <code>multivariate_time_series_corr</code> in module <code>utils</code>.</p> </li> <li> <p>Function <code>plot_multivariate_time_series_corr</code> in module <code>plot</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> allows to predict specific steps.</p> </li> <li> <p>Remove <code>ForecasterAutoregMultiOutput</code> in favor of <code>ForecasterAutoregDirect</code>, (deprecated since 0.5.0).</p> </li> <li> <p>Rename function <code>exog_to_multi_output</code> to <code>exog_to_direct</code> in <code>utils</code> module.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, rename parameter <code>series_levels</code> to <code>series_col_names</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code> change type of <code>out_sample_residuals</code> to a <code>dict</code> of numpy ndarrays.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, delete argument <code>level</code> from method <code>set_out_sample_residuals</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>check_predict_input</code> function, argument <code>level</code> renamed to <code>levels</code> and <code>series_levels</code> renamed to <code>series_col_names</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>metrics_levels</code> output is now a pandas DataFrame.</p> </li> <li> <p>In <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, argument <code>levels_weights</code> is deprecated since version 0.6.0, and will be removed in version 0.7.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Refactor <code>_create_lags_</code> in <code>ForecasterAutoreg</code>, <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiSeries</code>. (suggested by Bennett https://github.com/Bennett561)</p> </li> <li> <p>Refactor <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, <code>filter_train_X_y_for_step</code> now starts at 1 (before 0).</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, DataFrame <code>y_train</code> now start with 1, <code>y_step_1</code> (before <code>y_step_0</code>).</p> </li> <li> <p>Remove <code>cv_forecaster</code> from module <code>model_selection</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, argument <code>last_window</code> predict method now works when it is a pandas DataFrame.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, fix bug transformers initialization.</p> </li> </ul>"},{"location":"releases/releases.html#051-2022-10-05","title":"[0.5.1] - [2022-10-05]","text":"<p>Added</p> <ul> <li> <p>Check that <code>exog</code> and <code>y</code> have the same length in <code>_evaluate_grid_hyperparameters</code> and <code>bayesian_search_forecaster</code> to avoid fit exception when <code>return_best</code>.</p> </li> <li> <p>Check that <code>exog</code> and <code>series</code> have the same length in <code>_evaluate_grid_hyperparameters_multiseries</code> to avoid fit exception when <code>return_best</code>.</p> </li> </ul> <p>Changed</p> <ul> <li>Argument <code>levels_list</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code> renamed to <code>levels</code>.</li> </ul> <p>Fixed</p> <ul> <li> <p><code>ForecasterAutoregMultiOutput</code> updated to match <code>ForecasterAutoregDirect</code>.</p> </li> <li> <p>Fix Exception to raise when <code>level_weights</code> does not add up to a number close to 1.0 (before was exactly 1.0) in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p><code>Create_train_X_y</code> in <code>ForecasterAutoregMultiSeries</code> now works when the forecaster is not fitted.</p> </li> </ul>"},{"location":"releases/releases.html#050-2022-09-23","title":"[0.5.0] - [2022-09-23]","text":"<p>Added</p> <ul> <li> <p>New arguments <code>transformer_y</code> (<code>transformer_series</code> for multiseries) and <code>transformer_exog</code> in all forecaster classes. It is for transforming (scaling, max-min, ...) the modeled time series and exogenous variables inside the forecaster.</p> </li> <li> <p>Functions in utils <code>transform_series</code> and <code>transform_dataframe</code> to carry out the transformation of the modeled time series and exogenous variables.</p> </li> <li> <p>Functions <code>_backtesting_forecaster_verbose</code>, <code>random_search_forecaster</code>, <code>_evaluate_grid_hyperparameters</code>, <code>bayesian_search_forecaster</code>, <code>_bayesian_search_optuna</code> and <code>_bayesian_search_skopt</code> in model_selection.</p> </li> <li> <p>Created <code>ForecasterAutoregMultiSeries</code> class for modeling multiple time series simultaneously.</p> </li> <li> <p>Created module <code>model_selection_multiseries</code>. Functions: <code>_backtesting_forecaster_multiseries_refit</code>, <code>_backtesting_forecaster_multiseries_no_refit</code>, <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p>Function <code>_check_interval</code> in utils. (suggested by Thomas Karaouzene https://github.com/tkaraouzene)</p> </li> <li> <p><code>metric</code> can be a list in <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, <code>backtesting_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Skforecast works with python 3.10.</p> </li> <li> <p>Functions <code>save_forecaster</code> and <code>load_forecaster</code> to module utils.</p> </li> <li> <p><code>get_feature_importance()</code> method checks if the forecast is fitted.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster</code> change default value of argument <code>fixed_train_size: bool=True</code>.</p> </li> <li> <p>Remove argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster</code> (deprecated since 0.4.2).</p> </li> <li> <p><code>backtesting_forecaster</code> verbose now includes fold size.</p> </li> <li> <p><code>grid_search_forecaster</code> results include the name of the used metric as column name.</p> </li> <li> <p>Remove <code>get_coef</code> method from <code>ForecasterAutoreg</code>, <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiOutput</code> (deprecated since 0.4.3).</p> </li> <li> <p><code>_get_metric</code> now allows <code>mean_squared_log_error</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput</code> has been renamed to <code>ForecasterAutoregDirect</code>. <code>ForecasterAutoregMultiOutput</code> will be removed in version 0.6.0.</p> </li> <li> <p><code>check_predict_input</code> updated to check <code>ForecasterAutoregMultiSeries</code> inputs.</p> </li> <li> <p><code>set_out_sample_residuals</code> has a new argument <code>transform</code> to transform the residuals before being stored.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p><code>fit</code> now stores <code>last_window</code> values with len = forecaster.max_lag in ForecasterAutoreg and ForecasterAutoregCustom.</p> </li> <li> <p><code>in_sample_residuals</code> stored as a <code>pd.Series</code> when <code>len(residuals) &gt; 1000</code>.</p> </li> </ul>"},{"location":"releases/releases.html#043-2022-03-18","title":"[0.4.3] - [2022-03-18]","text":"<p>Added</p> <ul> <li> <p>Checks if all elements in lags are <code>int</code> when creating ForecasterAutoreg and ForecasterAutoregMultiOutput.</p> </li> <li> <p>Add <code>fixed_train_size: bool=False</code> argument to <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code></p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename <code>get_metric</code> to <code>_get_metric</code>.</p> </li> <li> <p>Functions in model_selection module allow custom metrics.</p> </li> <li> <p>Functions in model_selection_statsmodels module allow custom metrics.</p> </li> <li> <p>Change function <code>set_out_sample_residuals</code> (ForecasterAutoreg and ForecasterAutoregCustom), <code>residuals</code> argument must be a <code>pandas Series</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p>Returned value of backtesting functions (model_selection and model_selection_statsmodels) is now a <code>float</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p><code>get_coef</code> and <code>get_feature_importance</code> methods unified in <code>get_feature_importance</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Requirements versions.</p> </li> <li> <p>Method <code>fit</code> doesn't remove <code>out_sample_residuals</code> each time the forecaster is fitted.</p> </li> <li> <p>Added random seed to residuals downsampling (ForecasterAutoreg and ForecasterAutoregCustom)</p> </li> </ul>"},{"location":"releases/releases.html#042-2022-01-08","title":"[0.4.2] - [2022-01-08]","text":"<p>Added</p> <ul> <li> <p>Increased verbosity of function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Random state argument in <code>backtesting_forecaster()</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Function <code>backtesting_forecaster()</code> do not modify the original forecaster.</p> </li> <li> <p>Deprecated argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Function <code>model_selection.time_series_spliter</code> renamed to <code>model_selection.time_series_splitter</code></p> </li> </ul> <p>Fixed</p> <ul> <li>Methods <code>get_coef</code> and <code>get_feature_importance</code> of <code>ForecasterAutoregMultiOutput</code> class return proper feature names.</li> </ul>"},{"location":"releases/releases.html#041-2021-12-13","title":"[0.4.1] - [2021-12-13]","text":"<p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li><code>fit</code> and <code>predict</code> transform pandas series and dataframes to numpy arrays if regressor is XGBoost.</li> </ul>"},{"location":"releases/releases.html#040-2021-12-10","title":"[0.4.0] - [2021-12-10]","text":"<p>Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit).</p> <p>Added</p> <ul> <li><code>ForecasterBase</code> as parent class</li> </ul> <p>Changed</p> <ul> <li> <p>Argument <code>y</code> must be pandas Series. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Argument <code>exog</code> must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Output of <code>predict</code> is a pandas Series with index according to the steps predicted.</p> </li> <li> <p>Scikitlearn pipelines are allowed as regressors.</p> </li> <li> <p><code>backtesting_forecaster</code> and <code>backtesting_forecaster_intervals</code> have been combined in a single function.</p> <ul> <li>It is possible to backtest forecasters already trained.</li> <li><code>ForecasterAutoregMultiOutput</code> allows incomplete folds.</li> <li>It is possible to update <code>out_sample_residuals</code> with backtesting residuals.</li> </ul> </li> <li> <p><code>cv_forecaster</code> has the option to update <code>out_sample_residuals</code> with backtesting residuals.</p> </li> <li> <p><code>backtesting_sarimax_statsmodels</code> and <code>cv_sarimax_statsmodels</code> have been combined in a single function.</p> </li> <li> <p><code>gridsearch_forecaster</code> use backtesting as validation strategy with the option of refit.</p> </li> <li> <p>Extended information when printing <code>Forecaster</code> object.</p> </li> <li> <p>All static methods for checking and preprocessing inputs moved to module utils.</p> </li> <li> <p>Remove deprecated class <code>ForecasterCustom</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#030-2021-09-01","title":"[0.3.0] - [2021-09-01]","text":"<p>Added</p> <ul> <li> <p>New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library:</p> <ul> <li><code>backtesting_autoreg_statsmodels</code></li> <li><code>cv_autoreg_statsmodels</code></li> <li><code>backtesting_sarimax_statsmodels</code></li> <li><code>cv_sarimax_statsmodels</code></li> <li><code>grid_search_sarimax_statsmodels</code></li> </ul> </li> <li> <p>Added attribute window_size to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>. It is equal to <code>max_lag</code>.</p> </li> </ul> <p>Changed</p> <ul> <li><code>cv_forecaster</code> returns cross-validation metrics and cross-validation predictions.</li> <li>Added an extra column for each parameter in the dataframe returned by <code>grid_search_forecaster</code>.</li> <li>statsmodels 0.12.2 added to requirements</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#020-2021-08-26","title":"[0.2.0] - [2021-08-26]","text":"<p>Added</p> <ul> <li> <p>Multiple exogenous variables can be passed as pandas DataFrame.</p> </li> <li> <p>Documentation at https://joaquinamatrodrigo.github.io/skforecast/</p> </li> <li> <p>New unit test</p> </li> <li> <p>Increased typing</p> </li> </ul> <p>Changed</p> <ul> <li>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#019-2021-07-27","title":"[0.1.9] - [2021-07-27]","text":"<p>Added</p> <ul> <li> <p>Logging total number of models to fit in <code>grid_search_forecaster</code>.</p> </li> <li> <p>Class <code>ForecasterAutoregCustom</code>.</p> </li> <li> <p>Method <code>create_train_X_y</code> to facilitate access to the training data matrix created from <code>y</code> and <code>exog</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p>Class <code>ForecasterCustom</code> has been renamed to <code>ForecasterAutoregCustom</code>. However, <code>ForecasterCustom</code> will still remain to keep backward compatibility.</p> </li> <li> <p>Argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p>Check if argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p><code>time_series_spliter</code> doesn't include the remaining observations in the last complete fold but in a new one when <code>allow_incomplete_fold=True</code>. Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.</p> </li> </ul> <p>Fixed</p> <ul> <li>Update lags of  <code>ForecasterAutoregMultiOutput</code> after <code>grid_search_forecaster</code>.</li> </ul>"},{"location":"releases/releases.html#0181-2021-05-17","title":"[0.1.8.1] - [2021-05-17]","text":"<p>Added</p> <ul> <li><code>set_out_sample_residuals</code> method to store or update out of sample residuals used by <code>predict_interval</code>.</li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster_intervals</code> and <code>backtesting_forecaster</code> print number of steps per fold.</p> </li> <li> <p>Only stored up to 1000 residuals.</p> </li> <li> <p>Improved verbose in <code>backtesting_forecaster_intervals</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Warning of inclompleted folds when using <code>backtesting_forecast</code> with a  <code>ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput.predict</code> allow exog data longer than needed (steps).</p> </li> <li> <p><code>backtesting_forecast</code> prints correctly the number of folds when remainder observations are cero.</p> </li> <li> <p>Removed named argument X in <code>self.regressor.predict(X)</code> to allow using XGBoost regressor.</p> </li> <li> <p>Values stored in <code>self.last_window</code> when training <code>ForecasterAutoregMultiOutput</code>. </p> </li> </ul>"},{"location":"releases/releases.html#018-2021-04-02","title":"[0.1.8] - [2021-04-02]","text":"<p>Added</p> <ul> <li>Class <code>ForecasterAutoregMultiOutput.py</code>: forecaster with direct multi-step predictions.</li> <li>Method <code>ForecasterCustom.predict_interval</code> and  <code>ForecasterAutoreg.predict_interval</code>: estimate prediction interval using bootstrapping.</li> <li><code>skforecast.model_selection.backtesting_forecaster_intervals</code> perform backtesting and return prediction intervals.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"releases/releases.html#017-2021-03-19","title":"[0.1.7] - [2021-03-19]","text":"<p>Added</p> <ul> <li>Class <code>ForecasterCustom</code>: same functionalities as <code>ForecasterAutoreg</code> but allows custom definition of predictors.</li> </ul> <p>Changed</p> <ul> <li><code>grid_search forecaster</code> adapted to work with objects <code>ForecasterCustom</code> in addition to <code>ForecasterAutoreg</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#016-2021-03-14","title":"[0.1.6] - [2021-03-14]","text":"<p>Added</p> <ul> <li>Method <code>get_feature_importances</code> to <code>skforecast.ForecasterAutoreg</code>.</li> <li>Added backtesting strategy in <code>grid_search_forecaster</code>.</li> <li>Added <code>backtesting_forecast</code> to <code>skforecast.model_selection</code>.</li> </ul> <p>Changed</p> <ul> <li>Method <code>create_lags</code> return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor.</li> <li>Renamed argument <code>X</code> to <code>last_window</code> in method <code>predict</code>.</li> <li>Renamed <code>ts_cv_forecaster</code> to <code>cv_forecaster</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#014-2021-02-15","title":"[0.1.4] - [2021-02-15]","text":"<p>Added</p> <ul> <li>Method <code>get_coef</code> to <code>skforecast.ForecasterAutoreg</code>.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"user_guides/autoregresive-forecaster.html","title":"Recursive multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 12:53:49 \nLast fit date: 2023-05-29 12:53:49 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[4]: <pre>2005-07-01    0.921840\n2005-08-01    0.954921\n2005-09-01    1.101716\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.00429855684785846\n</pre> In\u00a0[7]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, interval=[5, 95], n_boot=100) predictions.head(3) Out[7]: pred lower_bound upper_bound 2005-07-01 0.921840 0.884362 0.960898 2005-08-01 0.954921 0.923163 0.980901 2005-09-01 1.101716 1.057791 1.143083 In\u00a0[8]: Copied! <pre># Plot prediction interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_test.plot(ax=ax, label=\"test\")\npredictions['pred'].plot(ax=ax, label=\"prediction\")\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'blue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.legend(loc=\"upper left\");\n</pre> # Plot prediction interval # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_test.plot(ax=ax, label=\"test\") predictions['pred'].plot(ax=ax, label=\"prediction\") ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'blue',     alpha = 0.3,     label = '80% interval' ) ax.legend(loc=\"upper left\"); In\u00a0[9]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[9]: feature importance 0 lag_1 0.012340 1 lag_2 0.085160 2 lag_3 0.013407 3 lag_4 0.004374 4 lag_5 0.003188 5 lag_6 0.003436 6 lag_7 0.003136 7 lag_8 0.007141 8 lag_9 0.007831 9 lag_10 0.012751 10 lag_11 0.009019 11 lag_12 0.807098 12 lag_13 0.004811 13 lag_14 0.016328 14 lag_15 0.009979 In\u00a0[10]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) X_train.head() Out[10]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1992-12-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1993-01-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 1993-02-01 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 In\u00a0[11]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[11]: <pre>datetime\n1992-10-01    0.568606\n1992-11-01    0.595223\n1992-12-01    0.771258\n1993-01-01    0.751503\n1993-02-01    0.387554\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(data_train)\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(data_train) In\u00a0[13]: Copied! <pre># Predict using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressor.predict(X_train)\npredictions_training[:4]\n</pre> # Predict using the internal regressor # ============================================================================== predictions_training = forecaster.regressor.predict(X_train) predictions_training[:4] Out[13]: <pre>array([0.55271468, 0.56539424, 0.72799541, 0.73649315])</pre> In\u00a0[14]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting\u00b6","text":"<p>Recursive multi-step forecasting involves using the predicted values from previous time steps as input to forecast the values for the subsequent time steps. The model initially predicts one time step ahead and then uses that forecast as an input for the next time step, continuing this recursive process until the desired forecast horizon is reached.</p> <p>To clarify, since the value of $t_{n-1}$ is required to predict $t_{n}$, and $t_{n-1}$ is unknown, a recursive process is applied in which, each new prediction is based on the previous one.</p> <p></p> Diagram of recursive multi-step forecasting. <p>When using machine learning models for recursive multi-step forecasting, one of the major challenges is transforming the time series data into a matrix format that relates each value of the series to the preceding time window (lags). This process can be complex and time-consuming, requiring careful feature engineering to ensure the model can accurately capture the underlying patterns in the data.</p> <p></p> Time series transformation including an exogenous variable. <p>By using the classes <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>, it is possible to build machine learning models for recursive multi-step forecasting with ease. These classes can automatically transform the time series data into a matrix format suitable for input to a machine learning algorithm, and provide a range of options for tuning the model parameters to achieve the best possible performance.</p>"},{"location":"user_guides/autoregresive-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction-intervals","title":"Prediction intervals\u00b6","text":"<p>A prediction interval defines the interval within which the true value of <code>y</code> is expected to be found with a given probability. For example, the prediction interval (1, 99) can be expected to contain the true prediction value with 98% probability.</p> <p>By default, every forecaster can estimate prediction intervals using the method <code>predict_interval</code>. For a detailed explanation of the different prediction intervals available in skforecast visit Probabilistic forecasting.</p>"},{"location":"user_guides/autoregresive-forecaster.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p>"},{"location":"user_guides/autoregresive-forecaster.html#prediction-on-training-data","title":"Prediction on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data, the <code>predict()</code> method of the regressor stored inside the forecaster object can be accessed, and the training matrix can be passed as an input to this method. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p>"},{"location":"user_guides/backtesting.html","title":"Backtesting forecaster","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='validation')\nax.legend()\nplt.show()\n\ndisplay(data.head(4))\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='validation') ax.legend() plt.show()  display(data.head(4)) <pre>Train dates      : 1991-07-01 00:00:00 --- 2002-01-01 00:00:00  (n=127)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 In\u00a0[3]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = True  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = True,                           show_progress         = True                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[4]: Copied! <pre>print(f\"Backtest error: {metric}\")\npredictions.head(4)\n</pre> print(f\"Backtest error: {metric}\") predictions.head(4) <pre>Backtest error: 0.00818535931502708\n</pre> Out[4]: pred 2002-02-01 0.594506 2002-03-01 0.785886 2002-04-01 0.698925 2002-05-01 0.790560 In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax) predictions.plot(ax=ax) ax.legend(); <p>  \u00a0 Note </p> <p>In this example, although only the last 10 predictions (steps) are stored for model evaluation, the total number of predicted steps in each fold is 15 (steps + gap).</p> In\u00a0[6]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 5,\n                          allow_incomplete_fold = False,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = False  \n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 5,                           allow_incomplete_fold = False,                           refit                 = True,                           verbose               = True,                           show_progress         = False                         ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 7\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 5\n    Last fold has been excluded because it was incomplete.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-07-01 00:00:00 -- 2003-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2003-05-01 00:00:00 -- 2004-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2004-03-01 00:00:00 -- 2004-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2005-01-01 00:00:00 -- 2005-10-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-11-01 00:00:00 -- 2006-08-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-09-01 00:00:00 -- 2007-06-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-07-01 00:00:00 -- 2008-04-01 00:00:00  (n=10)\n\n</pre> In\u00a0[7]: Copied! <pre>print(f\"Backtest error: {metric}\")\npredictions.head(4)\n</pre> print(f\"Backtest error: {metric}\") predictions.head(4) <pre>Backtest error: 0.011194317866594718\n</pre> Out[7]: pred 2002-07-01 0.870897 2002-08-01 0.920207 2002-09-01 0.942503 2002-10-01 1.022695 In\u00a0[8]: Copied! <pre># Backtesting forecaster with prediction intervals\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          interval              = [5, 95],\n                          n_boot                = 500,\n                          verbose               = True,\n                          show_progress         = False\n                      )\n</pre> # Backtesting forecaster with prediction intervals # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           interval              = [5, 95],                           n_boot                = 500,                           verbose               = True,                           show_progress         = False                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> In\u00a0[9]: Copied! <pre>predictions.head()\n</pre> predictions.head() Out[9]: pred lower_bound upper_bound 2002-02-01 0.703579 0.599636 0.834616 2002-03-01 0.673522 0.572605 0.787942 2002-04-01 0.698319 0.589196 0.815748 2002-05-01 0.703042 0.593719 0.831795 2002-06-01 0.733776 0.632476 0.842865 In\u00a0[10]: Copied! <pre># Plot prediction intervals\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'red',\n    alpha = 0.2,\n    label = 'prediction interval'\n)\nax.legend();\n</pre> # Plot prediction intervals # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'red',     alpha = 0.2,     label = 'prediction interval' ) ax.legend(); In\u00a0[11]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(6, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(6, 3)) data.plot(ax=ax); <pre>Train dates      : 1992-04-01 00:00:00 --- 2002-01-01 00:00:00  (n=118)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> In\u00a0[12]: Copied! <pre># Backtest forecaster exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          steps                 = 10,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = True,\n                          show_progress         = False\n                      )\n</pre> # Backtest forecaster exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           steps                 = 10,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = True,                           show_progress         = False                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 118\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2002-01-01 00:00:00  (n=118)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2002-11-01 00:00:00  (n=128)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2003-09-01 00:00:00  (n=138)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1992-04-01 00:00:00 -- 2004-07-01 00:00:00  (n=148)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1992-04-01 00:00:00 -- 2005-05-01 00:00:00  (n=158)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1992-04-01 00:00:00 -- 2006-03-01 00:00:00  (n=168)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1992-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=178)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1992-04-01 00:00:00 -- 2007-11-01 00:00:00  (n=188)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> In\u00a0[13]: Copied! <pre>print(f\"Backtest error with exogenous variables: {metric}\")\n</pre> print(f\"Backtest error with exogenous variables: {metric}\") <pre>Backtest error with exogenous variables: 0.007800037462113706\n</pre> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend(loc=\"upper left\");\n</pre> fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:].plot(ax=ax) predictions.plot(ax=ax) ax.legend(loc=\"upper left\"); In\u00a0[15]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          steps                 = 10,\n                          metric                = custom_metric,\n                          initial_train_size    = len(data.loc[:end_train]),\n                          fixed_train_size      = False,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          verbose               = False,\n                          show_progress         = False\n                      )\n\nprint(f\"Backtest error custom metric: {metric}\")\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric   metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           steps                 = 10,                           metric                = custom_metric,                           initial_train_size    = len(data.loc[:end_train]),                           fixed_train_size      = False,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           verbose               = False,                           show_progress         = False                       )  print(f\"Backtest error custom metric: {metric}\") <pre>Backtest error custom metric: 0.005482666107945366\n</pre> In\u00a0[16]: Copied! <pre>metrics, predictions = backtesting_forecaster(\n                           forecaster            = forecaster,\n                           y                     = data['y'],\n                           steps                 = 10,\n                           metric                = ['mean_squared_error', 'mean_absolute_error'],\n                           initial_train_size    = len(data.loc[:end_train]),\n                           fixed_train_size      = False,\n                           gap                   = 0,\n                           allow_incomplete_fold = True,\n                           refit                 = True,\n                           verbose               = False,\n                           show_progress         = False\n                       )\n\nprint(f\"Backtest error metrics: {metrics}\")\n</pre> metrics, predictions = backtesting_forecaster(                            forecaster            = forecaster,                            y                     = data['y'],                            steps                 = 10,                            metric                = ['mean_squared_error', 'mean_absolute_error'],                            initial_train_size    = len(data.loc[:end_train]),                            fixed_train_size      = False,                            gap                   = 0,                            allow_incomplete_fold = True,                            refit                 = True,                            verbose               = False,                            show_progress         = False                        )  print(f\"Backtest error metrics: {metrics}\") <pre>Backtest error metrics: [0.00804575658744146, 0.06502216041298702]\n</pre> In\u00a0[17]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15 \n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15               )  forecaster.fit(y=data['y']) In\u00a0[18]: Copied! <pre># Backtesting on training data\n# ==============================================================================\nmetric, predictions_training = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data['y'],\n                                   steps              = 1,\n                                   metric             = 'mean_squared_error',\n                                   initial_train_size = None,\n                                   refit              = False,\n                                   verbose            = False,\n                                   show_progress      = False\n                               )\n\nprint(f\"Backtest training error: {metric}\")\n</pre> # Backtesting on training data # ============================================================================== metric, predictions_training = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data['y'],                                    steps              = 1,                                    metric             = 'mean_squared_error',                                    initial_train_size = None,                                    refit              = False,                                    verbose            = False,                                    show_progress      = False                                )  print(f\"Backtest training error: {metric}\") <pre>Backtest training error: 0.0005647902993476632\n</pre> In\u00a0[19]: Copied! <pre>predictions_training.head(4)\n</pre> predictions_training.head(4) Out[19]: pred 1993-07-01 0.515375 1993-08-01 0.558055 1993-09-01 0.597601 1993-10-01 0.618857 <p>It is important to note that the first 15 observations are excluded from the predictions since they are required to generate the lags that serve as predictors in the model.</p> In\u00a0[20]: Copied! <pre># Plot training predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata['y'].plot(ax=ax)\npredictions_training.plot(ax=ax)\nax.set_title(\"Backtesting on training data\")\nax.legend();\n</pre> # Plot training predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data['y'].plot(ax=ax) predictions_training.plot(ax=ax) ax.set_title(\"Backtesting on training data\") ax.legend(); In\u00a0[21]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/backtesting.html#backtesting","title":"Backtesting\u00b6","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)\u00b6","text":"<p>In this strategy, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p></p> <p></p> Backtesting with refit and increasing training size (fixed origin)."},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)\u00b6","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p></p> <p></p> Backtesting with refit and fixed training size (rolling origin)."},{"location":"user_guides/backtesting.html#backtesting-without-refit","title":"Backtesting without refit\u00b6","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p></p> <p></p> Backtesting without refit."},{"location":"user_guides/backtesting.html#backtesting-including-gap","title":"Backtesting including gap\u00b6","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p></p> Backtesting with refit and gap."},{"location":"user_guides/backtesting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/backtesting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/backtesting.html#backtest","title":"Backtest\u00b6","text":"<p>An example of a backtesting with refit consists of the following steps:</p> <ol> <li><p>Train the model using an initial training set, the length of which is specified by <code>initial_train_size</code>.</p> </li> <li><p>Once the model is trained, it is used to make predictions for the next 10 steps (<code>steps=10</code>) in the data. These predictions are saved for later evaluation.</p> </li> <li><p>As <code>refit</code> is set to <code>True</code>, the size of the training set is increased by adding the lats 10 data points (the previously predicted 10 steps), while the next 10 steps are used as test data.</p> </li> <li><p>After expanding the training set, the model is retrained using the updated training data and then used to predict the next 10 steps.</p> </li> <li><p>Repeat steps 3 and 4 until the entire series has been tested.</p> </li> </ol> <p>By following these steps, you can ensure that the model is evaluated on multiple sets of test data, thereby providing a more accurate assessment of its predictive power.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-gap","title":"Backtesting with gap\u00b6","text":"<p>The gap size can be adjusted with the <code>gap</code> argument. In addition, the <code>allow_incomplete_fold</code> parameter allows the last fold to be excluded from the analysis if it doesn't have the same size as the required number of <code>steps</code>.</p> <p>These arguments can be used in conjunction with either refit set to <code>True</code> or <code>False</code>, depending on the needs and objectives of the use case.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>Backtesting can be used not only to obtain point estimate predictions but also to obtain prediction intervals. Prediction intervals provide a range of values within which the actual values are expected to fall with a certain level of confidence. By estimating prediction intervals during backtesting, one can get a better understanding of the uncertainty associated with your model's predictions. This information can be used to assess the reliability of the model's predictions and to make more informed decisions. To learn more about probabilistic forecasting, visit Probabilistic forecasting.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies discussed in this document can also be applied when incorporating exogenous variables in the forecasting model.</p> <p>Exogenous variables are additional independent variables that can impact the value of the target variable being forecasted. These variables can provide valuable information to the model and improve its forecasting accuracy.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-custom-metric","title":"Backtesting with custom metric\u00b6","text":"<p>In addition to the commonly used metrics such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-multiple-metrics","title":"Backtesting with multiple metrics\u00b6","text":"<p>The <code>backtesting_forecaster</code> function provides a convenient way to estimate multiple metrics simultaneously by accepting a list of metrics as an input argument. This list can include any combination of built-in metrics, such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, as well as user-defined custom metrics.</p> <p>By specifying multiple metrics, users can obtain a more comprehensive evaluation of the model's predictive performance, which can help in selecting the best model for a particular task. Additionally, the ability to include custom metrics allows users to tailor the evaluation to specific use cases and domain-specific requirements.</p>"},{"location":"user_guides/backtesting.html#backtesting-on-training-data","title":"Backtesting on training data\u00b6","text":"<p>While the main focus of creating forecasting models is predicting future values, it's also useful to evaluate whether the model is learning from the training data. Predictions on the training data can help with this evaluation.</p> <p>To obtain predictions on the training data,the forecaster must first be fitted using the training dataset. Then, backtesting can be performed using  <code>backtesting_forecaster</code> and specifying the arguments <code>initial_train_size=None</code> and <code>refit=False</code>. This configuration enables backtesting on the same data that was used to train the model.</p>"},{"location":"user_guides/categorical-features.html","title":"Categorical features","text":"<p>In the field of machine learning, categorical features play a crucial role in determining the predictive ability of a model. Categorical features are features that can take a limited number of values, such as color, gender or location. While these features can provide useful insights into patterns and relationships within data, they also present unique challenges for machine learning models.</p> <p>One of these challenges is the need to transform categorical features before they can be used by most models. This transformation involves converting categorical values into numerical values that can be processed by machine learning algorithms.</p> <p>Another challenge is dealing with infrequent categories, which can lead to biased models. If a categorical feature has a large number of categories, but some of them are rare or appear infrequently in the data, the model may not be able to learn accurately from these categories, resulting in biased predictions and inaccurate results.</p> <p>Despite these difficulties, categorical features are still an essential component in many use cases. When properly encoded and handled, machine learning models can effectively learn from patterns and relationships in categorical data, leading to better predictions.</p> <p>This document provides an overview of three of the most commonly used transformations: one-hot encoding, ordinal encoding, and target encoding. It explains how to apply them in the skforecast package using scikit-learn encoders, which provide a convenient and flexible way to pre-process data. It also shows how to use the native implementation of three popular gradient boosting frameworks \u2013 LightGBM, scikit-learn's HistogramGradientBoosting, and XGBoost \u2013 to handle categorical features directly in the model.</p> <p>For a comprehensive demonstration of the use of categorical features in time series forecasting, check out the article Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost.</p> <p>  \u00a0 Note </p> <p>All of the transformations described in this document can be applied to the entire dataset, regardless of the forecaster. However, it is important to ensure that the transformations are learned only from the training data to avoid information leakage. Furthermore, the same transformation should be applied to the input data during prediction. To reduce the likelihood of errors and to ensure consistent application of the transformations, it is advisable to include the transformation within the forecaster object, so that it is handled internally.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import make_pipeline\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd  import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5  from lightgbm import LGBMRegressor from sklearn.ensemble import HistGradientBoostingRegressor from xgboost import XGBRegressor from catboost import CatBoostRegressor from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import make_column_transformer from sklearn.compose import make_column_selector from sklearn.pipeline import make_pipeline  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'\n       'learning-python/master/data/bike_sharing_dataset_clean.csv')\ndata = pd.read_csv(url)\n\n# Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('H')\ndata = data.sort_index()\ndata['holiday'] = data['holiday'].astype(int)\ndata = data[['holiday', 'weather', 'temp', 'hum', 'users']]\ndata[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str)\nprint(data.dtypes)\ndata.head(3)\n</pre> # Downloading data # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-'        'learning-python/master/data/bike_sharing_dataset_clean.csv') data = pd.read_csv(url)  # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('H') data = data.sort_index() data['holiday'] = data['holiday'].astype(int) data = data[['holiday', 'weather', 'temp', 'hum', 'users']] data[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str) print(data.dtypes) data.head(3) <pre>holiday     object\nweather     object\ntemp       float64\nhum        float64\nusers      float64\ndtype: object\n</pre> Out[2]: holiday weather temp hum users date_time 2011-01-01 00:00:00 0 clear 9.84 81.0 16.0 2011-01-01 01:00:00 0 clear 9.02 80.0 40.0 2011-01-01 02:00:00 0 clear 9.02 80.0 32.0 <p>Only part of the data is used to simplify the example.</p> In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nstart_train = '2012-06-01 00:00:00'\nend_train = '2012-07-31 23:59:00'\nend_test = '2012-08-15 23:59:00'\ndata_train = data.loc[start_train:end_train, :]\ndata_test  = data.loc[end_train:end_test, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split train-test # ============================================================================== start_train = '2012-06-01 00:00:00' end_train = '2012-07-31 23:59:00' end_test = '2012-08-15 23:59:00' data_train = data.loc[start_train:end_train, :] data_test  = data.loc[end_train:end_test, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Dates train : 2012-06-01 00:00:00 --- 2012-07-31 23:00:00  (n=1464)\nDates test  : 2012-08-01 00:00:00 --- 2012-08-15 23:00:00  (n=360)\n</pre> In\u00a0[4]: Copied! <pre># ColumnTransformer with one-hot encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical features (no numerical)\n# using one-hot encoding. Numeric features are left untouched. For binary\n# features, only one column is created.\none_hot_encoder = make_column_transformer(\n                    (\n                        OneHotEncoder(sparse_output=False, drop='if_binary'),\n                        make_column_selector(dtype_exclude=np.number)\n                    ),\n                    remainder=\"passthrough\",\n                    verbose_feature_names_out=False,\n                ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with one-hot encoding # ============================================================================== # A ColumnTransformer is used to transform categorical features (no numerical) # using one-hot encoding. Numeric features are left untouched. For binary # features, only one column is created. one_hot_encoder = make_column_transformer(                     (                         OneHotEncoder(sparse_output=False, drop='if_binary'),                         make_column_selector(dtype_exclude=np.number)                     ),                     remainder=\"passthrough\",                     verbose_feature_names_out=False,                 ).set_output(transform=\"pandas\") In\u00a0[5]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=123),\n                 lags = 5,\n                 transformer_exog = one_hot_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=123),                  lags = 5,                  transformer_exog = one_hot_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002D2D73CEC50&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: H \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:08:20 \nLast fit date: 2023-05-29 13:08:20 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>Once the forecaster has been trained, the transformer can be inspected (feature_names_in, feature_names_out, ...) by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[6]: Copied! <pre># Access to the transformer used for exogenous features\n# ==============================================================================\nprint(forecaster.transformer_exog.get_feature_names_out())\nforecaster.transformer_exog\n</pre> # Access to the transformer used for exogenous features # ============================================================================== print(forecaster.transformer_exog.get_feature_names_out()) forecaster.transformer_exog <pre>['holiday_1' 'weather_clear' 'weather_mist' 'weather_rain' 'temp' 'hum']\n</pre> Out[6]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002D2D73CEC50&gt;)],\n                  verbose_feature_names_out=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002D2D73CEC50&gt;)],\n                  verbose_feature_names_out=False)</pre>onehotencoder<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002D2D73CEC50&gt;</pre>OneHotEncoder<pre>OneHotEncoder(drop='if_binary', sparse_output=False)</pre>remainder<pre>['temp', 'hum']</pre>passthrough<pre>passthrough</pre> In\u00a0[7]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[7]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>  \u00a0 Note </p> <p>It is possible to apply a transformation to the entire dataset independent of the forecaster. However, it is crucial to ensure that the transformations are only learned from the training data to avoid information leakage. In addition, the same transformation should be applied to the input data during prediction. It is therefore advisable to incorporate the transformation into the forecaster, so that it is handled internally. This approach ensures consistency in the application of transformations and reduces the likelihood of errors.</p> <p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y()</code> method to generate the matrices used by the forecaster to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p> In\u00a0[8]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) print(X_train.dtypes) X_train.head() <pre>lag_1            float64\nlag_2            float64\nlag_3            float64\nlag_4            float64\nlag_5            float64\nholiday_1        float64\nweather_clear    float64\nweather_mist     float64\nweather_rain     float64\ntemp             float64\nhum              float64\ndtype: object\n</pre> Out[8]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 0.0 1.0 0.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 1.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 1.0 0.0 0.0 13.12 76.0 In\u00a0[9]: Copied! <pre># Transform exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features])\nexog_transformed.head()\n</pre> # Transform exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features]) exog_transformed.head() Out[9]: holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 00:00:00 0.0 1.0 0.0 0.0 9.84 81.0 2011-01-01 01:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 02:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 03:00:00 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 04:00:00 0.0 1.0 0.0 0.0 9.84 75.0 In\u00a0[10]: Copied! <pre># ColumnTransformer with ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\nordinal_encoder = make_column_transformer(\n                    (\n                        OrdinalEncoder(\n                            handle_unknown='use_encoded_value',\n                            unknown_value=-1,\n                            encoded_missing_value=-1\n                        ),\n                        make_column_selector(dtype_exclude=np.number)\n                    ),\n                    remainder=\"passthrough\",\n                    verbose_feature_names_out=False,\n                ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. ordinal_encoder = make_column_transformer(                     (                         OrdinalEncoder(                             handle_unknown='use_encoded_value',                             unknown_value=-1,                             encoded_missing_value=-1                         ),                         make_column_selector(dtype_exclude=np.number)                     ),                     remainder=\"passthrough\",                     verbose_feature_names_out=False,                 ).set_output(transform=\"pandas\") In\u00a0[11]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                regressor = LGBMRegressor(random_state=123),\n                lags = 5,\n                transformer_exog = ordinal_encoder\n             )\n\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                 regressor = LGBMRegressor(random_state=123),                 lags = 5,                 transformer_exog = ordinal_encoder              )  forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[11]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('ordinalencoder',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002D2D74B36A0&gt;)],\n                  verbose_feature_names_out=False) \nWindow size: 5 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['holiday', 'weather', 'temp', 'hum'] \nTraining range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: H \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:08:20 \nLast fit date: 2023-05-29 13:08:21 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[12]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) print(X_train.dtypes) X_train.head() <pre>lag_1      float64\nlag_2      float64\nlag_3      float64\nlag_4      float64\nlag_5      float64\nholiday    float64\nweather    float64\ntemp       float64\nhum        float64\ndtype: object\n</pre> Out[12]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 1.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 0.0 13.12 76.0 <p>Once the forecaster has been trained, the transformer can be inspected by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[13]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[13]: <pre>2012-08-01 00:00:00    89.096098\n2012-08-01 01:00:00    57.749964\n2012-08-01 02:00:00    29.263922\nFreq: H, Name: pred, dtype: float64</pre> <p>  \u00a0 Warning </p> <p>We are currently working to allow this type of transformation within skforecast.</p> <p>  \u00a0 Warning </p> <p>When deploying models in production, it is strongly recommended to avoid using automatic detection based on pandas <code>category</code> type columns. Although pandas provides an internal coding for these columns, it is not consistent across different datasets and may vary depending on the categories present in each one. It is therefore crucial to be aware of this issue and to take appropriate measures to ensure consistency in the coding of categorical features when deploying models in production.</p> <p>At the time of writing, the only thing the authors have observed is that LightGBM internally manages changes in the coding of categories. More details on this issue can be found in  github issue and  stackoverflow.</p> <p>If the user still wishes to rely on automatic detection of categorical features based on pandas data types, categorical variables must first be encoded as integers (ordinal encoding) and then stored as category type. This is necessary because skforecast uses a numeric numpy array internally to speed up the calculation.</p> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features (recommended)</p> <p>When creating a forecaster with <code>LGBMRegressor</code>, it is necessary to specify the names of the categorical columns using the <code>fit_kwargs</code> argument. This is because the <code>categorical_feature</code> argument is only specified in the <code>fit</code> method of <code>LGBMRegressor</code>, and not during its initialization.</p> In\u00a0[14]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[15]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=963),\n                 lags = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs = {'categorical_feature': categorical_features}\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=963),                  lags = 5,                  transformer_exog = transformer_exog,                  fit_kwargs = {'categorical_feature': categorical_features}              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n  _log_warning('Using categorical_feature in Dataset.')\n</pre> <p>The UserWarning raised indicates that categorical features have been included. To suppress this warning use <code>warnings.filterwarnings('ignore')</code>. Warnings can then be restored using <code>warnings.filterwarnings('default')</code> or <code>warnings.resetwarnings()</code></p> In\u00a0[16]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[16]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>Allow the model to automatically detect categorical features (not recommended)</p> <p>  \u00a0 Warning </p> <p>Handling categorical variables by relying on the automatic detection of the <code>category</code> datatype can be achieved by setting <code>categorical_features='auto'</code> during model initialization. However, this approach can lead to significant problems when the model is exposed to new datasets that have a different pandas encoding for categorical columns than the one used during training. Therefore, it's crucial to ensure that the encoding is consistent between the training and the testing datasets to avoid any potential errors.</p> In\u00a0[17]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            FunctionTransformer(\n                                func=lambda x: x.astype('category'),\n                                feature_names_out= 'one-to-one'\n                            )\n                       )\n\ntransformer_exog = make_column_transformer(\n                        (\n                            pipeline_categorical,\n                            make_column_selector(dtype_exclude=np.number)\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             FunctionTransformer(                                 func=lambda x: x.astype('category'),                                 feature_names_out= 'one-to-one'                             )                        )  transformer_exog = make_column_transformer(                         (                             pipeline_categorical,                             make_column_selector(dtype_exclude=np.number)                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[18]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state=963),\n                 lags = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs = {'categorical_feature': 'auto'}\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state=963),                  lags = 5,                  transformer_exog = transformer_exog,                  fit_kwargs = {'categorical_feature': 'auto'}              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[19]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[19]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: H, Name: pred, dtype: float64</pre> <p>As with any other forecaster, the matrices used during model training can be created with <code>create_train_X_y</code>.</p> In\u00a0[20]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'],\n                        exog = data.loc[:end_train, exog_features]\n                   )\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'],                         exog = data.loc[:end_train, exog_features]                    ) X_train.head() Out[20]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0 0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0 0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0 0 13.12 76.0 <p>When creating a forecaster using <code>HistogramGradientBoosting</code>, the names of the categorical columns should be specified during the instantiation by passing them as a list to the <code>categorical_feature</code> argument.</p> In\u00a0[21]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[22]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(\n                                 categorical_features = categorical_features,\n                                 random_state = 963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(                                  categorical_features = categorical_features,                                  random_state = 963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) <p><code>HistGradientBoostingRegressor</code> stores a boolean mask indicating which features were considered categorical. It will be <code>None</code> if there are no categorical features.</p> In\u00a0[23]: Copied! <pre>forecaster.regressor.is_categorical_\n</pre> forecaster.regressor.is_categorical_ Out[23]: <pre>array([False, False, False, False, False,  True,  True, False, False])</pre> In\u00a0[24]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[24]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: H, Name: pred, dtype: float64</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features (recommended)</p> <p>At the time of writing, the <code>XGBRegressor</code> module does not provide an option to specify the names of categorical features. Instead, the feature types are specified by passing a list of strings to the <code>feature_types</code> argument, where 'c' denotes categorical and 'q' numeric features. The <code>enable_categorical</code> argument must also be set to <code>True</code>.</p> <p>Determining the positions of each column to create a list of feature types can be a challenging task. The shape of the data matrix depends on two factors, the number of lags used and the transformations applied to the exogenous variables. However, there is a workaround to this problem. First, create a forecaster without specifying the <code>feature_types</code> argument. Next, the <code>create_train_X_y</code> method can be used with a small sample of data to determine the position of each feature. Once the position of each feature has been determined, the <code>set_params()</code> method can be used to specify the values of <code>feature_types</code>. By following this approach it is possible to ensure that the feature types are correctly specified, thus avoiding any errors that may occur due to incorrect specification.</p> In\u00a0[25]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                        (\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            categorical_features\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                         (                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             categorical_features                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") <p>A forecaster is created without specifying the <code>feature_types</code> argument.</p> In\u00a0[26]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                               tree_method='hist',\n                               random_state=12345,\n                               enable_categorical=True,\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                tree_method='hist',                                random_state=12345,                                enable_categorical=True,                              ),                  lags = 5,                  transformer_exog = transformer_exog              ) <p>Once the forecaster is instantiated, its <code>create_train_X_y()</code> method is used to generate the training matrices that allow the user to identify the positions of the variables.</p> In\u00a0[27]: Copied! <pre># Create training matrices using a sample of the training data\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y = data.loc[:end_train, 'users'][:10],\n                        exog = data.loc[:end_train, exog_features][:10]\n                   )\nX_train.head(2)\n</pre> # Create training matrices using a sample of the training data # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y = data.loc[:end_train, 'users'][:10],                         exog = data.loc[:end_train, exog_features][:10]                    ) X_train.head(2) Out[27]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 <p>Create a list to identify which columns in the training matrix are numeric ('q') and categorical ('c').</p> In\u00a0[28]: Copied! <pre>feature_types = ['c' if col in categorical_features else 'q' for col in X_train.columns]\nfeature_types\n</pre> feature_types = ['c' if col in categorical_features else 'q' for col in X_train.columns] feature_types Out[28]: <pre>['q', 'q', 'q', 'q', 'q', 'c', 'c', 'q', 'q']</pre> <p>Update the regressor parameters using the forecaster's <code>set_params</code> method and fit.</p> In\u00a0[29]: Copied! <pre># Update regressor parameters\n# ==============================================================================\nforecaster.set_params({'feature_types': feature_types})\n</pre> # Update regressor parameters # ============================================================================== forecaster.set_params({'feature_types': feature_types}) In\u00a0[30]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[31]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[31]: <pre>2012-08-01 00:00:00    77.470787\n2012-08-01 01:00:00    40.735706\n2012-08-01 02:00:00    13.448755\nFreq: H, Name: pred, dtype: float64</pre> <p>Allow the model to automatically detect categorical features (not recommended)</p> <p>  \u00a0 Warning </p> <p>Handling categorical variables by relying on the automatic detection of the <code>category</code> datatype can be achieved by setting <code>enable_categorical=True</code> during model initialization. However, this approach can lead to significant problems when the model is exposed to new datasets that have a different pandas encoding for categorical columns than the one used during training. Therefore, it's crucial to ensure that the encoding is consistent between the training and the testing datasets to avoid any potential errors.</p> In\u00a0[32]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After the encoding, the features are converted back to category type so \n# that they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                            OrdinalEncoder(\n                                dtype=int,\n                                handle_unknown=\"use_encoded_value\",\n                                unknown_value=-1,\n                                encoded_missing_value=-1\n                            ),\n                            FunctionTransformer(\n                                func=lambda x: x.astype('category'),\n                                feature_names_out= 'one-to-one'\n                            )\n                       )\n\ntransformer_exog = make_column_transformer(\n                        (\n                            pipeline_categorical,\n                            make_column_selector(dtype_exclude=np.number)\n                        ),\n                        remainder=\"passthrough\",\n                        verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After the encoding, the features are converted back to category type so  # that they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                             OrdinalEncoder(                                 dtype=int,                                 handle_unknown=\"use_encoded_value\",                                 unknown_value=-1,                                 encoded_missing_value=-1                             ),                             FunctionTransformer(                                 func=lambda x: x.astype('category'),                                 feature_names_out= 'one-to-one'                             )                        )  transformer_exog = make_column_transformer(                         (                             pipeline_categorical,                             make_column_selector(dtype_exclude=np.number)                         ),                         remainder=\"passthrough\",                         verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[33]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(\n                                 enable_categorical=True,\n                                 tree_method='hist',\n                                 random_state=963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(                                  enable_categorical=True,                                  tree_method='hist',                                  random_state=963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[34]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[34]: <pre>2012-08-01 00:00:00    77.470787\n2012-08-01 01:00:00    40.735706\n2012-08-01 02:00:00    13.448755\nFreq: H, Name: pred, dtype: float64</pre> In\u00a0[35]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/categorical-features.html#categorical-features","title":"Categorical features\u00b6","text":""},{"location":"user_guides/categorical-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":"<p>The dataset used in this user guide consists of information on the number of users of a bicycle rental service, in addition to weather variables and holiday data. Two of the variables in the dataset, <code>holiday</code> and <code>weather</code>, are categorical.</p>"},{"location":"user_guides/categorical-features.html#one-hot-encoding","title":"One Hot Encoding\u00b6","text":"<p>One hot encoding, also known as dummy encoding or one-of-K encoding, consists of replacing the categorical variable with a set of binary variables that take the value 0 or 1 to indicate whether a particular category is present in an observation. For example, suppose a dataset contains a categorical variable called \"color\" with the possible values of \"red,\" \"blue,\" and \"green\". Using one hot encoding, this variable is converted into three binary variables such as <code>color_red</code>, <code>color_blue</code>, and <code>color_green</code>, where each variable takes a value of 0 or 1 depending on the category.</p> <p>The OneHotEncoder class in scikit-learn can be used to transform any categorical feature with n possible values into n new binary features, where one of them takes the value 1, and all the others take the value 0. The <code>OneHotEncoder</code> can be configured to handle certain corner cases, including unknown categories, missing values, and infrequent categories.</p> <ul> <li><p>When <code>handle_unknown='ignore'</code> and <code>drop</code> is not <code>None</code>, unknown categories are encoded as zeros. Additionally, if a feature contains both <code>np.nan</code> and <code>None</code>, they are considered separate categories.</p> </li> <li><p>It supports the aggregation of infrequent categories into a single output for each feature. The parameters to enable the aggregation of infrequent categories are <code>min_frequency</code> and <code>max_categories</code>. By setting <code>handle_unknown</code> to 'infrequent_if_exist', unknown categories are considered infrequent.</p> </li> <li><p>To avoid collinearity between features, it is possible to drop one of the categories per feature using the <code>drop</code> argument. This is especially important when using linear models.</p> </li> </ul> <p>ColumnTransformers in scikit-learn provide a powerful way to define transformations and apply them to specific features. By encapsulating the <code>OneHotEncoder</code> in a <code>ColumnTransformer</code> object, it can be passed to a forecaster using the <code>transformer_exog</code> argument.</p>"},{"location":"user_guides/categorical-features.html#ordinal-encoding","title":"Ordinal encoding\u00b6","text":"<p>Ordinal encoding is a technique used to convert categorical variables into numerical variables. Each category is assigned a unique numerical value based on its order or rank, as determined by a chosen criterion such as frequency or importance. This encoding method is particularly useful when categories have a natural order or ranking, such as educational qualifications. However, it is important to note that the numerical values assigned to each category do not represent any inherent numerical difference between them, but simply provide a numerical representation.</p> <p>The scikit-learn library provides the OrdinalEncoder class, which allows users to replace categorical variables with ordinal numbers ranging from 0 to n_categories-1. In addition, this class includes the <code>encoded_missing_value</code> parameter, which allows for the encoding of missing values. It is important to note that this implementation arbitrarily assigns numbers to categories on a first-seen-first-served basis. Users should therefore exercise caution when interpreting the numerical values assigned to the categories. Other implementations, such as the Feature-engine, numbers can be ordered based on the mean of the target.</p>"},{"location":"user_guides/categorical-features.html#target-encoding","title":"Target encoding\u00b6","text":"<p>Target encoding is a technic that encodes categorical variables based on the relationship between the categories and the target variable. Each category is encoded based on a shrinked estimate of the average target values for observations belonging to the category. The encoding scheme mixes the global target mean with the target mean conditioned on the value of the category.</p> <p>For example, suppose a categorical variable \"City\" with categories \"New York,\" \"Los Angeles,\" and \"Chicago,\" and a target variable \"Salary.\" One can calculate the mean salary for each city category based on the training data, and use these mean values to encode the categories.</p> <p>This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories are location based such as zip code or region.</p> <p>The TargetEncoder class is available in Scikit-learn (since version 1.3). <code>TargetEncoder</code> considers missing values, such as <code>np.nan</code> or <code>None</code>, as another category and encodes them like any other category. Categories that are not seen during fit are encoded with the target mean, i.e. <code>target_mean_</code>. A more detailed description of target encoding can be found in the scikit-learn user guide.</p>"},{"location":"user_guides/categorical-features.html#native-implementation-for-categorical-features","title":"Native implementation for categorical features\u00b6","text":"<p>Some machine learning models, including XGBoost, LightGBM, CatBoost, and HistGradientBoostingRegressor, provide built-in methods to handle categorical features, but they assume that the input categories are integers starting from 0 up to the number of categories [0, 1, ..., n_categories-1]. In practice, categorical variables are not coded with numbers but with strings, so an intermediate transformation step is necessary. Two options are:</p> <ul> <li><p>Set columns with categorical variables to the type <code>category</code>. For each column, the data structure consists of an array of categories and an array of integer values (codes) that point to the actual value of the array of categories. That is, internally it is a numeric array with a mapping that relates each value to a category. Models are able to automatically identify the columns of type <code>category</code> and access their internal codes. This approach is applicable to XGBoost, LightGBM and CatBoost.</p> </li> <li><p>Preprocess the categorical columns with an <code>OrdinalEncoder</code> to transform their values to integers and explicitly indicate that the columns should be treated as categorical. Skforecast allows this by using the <code>fit_kwargs</code> argument.</p> </li> </ul>"},{"location":"user_guides/categorical-features.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/categorical-features.html#scikit-learn-histogramgradientboosting","title":"Scikit-learn HistogramGradientBoosting\u00b6","text":""},{"location":"user_guides/categorical-features.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/categorical-features.html#catboost","title":"CatBoost\u00b6","text":"<p>Unfortunately, the current version of skforecast is not compatible with CatBoost's built-in handling of categorical features. The issue arises because CatBoost only accepts categorical features as integers, while skforecast converts input data to floats for faster computation using numpy arrays in the internal prediction process. If a CatBoost model is required, an external encoder should be used for the categorical variables.</p>"},{"location":"user_guides/custom-predictors.html","title":"Custom predictors","text":"<p>  \u00a0 Warning </p> <p>The <code>window_size</code> parameter specifies the size of the data window that <code>fun_predictors</code> uses to generate each row of predictors. Choosing the appropriate value for this parameter is crucial to avoid losing data when constructing the training matrices. Be sure to provide the correct value for <code>window_size</code> when using <code>fun_predictors</code>.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/' \n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'      'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Custom function to create predictors\n# ==============================================================================\ndef create_predictors(y):\n\"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    \"\"\"\n\n    lags = y[-1:-11:-1]     # window_size = 10\n    mean = np.mean(y[-20:]) # window_size = 20\n    predictors = np.hstack([lags, mean])\n\n    return predictors\n</pre> # Custom function to create predictors # ============================================================================== def create_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     \"\"\"      lags = y[-1:-11:-1]     # window_size = 10     mean = np.mean(y[-20:]) # window_size = 20     predictors = np.hstack([lags, mean])      return predictors In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregCustom(\n                 regressor      = RandomForestRegressor(random_state=123),\n                 fun_predictors = create_predictors,\n                 name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],\n                 window_size    = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom(                  regressor      = RandomForestRegressor(random_state=123),                  fun_predictors = create_predictors,                  name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],                  window_size    = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(y=data_train) forecaster Out[4]: <pre>======================= \nForecasterAutoregCustom \n======================= \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_predictors \nTransformer for y: None \nTransformer for exog: None \nWindow size: 20 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:09:08 \nLast fit date: 2023-05-29 13:09:08 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps) predictions.head(3) Out[5]: <pre>2005-07-01    0.926598\n2005-08-01    0.948202\n2005-09-01    1.020947\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.04487765885818191\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[8]: feature importance 0 lag 1 0.539720 1 lag 2 0.119097 2 lag 3 0.046404 3 lag 4 0.024165 4 lag 5 0.030567 5 lag 6 0.015139 6 lag 7 0.042883 7 lag 8 0.012742 8 lag 9 0.018938 9 lag 10 0.108639 10 moving_avg_20 0.041707 In\u00a0[9]: Copied! <pre>X, y = forecaster.create_train_X_y(data_train)\nX.head()\n</pre> X, y = forecaster.create_train_X_y(data_train) X.head() Out[9]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 datetime 1993-03-01 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.496401 1993-04-01 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.496275 1993-05-01 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.496924 1993-06-01 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.475463 0.496759 1993-07-01 0.470126 0.428859 0.413890 0.427283 0.387554 0.751503 0.771258 0.595223 0.568606 0.534761 0.495638 In\u00a0[10]: Copied! <pre>y.head()\n</pre> y.head() Out[10]: <pre>datetime\n1993-03-01    0.427283\n1993-04-01    0.413890\n1993-05-01    0.428859\n1993-06-01    0.470126\n1993-07-01    0.509210\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[11]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/simulated_items_sales.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/simulated_items_sales.csv') data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[11]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[12]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00  (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00  (n=170)\n</pre> In\u00a0[13]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7.5, 4), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); In\u00a0[14]: Copied! <pre># Custom function to create predictors\n# ==============================================================================\ndef create_complex_predictors(y):\n\"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    \"\"\"\n\n    lags = y[-1:-11:-1]     # window_size = 10  \n    mean = np.mean(y[-20:]) # window_size = 20\n    predictors = np.hstack([lags, mean])\n\n    return predictors\n</pre> # Custom function to create predictors # ============================================================================== def create_complex_predictors(y):     \"\"\"     Create first 10 lags of a time series.     Calculate moving average with window 20.     \"\"\"      lags = y[-1:-11:-1]     # window_size = 10       mean = np.mean(y[-20:]) # window_size = 20     predictors = np.hstack([lags, mean])      return predictors In\u00a0[15]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeriesCustom(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 fun_predictors  = create_complex_predictors,\n                 name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],\n                 window_size     = 20 # window_size needed by the mean is the most restrictive one\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiSeriesCustom(                  regressor       = RandomForestRegressor(random_state=123),                  fun_predictors  = create_complex_predictors,                  name_predictors = [f'lag {i}' for i in range(1, 11)] + ['moving_avg_20'],                  window_size     = 20 # window_size needed by the mean is the most restrictive one              )  forecaster.fit(series=data_train) forecaster Out[15]: <pre>================================== \nForecasterAutoregMultiSeriesCustom \n================================== \nRegressor: RandomForestRegressor(random_state=123) \nPredictors created with function: create_complex_predictors \nTransformer for series: None \nTransformer for exog: None \nWindow size: 20 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:09:10 \nLast fit date: 2023-05-29 13:09:13 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[16]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps, levels=None)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps, levels=None) predictions.head(3) Out[16]: item_1 item_2 item_3 2014-07-16 25.737550 11.265323 11.328083 2014-07-17 25.649209 10.784593 12.283007 2014-07-18 25.602333 11.273438 12.722012 In\u00a0[17]: Copied! <pre>X_train, y_train, _, _ = forecaster.create_train_X_y(data_train)\nX_train.head()\n</pre> X_train, y_train, _, _ = forecaster.create_train_X_y(data_train) X_train.head() Out[17]: lag 1 lag 2 lag 3 lag 4 lag 5 lag 6 lag 7 lag 8 lag 9 lag 10 moving_avg_20 item_1 item_2 item_3 0 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 21.717691 21.954065 1.0 0.0 0.0 1 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 21.751748 22.666583 1.0 0.0 0.0 2 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 21.758617 22.567441 1.0 0.0 0.0 3 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 20.784194 22.389038 1.0 0.0 0.0 4 28.018830 23.981037 20.794986 22.503533 24.018768 24.772249 29.245869 26.636444 20.228468 18.976196 22.495203 1.0 0.0 0.0 In\u00a0[18]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[18]: <pre>0    22.503533\n1    20.794986\n2    23.981037\n3    28.018830\n4    28.747482\nName: y, dtype: float64</pre> In\u00a0[19]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/custom-predictors.html#custom-predictors-for-recursive-multi-step-forecasting","title":"Custom predictors for recursive multi-step forecasting\u00b6","text":"<p>In forecasting time series data, it can be useful to consider additional characteristics beyond just the lagged values. For instance, the moving average of the previous n values can help capture the trend in the series.</p> <p>The <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes are similar to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregMultiSeries</code>, respectively, but they allow the user to define their own function for generating predictors.</p> <p>To create a custom predictor function, the user needs to write a function that takes a time series as input (a NumPy ndarray) and outputs another NumPy ndarray containing the predictors. The Forecaster parameters used to specify this function are <code>fun_predictors</code> and <code>window_size</code>.</p>"},{"location":"user_guides/custom-predictors.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/custom-predictors.html#forecasterautoregcustom","title":"ForecasterAutoregCustom\u00b6","text":""},{"location":"user_guides/custom-predictors.html#custom-predictors","title":"Custom predictors\u00b6","text":""},{"location":"user_guides/custom-predictors.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/custom-predictors.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/custom-predictors.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/custom-predictors.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":""},{"location":"user_guides/custom-predictors.html#forecasterautoregmultiseriescustom","title":"ForecasterAutoregMultiSeriesCustom\u00b6","text":"<p>All the methods available for <code>ForecasterAutoregMultiSeries</code> can also be applied to ForecasterAutoregMultiSeriesCustom. To learn more about these methods, please refer to the Multi-series forecasting page.</p>"},{"location":"user_guides/custom-predictors.html#train","title":"Train\u00b6","text":""},{"location":"user_guides/custom-predictors.html#predict","title":"Predict\u00b6","text":"<p>If no value is specified for the <code>levels</code> argument, predictions will be computed for all available levels.</p>"},{"location":"user_guides/custom-predictors.html#training-matrices","title":"Training matrices\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html","title":"Dependent multivariate series forecasting","text":"<p>  \u00a0 Note </p> <p>See ForecasterAutoregMultiSeries for independent multi-series forecasting.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import random_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries from skforecast.model_selection_multiseries import random_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/guangyuan_air_pollution.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata = data[['CO', 'SO2', 'PM2.5']]\ndata.head()\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/guangyuan_air_pollution.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data = data[['CO', 'SO2', 'PM2.5']] data.head() Out[2]: CO SO2 PM2.5 date 2013-03-01 9600.0 204.0 181.0 2013-03-02 20198.0 674.0 633.0 2013-03-03 47195.0 1661.0 1956.0 2013-03-04 15000.0 485.0 438.0 2013-03-05 59594.0 2001.0 3388.0 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2016-05-31 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}\"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2013-03-01 00:00:00 --- 2016-05-31 00:00:00(n=1188)\nTest dates  : 2016-06-01 00:00:00 --- 2017-02-28 00:00:00(n=273)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['CO'].plot(label='train', ax=axes[0])\ndata_test['CO'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('Concentration (ug/m^3)')\naxes[0].set_title('CO')\naxes[0].legend()\n\ndata_train['SO2'].plot(label='train', ax=axes[1])\ndata_test['SO2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('Concentration (ug/m^3)')\naxes[1].set_title('SO2')\n\ndata_train['PM2.5'].plot(label='train', ax=axes[2])\ndata_test['PM2.5'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('Concentration (ug/m^3)')\naxes[2].set_title('PM2.5')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['CO'].plot(label='train', ax=axes[0]) data_test['CO'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('Concentration (ug/m^3)') axes[0].set_title('CO') axes[0].legend()  data_train['SO2'].plot(label='train', ax=axes[1]) data_test['SO2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('Concentration (ug/m^3)') axes[1].set_title('SO2')  data_train['PM2.5'].plot(label='train', ax=axes[2]) data_test['PM2.5'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('Concentration (ug/m^3)') axes[2].set_title('PM2.5')  fig.tight_layout() plt.show(); In\u00a0[5]: Copied! <pre># Create and fit forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: None \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:09:25 \nLast fit date: 2023-05-29 13:09:25 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul> In\u00a0[6]: Copied! <pre># Predict with forecaster MultiVariate\n# ==============================================================================\n# Predict as many steps as defined in the forecaster initialization\npredictions = forecaster.predict()\ndisplay(predictions)\n</pre> # Predict with forecaster MultiVariate # ============================================================================== # Predict as many steps as defined in the forecaster initialization predictions = forecaster.predict() display(predictions) CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 2016-06-04 23111.643246 2016-06-05 24686.327128 2016-06-06 24023.210710 2016-06-07 24015.607873 In\u00a0[7]: Copied! <pre># Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) CO 2016-06-01 20244.442039 2016-06-05 24686.327128 In\u00a0[8]: Copied! <pre># Predict with prediction intervals\npredictions = forecaster.predict_interval()\ndisplay(predictions)\n</pre> # Predict with prediction intervals predictions = forecaster.predict_interval() display(predictions) CO lower_bound upper_bound 2016-06-01 20244.442039 -80.146735 47276.760018 2016-06-02 23307.697174 -2170.152743 63078.155420 2016-06-03 22487.345075 -4504.802798 64369.052985 2016-06-04 23111.643246 -1760.325312 58465.948430 2016-06-05 24686.327128 981.755673 55560.773066 2016-06-06 24023.210710 -327.970078 60295.211096 2016-06-07 24015.607873 -1438.003274 64567.853999 In\u00a0[9]: Copied! <pre># Backtesting MultiVariate\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data,\n                                           steps                 = 7,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_train),\n                                           fixed_train_size      = False,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = False,\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting MultiVariate # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data,                                            steps                 = 7,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_train),                                            fixed_train_size      = False,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = False,                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/39 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 CO 14931.121425 <pre>\nBacktest predictions\n</pre> Out[9]: CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 2016-06-04 23111.643246 In\u00a0[10]: Copied! <pre># Create and forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = RandomForestRegressor(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n</pre> # Create and forecaster MultiVariate # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = RandomForestRegressor(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None              ) In\u00a0[11]: Copied! <pre># Random search MultiVariate\n# ==============================================================================\nlags_grid = [7, 14]\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),\n    'max_depth': np.arange(start=3, stop=6, step=1, dtype=int)\n}\n\nresults = random_search_forecaster_multiseries(\n              forecaster            = forecaster,\n              series                = data,\n              exog                  = None,\n              lags_grid             = lags_grid,\n              param_distributions   = param_distributions,\n              steps                 = 7,\n              metric                = 'mean_absolute_error',\n              initial_train_size    = len(data_train),\n              fixed_train_size      = False,\n              gap                   = 0,\n              allow_incomplete_fold = True,\n              refit                 = False,\n              n_iter                = 5,\n              return_best           = False,\n              verbose               = False\n          )\n\nresults\n</pre> # Random search MultiVariate # ============================================================================== lags_grid = [7, 14] param_distributions = {     'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),     'max_depth': np.arange(start=3, stop=6, step=1, dtype=int) }  results = random_search_forecaster_multiseries(               forecaster            = forecaster,               series                = data,               exog                  = None,               lags_grid             = lags_grid,               param_distributions   = param_distributions,               steps                 = 7,               metric                = 'mean_absolute_error',               initial_train_size    = len(data_train),               fixed_train_size      = False,               gap                   = 0,               allow_incomplete_fold = True,               refit                 = False,               n_iter                = 5,               return_best           = False,               verbose               = False           )  results <pre>10 models compared for 1 level(s). Number of iterations: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[11]: levels lags params mean_absolute_error n_estimators max_depth 7 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 15, 'max_depth': 3} 15991.360494 15 3 5 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 17, 'max_depth': 3} 16001.390244 17 3 9 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 18, 'max_depth': 3} 16058.419766 18 3 8 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'max_depth': 5} 16160.441414 16 5 6 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 19, 'max_depth': 5} 16192.047506 19 5 3 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 16, 'max_depth': 5} 16195.444749 16 5 4 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 18, 'max_depth': 3} 16203.381611 18 3 1 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 19, 'max_depth': 5} 16269.039511 19 5 0 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 17, 'max_depth': 3} 16303.094588 17 3 2 [CO] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 15, 'max_depth': 3} 16354.631284 15 3 In\u00a0[12]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\ndisplay(predictions)\n</pre> # Create and fit forecaster MultiVariate Custom lags # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = {'CO': 7, 'SO2': [1, 7], 'PM2.5': 2},                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) display(predictions) CO 2016-06-01 19421.982426 2016-06-02 21856.950600 2016-06-03 21910.011418 2016-06-04 23197.353798 2016-06-05 23294.891733 2016-06-06 22478.158677 2016-06-07 23393.641623 <p>It is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific lags that have been created.</p> In\u00a0[13]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step    = 1,\n               X_train = X,\n               y_train = y,\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step    = 1,                X_train = X,                y_train = y,            )  X_1.head(4) Out[13]: CO_lag_1 CO_lag_2 CO_lag_3 CO_lag_4 CO_lag_5 CO_lag_6 CO_lag_7 SO2_lag_1 SO2_lag_7 PM2.5_lag_1 PM2.5_lag_2 date 2013-03-14 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 9600.0 2369.0000 204.0 5952.0 4934.0 2013-03-15 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 1535.5424 674.0 5039.0 5952.0 2013-03-16 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 881.0000 1661.0 1242.0 5039.0 2013-03-17 17600.0 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 553.0000 485.0 818.0 1242.0 In\u00a0[14]: Copied! <pre># Weights in MultiVariate\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = custom_weights\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=7).head(3)\n</pre> # Weights in MultiVariate # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = custom_weights              )  forecaster.fit(series=data_train) forecaster.predict(steps=7).head(3) Out[14]: CO 2016-06-01 20244.442039 2016-06-02 23307.697174 2016-06-03 22487.345075 <p>  \u00a0 Warning </p> <p>The <code>weight_func</code> argument will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>.</p> In\u00a0[15]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> In\u00a0[16]: Copied! <pre>forecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},\n                 transformer_exog   = None,\n                 weight_func        = None,\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7,                  transformer_series = {'CO': StandardScaler(), 'SO2': StandardScaler()},                  transformer_exog   = None,                  weight_func        = None,              )  forecaster.fit(series=data_train) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\skforecast\\ForecasterAutoregMultiVariate\\ForecasterAutoregMultiVariate.py:515: IgnoredArgumentWarning: {'PM2.5'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> Out[16]: <pre>============================= \nForecasterAutoregMultiVariate \n============================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for series: {'CO': StandardScaler(), 'SO2': StandardScaler()} \nTransformer for exog: None \nWeight function included: False \nWindow size: 7 \nTarget series, level: CO \nMultivariate series (names): ['CO', 'SO2', 'PM2.5'] \nMaximum steps predicted: 7 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2013-03-01 00:00:00'), Timestamp('2016-05-31 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:09:40 \nLast fit date: 2023-05-29 13:09:40 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[17]: Copied! <pre># Grid search MultiVariate with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'CO',\n                 lags               = 7,\n                 steps              = 7\n             )    \n\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [7, 14]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              exog                = None,\n              steps               = 7,\n              metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              lags_grid           = lags_grid,\n              param_grid          = param_grid,\n              initial_train_size  = len(data_train),\n              refit               = False,\n              fixed_train_size    = False,\n              return_best         = True,\n              verbose             = False\n          )\n\nresults\n</pre> # Grid search MultiVariate with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'CO',                  lags               = 7,                  steps              = 7              )      def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [7, 14] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               exog                = None,               steps               = 7,               metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],               lags_grid           = lags_grid,               param_grid          = param_grid,               initial_train_size  = len(data_train),               refit               = False,               fixed_train_size    = False,               return_best         = True,               verbose             = False           )  results <pre>6 models compared for 1 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7]\n  Parameters: {'alpha': 1}\n  Backtesting metric: 14931.12142535687\n  Levels: ['CO']\n\n</pre> Out[17]: levels lags params mean_absolute_error custom_metric mean_squared_error alpha 2 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 1} 14931.121425 21210.876663 5.519094e+08 1.00 1 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.1} 14931.121426 21210.876659 5.519094e+08 0.10 0 [CO] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.01} 14931.121426 21210.876659 5.519094e+08 0.01 5 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 1} 15019.782237 20996.998181 5.649121e+08 1.00 4 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.1} 15019.782238 20996.998178 5.649121e+08 0.10 3 [CO] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.01} 15019.782238 20996.998178 5.649121e+08 0.01 In\u00a0[18]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[18]: feature importance 0 CO_lag_1 0.655461 1 CO_lag_2 -0.072108 2 CO_lag_3 0.097851 3 CO_lag_4 0.055877 4 CO_lag_5 0.067536 5 CO_lag_6 -0.122215 6 CO_lag_7 0.115945 7 SO2_lag_1 5.336658 8 SO2_lag_2 -1.074241 9 SO2_lag_3 -3.647177 10 SO2_lag_4 0.020614 11 SO2_lag_5 1.483432 12 SO2_lag_6 2.971133 13 SO2_lag_7 -1.764458 14 PM2.5_lag_1 -1.000016 15 PM2.5_lag_2 -0.902531 16 PM2.5_lag_3 0.373182 17 PM2.5_lag_4 -0.767187 18 PM2.5_lag_5 -0.828407 19 PM2.5_lag_6 0.546431 20 PM2.5_lag_7 -0.401235 In\u00a0[19]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX, y = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step    = 1,\n               X_train = X,\n               y_train = y,\n           )\n\nX_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X, y = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step    = 1,                X_train = X,                y_train = y,            )  X_1.head(4) Out[19]: CO_lag_1 CO_lag_2 CO_lag_3 CO_lag_4 CO_lag_5 CO_lag_6 CO_lag_7 SO2_lag_1 SO2_lag_2 SO2_lag_3 ... SO2_lag_5 SO2_lag_6 SO2_lag_7 PM2.5_lag_1 PM2.5_lag_2 PM2.5_lag_3 PM2.5_lag_4 PM2.5_lag_5 PM2.5_lag_6 PM2.5_lag_7 date 2013-03-14 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 9600.0 2369.0000 3052.0000 2001.0000 ... 1661.0 674.0 204.0 5952.0 4934.0 3388.0 438.0 1956.0 633.0 181.0 2013-03-15 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 20198.0 1535.5424 2369.0000 3052.0000 ... 485.0 1661.0 674.0 5039.0 5952.0 4934.0 3388.0 438.0 1956.0 633.0 2013-03-16 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 47195.0 881.0000 1535.5424 2369.0000 ... 2001.0 485.0 1661.0 1242.0 5039.0 5952.0 4934.0 3388.0 438.0 1956.0 2013-03-17 17600.0 24596.0 78194.0 94792.0 81291.0 59594.0 15000.0 553.0000 881.0000 1535.5424 ... 3052.0 2001.0 485.0 818.0 1242.0 5039.0 5952.0 4934.0 3388.0 438.0 <p>4 rows \u00d7 21 columns</p> In\u00a0[20]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[20]: <pre>date\n2013-03-14    78194.0\n2013-03-15    24596.0\n2013-03-16    17600.0\n2013-03-17    40794.0\nFreq: D, Name: CO_step_1, dtype: float64</pre> <p>  \u00a0 Warning </p> <p><code>bayesian_search_forecaster_multiseries</code> will be released in a future version of skforecast.</p>  Stay tuned!  In\u00a0[21]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#dependent-multi-series-forecasting-multivariate-forecasting","title":"Dependent multi-series forecasting (Multivariate forecasting)\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model.</p> <p>In dependent multi-series forecasting (multivariate time series), all series are modeled together in a single model, considering that each time series depends not only on its past values but also on the past values of the other series. The forecaster is expected not only to learn the information of each series separately but also to relate them. An example is the measurements made by all the sensors (flow, temperature, pressure...) installed on an industrial machine such as a compressor.</p> <p></p> Time series transformation to train a forecaster with multiple dependent time series. <p>Since as many training matrices are created as there are series in the dataset, it must be decided on which level the forecasting will be performed. To predict the next n steps a model is trained for each step to be predicted, the selected level in the figure is <code>Series 1</code>. This strategy is of the type direct multi-step forecasting.</p> <p></p> Diagram of direct forecasting with multiple dependent time series. <p>Using the <code>ForecasterAutoregMultiVariate</code> class, it is possible to easily build machine learning models for dependent multi-series forecasting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#train-and-predict-forecasterautoregmultivariate","title":"Train and predict ForecasterAutoregMultiVariate\u00b6","text":"<p>When initializing the forecaster, the <code>level</code> to be predicted and the maximum number of <code>steps</code> must be indicated since a different model will be created for each step.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#backtesting-multivariate","title":"Backtesting MultiVariate\u00b6","text":"<p>See the backtesting user guide to learn more about backtesting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#hyperparameter-tuning-and-lags-selection-multivariate","title":"Hyperparameter tuning and lags selection MultiVariate\u00b6","text":"<p>Functions <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code> in the module <code>model_selection_multiseries</code> allow for lag and hyperparameter optimization. The optimization is performed in the same way as in the other forecasters, see the user guide here.</p> <p>The following example shows how to use <code>random_search_forecaster_multiseries</code> to find the best lags and model hyperparameters for all time series:</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#different-lags-for-each-time-series","title":"Different lags for each time series\u00b6","text":"<p>If a <code>dict</code> is passed to the <code>lags</code> argument, it allows setting different lags for each of the series.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#weights-in-multivariate","title":"Weights in MultiVariate\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model.</p> <p>Learn more about weighted time series forecasting with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#scikit-learn-transformers-in-multivariate","title":"Scikit-learn transformers in MultiVariate\u00b6","text":"<p>Learn more about using scikit-learn transformers with skforecast.</p> <ul> <li>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</li> <li>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them.</li> </ul>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, and <code>random_search_forecaster_multiseries</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregMultiVariate</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#extract-training-matrix","title":"Extract training matrix\u00b6","text":"<p>Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/direct-multi-step-forecasting.html","title":"Direct multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor = Ridge(),\n                 steps     = 36,\n                 lags      = 15\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor = Ridge(),                  steps     = 36,                  lags      = 15              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>======================= \nForecasterAutoregDirect \n======================= \nRegressor: Ridge() \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWeight function included: False \nWindow size: 15 \nMaximum steps predicted: 36 \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:10:10 \nLast fit date: 2023-05-29 13:10:11 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\n# Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict # ============================================================================== # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) <pre>2005-07-01    0.952051\n2005-11-01    1.179922\nName: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Predict all steps defined in the initialization.\npredictions = forecaster.predict()\ndisplay(predictions.head(3))\n</pre> # Predict all steps defined in the initialization. predictions = forecaster.predict() display(predictions.head(3)) <pre>2005-07-01    0.952051\n2005-08-01    1.004145\n2005-09-01    1.114590\nName: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\n\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n            \nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== predictions = forecaster.predict(steps=36)  error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )              print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.008419597278831958\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[8]: feature importance 0 lag_1 0.139299 1 lag_2 0.051089 2 lag_3 0.044192 3 lag_4 -0.019868 4 lag_5 -0.017935 5 lag_6 -0.013233 6 lag_7 -0.021063 7 lag_8 -0.012591 8 lag_9 0.011918 9 lag_10 0.020511 10 lag_11 0.154030 11 lag_12 0.551652 12 lag_13 0.057513 13 lag_14 -0.071071 14 lag_15 -0.035237 In\u00a0[9]: Copied! <pre># Create the whole train matrix\nX, y = forecaster.create_train_X_y(data_train)\n\n# Extract X and y to train the model for step 1\nX_1, y_1 = forecaster.filter_train_X_y_for_step(\n               step    = 1,\n               X_train = X,\n               y_train = y,\n           )\n\nX_1.head(4)\n</pre> # Create the whole train matrix X, y = forecaster.create_train_X_y(data_train)  # Extract X and y to train the model for step 1 X_1, y_1 = forecaster.filter_train_X_y_for_step(                step    = 1,                X_train = X,                y_train = y,            )  X_1.head(4) Out[9]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 datetime 1995-09-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 0.429795 1995-10-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 0.400906 1995-11-01 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 0.432159 1995-12-01 0.771258 0.595223 0.568606 0.534761 0.475463 0.483389 0.410534 0.361801 0.379808 0.351348 0.336220 0.660119 0.602652 0.502369 0.492543 In\u00a0[10]: Copied! <pre>y_1.head(4)\n</pre> y_1.head(4) Out[10]: <pre>datetime\n1995-09-01    0.568606\n1995-10-01    0.595223\n1995-11-01    0.771258\n1995-12-01    0.751503\nFreq: MS, Name: y_step_1, dtype: float64</pre> In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","title":"Direct multi-step forecaster\u00b6","text":"<p>This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the <code>ForecasterAutoregDirect</code> class and can also include one or multiple exogenous variables.</p> <p>Direct multi-step forecasting is a time series forecasting strategy in which a separate model is trained to predict each step in the forecast horizon. This is in contrast to recursive multi-step forecasting, where a single model is used to make predictions for all future time steps by recursively using its own output as input.</p> <p>Direct multi-step forecasting can be more computationally expensive than recursive forecasting since it requires training multiple models. However, it can often achieve better accuracy in certain scenarios, particularly when there are complex patterns and dependencies in the data that are difficult to capture with a single model.</p> <p>Direct multi-step forecasting can be performed using the <code>ForecasterAutoregDirect</code> class, which can also incorporate one or multiple exogenous variables to improve the accuracy of the forecasts.</p> <p></p> Diagram of direct multi-step forecasting. <p>To train a <code>ForecasterAutoregDirect</code> a different training matrix is created for each model.</p> <p></p> Transformation of a time series into matrices to train a direct multi-step forecasting model."},{"location":"user_guides/direct-multi-step-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#prediction","title":"Prediction\u00b6","text":"<p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul>"},{"location":"user_guides/direct-multi-step-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterAutoregDirect</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/direct-multi-step-forecasting.html#extract-training-matrices","title":"Extract training matrices\u00b6","text":"<p>Two steps are needed to extract the training matrices. One to create the whole training matrix and a second one to subset the data needed for each model (step).</p>"},{"location":"user_guides/exogenous-variables.html","title":"Exogenous variables","text":"<p>  \u00a0 Note </p> <p>When exogenous variables are used, their values must be aligned so that y[i] regresses on exog[i].</p> <p></p> Time series transformation including an exogenous variable. <p>  \u00a0 Warning </p> <p>When exogenous variables are included in a forecasting model, it is assumed that all exogenous inputs are known in the future. Do not include exogenous variables as predictors if their future value will not be known when making predictions.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data.plot(ax=ax); In\u00a0[3]: Copied! <pre># Split data in train and test\n# ==============================================================================\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Split data in train and test # ============================================================================== steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(\n    y    = data_train['y'],\n    exog = data_train[['exog_1', 'exog_2']]\n)\n\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(     y    = data_train['y'],     exog = data_train[['exog_1', 'exog_2']] )  forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:10:36 \nLast fit date: 2023-05-29 13:10:36 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(\n                  steps = steps,\n                  exog = data_test[['exog_1', 'exog_2']]\n              )\n\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(                   steps = steps,                   exog = data_test[['exog_1', 'exog_2']]               )  predictions.head(3) Out[5]: <pre>2005-07-01    0.908832\n2005-08-01    0.953925\n2005-09-01    1.100887\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3.5))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3.5)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.004022228812838391\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[8]: feature importance 0 lag_1 0.013354 1 lag_2 0.061120 2 lag_3 0.009086 3 lag_4 0.002721 4 lag_5 0.002478 5 lag_6 0.003155 6 lag_7 0.002179 7 lag_8 0.008154 8 lag_9 0.010319 9 lag_10 0.020587 10 lag_11 0.007036 11 lag_12 0.773389 12 lag_13 0.004583 13 lag_14 0.018127 14 lag_15 0.008732 15 exog_1 0.010364 16 exog_2 0.044616 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/exogenous-variables.html#exogenous-variables-features","title":"Exogenous variables  (features)\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In Skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p>"},{"location":"user_guides/exogenous-variables.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#train-forecaster","title":"Train forecaster\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#prediction","title":"Prediction\u00b6","text":"<p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p>"},{"location":"user_guides/exogenous-variables.html#feature-importances","title":"Feature importances\u00b6","text":"<p>If exogenous variables are included as predictors, they have a value of feature importances.</p>"},{"location":"user_guides/feature-importances.html","title":"Feature importances","text":"<p>  \u00a0 Warning </p> <p>The <code>get_feature_importances()</code> method will only return values if the forecaster's regressor has either the <code>coef_</code> or <code>feature_importances_</code> attribute, which is the default in scikit-learn. If your regressor does not follow this naming convention, please consider opening an issue on GitHub and we will strive to include it in future updates.</p> <p>  \u00a0 Note </p> <p>See also: SHAP values in skforecast models</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') In\u00a0[3]: Copied! <pre># Create and fit forecaster using a RandomForest regressor\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\n# Predictors importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Create and fit forecaster using a RandomForest regressor # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  # Predictors importances # ============================================================================== forecaster.get_feature_importances() Out[3]: feature importance 0 lag_1 0.530186 1 lag_2 0.100529 2 lag_3 0.023620 3 lag_4 0.070458 4 lag_5 0.063155 5 exog_1 0.047043 6 exog_2 0.165009 In\u00a0[4]: Copied! <pre># Create and fit forecaster using a linear regressor\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\nforecaster.get_feature_importances()\n</pre> # Create and fit forecaster using a linear regressor # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = Ridge(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  forecaster.get_feature_importances() Out[4]: feature importance 0 lag_1 0.327688 1 lag_2 -0.073593 2 lag_3 -0.152202 3 lag_4 -0.217106 4 lag_5 -0.145800 5 exog_1 0.379798 6 exog_2 0.668162 <p>To properly retrieve the feature importances in the <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>, it is essential to specify the model from which to extract the feature importances are to be extracted. This is because Direct Strategy Forecasters fit one model per step, and each model may have different important features. Therefore, the user must explicitly specify which model's feature importances wish to extract to ensure that the correct features are used.</p> In\u00a0[5]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoregDirect(\n                 regressor = Ridge(random_state=123),\n                 steps = 10,\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\n\n# Predictors importances of model for step 1\n# ==============================================================================\nforecaster.get_feature_importances(step=1)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregDirect(                  regressor = Ridge(random_state=123),                  steps = 10,                  lags = 5              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])  # Predictors importances of model for step 1 # ============================================================================== forecaster.get_feature_importances(step=1) Out[5]: feature importance 0 lag_1 0.326827 1 lag_2 -0.055386 2 lag_3 -0.155098 3 lag_4 -0.220415 4 lag_5 -0.138252 5 exog_1 0.386103 6 exog_2 0.635972 In\u00a0[6]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/feature-importances.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Feature importance is a technique used in machine learning to determine the relevance or importance of each feature (or variable) in a model's prediction. In other words, it measures how much each feature contributes to the model's output.</p> <p>Feature importance can be used for several purposes, such as identifying the most relevant features for a given prediction, understanding the behavior of a model, and selecting the best set of features for a given task. It can also help to identify potential biases or errors in the data used to train the model. It is important to note that feature importance is not a definitive measure of causality. Just because a feature is identified as important does not necessarily mean that it causes the outcome. Other factors, such as confounding variables, may also be at play.</p> <p>The method used to calculate feature importance may vary depending on the type of machine learning model being used. Different machine learning models may have different assumptions and characteristics that affect the calculation of feature importance. For example, decision tree-based models such as Random Forest and Gradient Boosting typically use mean decrease impurity or permutation feature importance methods to calculate feature importance.</p> <p>Linear regression models typically use coefficients or standardized coefficients to determine the importance of a feature. The magnitude of the coefficient reflects the strength and direction of the relationship between the feature and the target variable.</p> <p>The importance of the predictors included in a forecaster can be obtained using the method <code>get_feature_importances()</code>. This method accesses the <code>coef_</code> and <code>feature_importances_</code> attributes of the internal regressor.</p>"},{"location":"user_guides/feature-importances.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/feature-importances.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/feature-importances.html#extract-feature-importances-from-trained-forecaster","title":"Extract feature importances from trained forecaster\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html","title":"Forecaster in production","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata_train = data.loc[:'2005-01-01']\ndata_train.tail()\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data_train = data.loc[:'2005-01-01'] data_train.tail() Out[2]: y date 2004-09-01 1.134432 2004-10-01 1.181011 2004-11-01 1.216037 2004-12-01 1.257238 2005-01-01 1.170690 In\u00a0[3]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster.predict(steps=3) Out[4]: <pre>2005-02-01    0.927480\n2005-03-01    0.756215\n2005-04-01    0.692595\nFreq: MS, Name: pred, dtype: float64</pre> <p>As expected, predictions follow directly from the end of training data.</p> <p>When the <code>last_window</code> argument is provided, the forecaster uses this data to generate the necessary lags as predictors and starts the prediction thereafter.</p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nlast_window = data['y'].tail(5)\nforecaster.predict(steps=3, last_window=last_window)\n</pre> # Predict # ============================================================================== last_window = data['y'].tail(5) forecaster.predict(steps=3, last_window=last_window) Out[5]: <pre>2008-07-01    0.803853\n2008-08-01    0.870858\n2008-09-01    0.905003\nFreq: MS, Name: pred, dtype: float64</pre> <p>Since the provided <code>last_window</code> contains values from 2008-02-01 to 2008-06-01, the forecaster can create the needed lags and predict the next 5 steps.</p> <p>  \u00a0 Warning </p> <p>It is important to note that <code>last_window</code>'s length must be enough to include the maximum lag used by the forecaster. For example, if the forecaster uses lags 1, 24 and 48, <code>last_window</code> must include the last 48 values of the series.</p> <p>When using the <code>last_window</code> argument, it is crucial to ensure that the length of <code>last_window</code> is sufficient to include the maximum lag (or custom predictor) used by the forecaster. For instance, if the forecaster employs lags 1, 24, and 48, <code>last_window</code> must include the most recent 48 values of the series. Failing to include the required number of past observations may result in an error or incorrect predictions.</p> In\u00a0[6]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecaster-in-production.html#using-forecaster-models-in-production","title":"Using forecaster models in production\u00b6","text":"<p>When using a trained model in production, regular predictions need to be generated, for example, on a weekly basis every Monday. By default, the <code>predict</code> method on a trained forecaster object generates predictions starting right after the last training observation. Therefore, the model could be retrained weekly, just before the first prediction is needed, and its predict method called. However, this approach may not be practical due to reasons such as: expensive model training, unavailability of the training data history, or high prediction frequency.</p> <p>In such scenarios, the model must be able to predict at any time, even if it has not been recently trained. Fortunately, every model generated using skforecast has the <code>last_window</code> argument in its <code>predict</code> method. This argument allows providing only the past values needed to create the autoregressive predictors (lags or custom predictors), enabling prediction without the need to retrain the model. This feature is particularly useful when there are limitations in retraining the model regularly or when dealing with high-frequency predictions.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html","title":"Forecasting SARIMAX and ARIMA models","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ======================================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.ForecasterSarimax import ForecasterSarimax\nfrom skforecast.model_selection_sarimax import backtesting_sarimax\nfrom skforecast.model_selection_sarimax import grid_search_sarimax\nfrom pmdarima import ARIMA\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Libraries # ====================================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterSarimax import ForecasterSarimax from skforecast.model_selection_sarimax import backtesting_sarimax from skforecast.model_selection_sarimax import grid_search_sarimax from pmdarima import ARIMA from sklearn.metrics import mean_absolute_error import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Download data\n# ======================================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ======================================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Train-test dates\n# ======================================================================================\nend_train = '2005-06-01 23:59:00'\nprint(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\")\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\n# Plot\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ====================================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv') data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ====================================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Train-test dates # ====================================================================================== end_train = '2005-06-01 23:59:00' print(f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  (n={len(data.loc[end_train:])})\") data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  # Plot # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit ForecasterSarimax\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit ForecasterSarimax # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(y=data_train) forecaster Out[3]: <pre>================= \nForecasterSarimax \n================= \nRegressor:  ARIMA(12,1,1)(0,0,0)[0] intercept \nRegressor parameters: {'maxiter': 200, 'method': 'lbfgs', 'order': (12, 1, 1), 'out_of_sample_size': 0, 'scoring': 'mse', 'scoring_args': None, 'seasonal_order': (0, 0, 0, 0), 'start_params': None, 'suppress_warnings': False, 'trend': None, 'with_intercept': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2023-05-29 13:11:33 \nLast fit date: 2023-05-29 13:11:37 \nIndex seen by the forecaster: DatetimeIndex(['1991-07-01', '1991-08-01', '1991-09-01', '1991-10-01',\n               '1991-11-01', '1991-12-01', '1992-01-01', '1992-02-01',\n               '1992-03-01', '1992-04-01',\n               ...\n               '2004-09-01', '2004-10-01', '2004-11-01', '2004-12-01',\n               '2005-01-01', '2005-02-01', '2005-03-01', '2005-04-01',\n               '2005-05-01', '2005-06-01'],\n              dtype='datetime64[ns]', name='datetime', length=168, freq='MS') \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ======================================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ====================================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[4]: <pre>2005-07-01    0.972629\n2005-08-01    0.980605\n2005-09-01    1.135420\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ======================================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ====================================================================================== error_mse = mean_absolute_error(                 y_true = data_test,                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.08402132235228314\n</pre> In\u00a0[7]: Copied! <pre># Predict intervals\n# ======================================================================================\npredictions = forecaster.predict_interval(steps=36, interval=[5, 95])\npredictions.head(3)\n</pre> # Predict intervals # ====================================================================================== predictions = forecaster.predict_interval(steps=36, interval=[5, 95]) predictions.head(3) Out[7]: pred lower_bound upper_bound 2005-07-01 0.972629 0.875111 1.070146 2005-08-01 0.980605 0.877743 1.083467 2005-09-01 1.135420 1.029244 1.241595 In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[8]: feature importance 0 intercept 0.004855 1 ar.L1 -0.109838 2 ar.L2 -0.138647 3 ar.L3 -0.110524 4 ar.L4 -0.158482 5 ar.L5 -0.127197 6 ar.L6 -0.115794 7 ar.L7 -0.160912 8 ar.L8 -0.108582 9 ar.L9 -0.135261 10 ar.L10 -0.135297 11 ar.L11 -0.065233 12 ar.L12 0.784306 13 ma.L1 -0.554595 14 sigma2 0.002476 In\u00a0[9]: Copied! <pre># Backtest forecaster\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nmetric, predictions = backtesting_sarimax(\n                          forecaster         = forecaster,\n                          y                  = data,\n                          initial_train_size = len(data_train),\n                          fixed_train_size   = False,\n                          steps              = 12,\n                          metric             = 'mean_absolute_error',\n                          refit              = True,\n                          verbose            = True\n                      )\n</pre> # Backtest forecaster # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  metric, predictions = backtesting_sarimax(                           forecaster         = forecaster,                           y                  = data,                           initial_train_size = len(data_train),                           fixed_train_size   = False,                           steps              = 12,                           metric             = 'mean_absolute_error',                           refit              = True,                           verbose            = True                       ) <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number of steps per fold: 12\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=180)\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=192)\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre>print(f\"Error backtest: {metric}\")\n</pre> print(f\"Error backtest: {metric}\") <pre>Error backtest: 0.0696628451184841\n</pre> In\u00a0[11]: Copied! <pre>predictions.head(4)\n</pre> predictions.head(4) Out[11]: pred 2005-07-01 0.972629 2005-08-01 0.980605 2005-09-01 1.135420 2005-10-01 1.172995 In\u00a0[12]: Copied! <pre># Train-validation-test data\n# ======================================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\")\nprint(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")\n\n# Plot\n# ======================================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:end_val].plot(ax=ax, label='validation')\ndata.loc[end_val:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Train-validation-test data # ====================================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\") print(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")  # Plot # ====================================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:end_val].plot(ax=ax, label='validation') data.loc[end_val:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=29)\n</pre> In\u00a0[13]: Copied! <pre># Grid search hyperparameter\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nparam_grid = {\n    'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1), (14, 1, 4)],\n    'seasonal_order': [(0, 0, 0, 0)],\n    'trend': [None, 'n', 'c']\n}\n\nresults_grid = grid_search_sarimax(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val],\n                   param_grid         = param_grid,\n                   steps              = 12,\n                   refit              = True,\n                   metric             = 'mean_absolute_error',\n                   initial_train_size = len(data_train),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   verbose            = False\n               )\n\nresults_grid.head(5)\n</pre> # Grid search hyperparameter # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=ARIMA(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  param_grid = {     'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1), (14, 1, 4)],     'seasonal_order': [(0, 0, 0, 0)],     'trend': [None, 'n', 'c'] }  results_grid = grid_search_sarimax(                    forecaster         = forecaster,                    y                  = data.loc[:end_val],                    param_grid         = param_grid,                    steps              = 12,                    refit              = True,                    metric             = 'mean_absolute_error',                    initial_train_size = len(data_train),                    fixed_train_size   = False,                    return_best        = True,                    verbose            = False                )  results_grid.head(5) <pre>Number of models compared: 12.\n</pre> <pre>params grid:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found parameters, and the whole data set: \n  Parameters: {'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'trend': 'n'}\n  Backtesting metric: 0.06799312252877558\n\n</pre> Out[13]: params mean_absolute_error order seasonal_order trend 7 {'order': (12, 1, 1), 'seasonal_order': (0, 0,... 0.067993 (12, 1, 1) (0, 0, 0, 0) n 10 {'order': (14, 1, 4), 'seasonal_order': (0, 0,... 0.072158 (14, 1, 4) (0, 0, 0, 0) n 0 {'order': (12, 0, 0), 'seasonal_order': (0, 0,... 0.076132 (12, 0, 0) (0, 0, 0, 0) None 2 {'order': (12, 0, 0), 'seasonal_order': (0, 0,... 0.076132 (12, 0, 0) (0, 0, 0, 0) c 1 {'order': (12, 0, 0), 'seasonal_order': (0, 0,... 0.076674 (12, 0, 0) (0, 0, 0, 0) n <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[14]: Copied! <pre>forecaster\n</pre> forecaster Out[14]: <pre>================= \nForecasterSarimax \n================= \nRegressor:  ARIMA(12,1,1)(0,0,0)[0] intercept \nRegressor parameters: {'maxiter': 200, 'method': 'lbfgs', 'order': (12, 1, 1), 'out_of_sample_size': 0, 'scoring': 'mse', 'scoring_args': None, 'seasonal_order': (0, 0, 0, 0), 'start_params': None, 'suppress_warnings': False, 'trend': 'n', 'with_intercept': True} \nfit_kwargs: {} \nWindow size: 1 \nTransformer for y: None \nTransformer for exog: None \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nCreation date: 2023-05-29 13:11:49 \nLast fit date: 2023-05-29 13:12:30 \nIndex seen by the forecaster: DatetimeIndex(['1991-07-01', '1991-08-01', '1991-09-01', '1991-10-01',\n               '1991-11-01', '1991-12-01', '1992-01-01', '1992-02-01',\n               '1992-03-01', '1992-04-01',\n               ...\n               '2005-04-01', '2005-05-01', '2005-06-01', '2005-07-01',\n               '2005-08-01', '2005-09-01', '2005-10-01', '2005-11-01',\n               '2005-12-01', '2006-01-01'],\n              dtype='datetime64[ns]', name='datetime', length=175, freq='MS') \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[15]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecasting-sarimax-arima.html#sarimax-and-arima-forecasters","title":"SARIMAX and ARIMA forecasters\u00b6","text":"<p>SARIMAX (Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors) is a generalization of the ARIMA model that incorporates both seasonality and exogenous variables. SARIMAX models are among the most widely used statistical forecasting models with excellent forecasting performance.</p> <p>In the SARIMAX model notation, the parameters $p$, $d$, and $q$ represent the autoregressive, differencing, and moving-average components, respectively. $P$, $D$, and $Q$ denote the same components for the seasonal part of the model, with $m$ representing the number of periods in each season.</p> <ul> <li><p>$p$ is the order (number of time lags) of the autoregressive part of the model.</p> </li> <li><p>$d$ is the degree of differencing (the number of times that past values have been subtracted from the data).</p> </li> <li><p>$q$ is the order of the moving-average part of the model.</p> </li> <li><p>$P$ is the order (number of time lags) of the seasonal part of the model</p> </li> <li><p>$D$ is the degree of differencing (the number of times the data have had past values subtracted) of the seasonal part of the model.</p> </li> <li><p>$Q$ is the order of the moving-average of the seasonal part of the model.</p> </li> <li><p>$m$ refers to the number of periods in each season.</p> </li> </ul> <p>When two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping \"AR\", \"I\" or \"MA\" from the acronym describing the model. For example, $ARIMA(1,0,0)$ is $AR(1)$, $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$.</p> <p>The  <code>ForecasterSarimax</code> class allows for training and validating SARIMAX models from pmdarima (internally wraps the statsmodels SARIMAX) using the skforecast API. This facilitates the comparison of its performance with other machine learning models.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#forecastersarimax","title":"ForecasterSarimax\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#training","title":"Training\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#backtesting","title":"Backtesting\u00b6","text":"<p>SARIMAX models can be evaluated using any of the backtesting strategies implemented in skforecast.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#model-tunning","title":"Model tunning\u00b6","text":"<p>To find the optimal hyperparameters for the SARIMAX model, various hyperparameter optimization strategies can be used. It is crucial to conduct hyperparameter optimization using a validation dataset, rather than the test dataset, to ensure accurate evaluation of model performance.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html","title":"Forecasting with XGBoost and LightGBM","text":"<p>  \u00a0 Note </p> <p>All of the gradient boosting libraries mentioned above - XGBoost, Lightgbm, HistGradientBoostingRegressor, and CatBoost - can handle categorical features natively, but they require specific encoding techniques that may not be entirely intuitive. Detailed information can be found in categorical features and in this example.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from xgboost import XGBRegressor from lightgbm import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n        url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(         url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(random_state = 123),\n                 lags      = 8\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(random_state = 123),                  lags      = 8              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(random_state=123) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:13:14 \nLast fit date: 2023-05-29 13:13:14 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[4]: <pre>2005-07-01    0.939158\n2005-08-01    0.931943\n2005-09-01    1.072937\n2005-10-01    1.090429\n2005-11-01    1.087492\n2005-12-01    1.170073\n2006-01-01    0.964073\n2006-02-01    0.760841\n2006-03-01    0.829831\n2006-04-01    0.800095\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[5]: feature importance 0 lag_1 61 1 lag_2 91 2 lag_3 14 3 lag_4 38 4 lag_5 35 5 lag_6 49 6 lag_7 25 7 lag_8 26 8 exog_1 43 9 exog_2 127 In\u00a0[6]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = XGBRegressor(random_state = 123),\n                 lags      = 8\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = XGBRegressor(random_state = 123),                  lags      = 8              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[6]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=123, ...) \nLags: [1 2 3 4 5 6 7 8] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 8 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:13:15 \nLast fit date: 2023-05-29 13:13:16 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[7]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[7]: <pre>2005-07-01    0.882285\n2005-08-01    0.971786\n2005-09-01    1.106107\n2005-10-01    1.064638\n2005-11-01    1.094615\n2005-12-01    1.139401\n2006-01-01    0.948508\n2006-02-01    0.784839\n2006-03-01    0.774227\n2006-04-01    0.789593\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[8]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[8]: feature importance 0 lag_1 0.286422 1 lag_2 0.125064 2 lag_3 0.001548 3 lag_4 0.027828 4 lag_5 0.075020 5 lag_6 0.011337 6 lag_7 0.058954 7 lag_8 0.045198 8 exog_1 0.075610 9 exog_2 0.293018 In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecasting-with-xgboost-lightgbm-and-other-gradient-boosting-models","title":"Forecasting with XGBoost, LightGBM and other Gradient Boosting models\u00b6","text":"<p>Gradient boosting models have gained popularity in the machine learning community due to their ability to achieve excellent results in a wide range of use cases, including both regression and classification. Although these models have traditionally been less common in forecasting, recent research has shown that they can be highly effective in this domain. Some of the key advantages of using gradient boosting models for forecasting include:</p> <ul> <li><p>The ease with which exogenous variables, in addition to autoregressive variables, can be incorporated into the model.</p> </li> <li><p>The ability to capture non-linear relationships between variables.</p> </li> <li><p>High scalability, which enables the models to handle large volumes of data.</p> </li> </ul> <p>There are several popular implementations of gradient boosting in Python, with four of the most popular being XGBoost, LightGBM, scikit-learn HistGradientBoostingRegressor and CatBoost. All of these libraries follow the scikit-learn API, making them compatible with skforecast.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-lightgbm","title":"Forecaster LightGBM\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-xgboost","title":"Forecaster XGBoost\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html","title":"Hyperparameter tuning and lags selection","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import  random_search_forecaster\nfrom skforecast.model_selection import  bayesian_search_forecaster\nfrom sklearn.metrics import mean_squared_error\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt  from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import  random_search_forecaster from skforecast.model_selection import  bayesian_search_forecaster from sklearn.metrics import mean_squared_error In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\n\n# Train-val-test dates\n# ==============================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"\n    f\"  (n={len(data.loc[end_train:end_val])})\"\n)\nprint(\n    f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"\n    f\" (n={len(data.loc[end_val:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:end_val].plot(ax=ax, label='validation')\ndata.loc[end_val:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index()  # Train-val-test dates # ============================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"     f\"  (n={len(data.loc[end_train:end_val])})\" ) print(     f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"     f\" (n={len(data.loc[end_val:])})\" )  # Plot # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:end_val].plot(ax=ax, label='validation') data.loc[end_val:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00 (n=29)\n</pre> In\u00a0[3]: Copied! <pre># Grid search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val, 'y'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 12,\n                   refit              = True,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   verbose            = False\n               )\n</pre> # Grid search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data.loc[:end_val, 'y'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 12,                    refit              = True,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    verbose            = False                ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 5, 'n_estimators': 50}\n  Backtesting metric: 0.03344857370906804\n\n</pre> In\u00a0[4]: Copied! <pre>results_grid\n</pre> results_grid Out[4]: lags params mean_squared_error max_depth n_estimators 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.033449 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.039221 10 50 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.039266 15 100 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.039526 5 100 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.040241 10 100 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.040765 15 50 17 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.043909 15 100 13 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.044992 5 100 12 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.046224 5 50 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.048666 5 50 15 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.048991 10 100 14 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.050193 10 50 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.050556 15 100 16 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.051217 15 50 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.053123 5 100 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.060260 15 50 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.060951 10 50 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.067334 10 100 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[5]: Copied! <pre>forecaster\n</pre> forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(max_depth=5, n_estimators=50, random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 5, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:13:47 \nLast fit date: 2023-05-29 13:13:58 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[6]: Copied! <pre># Random search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),\n    'max_depth': np.arange(start=5, stop=30, step=1, dtype=int)\n}\n\nresults = random_search_forecaster(\n              forecaster           = forecaster,\n              y                    = data.loc[:end_val, 'y'],\n              steps                = 12,\n              lags_grid            = lags_grid,\n              param_distributions  = param_distributions,\n              n_iter               = 5,\n              metric               = 'mean_squared_error',\n              refit                = True,\n              initial_train_size   = len(data.loc[:end_train]),\n              fixed_train_size     = False,\n              return_best          = True,\n              random_state         = 123,\n              verbose              = False\n          )\n\nresults_grid.head(4)\n</pre> # Random search hyperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters param_distributions = {     'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),     'max_depth': np.arange(start=5, stop=30, step=1, dtype=int) }  results = random_search_forecaster(               forecaster           = forecaster,               y                    = data.loc[:end_val, 'y'],               steps                = 12,               lags_grid            = lags_grid,               param_distributions  = param_distributions,               n_iter               = 5,               metric               = 'mean_squared_error',               refit                = True,               initial_train_size   = len(data.loc[:end_train]),               fixed_train_size     = False,               return_best          = True,               random_state         = 123,               verbose              = False           )  results_grid.head(4) <pre>Number of models compared: 10.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'n_estimators': 77, 'max_depth': 17}\n  Backtesting metric: 0.03147248676391345\n\n</pre> Out[6]: lags params mean_squared_error max_depth n_estimators 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.033449 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.039221 10 50 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.039266 15 100 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.039526 5 100 In\u00a0[7]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters search space\ndef search_space(trial):\n    search_space  = {\n        'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1., 10),\n        'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    } \n    return search_space\n\nresults, frozen_trial = bayesian_search_forecaster(\n                            forecaster            = forecaster,\n                            y                     = data.loc[:end_val, 'y'],\n                            lags_grid             = lags_grid,\n                            search_space          = search_space,\n                            steps                 = 12,\n                            metric                = 'mean_absolute_error',\n                            refit                 = True,\n                            initial_train_size    = len(data.loc[:end_train]),\n                            fixed_train_size      = True,\n                            n_trials              = 10,\n                            random_state          = 123,\n                            return_best           = False,\n                            verbose               = False,\n                            engine                = 'optuna',\n                            kwargs_create_study   = {},\n                            kwargs_study_optimize = {}\n                        )\n\nresults_grid.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters search space def search_space(trial):     search_space  = {         'n_estimators'     : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1., 10),         'max_features'     : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  results, frozen_trial = bayesian_search_forecaster(                             forecaster            = forecaster,                             y                     = data.loc[:end_val, 'y'],                             lags_grid             = lags_grid,                             search_space          = search_space,                             steps                 = 12,                             metric                = 'mean_absolute_error',                             refit                 = True,                             initial_train_size    = len(data.loc[:end_train]),                             fixed_train_size      = True,                             n_trials              = 10,                             random_state          = 123,                             return_best           = False,                             verbose               = False,                             engine                = 'optuna',                             kwargs_create_study   = {},                             kwargs_study_optimize = {}                         )  results_grid.head(4) <pre>Number of models compared: 20,\n         10 bayesian search in each lag configuration.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> Out[7]: lags params mean_squared_error max_depth n_estimators 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.033449 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.039221 10 50 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.039266 15 100 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.039526 5 100 <p><code>frozen_trial</code> contains information of the trial which achived the best results: See more in Study class.</p> In\u00a0[8]: Copied! <pre>frozen_trial\n</pre> frozen_trial Out[8]: <pre>FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.15057479525044887], datetime_start=datetime.datetime(2023, 5, 29, 13, 14, 4, 782902), datetime_complete=datetime.datetime(2023, 5, 29, 13, 14, 4, 939367), params={'n_estimators': 17, 'min_samples_leaf': 3, 'max_features': 'sqrt'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=0, value=None)</pre> <p>  \u00a0 Warning </p> <p>This engine is deprecated since skforecast 0.7.0 in favor of optuna engine. To continue using it, install skforecast 0.6.0.  User guide: https://joaquinamatrodrigo.github.io/skforecast/0.6.0/user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize</p> In\u00a0[9]: Copied! <pre># Grid search hyperparameter and lags with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n    \nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val, 'y'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 12,\n                   refit              = True,\n                   metric             = custom_metric,\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   verbose            = False\n               )\n\nresults_grid.head(4)\n</pre> # Grid search hyperparameter and lags with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric      forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data.loc[:end_val, 'y'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 12,                    refit              = True,                    metric             = custom_metric,                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    verbose            = False                )  results_grid.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 50}\n  Backtesting metric: 0.04867459231626605\n\n</pre> Out[9]: lags params custom_metric max_depth n_estimators 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.048675 5 50 17 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.052172 15 100 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.055920 15 100 12 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.056981 5 50 In\u00a0[10]: Copied! <pre># Grid search hyperparameter and lags with multiple metrics\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 10 # Placeholder, the value will be overwritten\n             )\n\n# Metrics\nmetrics = ['mean_absolute_error', mean_squared_error, custom_metric]\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {'n_estimators': [50, 100],\n              'max_depth': [5, 10, 15]}\n\nresults_grid = grid_search_forecaster(\n                    forecaster         = forecaster,\n                    y                  = data.loc[:end_val, 'y'],\n                    param_grid         = param_grid,\n                    lags_grid          = lags_grid,\n                    steps              = 12,\n                    refit              = True,\n                    metric             = metrics,\n                    initial_train_size = len(data.loc[:end_train]),\n                    fixed_train_size   = False,\n                    return_best        = True,\n                    verbose            = False\n               )\n\nresults_grid.head(4)\n</pre> # Grid search hyperparameter and lags with multiple metrics # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric  forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 10 # Placeholder, the value will be overwritten              )  # Metrics metrics = ['mean_absolute_error', mean_squared_error, custom_metric]  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {'n_estimators': [50, 100],               'max_depth': [5, 10, 15]}  results_grid = grid_search_forecaster(                     forecaster         = forecaster,                     y                  = data.loc[:end_val, 'y'],                     param_grid         = param_grid,                     lags_grid          = lags_grid,                     steps              = 12,                     refit              = True,                     metric             = metrics,                     initial_train_size = len(data.loc[:end_train]),                     fixed_train_size   = False,                     return_best        = True,                     verbose            = False                )  results_grid.head(4) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 5, 'n_estimators': 50}\n  Backtesting metric: 0.14186925271863238\n\n</pre> Out[10]: lags params mean_absolute_error mean_squared_error custom_metric max_depth n_estimators 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.141869 0.033449 0.057507 5 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.144912 0.039221 0.066599 10 50 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.152188 0.040765 0.085193 15 50 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.154576 0.040241 0.071078 10 100 In\u00a0[11]: Copied! <pre># Models to compare\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\nmodels = [RandomForestRegressor(random_state=123), \n          GradientBoostingRegressor(random_state=123),\n          Ridge(random_state=123)]\n\n# Hyperparameter to search for each model\nparam_grids = {'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},\n               'GradientBoostingRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},\n               'Ridge': {'alpha': [0.01, 0.1, 1]}}\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\ndf_results = pd.DataFrame()\nfor i, model in enumerate(models):\n    print(f\"Grid search for regressor: {model}\")\n    print(f\"-------------------------\")\n\n    forecaster = ForecasterAutoreg(\n                     regressor = model,\n                     lags      = 3\n                 )\n\n    # Regressor hyperparameters\n    param_grid = param_grids[list(param_grids)[i]]\n\n    results_grid = grid_search_forecaster(\n                       forecaster         = forecaster,\n                       y                  = data.loc[:end_val, 'y'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 3,\n                       refit              = True,\n                       metric             = 'mean_squared_error',\n                       initial_train_size = len(data.loc[:end_train]),\n                       fixed_train_size   = True,\n                       return_best        = False,\n                       verbose            = False\n                   )\n    \n    # Create a column with model name\n    results_grid['model'] = list(param_grids)[i]\n    \n    df_results = pd.concat([df_results, results_grid])\n\ndf_results = df_results.sort_values(by='mean_squared_error')\ndf_results.head(10)\n</pre> # Models to compare from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.linear_model import Ridge  models = [RandomForestRegressor(random_state=123),            GradientBoostingRegressor(random_state=123),           Ridge(random_state=123)]  # Hyperparameter to search for each model param_grids = {'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},                'GradientBoostingRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},                'Ridge': {'alpha': [0.01, 0.1, 1]}}  # Lags used as predictors lags_grid = [3, 5]  df_results = pd.DataFrame() for i, model in enumerate(models):     print(f\"Grid search for regressor: {model}\")     print(f\"-------------------------\")      forecaster = ForecasterAutoreg(                      regressor = model,                      lags      = 3                  )      # Regressor hyperparameters     param_grid = param_grids[list(param_grids)[i]]      results_grid = grid_search_forecaster(                        forecaster         = forecaster,                        y                  = data.loc[:end_val, 'y'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 3,                        refit              = True,                        metric             = 'mean_squared_error',                        initial_train_size = len(data.loc[:end_train]),                        fixed_train_size   = True,                        return_best        = False,                        verbose            = False                    )          # Create a column with model name     results_grid['model'] = list(param_grids)[i]          df_results = pd.concat([df_results, results_grid])  df_results = df_results.sort_values(by='mean_squared_error') df_results.head(10) <pre>Grid search for regressor: RandomForestRegressor(random_state=123)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: GradientBoostingRegressor(random_state=123)\n-------------------------\nNumber of models compared: 8.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: Ridge(random_state=123)\n-------------------------\nNumber of models compared: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[11]: lags params mean_squared_error max_depth n_estimators model alpha 4 [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 50} 0.045461 5.0 50.0 RandomForestRegressor NaN 6 [1, 2, 3, 4, 5] {'max_depth': 15, 'n_estimators': 50} 0.045826 15.0 50.0 RandomForestRegressor NaN 5 [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 100} 0.048252 5.0 100.0 RandomForestRegressor NaN 7 [1, 2, 3, 4, 5] {'max_depth': 15, 'n_estimators': 100} 0.048994 15.0 100.0 RandomForestRegressor NaN 0 [1, 2, 3] {'alpha': 0.01} 0.054196 NaN NaN Ridge 0.01 3 [1, 2, 3, 4, 5] {'alpha': 0.01} 0.054255 NaN NaN Ridge 0.01 4 [1, 2, 3, 4, 5] {'alpha': 0.1} 0.054266 NaN NaN Ridge 0.10 1 [1, 2, 3] {'alpha': 0.1} 0.054287 NaN NaN Ridge 0.10 7 [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 50} 0.054291 10.0 50.0 GradientBoostingRegressor NaN 6 [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 20} 0.054773 10.0 20.0 GradientBoostingRegressor NaN In\u00a0[12]: Copied! <pre>from tqdm.auto import tqdm\nfrom functools import partialmethod\ntqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data.loc[:end_val, 'y'],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 12,\n                   refit              = True,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   verbose            = False\n               )\n</pre> from tqdm.auto import tqdm from functools import partialmethod tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data.loc[:end_val, 'y'],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 12,                    refit              = True,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    verbose            = False                ) <pre>Number of models compared: 6.\n`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'alpha': 1}\n  Backtesting metric: 0.08758355918903007\n\n</pre> In\u00a0[13]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The Skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search, that can be combined with backtesting to identify the optimal combination of lags and hyperparameters that achieve the best prediction performance.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#grid-search","title":"Grid search\u00b6","text":"<p>Grid search is a popular hyperparameter tuning technique that evaluate an exaustive list of combinations of hyperparameters and lags to find the optimal configuration for a forecasting model. To perform a grid search with the Skforecast library, two grids are needed: one with different lags (<code>lags_grid</code>) and another with the hyperparameters (<code>param_grid</code>).</p> <p>The grid search process involves the following steps:</p> <ol> <li><p><code>grid_search_forecaster</code> creates a copy of the forecaster object and replaces the <code>lags</code> argument with the first option appearing in <code>lags_grid</code>.</p> </li> <li><p>The function validates all combinations of hyperparameters presented in <code>param_grid</code> using backtesting.</p> </li> <li><p>The function repeats these two steps until it has evaluated all possible combinations of lags and hyperparameters.</p> </li> <li><p>If <code>return_best = True</code>, the original forecaster is trained with the best lags and hyperparameters configuration found during the grid search process.</p> </li> </ol>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#random-search","title":"Random search\u00b6","text":"<p>Random search is another hyperparameter tuning strategy available in the Skforecast library. In contrast to grid search, which tries out all possible combinations of hyperparameters and lags, randomized search samples a fixed number of values from the specified possibilities. The number of combinations that are evaluated is given by <code>n_iter</code>.</p> <p>It is important to note that random sampling is only applied to the model hyperparameters, but not to the lags. All lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#bayesian-search","title":"Bayesian search\u00b6","text":"<p>Grid and random search can generate good results, especially when the search range is narrowed down. However, neither of them takes into account the results obtained so far, which prevents them from focusing the search on the regions of greatest interest while avoiding unnecessary ones.</p> <p>An alternative is to use Bayesian optimization methods to search for hyperparameters. In general terms, bayesian hyperparameter optimization consists of creating a probabilistic model in which the objective function is the model validation metric (RMSE, AUC, accuracy...). With this strategy, the search is redirected at each iteration to the regions of greatest interest. The ultimate goal is to reduce the number of hyperparameter combinations with which the model is evaluated, choosing only the best candidates. This approach is particularly advantageous when the search space is very large or the model evaluation is very slow.</p> <p>Skforecast offers two Bayesian optimization engines: Scikit-Optimize and Optuna. It is worth noting that, in the context of skforecast, bayesian search is only applied to the hyperparameters of the model, and not to the lags, as all lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#optuna","title":"Optuna\u00b6","text":"<p>In skforecast, Bayesian optimization with Optuna is performed using its Study object. The objective of the optimization is to minimize the metric generated by backtesting.</p> <p>Additional parameters can be included by passing a dictionary to <code>kwargs_create_study</code> and <code>kwargs_study_optimize</code> arguments to create_study and optimize method, respectively. These arguments are used to configure the study object and optimization algorithm.</p> <p>To use Optuna in skforecast, the <code>search_space</code> argument must be a python function that defines the hyperparameters to optimize over. Optuna uses the Trial object object to generate each search space.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#scikit-optimize","title":"Scikit-optimize\u00b6","text":"<p>Skopt performs bayesian optimization with Gaussian processes. This is done with the function gp_minimize where the objective value to be minimized is calculated by backtesting.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-with-custom-metric","title":"Hyperparameter tuning with custom metric\u00b6","text":"<p>Besides to the commonly used metrics such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p> <p>The example below demonstrates how to use hyperparameter optimization to find the optimal parameters for a custom metric that considers only the last three months of each year.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p> <p>All three functions (<code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code>) enable users to calculate multiple metrics for each forecaster configuration if a list is provided. This list may include any combination of built-in metrics, such as <code>mean_squared_error</code>, <code>mean_absolute_error</code>, and <code>mean_absolute_percentage_error</code>, as well as user-defined custom metrics.</p> <p>Note that if multiple metrics are specified, these functions will select the best model based on the first metric in the list.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-regressors","title":"Compare multiple regressors\u00b6","text":"<p>The grid search process can be easily extended to compare several machine learning models. This can be achieved by using a simple for loop that iterates over each regressor and applying the <code>grid_search_forecaster</code> function. This approach allows for a more thorough exploration and can help you select the best model.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hide-progress-bar","title":"Hide progress bar\u00b6","text":"<p>It is possible to hide the progress bar using the following code:</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html","title":"Independent multi-time series forecasting","text":"<p>  \u00a0 Note </p> <p>See ForecasterAutoregMultiVariate for dependent multi-series forecasting (multivariate time series).</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\n\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\nfrom skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\nfrom skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_absolute_error  from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/simulated_items_sales.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/simulated_items_sales.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00   (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00   (n=170)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\ndata_train['item_1'].plot(label='train', ax=axes[0])\ndata_test['item_1'].plot(label='test', ax=axes[0])\naxes[0].set_xlabel('')\naxes[0].set_ylabel('sales')\naxes[0].set_title('Item 1')\naxes[0].legend()\n\ndata_train['item_2'].plot(label='train', ax=axes[1])\ndata_test['item_2'].plot(label='test', ax=axes[1])\naxes[1].set_xlabel('')\naxes[1].set_ylabel('sales')\naxes[1].set_title('Item 2')\n\ndata_train['item_3'].plot(label='train', ax=axes[2])\ndata_test['item_3'].plot(label='test', ax=axes[2])\naxes[2].set_xlabel('')\naxes[2].set_ylabel('sales')\naxes[2].set_title('Item 3')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  data_train['item_1'].plot(label='train', ax=axes[0]) data_test['item_1'].plot(label='test', ax=axes[0]) axes[0].set_xlabel('') axes[0].set_ylabel('sales') axes[0].set_title('Item 1') axes[0].legend()  data_train['item_2'].plot(label='train', ax=axes[1]) data_test['item_2'].plot(label='test', ax=axes[1]) axes[1].set_xlabel('') axes[1].set_ylabel('sales') axes[1].set_title('Item 2')  data_train['item_3'].plot(label='train', ax=axes[2]) data_test['item_3'].plot(label='test', ax=axes[2]) axes[2].set_xlabel('') axes[2].set_ylabel('sales') axes[2].set_title('Item 3')  fig.tight_layout() plt.show(); In\u00a0[5]: Copied! <pre># Create and fit forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              )  forecaster.fit(series=data_train) forecaster Out[5]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: Ridge(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: None \nTransformer for exog: None \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:15:11 \nLast fit date: 2023-05-29 13:15:11 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>Two methods can be use to predict the next n steps: <code>predict()</code> or <code>predict_interval()</code>. The argument <code>levels</code> is used to indicate for which series estimate predictions. If <code>None</code> all series will be predicted.</p> In\u00a0[6]: Copied! <pre># Predict and predict_interval\n# ==============================================================================\nsteps = 24\n\n# Predictions for item_1\npredictions_item_1 = forecaster.predict(steps=steps, levels='item_1')\ndisplay(predictions_item_1.head(3))\n\n# Interval predictions for item_1 and item_2\npredictions_intervals = forecaster.predict_interval(steps=steps, levels=['item_1', 'item_2'])\ndisplay(predictions_intervals.head(3))\n</pre> # Predict and predict_interval # ============================================================================== steps = 24  # Predictions for item_1 predictions_item_1 = forecaster.predict(steps=steps, levels='item_1') display(predictions_item_1.head(3))  # Interval predictions for item_1 and item_2 predictions_intervals = forecaster.predict_interval(steps=steps, levels=['item_1', 'item_2']) display(predictions_intervals.head(3)) item_1 2014-07-16 25.497376 2014-07-17 24.866972 2014-07-18 24.281173 item_1 item_1_lower_bound item_1_upper_bound item_2 item_2_lower_bound item_2_upper_bound 2014-07-16 25.497376 23.220087 28.226068 10.694506 7.093046 15.518896 2014-07-17 24.866972 22.141168 27.389805 11.080091 6.467676 16.534679 2014-07-18 24.281173 21.688393 26.981395 11.490882 7.077863 16.762530 In\u00a0[7]: Copied! <pre># Backtesting Multi Series\n# ==============================================================================\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n                                           forecaster            = forecaster,\n                                           series                = data,\n                                           levels                = None,\n                                           steps                 = 24,\n                                           metric                = 'mean_absolute_error',\n                                           initial_train_size    = len(data_train),\n                                           fixed_train_size      = True,\n                                           gap                   = 0,\n                                           allow_incomplete_fold = True,\n                                           refit                 = True,\n                                           verbose               = False,\n                                           show_progress         = True\n                                       )\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting Multi Series # ============================================================================== metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(                                            forecaster            = forecaster,                                            series                = data,                                            levels                = None,                                            steps                 = 24,                                            metric                = 'mean_absolute_error',                                            initial_train_size    = len(data_train),                                            fixed_train_size      = True,                                            gap                   = 0,                                            allow_incomplete_fold = True,                                            refit                 = True,                                            verbose               = False,                                            show_progress         = True                                        )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.360675 1 item_2 2.332392 2 item_3 3.155592 <pre>\nBacktest predictions\n</pre> Out[7]: item_1 item_2 item_3 2014-07-16 25.497376 10.694506 11.275026 2014-07-17 24.866972 11.080091 11.313510 2014-07-18 24.281173 11.490882 13.030112 2014-07-19 23.515499 11.548922 13.378282 In\u00a0[8]: Copied! <pre># Create Forecaster multi series\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n</pre> # Create Forecaster multi series # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = StandardScaler(),                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              ) In\u00a0[9]: Copied! <pre># Grid search Multi Series\n# ==============================================================================\nlags_grid = [24, 48]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nlevels = ['item_1', 'item_2', 'item_3']\n\nresults = grid_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              exog                = None,\n              levels              = levels,\n              lags_grid           = lags_grid,\n              param_grid          = param_grid,\n              steps               = 24,\n              metric              = 'mean_absolute_error',\n              initial_train_size  = len(data_train),\n              refit               = True,\n              fixed_train_size    = True,\n              return_best         = False,\n              verbose             = False\n          )\n\nresults\n</pre> # Grid search Multi Series # ============================================================================== lags_grid = [24, 48] param_grid = {'alpha': [0.01, 0.1, 1]}  levels = ['item_1', 'item_2', 'item_3']  results = grid_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               exog                = None,               levels              = levels,               lags_grid           = lags_grid,               param_grid          = param_grid,               steps               = 24,               metric              = 'mean_absolute_error',               initial_train_size  = len(data_train),               refit               = True,               fixed_train_size    = True,               return_best         = False,               verbose             = False           )  results <pre>6 models compared for 3 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[9]: levels lags params mean_absolute_error alpha 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.207648 1.00 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.207700 0.10 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.207706 0.01 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.335039 1.00 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.335149 0.10 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.335161 0.01 In\u00a0[10]: Copied! <pre># Weights in Multi-Series\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = custom_weights,\n                 series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.} # Same as {'item_2': 2.}\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=24).head(3)\n</pre> # Weights in Multi-Series # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights  forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = custom_weights,                  series_weights     = {'item_1': 1., 'item_2': 2., 'item_3': 1.} # Same as {'item_2': 2.}              )  forecaster.fit(series=data_train) forecaster.predict(steps=24).head(3) Out[10]: item_1 item_2 item_3 2014-07-16 25.547560 10.527454 11.195018 2014-07-17 24.779779 10.987891 11.424717 2014-07-18 24.182702 11.375158 13.090853 <p>  \u00a0 Warning </p> <p>The <code>weight_func</code> and <code>series_weights</code> arguments will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>. If <code>weight_func</code> is a <code>dict</code>, it will be a <code>dict</code> of the form <code>{'series_column_name': source_code_weight_func}</code> .</p> In\u00a0[11]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n</pre> In\u00a0[12]: Copied! <pre>forecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24,\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': StandardScaler()},\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24,                  transformer_series = {'item_1': StandardScaler(), 'item_2': StandardScaler()},                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None              )  forecaster.fit(series=data_train) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\skforecast\\ForecasterAutoregMultiSeries\\ForecasterAutoregMultiSeries.py:453: IgnoredArgumentWarning: {'item_3'} not present in `transformer_series`. No transformation is applied to these series. \n You can suppress this warning using: warnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n  warnings.warn(\n</pre> Out[12]: <pre>============================ \nForecasterAutoregMultiSeries \n============================ \nRegressor: Ridge(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \nTransformer for series: {'item_1': StandardScaler(), 'item_2': StandardScaler()} \nTransformer for exog: None \nWindow size: 24 \nSeries levels (names): ['item_1', 'item_2', 'item_3'] \nSeries weights: None \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2012-01-01 00:00:00'), Timestamp('2014-07-15 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:15:14 \nLast fit date: 2023-05-29 13:15:14 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[13]: Copied! <pre># Grid search Multi-Series with multiple metrics\n# ==============================================================================\nforecaster = ForecasterAutoregMultiSeries(\n                 regressor          = Ridge(random_state=123),\n                 lags               = 24\n             )\n\ndef custom_metric(y_true, y_pred):\n\"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\nlags_grid = [24, 48]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              lags_grid           = lags_grid,\n              param_grid          = param_grid,\n              steps               = 24,\n              metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              initial_train_size  = len(data_train),\n              fixed_train_size    = True,\n              levels              = None,\n              exog                = None,\n              refit               = True,\n              return_best         = False,\n              verbose             = False\n          )\n\nresults\n</pre> # Grid search Multi-Series with multiple metrics # ============================================================================== forecaster = ForecasterAutoregMultiSeries(                  regressor          = Ridge(random_state=123),                  lags               = 24              )  def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  lags_grid = [24, 48] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               lags_grid           = lags_grid,               param_grid          = param_grid,               steps               = 24,               metric              = [mean_absolute_error, custom_metric, 'mean_squared_error'],               initial_train_size  = len(data_train),               fixed_train_size    = True,               levels              = None,               exog                = None,               refit               = True,               return_best         = False,               verbose             = False           )  results <pre>6 models compared for 3 level(s). Number of iterations: 6.\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[13]: levels lags params mean_absolute_error custom_metric mean_squared_error alpha 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.190420 2.290771 9.250861 1.00 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.190493 2.290853 9.251522 0.10 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.190500 2.290861 9.251589 0.01 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 1} 2.282886 2.358415 9.770826 1.00 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.1} 2.282948 2.358494 9.771567 0.10 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'alpha': 0.01} 2.282954 2.358502 9.771641 0.01 <p>  \u00a0 Warning </p> <p><code>bayesian_search_forecaster_multiseries</code> will be released in a future version of skforecast.  Stay tuned!</p> In\u00a0[14]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/independent-multi-time-series-forecasting.html#independent-multi-series-forecasting","title":"Independent multi-series forecasting\u00b6","text":"<p>In univariate time series forecasting, a single time series is modeled as a linear or nonlinear combination of its lags, where past values of the series are used to forecast its future. In multi-series forecasting, two or more time series are modeled together using a single model.</p> <p>In independent multi-series forecasting a single model is trained for all time series, but each time series remains independent of the others, meaning that past values of one series are not used as predictors of other series. However, modeling them together is useful because the series may follow the same intrinsic pattern regarding their past and future values. For instance, the sales of products A and B in the same store may not be related, but they follow the same dynamics, that of the store.</p> <p></p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied, with the only difference being that the series name for which to estimate the predictions needs to be indicated.</p> <p></p> Diagram of recursive forecasting with multiple independent time series. <p>Using the <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> classes, it is possible to easily build machine learning models for independent multi-series forecasting.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#train-and-predict-forecasterautoregmultiseries","title":"Train and predict ForecasterAutoregMultiSeries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#backtesting-multi-series","title":"Backtesting Multi Series\u00b6","text":"<p>As in the <code>predict</code> method, the <code>levels</code> at which backtesting is performed must be indicated. The argument can also be set to <code>None</code> to perform backtesting at all levels.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#hyperparameter-tuning-and-lags-selection-multi-series","title":"Hyperparameter tuning and lags selection Multi Series\u00b6","text":"<p>Functions <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code> in the module <code>model_selection_multiseries</code> allow for lag and hyperparameter optimization. The optimization is performed in the same way as in the other forecasters, see the user guide here, except for the <code>levels</code> argument:</p> <ul> <li><p><code>levels</code>: level(s) at which the forecaster is optimized, for example:</p> <ul> <li><p>If <code>levels = ['item_1', 'item_2', 'item_3']</code> (Same as <code>levels = None</code>), the function will search for the lags and hyperparameters that minimize the average error of the predictions of all the time series. The resulting metric will be the average of all levels.</p> </li> <li><p>If <code>levels = 'item_1'</code> (Same as <code>levels = ['item_1']</code>), the function will search for the lags and hyperparameters that minimize the error of the <code>item_1</code> predictions. The resulting metric will be the one calculated for <code>item_1</code>.</p> </li> </ul> </li> </ul> <p>The following example shows how to use <code>grid_search_forecaster_multiseries</code> to find the best lags and model hyperparameters for all time series:</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#weights-in-multi-series","title":"Weights in multi-series\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterAutoregMultiseries</code> accepts two types of weights:</p> <ul> <li><p><code>series_weights</code> controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.</p> </li> <li><p><code>weight_func</code> controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.</p> </li> </ul> <p>If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <p></p> <ul> <li><p><code>series_weights</code> is a dict of the form <code>{'series_column_name': float}</code>. If a series is used during <code>fit</code> and is not present in <code>series_weights</code>, it will have a weight of 1.</p> </li> <li><p><code>weight_func</code> is a function that defines the individual weights of each sample based on the index.</p> <ul> <li><p>If it is a <code>callable</code>, the same function will apply to all series.</p> </li> <li><p>If it is a <code>dict</code> of the form <code>{'series_column_name': callable}</code>, a different function can be used for each series. A weight of 1 is given to all series not present in <code>weight_func</code>.</p> </li> </ul> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#scikit-learn-transformers-in-multi-series","title":"Scikit-learn transformers in multi-series\u00b6","text":"<p>Learn more about using scikit-learn transformers with skforecast.</p> <ul> <li>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</li> <li>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them.</li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>All three functions (<code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, and <code>random_search_forecaster_multiseries</code>) allow the calculation of multiple metrics for each forecaster configuration if a list is provided. This list may include custom metrics and the best model selection is done based on the first metric of the list.</p>"},{"location":"user_guides/input-data.html","title":"Input data","text":"<p>Skforecast only allows pandas series and dataframes as input (although numpy arrays are used internally for better performance). The type of pandas index is used to determine how the data is processed:</p> <ul> <li><p>If the index is not of type DatetimeIndex, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex but has no frequency, a RangeIndex is created.</p> </li> <li><p>If the index is of type DatetimeIndex and has a frequency, it remains unchanged.</p> </li> </ul> <p>  \u00a0 Note </p> <p>Although it is possible to use data without an associated date/time index, when using a pandas series with an associated frequency prediction results will have a more useful index.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date']) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data Out[2]: y date 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 1991-11-01 0.502369 ... ... 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags = 5\n             )\n\nforecaster.fit(y=data['y'])\n\n# Predictions\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags = 5              )  forecaster.fit(y=data['y'])  # Predictions # ============================================================================== forecaster.predict(steps=5) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\n2008-10-01    0.845027\n2008-11-01    0.914621\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre>data = data.reset_index(drop=True)\ndata\n</pre> data = data.reset_index(drop=True) data Out[4]: y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 ... ... 199 0.761822 200 0.649435 201 0.827887 202 0.816255 203 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[5]: Copied! <pre>forecaster.fit(y=data['y'])\nforecaster.predict(steps=5)\n</pre> forecaster.fit(y=data['y']) forecaster.predict(steps=5) Out[5]: <pre>204    0.714526\n205    0.789144\n206    0.818433\n207    0.845027\n208    0.914621\nName: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/input-data.html#input-data","title":"Input data\u00b6","text":""},{"location":"user_guides/input-data.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/input-data.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-with-datetime-and-frequency-index","title":"Train and predict using input with datetime and frequency index\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-without-datetime-index","title":"Train and predict using input without datetime index\u00b6","text":""},{"location":"user_guides/plot-forecaster-residuals.html","title":"Plot forecaster residuals","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import plot_residuals\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.plot import plot_residuals In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\n# Plot data\n# ==============================================================================\nfig, ax=plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  # Plot data # ============================================================================== fig, ax=plt.subplots(figsize=(7, 3)) data.plot(ax=ax); In\u00a0[3]: Copied! <pre># Train and backtest forecaster\n# ==============================================================================\nn_backtest = 36*3\ndata_train = data[:-n_backtest]\ndata_test  = data[-n_backtest:]\n\nforecaster = ForecasterAutoreg(\n                 regressor = Ridge(),\n                 lags      = 5 \n             )\n\nmetric, predictions = backtesting_forecaster(\n                        forecaster         = forecaster,\n                        y                  = data.y,\n                        initial_train_size = len(data_train),\n                        steps              = 36,\n                        metric             = 'mean_squared_error',\n                        verbose            = True\n                     )\n \npredictions.head()\n</pre> # Train and backtest forecaster # ============================================================================== n_backtest = 36*3 data_train = data[:-n_backtest] data_test  = data[-n_backtest:]  forecaster = ForecasterAutoreg(                  regressor = Ridge(),                  lags      = 5               )  metric, predictions = backtesting_forecaster(                         forecaster         = forecaster,                         y                  = data.y,                         initial_train_size = len(data_train),                         steps              = 36,                         metric             = 'mean_squared_error',                         verbose            = True                      )   predictions.head() <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 96\nNumber of observations used for backtesting: 108\n    Number of folds: 3\n    Number of steps per fold: 36\n    Number of steps to exclude from the end of each train set before test (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 1999-07-01 00:00:00 -- 2002-06-01 00:00:00  (n=36)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2002-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=36)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 2005-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=36)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[3]: pred 1999-07-01 0.667651 1999-08-01 0.655759 1999-09-01 0.652177 1999-10-01 0.641377 1999-11-01 0.635245 <p>The <code>predictions_backtest</code> function can be used in two ways. Firstly, it can be utilized with pre-calculated residuals to perform a backtest on the forecast model. Secondly, it can be used with both predicted values and actual values of the series to evaluate the accuracy of the forecast.</p> In\u00a0[4]: Copied! <pre># Plot residuals\n# ======================================================================================\nresiduals = predictions['pred'] - data_test['y']\n_ = plot_residuals(residuals=residuals)\n</pre> # Plot residuals # ====================================================================================== residuals = predictions['pred'] - data_test['y'] _ = plot_residuals(residuals=residuals) In\u00a0[5]: Copied! <pre>_ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'])\n</pre> _ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred']) In\u00a0[6]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n_ = plot_residuals(residuals=residuals, fig=fig)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 5)) _ = plot_residuals(residuals=residuals, fig=fig) In\u00a0[7]: Copied! <pre>_ = plot_residuals(residuals=residuals, edgecolor=\"black\", linewidth=15, figsize=(8, 5))\n</pre> _ = plot_residuals(residuals=residuals, edgecolor=\"black\", linewidth=15, figsize=(8, 5)) <p>It is also possible to customize the internal aesthetics of the plots by changing <code>plt.rcParams</code>.</p> In\u00a0[8]: Copied! <pre>plt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor'  : '#212946',\n    'axes.facecolor'    : '#212946',\n    'savefig.facecolor' :'#212946',\n    'axes.grid'         : True,\n    'axes.grid.which'   : 'both',\n    'axes.spines.left'  : False,\n    'axes.spines.right' : False,\n    'axes.spines.top'   : False,\n    'axes.spines.bottom': False,\n    'grid.color'        : '#2A3459',\n    'grid.linewidth'    : '1',\n    'text.color'        : '0.9',\n    'axes.labelcolor'   : '0.9',\n    'xtick.color'       : '0.9',\n    'ytick.color'       : '0.9',\n    'font.size'         : 12\n}\nplt.rcParams.update(dark_style)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\n_ = plot_residuals(residuals=residuals, fig=fig)\n</pre> plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 dark_style = {     'figure.facecolor'  : '#212946',     'axes.facecolor'    : '#212946',     'savefig.facecolor' :'#212946',     'axes.grid'         : True,     'axes.grid.which'   : 'both',     'axes.spines.left'  : False,     'axes.spines.right' : False,     'axes.spines.top'   : False,     'axes.spines.bottom': False,     'grid.color'        : '#2A3459',     'grid.linewidth'    : '1',     'text.color'        : '0.9',     'axes.labelcolor'   : '0.9',     'xtick.color'       : '0.9',     'ytick.color'       : '0.9',     'font.size'         : 12 } plt.rcParams.update(dark_style)  fig, ax = plt.subplots(1, 1, figsize=(8, 5)) _ = plot_residuals(residuals=residuals, fig=fig) In\u00a0[9]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/plot-forecaster-residuals.html#plot-forecasting-residuals","title":"Plot forecasting residuals\u00b6","text":"<p>Analyzing the residuals (errors) of predictions is useful to understand the behavior of a forecaster. The function <code>skforecast.plot.plot_residuals</code> creates 3 plots:</p> <ul> <li><p>A time-ordered plot of residual values</p> </li> <li><p>A distribution plot that showcases the distribution of residuals</p> </li> <li><p>A plot showcasing the autocorrelation of residuals</p> </li> </ul> <p>By examining the residual values over time, you can determine whether there is a pattern in the errors made by the forecast model. The distribution plot helps you understand whether the residuals are normally distributed, and the autocorrelation plot helps you identify whether there are any dependencies or relationships between the residuals.</p>"},{"location":"user_guides/plot-forecaster-residuals.html#customizing-plots","title":"Customizing plots\u00b6","text":"<p>It is possible to customize the plot by by either passing a pre-existing matplotlib figure object or using additional keyword arguments that are passed to <code>matplotlib.pyplot.figure()</code>.</p>"},{"location":"user_guides/probabilistic-forecasting.html","title":"Probabilistic forecasting","text":"<p>  \u00a0 Warning </p> <p>As Rob J Hyndman explains in his blog, in real-world problems, almost all prediction intervals are too narrow. For example, nominal 95% intervals may only provide coverage between 71% and 87%. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. With forecasting models, there are at least four sources of uncertainty:</p> <ul> <li>The random error term</li> <li>The parameter estimates</li> <li>The choice of model for the historical data</li> <li>The continuation of the historical data generating process into the future</li> </ul> <p>When producing prediction intervals for time series models, generally only the first of these sources is taken into account. Therefore, it is advisable to use test data to validate the empirical coverage of the interval and not solely rely on the expected coverage.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ndark_style = {\n    'figure.facecolor'  : '#212946',\n    'axes.facecolor'    : '#212946',\n    'savefig.facecolor' : '#212946',\n    'axes.grid'         : True,\n    'axes.grid.which'   : 'both',\n    'axes.spines.left'  : False,\n    'axes.spines.right' : False,\n    'axes.spines.top'   : False,\n    'axes.spines.bottom': False,\n    'grid.color'        : '#2A3459',\n    'grid.linewidth'    : '1',\n    'text.color'        : '0.9',\n    'axes.labelcolor'   : '0.9',\n    'xtick.color'       : '0.9',\n    'ytick.color'       : '0.9',\n    'font.size'         : 12\n}\nplt.rcParams.update(dark_style)\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.plot import plot_prediction_distribution\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_pinball_loss\nfrom skforecast.exceptions import LongTrainingWarning\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt import matplotlib.ticker as ticker from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 dark_style = {     'figure.facecolor'  : '#212946',     'axes.facecolor'    : '#212946',     'savefig.facecolor' : '#212946',     'axes.grid'         : True,     'axes.grid.which'   : 'both',     'axes.spines.left'  : False,     'axes.spines.right' : False,     'axes.spines.top'   : False,     'axes.spines.bottom': False,     'grid.color'        : '#2A3459',     'grid.linewidth'    : '1',     'text.color'        : '0.9',     'axes.labelcolor'   : '0.9',     'xtick.color'       : '0.9',     'ytick.color'       : '0.9',     'font.size'         : 12 } plt.rcParams.update(dark_style)  # Modelling and Forecasting # ============================================================================== from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect from skforecast.plot import plot_prediction_distribution from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from lightgbm import LGBMRegressor from sklearn.metrics import mean_pinball_loss from skforecast.exceptions import LongTrainingWarning  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n       'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/vic_elec.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preparation (aggregation at daily level)\n# ==============================================================================\ndata['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ')\ndata = data.set_index('Time')\ndata = data.asfreq('30min')\ndata = data.sort_index()\ndata = data.drop(columns='Date')\ndata = data.resample(rule='D', closed='left', label ='right')\\\n       .agg({'Demand': 'sum', 'Temperature': 'mean', 'Holiday': 'max'})\n\n# Split data into train-validation-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\nend_validation = '2014-9-30 23:59:00'\ndata_train = data.loc[: end_train, :].copy()\ndata_val   = data.loc[end_train:end_validation, :].copy()\ndata_test  = data.loc[end_validation:, :].copy()\n\nprint(\n    f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"\n    f\"  (n={len(data_val)})\"\n)\nprint(\n    f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Data download # ============================================================================== url = (        'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/vic_elec.csv' ) data = pd.read_csv(url, sep=',')  # Data preparation (aggregation at daily level) # ============================================================================== data['Time'] = pd.to_datetime(data['Time'], format='%Y-%m-%dT%H:%M:%SZ') data = data.set_index('Time') data = data.asfreq('30min') data = data.sort_index() data = data.drop(columns='Date') data = data.resample(rule='D', closed='left', label ='right')\\        .agg({'Demand': 'sum', 'Temperature': 'mean', 'Holiday': 'max'})  # Split data into train-validation-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' end_validation = '2014-9-30 23:59:00' data_train = data.loc[: end_train, :].copy() data_val   = data.loc[end_train:end_validation, :].copy() data_test  = data.loc[end_validation:, :].copy()  print(     f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}\"     f\"  (n={len(data_val)})\" ) print(     f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Train dates      : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00  (n=731)\nValidation dates : 2014-01-01 00:00:00 --- 2014-09-30 00:00:00  (n=273)\nTest dates       : 2014-10-01 00:00:00 --- 2014-12-30 00:00:00  (n=91)\n</pre> In\u00a0[3]: Copied! <pre># Plot time series partition\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 3))\ndata_train['Demand'].plot(label='train', ax=ax)\ndata_val['Demand'].plot(label='validation', ax=ax)\ndata_test['Demand'].plot(label='test', ax=ax)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylim(bottom=160_000)\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== fig, ax = plt.subplots(figsize=(9, 3)) data_train['Demand'].plot(label='train', ax=ax) data_val['Demand'].plot(label='validation', ax=ax) data_test['Demand'].plot(label='test', ax=ax) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylim(bottom=160_000) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand') ax.legend(); In\u00a0[4]: Copied! <pre># Create and train a ForecasterAutoreg\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(\n                                 learning_rate = 0.01,\n                                 max_depth     = 10,\n                                 n_estimators  = 500,\n                                 random_state  = 123\n                             ),\n                 lags      = 7\n             )\n\nforecaster.fit(y=data.loc[end_train:end_validation, 'Demand'])\nforecaster\n</pre> # Create and train a ForecasterAutoreg # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(                                  learning_rate = 0.01,                                  max_depth     = 10,                                  n_estimators  = 500,                                  random_state  = 123                              ),                  lags      = 7              )  forecaster.fit(y=data.loc[end_train:end_validation, 'Demand']) forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: LGBMRegressor(learning_rate=0.01, max_depth=10, n_estimators=500,\n              random_state=123) \nLags: [1 2 3 4 5 6 7] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 7 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-09-30 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.01, 'max_depth': 10, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:17:23 \nLast fit date: 2023-05-29 13:17:23 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>Once the Forecaster has been trained, the method <code>predict_bootstraping</code> can be use to simulate <code>n_boot</code> values of the forecasting distribution.</p> In\u00a0[5]: Copied! <pre># Predict 10 different forecasting sequences of 7 steps each using bootstrapping\n# ==============================================================================\nboot_predictions = forecaster.predict_bootstrapping(steps=7, n_boot=10)\nboot_predictions\n</pre> # Predict 10 different forecasting sequences of 7 steps each using bootstrapping # ============================================================================== boot_predictions = forecaster.predict_bootstrapping(steps=7, n_boot=10) boot_predictions Out[5]: pred_boot_0 pred_boot_1 pred_boot_2 pred_boot_3 pred_boot_4 pred_boot_5 pred_boot_6 pred_boot_7 pred_boot_8 pred_boot_9 2014-10-01 208783.097577 204764.999003 201367.203420 212280.661192 204622.100582 202889.246579 210078.609982 210151.742392 200990.348257 203137.232359 2014-10-02 212340.130913 208320.030667 217221.147681 209996.152477 218464.949614 212545.697772 212061.596039 216579.376162 226902.326444 212847.017644 2014-10-03 221380.131363 223890.630246 224625.204840 207260.018970 206099.826214 288939.369743 220085.831843 229514.853352 230476.360893 224059.729506 2014-10-04 209943.773973 212946.528553 200027.296792 203592.696069 194120.730680 262225.653280 212516.896678 222042.271003 215236.033624 222487.580478 2014-10-05 193372.607408 177195.227257 194351.934752 206606.909536 202654.435499 297740.735825 192454.576530 199372.141645 208527.690318 197830.624380 2014-10-06 199626.367544 203338.054671 207145.334747 208993.695577 204766.177714 258050.416951 184903.916795 201473.420885 204790.425542 183312.907014 2014-10-07 201546.003371 212477.910879 209639.468951 204102.852012 225036.945159 257847.869040 197235.769115 196990.134684 200007.936217 207116.299541 <p>Using a ridge plot is a useful way to visualize the uncertainty of a forecasting model. This plot estimates a kernel density for each step by using the bootstrapped predictions.</p> In\u00a0[6]: Copied! <pre># Ridge plot of bootstrapping predictions\n# ==============================================================================\n_ = plot_prediction_distribution(boot_predictions, figsize=(5, 4))\n</pre> # Ridge plot of bootstrapping predictions # ============================================================================== _ = plot_prediction_distribution(boot_predictions, figsize=(5, 4)) <p>In most cases, the user is interested in a specific interval rather than the entire bootstrapping simulation matrix. To address this need, skforecast provides the <code>predict_interval</code> method. This method internally uses <code>predict_bootstrapping</code> to obtain the bootstrapping matrix and estimates the upper and lower quantiles for each step, thus providing the user with the desired prediction intervals.</p> In\u00a0[7]: Copied! <pre># Predict intervals for next 7 steps, quantiles 10th and 90th\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=7, interval=[10, 90], n_boot=1000)\npredictions\n</pre> # Predict intervals for next 7 steps, quantiles 10th and 90th # ============================================================================== predictions = forecaster.predict_interval(steps=7, interval=[10, 90], n_boot=1000) predictions Out[7]: pred lower_bound upper_bound 2014-10-01 205723.923923 195483.862851 214883.472075 2014-10-02 215167.163121 204160.738452 225623.750301 2014-10-03 225144.443075 212599.136391 237675.514362 2014-10-04 211406.440681 196863.841123 234950.293307 2014-10-05 194848.766987 185544.868289 225497.131653 2014-10-06 201901.819903 188334.486578 226720.998742 2014-10-07 208648.526025 191797.716429 231948.852094 <p>The intervals estimated so far are distribution-free, which means that no assumptions are made about a particular distribution. The <code>predict_dist</code> method in skforecast allows fitting a parametric distribution to the bootstrapped prediction samples obtained with <code>predict_bootstrapping</code>. This is useful when there is reason to believe that the forecast errors follow a particular distribution, such as the normal distribution or the student's t-distribution. The <code>predict_dist</code> method allows the user to specify any continuous distribution from the scipy.stats module.</p> In\u00a0[8]: Copied! <pre># Predict the parameters of a normal distribution for the next 7 steps\n# ==============================================================================\nfrom scipy.stats import norm\n\npredictions = forecaster.predict_dist(steps=7, distribution=norm, n_boot=1000)\npredictions\n</pre> # Predict the parameters of a normal distribution for the next 7 steps # ============================================================================== from scipy.stats import norm  predictions = forecaster.predict_dist(steps=7, distribution=norm, n_boot=1000) predictions Out[8]: loc scale 2014-10-01 205374.632063 12290.061732 2014-10-02 215791.488070 13607.935838 2014-10-03 225631.039103 14206.970100 2014-10-04 215210.987777 18980.776421 2014-10-05 202613.715687 19980.302998 2014-10-06 205391.493409 19779.732794 2014-10-07 208579.269999 18950.289467 In\u00a0[9]: Copied! <pre># Backtesting on test data \n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['Demand'],\n                          steps                 = 7,\n                          metric                = 'mean_squared_error',\n                          initial_train_size    = len(data_train) + len(data_val),\n                          fixed_train_size      = True,\n                          gap                   = 0,\n                          allow_incomplete_fold = True,\n                          refit                 = True,\n                          interval              = [10, 90],\n                          n_boot                = 1000,\n                          random_state          = 123,\n                          verbose               = False\n                      )\npredictions.head(4)\n</pre> # Backtesting on test data  # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['Demand'],                           steps                 = 7,                           metric                = 'mean_squared_error',                           initial_train_size    = len(data_train) + len(data_val),                           fixed_train_size      = True,                           gap                   = 0,                           allow_incomplete_fold = True,                           refit                 = True,                           interval              = [10, 90],                           n_boot                = 1000,                           random_state          = 123,                           verbose               = False                       ) predictions.head(4) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> Out[9]: pred lower_bound upper_bound 2014-10-01 223264.432587 214633.692784 231376.785240 2014-10-02 233596.210854 223051.862727 244752.538248 2014-10-03 242653.671597 226947.654906 258361.695192 2014-10-04 220842.603128 209967.438912 236404.408845 In\u00a0[10]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[predictions.index, 'Demand'] &gt;= predictions['lower_bound']) &amp; \\\n                      (data.loc[predictions.index, 'Demand'] &lt;= predictions['upper_bound']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval on test data: {100 * coverage}\")\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[predictions.index, 'Demand'] &gt;= predictions['lower_bound']) &amp; \\                       (data.loc[predictions.index, 'Demand'] &lt;= predictions['upper_bound']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval on test data: {100 * coverage}\") <pre>Coverage of the predicted interval on test data: 65.93406593406593\n</pre> In\u00a0[11]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend();\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(); <p>The coverage of the predicted interval is much lower than expected (80%).</p> <p>By default, the training residuals are used to create the prediction intervals. However, this is not the most recommended option as the estimated interval tends to be too narrow. This is because the model's performance on training data is usually better than on new observations, leading to smaller residuals and, in turn, narrower prediction intervals.</p> <p>To overcome this issue, it is advisable to use other residuals, such as those obtained from a validation set. This can be achieved by storing the new residuals inside the forecaster using the <code>set_out_sample_residuals</code> method. Once the new residuals have been added to the forecaster, use <code>in_sample_residuals = False</code> when calling the <code>predict_interval</code> method.</p> In\u00a0[12]: Copied! <pre># Backtesting using validation data\n# ==============================================================================\nmetric, backtest_predictions = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data.loc[:end_validation, 'Demand'],\n                                   initial_train_size = len(data_train),\n                                   steps              = 7,\n                                   refit              = True,\n                                   metric             = 'mean_squared_error',\n                                   verbose            = False\n                               )\n\n# Calculate residuals in validation data\n# ==============================================================================\nresiduals = (data_val['Demand'] - backtest_predictions['pred']).to_numpy()\nresiduals[:5]\n</pre> # Backtesting using validation data # ============================================================================== metric, backtest_predictions = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data.loc[:end_validation, 'Demand'],                                    initial_train_size = len(data_train),                                    steps              = 7,                                    refit              = True,                                    metric             = 'mean_squared_error',                                    verbose            = False                                )  # Calculate residuals in validation data # ============================================================================== residuals = (data_val['Demand'] - backtest_predictions['pred']).to_numpy() residuals[:5] <pre>  0%|          | 0/39 [00:00&lt;?, ?it/s]</pre> Out[12]: <pre>array([-16084.02669559, -14168.47600418,  -4416.31618072, -15101.78356121,\n       -31133.70782794])</pre> <p>Once the residuals of the validation set are calculated, they can be stored inside the forecaster. This allows the forecaster to use the out-of-sample residuals for prediction interval calculation instead of the default in-sample residuals. By doing so, the estimated prediction intervals will be less optimistic and more representative of the performance of the model on new observations.</p> In\u00a0[13]: Copied! <pre># Set out-sample residuals\n# ==============================================================================\nforecaster.set_out_sample_residuals(residuals=residuals)\n</pre> # Set out-sample residuals # ============================================================================== forecaster.set_out_sample_residuals(residuals=residuals) <p>Finally, a backtesting process is applied to the test data, this time indicating <code>in_sample_residuals = False</code> to use the out-of-sample residuals in the interval estimation.</p> In\u00a0[14]: Copied! <pre># Backtesting using test data\n# ==============================================================================\nmetric, predictions_2 = backtesting_forecaster(\n                            forecaster          = forecaster,\n                            y                   = data['Demand'],\n                            initial_train_size  = len(data_train) + len(data_val),\n                            steps               = 7,\n                            refit               = True,\n                            interval            = [10, 90],\n                            n_boot              = 1000,\n                            in_sample_residuals = False,\n                            random_state        = 123,\n                            metric              = 'mean_squared_error',\n                            verbose             = False\n                        )\n\npredictions_2.head(4)\n</pre> # Backtesting using test data # ============================================================================== metric, predictions_2 = backtesting_forecaster(                             forecaster          = forecaster,                             y                   = data['Demand'],                             initial_train_size  = len(data_train) + len(data_val),                             steps               = 7,                             refit               = True,                             interval            = [10, 90],                             n_boot              = 1000,                             in_sample_residuals = False,                             random_state        = 123,                             metric              = 'mean_squared_error',                             verbose             = False                         )  predictions_2.head(4) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> Out[14]: pred lower_bound upper_bound 2014-10-01 223264.432587 203983.457029 236841.748720 2014-10-02 233596.210854 207273.919217 255429.172682 2014-10-03 242653.671597 211668.258043 265757.229322 2014-10-04 220842.603128 195869.206779 245921.530106 In\u00a0[15]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[predictions_2.index, 'Demand'] &gt;= predictions_2['lower_bound']) &amp; \\\n                      (data.loc[predictions_2.index, 'Demand'] &lt;= predictions_2['upper_bound']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval on test data: {100 * coverage}\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    predictions.index,\n    predictions['lower_bound'],\n    predictions['upper_bound'],\n    color = 'orange',\n    alpha = 0.3,\n    label = '80% interval in-sample residuals'\n)\nax.fill_between(\n    predictions_2.index,\n    predictions_2['lower_bound'],\n    predictions_2['upper_bound'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval out-sample residuals'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend(fontsize=10);\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[predictions_2.index, 'Demand'] &gt;= predictions_2['lower_bound']) &amp; \\                       (data.loc[predictions_2.index, 'Demand'] &lt;= predictions_2['upper_bound']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval on test data: {100 * coverage}\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     predictions.index,     predictions['lower_bound'],     predictions['upper_bound'],     color = 'orange',     alpha = 0.3,     label = '80% interval in-sample residuals' ) ax.fill_between(     predictions_2.index,     predictions_2['lower_bound'],     predictions_2['upper_bound'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval out-sample residuals' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(fontsize=10); <pre>Coverage of the predicted interval on test data: 92.3076923076923\n</pre> <p>Using out-of-sample residuals, the interval coverage is much closer to that expected than when using in-sample residuals.</p> <p>  \u00a0 Note </p> <p>In the case of <code>ForecasterAutoregDirect</code>, <code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>, the residuals must be provided using a dictionary to indicate to which step or level (series) they belong to.</p> <p>As in the previous example, an 80% prediction interval is estimated for 7 steps-ahead predictions but, this time, using quantile regression. In this example a LightGBM gradient boosting model is trained, however, the reader can use any other model by simply substituting the regressor definition.</p> In\u00a0[16]: Copied! <pre># Create forecasters: one for each bound of the interval\n# ==============================================================================\n# The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence\n# interval (90% - 10% = 80%).\n\n# Forecaster for quantile 10%\nforecaster_q10 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.1,\n                                     learning_rate = 0.1,\n                                     max_depth     = 10,\n                                     n_estimators  = 100\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n                  \n# Forecaster for quantile 90%\nforecaster_q90 = ForecasterAutoregDirect(\n                     regressor = LGBMRegressor(\n                                     objective     = 'quantile',\n                                     metric        = 'quantile',\n                                     alpha         = 0.9,\n                                     learning_rate = 0.1,\n                                     max_depth     = 10,\n                                     n_estimators  = 100\n                                 ),\n                     lags = 7,\n                     steps = 7\n                 )\n\nforecaster_q10.fit(y=data['Demand'])\nforecaster_q90.fit(y=data['Demand'])\n</pre> # Create forecasters: one for each bound of the interval # ============================================================================== # The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence # interval (90% - 10% = 80%).  # Forecaster for quantile 10% forecaster_q10 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.1,                                      learning_rate = 0.1,                                      max_depth     = 10,                                      n_estimators  = 100                                  ),                      lags = 7,                      steps = 7                  )                    # Forecaster for quantile 90% forecaster_q90 = ForecasterAutoregDirect(                      regressor = LGBMRegressor(                                      objective     = 'quantile',                                      metric        = 'quantile',                                      alpha         = 0.9,                                      learning_rate = 0.1,                                      max_depth     = 10,                                      n_estimators  = 100                                  ),                      lags = 7,                      steps = 7                  )  forecaster_q10.fit(y=data['Demand']) forecaster_q90.fit(y=data['Demand']) <p>Once the quantile forecasters are trained, they can be used to predict each of the bounds of the forecasting interval.</p> In\u00a0[17]: Copied! <pre># Predict intervals for next 7 steps\n# ==============================================================================\npredictions_q10 = forecaster_q10.predict(steps=7)\npredictions_q90 = forecaster_q90.predict(steps=7)\npredictions = pd.DataFrame({\n                  'lower_bound': predictions_q10,\n                  'upper_bound': predictions_q90,\n              })\npredictions\n</pre> # Predict intervals for next 7 steps # ============================================================================== predictions_q10 = forecaster_q10.predict(steps=7) predictions_q90 = forecaster_q90.predict(steps=7) predictions = pd.DataFrame({                   'lower_bound': predictions_q10,                   'upper_bound': predictions_q90,               }) predictions  Out[17]: lower_bound upper_bound 2014-12-31 177607.329878 218885.714906 2015-01-01 178062.602517 242582.788349 2015-01-02 186213.019530 220677.491829 2015-01-03 184901.085939 204261.638256 2015-01-04 193237.899189 235310.200573 2015-01-05 196673.050873 284881.068713 2015-01-06 184148.733152 293018.478848 <p>When validating a quantile regression model, a custom metric must be provided depending on the quantile being estimated. These metrics will be used again when tuning the hyper-parameters of each model.</p> In\u00a0[18]: Copied! <pre># Loss function for each quantile (pinball_loss)\n# ==============================================================================\ndef mean_pinball_loss_q10(y_true, y_pred):\n\"\"\"\n    Pinball loss for quantile 10.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.1)\n\n\ndef mean_pinball_loss_q90(y_true, y_pred):\n\"\"\"\n    Pinball loss for quantile 90.\n    \"\"\"\n    return mean_pinball_loss(y_true, y_pred, alpha=0.9)\n</pre> # Loss function for each quantile (pinball_loss) # ============================================================================== def mean_pinball_loss_q10(y_true, y_pred):     \"\"\"     Pinball loss for quantile 10.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.1)   def mean_pinball_loss_q90(y_true, y_pred):     \"\"\"     Pinball loss for quantile 90.     \"\"\"     return mean_pinball_loss(y_true, y_pred, alpha=0.9) In\u00a0[19]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nwarnings.simplefilter('ignore', category=LongTrainingWarning)\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster         = forecaster_q10,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = True,\n                                  metric             = mean_pinball_loss_q10,\n                                  verbose            = False\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster         = forecaster_q90,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = True,\n                                  metric             = mean_pinball_loss_q90,\n                                  verbose            = False\n                              )\n</pre> # Backtesting on test data # ============================================================================== warnings.simplefilter('ignore', category=LongTrainingWarning) metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster         = forecaster_q10,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = True,                                   metric             = mean_pinball_loss_q10,                                   verbose            = False                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster         = forecaster_q90,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = True,                                   metric             = mean_pinball_loss_q90,                                   verbose            = False                               ) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> <p>Predictions generated for each model are used to define the upper and lower limits of the interval.</p> In\u00a0[20]: Copied! <pre># Interval coverage on test data\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\\n                      (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval: {100 * coverage}\")\n</pre> # Interval coverage on test data # ============================================================================== inside_interval = np.where(                       (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\                       (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval: {100 * coverage}\") <pre>Coverage of the predicted interval: 61.53846153846154\n</pre> <p>The coverage of the predicted interval is much lower than expected (80%).</p> <p>The hyperparameters of the model were hand-tuned and there is no reason that the same hyperparameters are suitable for the 10th and 90th percentiles regressors. Therefore, a grid search is carried out for each forecaster.</p> In\u00a0[21]: Copied! <pre># Grid search of hyper-parameters and lags for each quantile forecaster\n# ==============================================================================\n# Regressor hyper-parameters\nparam_grid = {\n    'n_estimators': [100, 500],\n    'max_depth': [3, 5, 10],\n    'learning_rate': [0.01, 0.1]\n}\n\n# Lags used as predictors\nlags_grid = [7]\n\nresults_grid_q10 = grid_search_forecaster(\n                       forecaster         = forecaster_q10,\n                       y                  = data.loc[:end_validation, 'Demand'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 7,\n                       refit              = False,\n                       metric             = mean_pinball_loss_q10,\n                       initial_train_size = int(len(data_train)),\n                       return_best        = True,\n                       verbose            = False\n                   )\n\nresults_grid_q90 = grid_search_forecaster(\n                       forecaster         = forecaster_q90,\n                       y                  = data.loc[:end_validation, 'Demand'],\n                       param_grid         = param_grid,\n                       lags_grid          = lags_grid,\n                       steps              = 7,\n                       refit              = False,\n                       metric             = mean_pinball_loss_q90,\n                       initial_train_size = int(len(data_train)),\n                       return_best        = True,\n                       verbose            = False\n                   )\n</pre> # Grid search of hyper-parameters and lags for each quantile forecaster # ============================================================================== # Regressor hyper-parameters param_grid = {     'n_estimators': [100, 500],     'max_depth': [3, 5, 10],     'learning_rate': [0.01, 0.1] }  # Lags used as predictors lags_grid = [7]  results_grid_q10 = grid_search_forecaster(                        forecaster         = forecaster_q10,                        y                  = data.loc[:end_validation, 'Demand'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 7,                        refit              = False,                        metric             = mean_pinball_loss_q10,                        initial_train_size = int(len(data_train)),                        return_best        = True,                        verbose            = False                    )  results_grid_q90 = grid_search_forecaster(                        forecaster         = forecaster_q90,                        y                  = data.loc[:end_validation, 'Demand'],                        param_grid         = param_grid,                        lags_grid          = lags_grid,                        steps              = 7,                        refit              = False,                        metric             = mean_pinball_loss_q90,                        initial_train_size = int(len(data_train)),                        return_best        = True,                        verbose            = False                    ) <pre>Number of models compared: 12.\n</pre> <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7] \n  Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n  Backtesting metric: 2713.0192469016706\n\nNumber of models compared: 12.\n</pre> <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5 6 7] \n  Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500}\n  Backtesting metric: 4094.3047516967745\n\n</pre> <p>Once the best hyperparameters have been found for each forecaster, a backtesting process is applied again using the test data.</p> In\u00a0[22]: Copied! <pre># Backtesting on test data\n# ==============================================================================\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster         = forecaster_q10,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = False,\n                                  metric             = mean_pinball_loss_q10,\n                                  verbose            = False\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster         = forecaster_q90,\n                                  y                  = data['Demand'],\n                                  initial_train_size = len(data_train) + len(data_val),\n                                  steps              = 7,\n                                  refit              = False,\n                                  metric             = mean_pinball_loss_q90,\n                                  verbose            = False\n                              )\n</pre> # Backtesting on test data # ============================================================================== metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster         = forecaster_q10,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = False,                                   metric             = mean_pinball_loss_q10,                                   verbose            = False                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster         = forecaster_q90,                                   y                  = data['Demand'],                                   initial_train_size = len(data_train) + len(data_val),                                   steps              = 7,                                   refit              = False,                                   metric             = mean_pinball_loss_q90,                                   verbose            = False                               ) <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[23]: Copied! <pre># Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand')\nax.fill_between(\n    data.loc[end_validation:].index,\n    predictions_q10['pred'],\n    predictions_q90['pred'],\n    color = 'deepskyblue',\n    alpha = 0.3,\n    label = '80% interval'\n)\nax.yaxis.set_major_formatter(ticker.EngFormatter())\nax.set_ylabel('MW')\nax.set_xlabel('')\nax.set_title('Energy demand forecast')\nax.legend();\n</pre> # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'Demand'].plot(ax=ax, label='Demand') ax.fill_between(     data.loc[end_validation:].index,     predictions_q10['pred'],     predictions_q90['pred'],     color = 'deepskyblue',     alpha = 0.3,     label = '80% interval' ) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.set_ylabel('MW') ax.set_xlabel('') ax.set_title('Energy demand forecast') ax.legend(); In\u00a0[24]: Copied! <pre># Interval coverage\n# ==============================================================================\ninside_interval = np.where(\n                      (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\\n                      (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),\n                      True,\n                      False\n                  )\n\ncoverage = inside_interval.mean()\nprint(f\"Coverage of the predicted interval: {100 * coverage}\")\n</pre> # Interval coverage # ============================================================================== inside_interval = np.where(                       (data.loc[end_validation:, 'Demand'] &gt;= predictions_q10['pred']) &amp; \\                       (data.loc[end_validation:, 'Demand'] &lt;= predictions_q90['pred']),                       True,                       False                   )  coverage = inside_interval.mean() print(f\"Coverage of the predicted interval: {100 * coverage}\") <pre>Coverage of the predicted interval: 75.82417582417582\n</pre> <p>After optimizing the hyper-parameters of each quantile forecaster, the coverage is closer to the expected one (80%).</p> In\u00a0[25]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/probabilistic-forecasting.html#probabilistic-forecasting-prediction-intervals-and-prediction-distribution","title":"Probabilistic forecasting: prediction intervals and prediction distribution\u00b6","text":"<p>When trying to anticipate future values, most forecasting models focus on predicting the most likely value, which is called point-forecasting. Although knowing the expected value of a time series is useful for most business cases, this type of forecasting does not provide any information about the model's confidence or the prediction's uncertainty.</p> <p>Probabilistic forecasting, as opposed to point-forecasting, is a family of techniques that enable the prediction of the expected distribution of the outcome, rather than a single future value. This type of forecasting provides much richer information, as it reports the range of probable values into which the true value may fall, allowing the estimation of prediction intervals.</p> <p>A prediction interval is the interval within which the true value of the response variable is expected to be found with a given probability. There are multiple ways to estimate prediction intervals, most of which require that the residuals (errors) of the model follow a normal distribution. However, when this assumption cannot be made, two commonly used alternatives are bootstrapping and quantile regression.</p> <p>To illustrate how skforecast enables the estimation of prediction intervals for multi-step forecasting, examples attempting to predict energy demand for a 7-day horizon are provided. Two strategies are showcased:</p> <ul> <li><p>Prediction intervals based on bootstrapped residuals.</p> </li> <li><p>Prediction intervals based on quantile regression.</p> </li> </ul>"},{"location":"user_guides/probabilistic-forecasting.html#predicting-distribution-and-intervals-using-bootstrapped-residuals","title":"Predicting distribution and intervals using bootstrapped residuals\u00b6","text":"<p>The error of a one-step-ahead forecast is defined as the difference between the actual value and the predicted value ($e_t = y_t - \\hat{y}_{t|t-1}$). By assuming that future errors will be similar to past errors, it is possible to simulate different predictions by taking samples from the collection of errors previously seen in the past (i.e., the residuals) and adding them to the predictions.</p> <p></p> <p>Repeatedly performing this process creates a collection of slightly different predictions, which represent the distribution of possible outcomes due to the expected variance in the forecasting process.</p> <p></p> <p>Using the outcome of the bootstrapping process, prediction intervals can be computed by calculating the $\u03b1/2$ and $1 \u2212 \u03b1/2$ percentiles at each forecasting horizon.</p> <p></p> <p>Alternatively, it is also possible to fit a parametric distribution for each forecast horizon.</p> <p>One of the main advantages of this strategy is that it requires only a single model to estimate any interval. However, performing hundreds or thousands of bootstrapping iterations can be computationally expensive and may not always be feasible.</p> <p>In skforecast, all forecasters have three different methods that allow for probabilistic forecasting:</p> <ul> <li><p><code>predict_bootstrapping</code>: this method generates multiple forecasting predictions through a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each bootstrapping iteration generates a different set of predictions. The output is a <code>pandas DataFrame</code> with one row for each predicted step and one column for each bootstrapping iteration.</p> </li> <li><p><code>predict_intervals</code>: this method estimates quantile predictions intervals using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_dist</code>: this method fits a parametric distribution using the values generated with <code>predict_bootstrapping</code>. Any of the continuous distributions available in scipy.stats can be used.</p> </li> </ul> <p>The following example demonstrates how to use these methods with a <code>ForecasterAutoreg</code> (note that this also works for all other types of forecasters).</p>"},{"location":"user_guides/probabilistic-forecasting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>A backtesting process can be applied to evaluate the performance of the forecaster over a period of time, for example on test data, and calculate the actual coverage of the estimated range.</p>"},{"location":"user_guides/probabilistic-forecasting.html#using-out-sample-residuals","title":"Using out-sample-residuals\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting.html#prediction-intervals-using-quantile-regression-models","title":"Prediction intervals using quantile regression models\u00b6","text":"<p>As opposed to linear regression, which is intended to estimate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating the conditional quantiles of the response variable. For a continuous distribution function, the $\\alpha$-quantile $Q_{\\alpha}(x)$ is defined such that the probability of $Y$ being smaller than $Q_{\\alpha}(x)$ is, for a given $X=x$, equal to $\\alpha$. For example, 36% of the population values are lower than the quantile  $Q=0.36$. The most known quantile is the 50%-quantile, more commonly called the median.</p> <p>By combining the predictions of two quantile regressors, it is possible to build an interval. Each model estimates one of the limits of the interval. For example, the models obtained for $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval (90% - 10% = 80%).</p> <p>Several machine learning algorithms are capable of modeling quantiles. Some of them are:</p> <ul> <li><p>LightGBM</p> </li> <li><p>XGBoost</p> </li> <li><p>CatBoost</p> </li> <li><p>sklearn GradientBoostingRegressor</p> </li> <li><p>sklearn QuantileRegressor</p> </li> <li><p>skranger quantile RandomForest</p> </li> </ul> <p>Just as the squared-error loss function is used to train models that predict the mean value, a specific loss function is needed in order to train models that predict quantiles. The most common metric used for quantile regression is calles quantile loss  or pinball loss:</p> $$\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)$$<p>where $\\alpha$ is the target quantile, $y$ the real value and $\\hat{y}$ the quantile prediction.</p> <p>It can be seen that loss differs depending on the evaluated quantile. The higher the quantile, the more the loss function penalizes underestimates, and the less it penalizes overestimates. As with MSE and MAE, the goal is to minimize its values (the lower loss, the better).</p> <p>Two disadvantages of quantile regression, compared to the bootstrap approach to prediction intervals, are that each quantile needs its regressor and quantile regression is not available for all types of regression models. However, once the models are trained, the inference is much faster since no iterative process is needed.</p> <p>This type of prediction intervals can be easily estimated using ForecasterAutoregDirect and ForecasterAutoregMultiVariate models.</p>"},{"location":"user_guides/quick-start-skforecast.html","title":"Quick start","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error  from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Data partition train-test\n# ==============================================================================\nend_train = '2005-06-01 23:59:00'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Data partition train-test # ============================================================================== end_train = '2005-06-01 23:59:00' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = RandomForestRegressor(random_state=123),\n                 lags      = 15\n             )\n\nforecaster.fit(y=data.loc[:end_train])\nforecaster\n</pre> # Create and fit a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = RandomForestRegressor(random_state=123),                  lags      = 15              )  forecaster.fit(y=data.loc[:end_train]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 15 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:19:10 \nLast fit date: 2023-05-29 13:19:10 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data.loc[end_train:]))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data.loc[end_train:])) predictions.head(3) Out[4]: <pre>2005-07-01    0.921840\n2005-08-01    0.954921\n2005-09-01    1.101716\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error on test data\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data.loc[end_train:],\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error on test data # ============================================================================== error_mse = mean_squared_error(                 y_true = data.loc[end_train:],                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.00429855684785846\n</pre> In\u00a0[7]: Copied! <pre># Backtesting\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster            = forecaster,\n                                   y                     = data,\n                                   steps                 = 10,\n                                   metric                = 'mean_squared_error',\n                                   initial_train_size    = len(data.loc[:end_train]),\n                                   fixed_train_size      = False,\n                                   gap                   = 0,\n                                   allow_incomplete_fold = True,\n                                   refit                 = True,\n                                   verbose               = True,\n                                   show_progress         = True\n                               )\n\nprint(f\"Backtest error: {metric}\")\n</pre> # Backtesting # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster            = forecaster,                                    y                     = data,                                    steps                 = 10,                                    metric                = 'mean_squared_error',                                    initial_train_size    = len(data.loc[:end_train]),                                    fixed_train_size      = False,                                    gap                   = 0,                                    allow_incomplete_fold = True,                                    refit                 = True,                                    verbose               = True,                                    show_progress         = True                                )  print(f\"Backtest error: {metric}\") <pre>Information of backtesting process\n----------------------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 4\n    Number of steps per fold: 10\n    Number of steps to exclude from the end of each train set before test (gap): 0\n    Last fold only includes 6 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=178)\n    Validation: 2006-05-01 00:00:00 -- 2007-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2007-02-01 00:00:00  (n=188)\n    Validation: 2007-03-01 00:00:00 -- 2007-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2007-12-01 00:00:00  (n=198)\n    Validation: 2008-01-01 00:00:00 -- 2008-06-01 00:00:00  (n=6)\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error: 0.004810878299401304\n</pre> In\u00a0[8]: Copied! <pre># Grid search hyperparameter and lags\n# ==============================================================================\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data,\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   steps              = 10,\n                   refit              = True,\n                   metric             = 'mean_squared_error',\n                   initial_train_size = len(data.loc[:end_train]),\n                   fixed_train_size   = False,\n                   return_best        = True,\n                   verbose            = False\n               )\n</pre> # Grid search hyperparameter and lags # ============================================================================== # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data,                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    steps              = 10,                    refit              = True,                    metric             = 'mean_squared_error',                    initial_train_size = len(data.loc[:end_train]),                    fixed_train_size   = False,                    return_best        = True,                    verbose            = False                ) <pre>Number of models compared: 18.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 10, 'n_estimators': 100}\n  Backtesting metric: 0.015161925563451037\n\n</pre> In\u00a0[9]: Copied! <pre># Grid results\n# ==============================================================================\nresults_grid\n</pre> # Grid results # ============================================================================== results_grid Out[9]: lags params mean_squared_error max_depth n_estimators 9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.015162 10 100 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.015559 15 100 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.016284 15 50 8 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.018412 10 50 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.018674 5 50 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.018912 5 100 14 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.037743 10 50 17 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.043362 15 100 13 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.045687 5 100 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.047272 10 50 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.047515 10 100 5 [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.050939 15 100 16 [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.051572 15 50 4 [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.052900 15 50 12 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.055255 5 50 15 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.055293 10 100 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.056037 5 100 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.068426 5 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[10]: Copied! <pre>forecaster\n</pre> forecaster Out[10]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(max_depth=10, random_state=123) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 10, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:19:10 \nLast fit date: 2023-05-29 13:19:20 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/quick-start-skforecast.html#quick-start-skforecast","title":"Quick start skforecast\u00b6","text":"<p>Welcome to a quick start guide to using skforecast! In this guide, we will provide you with a code example that demonstrates how to create, validate, and optimize a recursive multi-step forecaster, <code>ForecasterAutoreg</code>, using skforecast.</p> <p>If you need more detailed documentation or guidance, you can visit the User Guides section.</p> <p>Without further ado, let's jump into the code example!</p>"},{"location":"user_guides/quick-start-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/quick-start-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/quick-start-skforecast.html#train-a-forecaster","title":"Train a forecaster\u00b6","text":"<p>Let's start by training a forecaster! For a more in-depth guide to using <code>ForecasterAutoreg</code>, visit the User guide ForecasterAutoreg.</p>"},{"location":"user_guides/quick-start-skforecast.html#prediction","title":"Prediction\u00b6","text":"<p>After training the forecaster, the <code>predict</code> method can be used to make predictions for the future $n$ steps.</p>"},{"location":"user_guides/quick-start-skforecast.html#backtesting-forecaster-validation","title":"Backtesting: forecaster validation\u00b6","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data. For more detailed documentation on backtesting, visit: User guide Backtesting forecaster.</p>"},{"location":"user_guides/quick-start-skforecast.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The Skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search. For more detailed documentation on Hyperparameter tuning, visit: Hyperparameter tuning and lags selection.</p>"},{"location":"user_guides/save-load-forecaster.html","title":"Save and load forecaster","text":"<p>  \u00a0 Note </p> <p>Since version 0.7.0, when initializing the forecaster, a <code>forecaster_id</code> can be included to help identify the target of the forecaster.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o.csv'\n)\ndata = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o.csv' ) data = pd.read_csv(url, sep=',', header=0, names=['y', 'date']) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') In\u00a0[3]: Copied! <pre># Create and train forecaster\nforecaster = ForecasterAutoreg(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = \"forecaster_001\"\n             )\n\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=3)\n</pre> # Create and train forecaster forecaster = ForecasterAutoreg(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = \"forecaster_001\"              )  forecaster.fit(y=data['y']) forecaster.predict(steps=3) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Save model\nsave_forecaster(forecaster, file_name='forecaster_001.py', verbose=False)\n</pre> # Save model save_forecaster(forecaster, file_name='forecaster_001.py', verbose=False) In\u00a0[5]: Copied! <pre># Load model\nforecaster_loaded = load_forecaster('forecaster_001.py', verbose=True)\n</pre> # Load model forecaster_loaded = load_forecaster('forecaster_001.py', verbose=True) <pre>================= \nForecasterAutoreg \n================= \nRegressor: RandomForestRegressor(random_state=123) \nLags: [1 2 3 4 5] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 5 \nWeight function included: False \nExogenous included: False \nType of exogenous variable: None \nExogenous variables names: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:19:47 \nLast fit date: 2023-05-29 13:19:47 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: forecaster_001 \n\n</pre> In\u00a0[6]: Copied! <pre># Predict\nforecaster_loaded.predict(steps=3)\n</pre> # Predict forecaster_loaded.predict(steps=3) Out[6]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[7]: Copied! <pre># Forecaster identifier\n# ==============================================================================\nforecaster.forecaster_id\n</pre> # Forecaster identifier # ============================================================================== forecaster.forecaster_id Out[7]: <pre>'forecaster_001'</pre> In\u00a0[8]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecasters","title":"Save and load forecasters\u00b6","text":"<p>Skforecast models can be easily saved and loaded from disk using the pickle or joblib library. To streamline this process, two convenient functions, <code>save_forecaster</code> and <code>load_forecaster</code>, are available. See below for a simple example.</p> <p>A <code>forecaster_id</code> has been included when initializing the forecaster, this may help to identify the target of the forecaster.</p>"},{"location":"user_guides/save-load-forecaster.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecaster-model","title":"Save and load forecaster model\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html","title":"Shap-values","text":"<p>  \u00a0 Note </p> <p>See also: Feature importances</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport shap\nshap.initjs()\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import pandas as pd import shap shap.initjs() from sklearn.ensemble import HistGradientBoostingRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n            url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(             url, sep=',', header=0, names=['datetime', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index() data.head(3) Out[2]: y exog_1 exog_2 datetime 1992-04-01 0.379808 0.958792 1.166029 1992-05-01 0.361801 0.951993 1.117859 1992-06-01 0.410534 0.952955 1.067942 In\u00a0[3]: Copied! <pre># Create recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = 5\n             )\n\nforecaster.fit(\n    y    = data['y'],\n    exog = data[['exog_1', 'exog_2']]\n)\n</pre> # Create recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = 5              )  forecaster.fit(     y    = data['y'],     exog = data[['exog_1', 'exog_2']] ) In\u00a0[4]: Copied! <pre># Training matrices used by the forecaster to fit the internal regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                        y    = data['y'],\n                        exog = data[['exog_1', 'exog_2']]\n                    )\ndisplay(X_train.head(3))\ndisplay(y_train.head(3))\n</pre> # Training matrices used by the forecaster to fit the internal regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                         y    = data['y'],                         exog = data[['exog_1', 'exog_2']]                     ) display(X_train.head(3)) display(y_train.head(3)) lag_1 lag_2 lag_3 lag_4 lag_5 exog_1 exog_2 datetime 1992-09-01 0.475463 0.483389 0.410534 0.361801 0.379808 0.959610 1.153190 1992-10-01 0.534761 0.475463 0.483389 0.410534 0.361801 0.956205 1.194551 1992-11-01 0.568606 0.534761 0.475463 0.483389 0.410534 0.949715 1.231489 <pre>datetime\n1992-09-01    0.534761\n1992-10-01    0.568606\n1992-11-01    0.595223\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Create SHAP explainer\n# ==============================================================================\nexplainer = shap.TreeExplainer(forecaster.regressor)\nshap_values = explainer.shap_values(X_train)\n</pre> # Create SHAP explainer # ============================================================================== explainer = shap.TreeExplainer(forecaster.regressor) shap_values = explainer.shap_values(X_train) In\u00a0[6]: Copied! <pre>shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n</pre> shap.summary_plot(shap_values, X_train, plot_type=\"bar\") In\u00a0[7]: Copied! <pre>shap.summary_plot(shap_values, X_train)\n</pre> shap.summary_plot(shap_values, X_train) <p>Visualize a single prediction</p> In\u00a0[8]: Copied! <pre># visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n</pre> # visualize the first prediction's explanation shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:]) Out[8]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>Visualize many predictions</p> In\u00a0[9]: Copied! <pre># visualize the first 200 training set predictions\nshap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:])\n</pre> # visualize the first 200 training set predictions shap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:]) Out[9]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  In\u00a0[10]: Copied! <pre>shap.dependence_plot(\"exog_2\", shap_values, X_train)\n</pre> shap.dependence_plot(\"exog_2\", shap_values, X_train) In\u00a0[11]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/shap-values-skforecast.html#shap-values-for-skforecast-models","title":"SHAP values for skforecast models\u00b6","text":"<p>SHAP (SHapley Additive exPlanations) values are a popular method for explaining machine learning models, as they help to understand how variables and values influence predictions visually and quantitatively.</p> <p>It is possible to generate SHAP-values explanations from Skforecast models with just two essential elements:</p> <ul> <li><p>The internal regressor of the forecaster.</p> </li> <li><p>The training matrices created from the time series and used to fit the forecaster.</p> </li> </ul> <p>By leveraging these two components, users can create insightful and interpretable explanations for their skforecast models. These explanations can be used to verify the reliability of the model, identify the most significant factors that contribute to model predictions, and gain a deeper understanding of the underlying relationship between the input variables and the target variable.</p>"},{"location":"user_guides/shap-values-skforecast.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#training-matrices","title":"Training matrices\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-values","title":"Shap Values\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-explainer","title":"Shap explainer\u00b6","text":"<p>The python implementation of SHAP is built along the explainers. These explainers are appropriate only for certain types or classes of algorithms. For example, the TreeExplainer is used for tree-based models.</p>"},{"location":"user_guides/shap-values-skforecast.html#shap-summary-plot","title":"SHAP Summary Plot\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#explain-predictions","title":"Explain predictions\u00b6","text":""},{"location":"user_guides/shap-values-skforecast.html#shap-dependence-plots","title":"SHAP Dependence Plots\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html","title":"Skforecast in GPU","text":"<p>Traditionally, machine learning algorithms are executed on CPUs (Central Processing Units), which are general-purpose processors that are designed to handle a wide range of tasks. However, CPUs are not optimized for the highly parallelized matrix operations that are required by many machine learning algorithms, which can result in slow training times and limited scalability. GPUs, on the other hand, are designed specifically for parallel processing and can perform thousands of mathematical operations simultaneously, making them ideal for training and deploying large-scale machine learning models.</p> <p>Two popular machine learning libraries that have implemented GPU acceleration are XGBoost and LightGBM. These libraries are used for building gradient boosting models, which are a type of machine learning algorithm that is highly effective for a wide range of tasks, including forecasting. With GPU acceleration, these libraries can significantly reduce the training time required to build these models and improve their scalability.</p> <p>Despite the significant advantages offered by GPUs (specifically Nvidia GPUs) in accelerating machine learning computations, access to them is often limited due to high costs or other practical constraints. Fortunatelly, Google Colaboratory (Colab), a free Jupyter notebook environment, allows users to run Python code in the cloud, with access to powerful hardware resources such as GPUs. This makes it an excellent platform for experimenting with machine learning models, especially those that require intensive computations.</p> <p>The following sections demonstrate how to install and use XGBoost and LightGBM with GPU acceleration to create powerful forecasting models.</p> <p>  \u00a0 Warning </p> <p>The following code assumes that the user is executing it in Google Colab with an activated GPU runtime.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom xgboost import XGBRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nimport torch\nimport os\nimport sys\nimport psutil\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from xgboost import XGBRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg import torch import os import sys import psutil In\u00a0[14]: Copied! <pre># Setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# GPU info\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n\n# CPU info\nprint(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n</pre> # Setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device)  # GPU info if device.type == 'cuda':     print(torch.cuda.get_device_name(0))     print('Memory Usage:')     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')     print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')  # CPU info print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\") <pre>Using device: cpu\nCPU RAM Free: 6.81 GB\n</pre> In\u00a0[16]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a XGBRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                regressor = XGBRegressor(\n                              n_estimators=5000,\n                              tree_method='gpu_hist',\n                              gpu_id=0\n                            ),\n                lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a XGBRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                 regressor = XGBRegressor(                               n_estimators=5000,                               tree_method='gpu_hist',                               gpu_id=0                             ),                 lags = 20              )  forecaster.fit(y=data) In\u00a0[\u00a0]: Copied! <pre>!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n</pre> !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm In\u00a0[\u00a0]: Copied! <pre>!git clone --recursive https://github.com/Microsoft/LightGBM\n</pre> !git clone --recursive https://github.com/Microsoft/LightGBM In\u00a0[\u00a0]: Copied! <pre>!apt-get install -y -qq libboost-all-dev\n</pre> !apt-get install -y -qq libboost-all-dev In\u00a0[\u00a0]: Copied! <pre>%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)\n</pre> %%bash cd LightGBM rm -r build mkdir build cd build cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc) In\u00a0[\u00a0]: Copied! <pre>!cd LightGBM/python-package/;python3 setup.py install --precompile\n</pre> !cd LightGBM/python-package/;python3 setup.py install --precompile In\u00a0[\u00a0]: Copied! <pre>!mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM\n</pre> !mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd !rm -r LightGBM <p>Once all the above installation has been executed, it is necessary to restart the runtime (kernel).</p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nfrom lightgbm  import LGBMRegressor\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import time from lightgbm  import LGBMRegressor from skforecast.ForecasterAutoreg import ForecasterAutoreg In\u00a0[\u00a0]: Copied! <pre># Data\n# ==============================================================================\ndata = pd.Series(np.random.normal(size=1000000))\n</pre> # Data # ============================================================================== data = pd.Series(np.random.normal(size=1000000)) In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a LGBMRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),\n                 lags = 20\n             )\n\nforecaster.fit(y=data)\n</pre> # Create and train forecaster with a LGBMRegressor using GPU # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = LGBMRegressor(n_estimators=5000, device_type='gpu'),                  lags = 20              )  forecaster.fit(y=data) In\u00a0[\u00a0]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/skforecast-in-GPU.html#skforecast-in-gpu","title":"Skforecast in GPU\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#xgboost","title":"XGBoost\u00b6","text":"<p>When creating the model, only two arguments are need to indicate XGBoost to GPU, if it available: <code>tree_method='gpu_hist'</code> and <code>gpu_id=0</code>.</p>"},{"location":"user_guides/skforecast-in-GPU.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html","title":"Scikit-learn Transformers and Pipelines","text":"<p>  \u00a0 Note </p> <p>When using ForecasterAutoregMultiSeries or ForecasterAutoregMultiVariate the <code>transformer_series</code> argument replaces <code>transformer_y</code>. If it is a <code>transformer</code>, the same transformation will be applied to all series. If it is a <code>dict</code> a different transformation can be set for each series.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n    'data/h2o_exog.csv'\n)\ndata = pd.read_csv(\n           url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']\n       )\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n# Add an extra categorical variable\ndata['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1))\ndata.head()\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'     'data/h2o_exog.csv' ) data = pd.read_csv(            url, sep=',', header=0, names=['date', 'y', 'exog_1', 'exog_2']        )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') # Add an extra categorical variable data['exog_3'] = ([\"A\"] * int(len(data)/2)) + ([\"B\"] * (int(len(data)/2) +1)) data.head() Out[2]: y exog_1 exog_2 exog_3 date 1992-04-01 0.379808 0.958792 1.166029 A 1992-05-01 0.361801 0.951993 1.117859 A 1992-06-01 0.410534 0.952955 1.067942 A 1992-07-01 0.483389 0.958078 1.097376 A 1992-08-01 0.475463 0.956370 1.122199 A In\u00a0[3]: Copied! <pre># Create and fit forecaster that scales the input series\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster that scales the input series # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[3]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: StandardScaler() \nTransformer for exog: None \nWindow size: 3 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:21:40 \nLast fit date: 2023-05-29 13:21:40 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> In\u00a0[4]: Copied! <pre># Create and fit forecaster with same transformer for all exogenous variables\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = StandardScaler()\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster with same transformer for all exogenous variables # ============================================================================== forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = StandardScaler()              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[4]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: StandardScaler() \nWindow size: 3 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:21:40 \nLast fit date: 2023-05-29 13:21:40 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>It is also possible to apply a different transformation to each exogenous variable making use of <code>ColumnTransformer</code>.</p> In\u00a0[5]: Copied! <pre># Create and fit forecaster with different transformer for each exog variable\n# ==============================================================================\ntransformer_exog = ColumnTransformer(\n                       [('scale_1', StandardScaler(), ['exog_1']),\n                        ('scale_2', StandardScaler(), ['exog_2']),\n                        ('onehot', OneHotEncoder(), ['exog_3']),\n                       ],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\nforecaster = ForecasterAutoreg(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = transformer_exog\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']])\nforecaster\n</pre> # Create and fit forecaster with different transformer for each exog variable # ============================================================================== transformer_exog = ColumnTransformer(                        [('scale_1', StandardScaler(), ['exog_1']),                         ('scale_2', StandardScaler(), ['exog_2']),                         ('onehot', OneHotEncoder(), ['exog_3']),                        ],                        remainder = 'passthrough',                        verbose_feature_names_out = False                    )  forecaster = ForecasterAutoreg(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = transformer_exog              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']]) forecaster Out[5]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Ridge(random_state=123) \nLags: [1 2 3] \nTransformer for y: None \nTransformer for exog: ColumnTransformer(remainder='passthrough',\n                  transformers=[('scale_1', StandardScaler(), ['exog_1']),\n                                ('scale_2', StandardScaler(), ['exog_2']),\n                                ('onehot', OneHotEncoder(), ['exog_3'])],\n                  verbose_feature_names_out=False) \nWindow size: 3 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2', 'exog_3'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:21:40 \nLast fit date: 2023-05-29 13:21:40 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>It is possible to verify if the data transformation has been applied correctly by examining the training matrices. The training matrices should reflect the data transformation that was specified using the <code>transformer_y</code> or <code>transformer_exog</code> arguments.</p> In\u00a0[6]: Copied! <pre>X_train, y_train = forecaster.create_train_X_y(\n                       y    = data['y'],\n                       exog = data[['exog_1', 'exog_2', 'exog_3']]\n                   )\n</pre> X_train, y_train = forecaster.create_train_X_y(                        y    = data['y'],                        exog = data[['exog_1', 'exog_2', 'exog_3']]                    ) In\u00a0[7]: Copied! <pre>X_train.head(4)\n</pre> X_train.head(4) Out[7]: lag_1 lag_2 lag_3 exog_1 exog_2 exog_3_A exog_3_B date 1992-07-01 0.410534 0.361801 0.379808 -2.119529 -2.135088 1.0 0.0 1992-08-01 0.483389 0.410534 0.361801 -2.131024 -1.996017 1.0 0.0 1992-09-01 0.475463 0.483389 0.410534 -2.109222 -1.822392 1.0 0.0 1992-10-01 0.534761 0.475463 0.483389 -2.132137 -1.590667 1.0 0.0 In\u00a0[8]: Copied! <pre>y_train.head(4)\n</pre> y_train.head(4) Out[8]: <pre>date\n1992-07-01    0.483389\n1992-08-01    0.475463\n1992-09-01    0.534761\n1992-10-01    0.568606\nFreq: MS, Name: y, dtype: float64</pre> <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation.</p> <p>Scikit-learn's FunctionTransformer can be used to incorporate custom transformers, such as a logarithmic transformation, in the forecaster object. To implement this, a user-defined transformation function can be created and then passed to the <code>FunctionTransformer</code>. Detailed information on how to use FunctionTransformer can be found in the scikit-learn documentation.</p> <p>  \u00a0 Warning </p> <p>For versions 1.1.0 &gt;= scikit-learn &lt;= 1.2.0 <code>sklearn.preprocessing.FunctionTransformer.inverse_transform</code> does not support DataFrames that are all numerical when <code>check_inverse=True</code>. It will raise an Exception which is fixed in scikit-learn 1.2.1.</p> <p>More info: https://scikit-learn.org/stable/whats_new/v1.2.html#version-1-2-1</p> In\u00a0[9]: Copied! <pre># Create custom transformer\n# =============================================================================\ndef log_transform(x):\n\"\"\" \n    Calculate log adding 1 to avoid calculation errors if x is very close to 0.\n    \"\"\"\n    return np.log(x+1)\n\ndef exp_transform(x):\n\"\"\"\n    Inverse of log_transform.\n    \"\"\"\n    return np.exp(x) - 1\n\ntransformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)\n\n# Create forecaster and train\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor     = Ridge(random_state=123),\n                 lags          = 3,\n                 transformer_y = transformer_y\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create custom transformer # ============================================================================= def log_transform(x):     \"\"\"      Calculate log adding 1 to avoid calculation errors if x is very close to 0.     \"\"\"     return np.log(x+1)  def exp_transform(x):     \"\"\"     Inverse of log_transform.     \"\"\"     return np.exp(x) - 1  transformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)  # Create forecaster and train # ============================================================================== forecaster = ForecasterAutoreg(                  regressor     = Ridge(random_state=123),                  lags          = 3,                  transformer_y = transformer_y              )  forecaster.fit(y=data['y']) <p>If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[10]: Copied! <pre>forecaster.predict(steps=4)\n</pre> forecaster.predict(steps=4) Out[10]: <pre>2008-07-01    0.776206\n2008-08-01    0.775471\n2008-09-01    0.777200\n2008-10-01    0.777853\nFreq: MS, Name: pred, dtype: float64</pre> <p>  \u00a0 Warning </p> <p>Starting from version 0.4.0, skforecast permits the usage of scikit-learn pipelines as regressors. It is important to note that ColumnTransformer cannot be included in the pipeline; thus, the same transformation will be applied to both the modeled series and all exogenous variables. However, if the preprocessing transformations only apply to specific columns, then they need to be applied separately using <code>transformer_y</code> and <code>transformer_exog</code>.</p> In\u00a0[11]: Copied! <pre>pipe = make_pipeline(StandardScaler(), Ridge())\npipe\n</pre> pipe = make_pipeline(StandardScaler(), Ridge()) pipe Out[11]: <pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])</pre>StandardScaler<pre>StandardScaler()</pre>Ridge<pre>Ridge()</pre> In\u00a0[12]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags      = 10\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags      = 10              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[12]: <pre>================= \nForecasterAutoreg \n================= \nRegressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) \nLags: [ 1  2  3  4  5  6  7  8  9 10] \nTransformer for y: None \nTransformer for exog: None \nWindow size: 10 \nWeight function included: False \nExogenous included: True \nType of exogenous variable: &lt;class 'pandas.core.frame.DataFrame'&gt; \nExogenous variables names: ['exog_1', 'exog_2'] \nTraining range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.0001} \nfit_kwargs: {} \nCreation date: 2023-05-29 13:21:41 \nLast fit date: 2023-05-29 13:21:41 \nSkforecast version: 0.8.1 \nPython version: 3.10.11 \nForecaster id: None </pre> <p>When performing a grid search over a scikit-learn pipeline, the model's name precedes the parameters' name.</p> In\u00a0[13]: Copied! <pre># Hyperparameter grid search using a scikit-learn pipeline\n# ==============================================================================\npipe = make_pipeline(StandardScaler(), Ridge())\nforecaster = ForecasterAutoreg(\n                 regressor = pipe,\n                 lags = 10  # This value will be replaced in the grid search\n             )\n\n# Regressor's hyperparameters\nparam_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}\n\n# Lags used as predictors\nlags_grid = [5, 24, [1, 2, 3, 23, 24]]\n\nresults_grid = grid_search_forecaster(\n                   forecaster  = forecaster,\n                   y           = data['y'],\n                   exog        = data[['exog_1', 'exog_2']],\n                   param_grid  = param_grid,\n                   lags_grid   = lags_grid,\n                   steps       = 5,\n                   metric      = 'mean_absolute_error',\n                   refit       = False,\n                   initial_train_size = len(data.loc[:'2000-04-01']),\n                   return_best = True,\n                   verbose     = False\n               )\n\nresults_grid.head(4)\n</pre> # Hyperparameter grid search using a scikit-learn pipeline # ============================================================================== pipe = make_pipeline(StandardScaler(), Ridge()) forecaster = ForecasterAutoreg(                  regressor = pipe,                  lags = 10  # This value will be replaced in the grid search              )  # Regressor's hyperparameters param_grid = {'ridge__alpha': np.logspace(-3, 5, 10)}  # Lags used as predictors lags_grid = [5, 24, [1, 2, 3, 23, 24]]  results_grid = grid_search_forecaster(                    forecaster  = forecaster,                    y           = data['y'],                    exog        = data[['exog_1', 'exog_2']],                    param_grid  = param_grid,                    lags_grid   = lags_grid,                    steps       = 5,                    metric      = 'mean_absolute_error',                    refit       = False,                    initial_train_size = len(data.loc[:'2000-04-01']),                    return_best = True,                    verbose     = False                )  results_grid.head(4) <pre>Number of models compared: 30.\n</pre> <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'ridge__alpha': 0.001}\n  Backtesting metric: 6.845311709632476e-05\n\n</pre> Out[13]: lags params mean_absolute_error ridge__alpha 0 [1, 2, 3, 4, 5] {'ridge__alpha': 0.001} 0.000068 0.001000 10 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.001} 0.000188 0.001000 1 [1, 2, 3, 4, 5] {'ridge__alpha': 0.007742636826811269} 0.000526 0.007743 11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'ridge__alpha': 0.007742636826811269} 0.001413 0.007743 In\u00a0[14]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#scikit-learn-transformers-and-pipelines","title":"Scikit-learn transformers and pipelines\u00b6","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <ul> <li><p><code>transformer_y</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform. Scikit-learn ColumnTransformer is not allowed since they do not have the inverse_transform method.</p> </li> <li><p><code>transformer_exog</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. Scikit-learn ColumnTransformer can be used if the preprocessing transformations only apply to some specific columns or if different transformations are needed for different columns. For example, scale numeric features and one hot encode categorical ones.</p> </li> </ul> <p>Transformations are learned and applied before training the forecaster and are automatically used when calling <code>predict</code>. The output of <code>predict</code> is always on the same scale as the original series y.</p> <p>Although skforecast has allowed using scikit-learn pipelines as regressors since version 0.4.0, it is recommended to use <code>transformer_y</code> and <code>transformer_exog</code> instead.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-input-series","title":"Transforming input series\u00b6","text":"<p>The following example shows how include a transformer that scales the input series y.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-exogenous-variables","title":"Transforming exogenous variables\u00b6","text":"<p>The following example shows how to apply the same transformation (scaling) to all exogenous variables.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#custom-transformers","title":"Custom transformers\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#scikit-learn-pipelines","title":"Scikit-learn pipelines\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html","title":"Weighted time series forecasting","text":"<p>  \u00a0 Note </p> <p>The examples that follow demonstrate how a portion of the time series can be excluded from model training by assigning it a weight of zero. However, the use of weights extends beyond the inclusion or exclusion of observations and can also balance the degree of influence that each observation has on the forecasting model. For instance, an observation assigned a weight of 10 will have ten times more impact on the model training than an observation assigned a weight of 1.</p> <p>  \u00a0 Warning </p> <p>In most gradient boosting implementations, such as LightGBM, XGBoost, and CatBoost, samples with zero weight are typically excluded when calculating gradients and Hessians. However, these samples are still taken into account when constructing the feature histograms, which can result in a model that differs from one trained without zero-weighted samples. For more information on this issue, please refer to this GitHub issue.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\nfrom sklearn.linear_model import Ridge\nfrom skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 from sklearn.linear_model import Ridge from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n       'data/energy_production_shutdown.csv')\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'        'data/energy_production_shutdown.csv') data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: production date 2012-01-01 375.1 2012-01-02 474.5 2012-01-03 573.9 2012-01-04 539.5 2012-01-05 445.4 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Dates train : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00   (n=731)\nDates test  : 2014-01-01 00:00:00 --- 2014-12-30 00:00:00   (n=364)\n</pre> In\u00a0[4]: Copied! <pre># Time series plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train.production.plot(ax=ax, label='train', linewidth=1)\ndata_test.production.plot(ax=ax, label='test', linewidth=1)\nax.axvspan(\n    pd.to_datetime('2012-06'),\n    pd.to_datetime('2012-10'), \n    label=\"Shutdown\",\n    color=\"red\",\n    alpha=0.1\n)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Time series plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train.production.plot(ax=ax, label='train', linewidth=1) data_test.production.plot(ax=ax, label='test', linewidth=1) ax.axvspan(     pd.to_datetime('2012-06'),     pd.to_datetime('2012-10'),      label=\"Shutdown\",     color=\"red\",     alpha=0.1 ) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[5]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n\"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between 2012-06-01 and 2012-10-21.     \"\"\"     weights = np.where(                   (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),                    0,                    1               )      return weights <p>A <code>ForecasterAutoreg</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[6]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterAutoreg)\n# ==============================================================================\nforecaster = ForecasterAutoreg(\n                 regressor   = Ridge(random_state=123),\n                 lags        = 21,\n                 weight_func = custom_weights\n             )\n</pre> # Create a recursive multi-step forecaster (ForecasterAutoreg) # ============================================================================== forecaster = ForecasterAutoreg(                  regressor   = Ridge(random_state=123),                  lags        = 21,                  weight_func = custom_weights              ) <p>  \u00a0 Warning </p> <p>If the regressor used in the model's fitting method does not support <code>sample_weight</code> within its <code>fit</code> method, the <code>weight_func</code> argument will be ignored.</p> <p>The <code>source_code_weight_func</code> argument stores the source code of the <code>weight_func</code> added to the forecaster.</p> In\u00a0[7]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p>After creating the forecaster, a backtesting process is performed to simulate its behavior if the test set had been predicted in batches of 12 days.</p> In\u00a0[8]: Copied! <pre># Backtesting: predict batches of 12 days\n# ==============================================================================\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster         = forecaster,\n                                   y                  = data.production,\n                                   initial_train_size = len(data.loc[:end_train]),\n                                   fixed_train_size   = False,\n                                   steps              = 12,\n                                   metric             = 'mean_absolute_error',\n                                   refit              = False,\n                                   verbose            = False\n                               )\n\nprint(f\"Backtest error: {metric}\")\npredictions_backtest.head()\n</pre> # Backtesting: predict batches of 12 days # ============================================================================== metric, predictions_backtest = backtesting_forecaster(                                    forecaster         = forecaster,                                    y                  = data.production,                                    initial_train_size = len(data.loc[:end_train]),                                    fixed_train_size   = False,                                    steps              = 12,                                    metric             = 'mean_absolute_error',                                    refit              = False,                                    verbose            = False                                )  print(f\"Backtest error: {metric}\") predictions_backtest.head() <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error: 26.82118940346796\n</pre> Out[8]: pred 2014-01-01 406.122211 2014-01-02 444.103631 2014-01-03 469.424876 2014-01-04 449.407001 2014-01-05 414.945674 In\u00a0[9]: Copied! <pre># Predictions plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test.production.plot(ax=ax, label='test', linewidth=1)\npredictions_backtest.plot(ax=ax, label='predictions', linewidth=1)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Predictions plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test.production.plot(ax=ax, label='test', linewidth=1) predictions_backtest.plot(ax=ax, label='predictions', linewidth=1) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[10]: Copied! <pre>%%html\n&lt;style&gt;\n.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt {display: none;}\n&lt;/style&gt;\n</pre> %%html"},{"location":"user_guides/weighted-time-series-forecasting.html#weighted-time-series-forecasting","title":"Weighted time series forecasting\u00b6","text":"<p>In many real-world scenarios, historical data is available for forecasting, but not all of it is reliable. For example, IoT sensors capture raw data from the physical world, but they are often prone to failure, malfunction, and attrition due to harsh deployment environments, leading to unusual or erroneous readings. Similarly, factories may shut down for maintenance, repair, or overhaul, resulting in gaps in the data. The Covid-19 pandemic has also affected population behavior, impacting many time series such as production, sales, and transportation.</p> <p>The presence of unreliable or unrepresentative values in the data history poses a significant challenge, as it hinders model learning. However, most forecasting algorithms require complete time series data, making it impossible to remove these observations. An alternative solution is to reduce the weight of the affected observations during model training. This document demonstrates how skforecast makes it easy to implement this strategy with two examples.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html#data","title":"Data\u00b6","text":"<p>Power plants used for energy generation are complex installations that require a high level of maintenance. It is typical for power plants to require periodic shutdowns for maintenance, repair, or overhaul activities. The following data set simulates these events, where energy production experiences a decrease.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#exclude-part-of-the-time-series","title":"Exclude part of the time series\u00b6","text":"<p>Between 2012-06-01 and 2012-09-30, the factory underwent a shutdown. To reduce the impact of these dates on the model, a custom function is created. This function assigns a value of 0 to any index date that falls within the shutdown period or up to 21 days later (lags used by the model), and a value of 1 to all other dates. Observations assigned a weight of 0 have no influence on the model training.</p>"}]}