{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"skforecast \u00b6 Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...). Installation \u00b6 1 pip install skforecast Specific version: 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast@v0.1.9 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24 Dependencies \u00b6 1 2 3 4 5 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24 Features \u00b6 Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance TODO \u00b6 [ ] Pandas dataframe as input of multiple exogenous variables [ ] Parallel grid search [ ] Speed lag creation with numba [ ] Increase unit test coverage Tutorials (spanish) \u00b6 Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python References \u00b6 Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance Licence \u00b6 joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Readme"},{"location":"index.html#skforecast","text":"Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...).","title":"skforecast"},{"location":"index.html#installation","text":"1 pip install skforecast Specific version: 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast@v0.1.9 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24","title":"Installation"},{"location":"index.html#dependencies","text":"1 2 3 4 5 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24","title":"Dependencies"},{"location":"index.html#features","text":"Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance","title":"Features"},{"location":"index.html#todo","text":"[ ] Pandas dataframe as input of multiple exogenous variables [ ] Parallel grid search [ ] Speed lag creation with numba [ ] Increase unit test coverage","title":"TODO"},{"location":"index.html#tutorials-spanish","text":"Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python","title":"Tutorials (spanish)"},{"location":"index.html#references","text":"Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance","title":"References"},{"location":"index.html#licence","text":"joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Licence"},{"location":"api/ForecasterAutoreg.html","text":"ForecasterAutoreg \u00b6 class skforecast.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) This class turns a scikit-learn regressor into a recursive autoregressive (multi-step) forecaster. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. lags (int, list, 1D np.array, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. in_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. lags (1D np.array) \u2014 Lags used as predictors. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. max_lag (int) \u2014 Maximum value of lag included in lags. out_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_lags ( y ) (X_data : 2D np.ndarray, shape (samples, len(self.lags))) \u2014 Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices X, y fit ( y , exog ) (self : ForecasterAutoreg) \u2014 Training ForecasterAutoreg get_coef ( ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importances ( ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . predict ( steps , last_window , exog ) (predictions : 1D np.array, shape (steps,)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , in_sample_residuals ) (predictions : np.array, shape (steps, 3)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attribute max_lag is also updated. set_out_sample_residuals ( residuals ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_lags ( y ) Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. Returns (X_data : 2D np.ndarray, shape (samples, len(self.lags))) 2D array with the lag values (predictors). ta : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of X_data . method create_train_X_y ( y , exog=None ) Create training matrices X, y Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoreg Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoreg) Trained ForecasterAutoreg method predict ( steps , last_window=None , exog=None ) Iterative process in which, each prediction, is used as a predictor for the next step. Returns (predictions : 1D np.array, shape (steps,)) Values predicted. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Returns (predictions : np.array, shape (steps, 3)) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. Parameters params (dict) \u2014 Parameters values. method set_lags ( lags ) Set new value to the attribute lags . Attribute max_lag is also updated. Parameters Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \u2014 int : include lags from 1 to lags . list or np.array : include only lags present in lags . method set_out_sample_residuals ( residuals ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . method get_feature_importances ( ) Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] .","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#forecasterautoreg","text":"class skforecast.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) This class turns a scikit-learn regressor into a recursive autoregressive (multi-step) forecaster. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. lags (int, list, 1D np.array, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. in_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. lags (1D np.array) \u2014 Lags used as predictors. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. max_lag (int) \u2014 Maximum value of lag included in lags. out_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_lags ( y ) (X_data : 2D np.ndarray, shape (samples, len(self.lags))) \u2014 Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices X, y fit ( y , exog ) (self : ForecasterAutoreg) \u2014 Training ForecasterAutoreg get_coef ( ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importances ( ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . predict ( steps , last_window , exog ) (predictions : 1D np.array, shape (steps,)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , in_sample_residuals ) (predictions : np.array, shape (steps, 3)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attribute max_lag is also updated. set_out_sample_residuals ( residuals ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_lags ( y ) Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. Returns (X_data : 2D np.ndarray, shape (samples, len(self.lags))) 2D array with the lag values (predictors). ta : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of X_data . method create_train_X_y ( y , exog=None ) Create training matrices X, y Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoreg Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoreg) Trained ForecasterAutoreg method predict ( steps , last_window=None , exog=None ) Iterative process in which, each prediction, is used as a predictor for the next step. Returns (predictions : 1D np.array, shape (steps,)) Values predicted. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Returns (predictions : np.array, shape (steps, 3)) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. Parameters params (dict) \u2014 Parameters values. method set_lags ( lags ) Set new value to the attribute lags . Attribute max_lag is also updated. Parameters Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \u2014 int : include lags from 1 to lags . list or np.array : include only lags present in lags . method set_out_sample_residuals ( residuals ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . method get_feature_importances ( ) Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] .","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoregCustom.html","text":"ForecasterAutoregCustom \u00b6 class skforecast.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) This class turns a scikit-learn regressor into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. fun_predictors (Callable) \u2014 Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. fun_predictors (Callable) \u2014 Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. in_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the predictors for the next step after the training data. out_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices X, y fit ( y , exog ) (self : ForecasterAutoregCustom) \u2014 Training ForecasterAutoregCustom get_coef ( ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importances ( ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . predict ( steps , last_window , exog ) (predicciones : 1D np.array, shape (steps,)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , in_sample_residuals ) (predictions : np.array, shape (steps, 3)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_out_sample_residuals ( residuals ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices X, y Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoregCustom Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoregCustom) Trained ForecasterAutoregCustom method predict ( steps , last_window=None , exog=None ) Iterative process in which, each prediction, is used as a predictor for the next step. Returns (predicciones : 1D np.array, shape (steps,)) Values predicted. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Returns (predictions : np.array, shape (steps, 3)) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. Parameters params (dict) \u2014 Parameters values. method set_out_sample_residuals ( residuals ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor. Coefficients are aligned so that coef[i] is the value associated with predictor i returned by self.create_predictors . method get_feature_importances ( ) Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor. Values are aligned so that feature_importances[i] is the value associated with predictor i returned by self.create_predictors .","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#forecasterautoregcustom","text":"class skforecast.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) This class turns a scikit-learn regressor into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. fun_predictors (Callable) \u2014 Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. fun_predictors (Callable) \u2014 Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. in_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the predictors for the next step after the training data. out_sample_residuals (np.ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices X, y fit ( y , exog ) (self : ForecasterAutoregCustom) \u2014 Training ForecasterAutoregCustom get_coef ( ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importances ( ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . predict ( steps , last_window , exog ) (predicciones : 1D np.array, shape (steps,)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , in_sample_residuals ) (predictions : np.array, shape (steps, 3)) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_out_sample_residuals ( residuals ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices X, y Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags))) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoregCustom Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoregCustom) Trained ForecasterAutoregCustom method predict ( steps , last_window=None , exog=None ) Iterative process in which, each prediction, is used as a predictor for the next step. Returns (predicciones : 1D np.array, shape (steps,)) Values predicted. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Returns (predictions : np.array, shape (steps, 3)) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. Parameters params (dict) \u2014 Parameters values. method set_out_sample_residuals ( residuals ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor. Coefficients are aligned so that coef[i] is the value associated with predictor i returned by self.create_predictors . method get_feature_importances ( ) Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor. Values are aligned so that feature_importances[i] is the value associated with predictor i returned by self.create_predictors .","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregMultiOutput.html","text":"ForecasterAutoregMultiOutput \u00b6 class skforecast.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) This class turns a scikit-learn regressor into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. lags (int, list, 1D np.array, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. lags (1D np.array) \u2014 Lags used as predictors. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. max_lag (int) \u2014 Maximum value of lag included in lags. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. One instance of this regressor is trainned for each step. All them are stored in slef.regressors_ . regressors_ (dict) \u2014 Dictionary with scikit-learn regressors trained for each step. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hiperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_lags ( y ) (X_data : 2D np.ndarray) \u2014 Transforms a time series into two 2D arrays of pairs predictor-response. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : 2D np.ndarray) \u2014 Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() . fit ( y , exog ) (self : ForecasterAutoregMultiOutput) \u2014 Training ForecasterAutoregMultiOutput get_coef ( step ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. get_feature_importances ( step ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. predict ( last_window , exog , steps ) (predictions : 1D np.array, shape (steps,)) \u2014 Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attribute max_lag is also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_lags ( y ) Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. Returns (X_data : 2D np.ndarray) 2D array with the lag values (predictors). ta : 2D np.ndarray Values of the time series related to each row of X_data . method create_train_X_y ( y , exog=None ) Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (2D np.ndarray) \u2014 2D array with the training values (predictors). y_train (1D np.ndarray) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : 2D np.ndarray) 2D array with the training values (predictors) for step. ain_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoregMultiOutput Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoregMultiOutput) Trained ForecasterAutoregMultiOutput method predict ( last_window=None , exog=None , steps=None ) Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. Returns (predictions : 1D np.array, shape (steps,)) Values predicted. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. Parameters params (dict) \u2014 Parameters values. method set_lags ( lags ) Set new value to the attribute lags . Attribute max_lag is also updated. Parameters Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \u2014 int : include lags from 1 to lags . list or np.array : include only lags present in lags . method get_coef ( step ) Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters step (int) \u2014 Model from which retireve information (a separate model is created for each forecast time step). Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . method get_feature_importances ( step ) Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Parameters step (int) \u2014 Model from which retireve information (a separate model is created for each forecast time step). Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] .","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#forecasterautoregmultioutput","text":"class skforecast.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) This class turns a scikit-learn regressor into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. lags (int, list, 1D np.array, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . Attributes exog_shape (tuple) \u2014 Shape of exog used in training. exog_type (type) \u2014 Type used for the exogenous variable/s. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. lags (1D np.array) \u2014 Lags used as predictors. last_window (1D np.ndarray) \u2014 Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. max_lag (int) \u2014 Maximum value of lag included in lags. regressor (scikit-learn regressor) \u2014 An instance of a scikit-learn regressor. One instance of this regressor is trainned for each step. All them are stored in slef.regressors_ . regressors_ (dict) \u2014 Dictionary with scikit-learn regressors trained for each step. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hiperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_lags ( y ) (X_data : 2D np.ndarray) \u2014 Transforms a time series into two 2D arrays of pairs predictor-response. create_train_X_y ( y , exog ) (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : 2D np.ndarray) \u2014 Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() . fit ( y , exog ) (self : ForecasterAutoregMultiOutput) \u2014 Training ForecasterAutoregMultiOutput get_coef ( step ) (coef : 1D np.ndarray) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. get_feature_importances ( step ) (feature_importances : 1D np.ndarray) \u2014 Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. predict ( last_window , exog , steps ) (predictions : 1D np.array, shape (steps,)) \u2014 Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attribute max_lag is also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_lags ( y ) Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. Returns (X_data : 2D np.ndarray) 2D array with the lag values (predictors). ta : 2D np.ndarray Values of the time series related to each row of X_data . method create_train_X_y ( y , exog=None ) Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) 2D array with the training values (predictors). ain : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (2D np.ndarray) \u2014 2D array with the training values (predictors). y_train (1D np.ndarray) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : 2D np.ndarray) 2D array with the training values (predictors) for step. ain_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training ForecasterAutoregMultiOutput Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns (self : ForecasterAutoregMultiOutput) Trained ForecasterAutoregMultiOutput method predict ( last_window=None , exog=None , steps=None ) Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. Returns (predictions : 1D np.array, shape (steps,)) Values predicted. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. Parameters params (dict) \u2014 Parameters values. method set_lags ( lags ) Set new value to the attribute lags . Attribute max_lag is also updated. Parameters Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \u2014 int : include lags from 1 to lags . list or np.array : include only lags present in lags . method get_coef ( step ) Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters step (int) \u2014 Model from which retireve information (a separate model is created for each forecast time step). Returns (coef : 1D np.ndarray) Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . method get_feature_importances ( step ) Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Parameters step (int) \u2014 Model from which retireve information (a separate model is created for each forecast time step). Returns (feature_importances : 1D np.ndarray) Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] .","title":"ForecasterAutoregMultiOutput"},{"location":"api/model_selection.html","text":"model_selection \u00b6 function skforecast.model_selection. backtesting_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , verbose=False ) Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (backtest_predictions: 1D np.ndarray) Value of predictions. ic_value: np.ndarray shape (1,) Value of the metric. function skforecast.model_selection. backtesting_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , verbose=False ) Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (backtest_predictions: 1D np.ndarray) Value of predictions. ic_value: np.ndarray shape (1,) Value of the metric. function skforecast.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , verbose=True ) Cross-validation of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The order of is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_results: 1D np.ndarray) Value of the metric for each partition. function skforecast.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , method='cv' , allow_incomplete_fold=True , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting. Returns (results: pandas.DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1D np.ndarray) Training indices. : 1D np.ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection.html#model_selection","text":"function skforecast.model_selection. backtesting_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , verbose=False ) Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (backtest_predictions: 1D np.ndarray) Value of predictions. ic_value: np.ndarray shape (1,) Value of the metric. function skforecast.model_selection. backtesting_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , verbose=False ) Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (backtest_predictions: 1D np.ndarray) Value of predictions. ic_value: np.ndarray shape (1,) Value of the metric. function skforecast.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , verbose=True ) Cross-validation of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The order of is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom,) \u2014 ForecasterAutoregMultiOutput erAutoreg , ForecasterCustom ForecasterAutoregCustom or erAutoregMultiOutput object. y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_results: 1D np.ndarray) Value of the metric for each partition. function skforecast.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , method='cv' , allow_incomplete_fold=True , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting. Returns (results: pandas.DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1D np.ndarray, pd.Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1D np.ndarray) Training indices. : 1D np.ndarray Test indices.","title":"model_selection"},{"location":"examples/examples.html","text":"Examples \u00b6 Use cases where skforecast library is used for forecasting. Forecasting series temporales con Python y Scikit-learn \u00b6 Forecasting de la demanda el\u00e9ctrica \u00b6","title":"Forecasting energy demand"},{"location":"examples/examples.html#examples","text":"Use cases where skforecast library is used for forecasting.","title":"Examples"},{"location":"examples/examples.html#forecasting-series-temporales-con-python-y-scikit-learn","text":"","title":"Forecasting series temporales con Python y Scikit-learn"},{"location":"examples/examples.html#forecasting-de-la-demanda-electrica","text":"","title":"Forecasting de la demanda el\u00e9ctrica"},{"location":"guides/autoregresive_forecaster.html","text":"Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Prediction \u00b6 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005 - 07 - 01 0.973131 2005 - 08 - 01 1.022154 2005 - 09 - 01 1.151334 Freq : MS , dtype : float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009918738501371805 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 1.58096176e-01, 6.18241513e-02, 6.44665806e-02, -2.41792429e-02, -2.60679572e-02, 7.04191008e-04, -4.28090339e-02, 4.87464352e-04, 1.66853207e-02, 1.00022527e-02, 1.62219885e-01, 6.15595305e-01, 2.85168042e-02, -7.31915864e-02, -5.38785052e-02]) Extract training matrix \u00b6 1 X , y = forecaster . create_train_X_y ( data_train )","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive_forecaster.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom .","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive_forecaster.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive_forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/autoregresive_forecaster.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}","title":"Create and train forecaster"},{"location":"guides/autoregresive_forecaster.html#prediction","text":"1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005 - 07 - 01 0.973131 2005 - 08 - 01 1.022154 2005 - 09 - 01 1.151334 Freq : MS , dtype : float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009918738501371805","title":"Prediction"},{"location":"guides/autoregresive_forecaster.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 1.58096176e-01, 6.18241513e-02, 6.44665806e-02, -2.41792429e-02, -2.60679572e-02, 7.04191008e-04, -4.28090339e-02, 4.87464352e-04, 1.66853207e-02, 1.00022527e-02, 1.62219885e-01, 6.15595305e-01, 2.85168042e-02, -7.31915864e-02, -5.38785052e-02])","title":"Feature importance"},{"location":"guides/autoregresive_forecaster.html#extract-training-matrix","text":"1 X , y = forecaster . create_train_X_y ( data_train )","title":"Extract training matrix"},{"location":"guides/autoregresive_forecaster_exogenous.html","text":"Recursive multi-step forecasting with exogenous variables \u00b6 ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i]. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :] Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: True Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001} Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 8 9 10 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] . values ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.965051 2005-08-01 1.014752 2005-09-01 1.140090 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.008474661390588431 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 0.1173852 , 0.0290203 , 0.02626445, -0.06322356, -0.04009523, -0.00921489, -0.04732691, -0.00662549, 0.01139862, 0.01485692, 0.15042588, 0.56962708, 0.01581026, -0.08791199, -0.06785373, 0.09607342, 0.21051962]) Extract training matrix \u00b6 1 2 3 4 X , y = forecaster . create_train_X_y ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values )","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive_forecaster_exogenous.html#recursive-multi-step-forecasting-with-exogenous-variables","text":"ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i].","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive_forecaster_exogenous.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive_forecaster_exogenous.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :]","title":"Data"},{"location":"guides/autoregresive_forecaster_exogenous.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values ) forecaster 1 2 3 4 5 =======================ForecasterAutoreg======================= Regressor: Ridge() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: True Parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}","title":"Create and train forecaster"},{"location":"guides/autoregresive_forecaster_exogenous.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 8 9 10 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] . values ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.965051 2005-08-01 1.014752 2005-09-01 1.140090 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.008474661390588431","title":"Prediction"},{"location":"guides/autoregresive_forecaster_exogenous.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso forecaster . get_coef () # When using as regressor RandomForestRegressor or GradientBoostingRegressor # forecaster.get_feature_importances() 1 2 3 4 array([ 0.1173852 , 0.0290203 , 0.02626445, -0.06322356, -0.04009523, -0.00921489, -0.04732691, -0.00662549, 0.01139862, 0.01485692, 0.15042588, 0.56962708, 0.01581026, -0.08791199, -0.06785373, 0.09607342, 0.21051962])","title":"Feature importance"},{"location":"guides/autoregresive_forecaster_exogenous.html#extract-training-matrix","text":"1 2 3 4 X , y = forecaster . create_train_X_y ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] . values )","title":"Extract training matrix"},{"location":"guides/cross_validation_backtest.html","text":"Cross validation and backtest \u00b6","title":"Cross validation and Backtest"},{"location":"guides/cross_validation_backtest.html#cross-validation-and-backtest","text":"","title":"Cross validation and backtest"},{"location":"guides/custom_predictors.html","text":"Custom predictors for recursive multi-step forecasting \u00b6 It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 =======================ForecasterAutoregCustom======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with: create_predictors Window size: 20 Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Prediction \u00b6 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191 Feature importance \u00b6 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances () 1 2 3 array([0.53971953, 0.11909726, 0.04640365, 0.0241653 , 0.03056669, 0.01513909, 0.04288317, 0.012742 , 0.01893797, 0.10863873, 0.04170661]) Extract training matrix \u00b6 1 X , y = forecaster . create_train_X_y ( data_train )","title":"Custom predictors"},{"location":"guides/custom_predictors.html#custom-predictors-for-recursive-multi-step-forecasting","text":"It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors.","title":"Custom predictors for  recursive multi-step forecasting"},{"location":"guides/custom_predictors.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/custom_predictors.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/custom_predictors.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 =======================ForecasterAutoregCustom======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with: create_predictors Window size: 20 Exogenous variable: False Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Create and train forecaster"},{"location":"guides/custom_predictors.html#prediction","text":"1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191","title":"Prediction"},{"location":"guides/custom_predictors.html#feature-importance","text":"1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances () 1 2 3 array([0.53971953, 0.11909726, 0.04640365, 0.0241653 , 0.03056669, 0.01513909, 0.04288317, 0.012742 , 0.01893797, 0.10863873, 0.04170661])","title":"Feature importance"},{"location":"guides/custom_predictors.html#extract-training-matrix","text":"1 X , y = forecaster . create_train_X_y ( data_train )","title":"Extract training matrix"},{"location":"guides/direct_multi_step_forecasting.html","text":"Direct multi-step forecaster \u00b6 ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = GradientBoostingRegressor (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 ============================ForecasterAutoregMultiOutput============================ Regressor: GradientBoostingRegressor() Steps: 36 Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False} Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.877073 2005-08-01 0.974353 2005-09-01 1.021718 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.006208145903529839 Feature importance \u00b6 Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances ( step = 1 ) 1 2 3 array([0.0070333 , 0.07330653, 0.03192484, 0.00284055, 0.00525716, 0.0039042 , 0.00717631, 0.00676659, 0.00587334, 0.00856639, 0.01275252, 0.81823596, 0.005138 , 0.00413031, 0.007094]) Extract training matrix \u00b6 Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , )","title":"Direct multi-step forecasting"},{"location":"guides/direct_multi_step_forecasting.html#direct-multi-step-forecaster","text":"ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model.","title":"Direct multi-step forecaster"},{"location":"guides/direct_multi_step_forecasting.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/direct_multi_step_forecasting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/direct_multi_step_forecasting.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = GradientBoostingRegressor (), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 ============================ForecasterAutoregMultiOutput============================ Regressor: GradientBoostingRegressor() Steps: 36 Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Exogenous variable: False Parameters: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}","title":"Create and train forecaster"},{"location":"guides/direct_multi_step_forecasting.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 5 6 7 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = data_test . index ) predictions . head ( 3 ) 1 2 3 4 5 fecha 2005-07-01 0.877073 2005-08-01 0.974353 2005-09-01 1.021718 Freq: MS, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.006208145903529839","title":"Prediction"},{"location":"guides/direct_multi_step_forecasting.html#feature-importance","text":"Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 2 3 4 5 # When using as regressor LinearRegression, Ridge or Lasso # forecaster.get_coef() # When using as regressor RandomForestRegressor or GradientBoostingRegressor forecaster . get_feature_importances ( step = 1 ) 1 2 3 array([0.0070333 , 0.07330653, 0.03192484, 0.00284055, 0.00525716, 0.0039042 , 0.00717631, 0.00676659, 0.00587334, 0.00856639, 0.01275252, 0.81823596, 0.005138 , 0.00413031, 0.007094])","title":"Feature importance"},{"location":"guides/direct_multi_step_forecasting.html#extract-training-matrix","text":"Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , )","title":"Extract training matrix"},{"location":"guides/forecasting-methods.html","text":"Introduction to forcasting \u00b6 Time Series and forecasting \u00b6 A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Direct multi-step forecasting \u00b6 This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Introduction to forcasting"},{"location":"guides/forecasting-methods.html#introduction-to-forcasting","text":"","title":"Introduction to forcasting"},{"location":"guides/forecasting-methods.html#time-series-and-forecasting","text":"A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions.","title":"Time Series and forecasting"},{"location":"guides/forecasting-methods.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix.","title":"Recursive multi-step forecasting"},{"location":"guides/forecasting-methods.html#direct-multi-step-forecasting","text":"This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Direct multi-step forecasting"},{"location":"guides/prediction_intervals.html","text":"Prediction intervals \u00b6","title":"Prediction intervals"},{"location":"guides/prediction_intervals.html#prediction-intervals","text":"","title":"Prediction intervals"},{"location":"guides/releases.html","text":"Releases \u00b6 All notable changes to this project will be documented in this file. [0.2.0] - [unreleased] \u00b6 Added \u00b6 Changed \u00b6 Installation using pip install skforecast Fixed \u00b6 [0.1.9] - 2121-07-27 \u00b6 Added \u00b6 Logging total number of models to fit in grid_search_forecaster() . Class ForecasterAutoregCustom . Method create_train_X_y() to facilitate access to the training data matrix created from y and exog . Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster() , backtesting_forecaster() , grid_search_forecaster() and backtesting_forecaster_intervals() changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster() , backtesting_forecaster() , grid_search_forecaster() and backtesting_forecaster_intervals() is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric. Fixed \u00b6 Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster() . [0.1.8.1] - 2021-05-17 \u00b6 Added \u00b6 set_out_sample_residuals() method to store or update out of sample residuals used by predict_interval() . Changed \u00b6 backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals . Fixed \u00b6 Warning of inclompleted folds when using backtesting_forecast() with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict() allow exog data longer than needed (steps). backtesting_forecast() prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput . [0.1.8] - 2021-04-02 \u00b6 Added \u00b6 Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval() and ForecasterAutoreg.predict_interval() : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals() perform backtesting and return prediction intervals. Changed \u00b6 Fixed \u00b6 [0.1.7] - 2021-03-19 \u00b6 Added \u00b6 Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors. Changed \u00b6 grid_search forecaster() adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg . Fixed \u00b6 [0.1.6] - 2021-03-14 \u00b6 Added \u00b6 Method get_feature_importances() to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster() . Added backtesting_forecast() to skforecast.model_selection . Changed \u00b6 Method create_lags() return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict() . Renamed ts_cv_forecaster() to cv_forecaster() . Fixed \u00b6 [0.1.4] - 2021-02-15 \u00b6 Added \u00b6 Method get_coef() to skforecast.ForecasterAutoreg . Changed \u00b6 Fixed \u00b6","title":"Releases"},{"location":"guides/releases.html#releases","text":"All notable changes to this project will be documented in this file.","title":"Releases"},{"location":"guides/releases.html#020-unreleased","text":"","title":"[0.2.0] - [unreleased]"},{"location":"guides/releases.html#added","text":"","title":"Added"},{"location":"guides/releases.html#changed","text":"Installation using pip install skforecast","title":"Changed"},{"location":"guides/releases.html#fixed","text":"","title":"Fixed"},{"location":"guides/releases.html#019-2121-07-27","text":"","title":"[0.1.9] - 2121-07-27"},{"location":"guides/releases.html#added_1","text":"Logging total number of models to fit in grid_search_forecaster() . Class ForecasterAutoregCustom . Method create_train_X_y() to facilitate access to the training data matrix created from y and exog .","title":"Added"},{"location":"guides/releases.html#changed_1","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster() , backtesting_forecaster() , grid_search_forecaster() and backtesting_forecaster_intervals() changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster() , backtesting_forecaster() , grid_search_forecaster() and backtesting_forecaster_intervals() is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.","title":"Changed"},{"location":"guides/releases.html#fixed_1","text":"Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster() .","title":"Fixed"},{"location":"guides/releases.html#0181-2021-05-17","text":"","title":"[0.1.8.1] - 2021-05-17"},{"location":"guides/releases.html#added_2","text":"set_out_sample_residuals() method to store or update out of sample residuals used by predict_interval() .","title":"Added"},{"location":"guides/releases.html#changed_2","text":"backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals .","title":"Changed"},{"location":"guides/releases.html#fixed_2","text":"Warning of inclompleted folds when using backtesting_forecast() with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict() allow exog data longer than needed (steps). backtesting_forecast() prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput .","title":"Fixed"},{"location":"guides/releases.html#018-2021-04-02","text":"","title":"[0.1.8] - 2021-04-02"},{"location":"guides/releases.html#added_3","text":"Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval() and ForecasterAutoreg.predict_interval() : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals() perform backtesting and return prediction intervals.","title":"Added"},{"location":"guides/releases.html#changed_3","text":"","title":"Changed"},{"location":"guides/releases.html#fixed_3","text":"","title":"Fixed"},{"location":"guides/releases.html#017-2021-03-19","text":"","title":"[0.1.7] - 2021-03-19"},{"location":"guides/releases.html#added_4","text":"Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors.","title":"Added"},{"location":"guides/releases.html#changed_4","text":"grid_search forecaster() adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg .","title":"Changed"},{"location":"guides/releases.html#fixed_4","text":"","title":"Fixed"},{"location":"guides/releases.html#016-2021-03-14","text":"","title":"[0.1.6] - 2021-03-14"},{"location":"guides/releases.html#added_5","text":"Method get_feature_importances() to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster() . Added backtesting_forecast() to skforecast.model_selection .","title":"Added"},{"location":"guides/releases.html#changed_5","text":"Method create_lags() return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict() . Renamed ts_cv_forecaster() to cv_forecaster() .","title":"Changed"},{"location":"guides/releases.html#fixed_5","text":"","title":"Fixed"},{"location":"guides/releases.html#014-2021-02-15","text":"","title":"[0.1.4] - 2021-02-15"},{"location":"guides/releases.html#added_6","text":"Method get_coef() to skforecast.ForecasterAutoreg .","title":"Added"},{"location":"guides/releases.html#changed_6","text":"","title":"Changed"},{"location":"guides/releases.html#fixed_6","text":"","title":"Fixed"},{"location":"guides/tuning.html","text":"Tuning forecaster \u00b6 Skforecast library allows to combine grid search strategy with cross-validation and backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediccion performance. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Tuning forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 14:28:00,536 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.39it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.11s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:02<00:00, 1.08it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.09s/it] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:04<00:08, 4.16s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.22it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.40s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.32s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00, 1.42s/it] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:09<00:04, 4.91s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.18it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.23s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.01s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.27s/it] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.78s/it] 2021-08-13 14:28:14,897 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 4 5 6 7 8 9 10] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0265202 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0269665 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0280916 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0286928 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0295003 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0332516 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0338282 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0341536 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0365391 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.036623 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0393264 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.046249","title":"Tuning forecaster"},{"location":"guides/tuning.html#tuning-forecaster","text":"Skforecast library allows to combine grid search strategy with cross-validation and backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediccion performance.","title":"Tuning forecaster"},{"location":"guides/tuning.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/tuning.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/tuning.html#tuning-forecaster_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( data_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2021-08-13 14:28:00,536 root INFO Number of models to fit: 12 loop lags_grid: 0%| | 0/3 [00:00<?, ?it/s] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.39it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.11s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:02<00:00, 1.08it/s] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.09s/it] loop lags_grid: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:04<00:08, 4.16s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.22it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.40s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.32s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00, 1.42s/it] loop lags_grid: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:09<00:04, 4.91s/it] loop param_grid: 0%| | 0/4 [00:00<?, ?it/s] loop param_grid: 25%|\u2588\u2588\u258c | 1/4 [00:00<00:02, 1.18it/s] loop param_grid: 50%|\u2588\u2588\u2588\u2588\u2588 | 2/4 [00:02<00:02, 1.23s/it] loop param_grid: 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3/4 [00:03<00:01, 1.01s/it] loop param_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:04<00:00, 1.27s/it] loop lags_grid: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14<00:00, 4.78s/it] 2021-08-13 14:28:14,897 root INFO Refitting `forecaster` using the best found parameters and the whole data set: lags: [ 1 2 3 4 5 6 7 8 9 10] params: {'max_depth': 10, 'n_estimators': 50} 1 results_grid lags params metric 6 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 50} 0.0265202 4 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 50} 0.0269665 7 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 10, 'n_estimators': 100} 0.0280916 5 [ 1 2 3 4 5 6 7 8 9 10] {'max_depth': 5, 'n_estimators': 100} 0.0286928 11 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 100} 0.0295003 0 [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.0332516 9 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 100} 0.0338282 1 [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.0341536 8 [ 1 2 3 20] {'max_depth': 5, 'n_estimators': 50} 0.0365391 3 [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.036623 10 [ 1 2 3 20] {'max_depth': 10, 'n_estimators': 50} 0.0393264 2 [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.046249","title":"Tuning forecaster"}]}