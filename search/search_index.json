{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"skforecast \u00b6 Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...). Installation \u00b6 pip install skforecast pip install git + https : // github . com / JoaquinAmatRodrigo / skforecast @v0 .1.9 Latest (unstable): pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24 Dependencies \u00b6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24 Features \u00b6 Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance TODO \u00b6 [ ] Pandas dataframe as input of multiple exogenous variables [ ] Parallel grid search [ ] Speed lag creation with numba [ ] Increase unit test coverage Tutorials (spanish) \u00b6 Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python References \u00b6 Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance Licence \u00b6 joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Index"},{"location":"index.html#skforecast","text":"Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (XGBoost, LightGBM, Ranger...).","title":"skforecast"},{"location":"index.html#installation","text":"pip install skforecast pip install git + https : // github . com / JoaquinAmatRodrigo / skforecast @v0 .1.9 Latest (unstable): pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master The most common error when importing the library is: 'cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' . This is because the scikit-learn installation is lower than 0.24. Try to upgrade scikit-learn with pip install scikit-learn==0.24","title":"Installation"},{"location":"index.html#dependencies","text":"python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=0.24","title":"Dependencies"},{"location":"index.html#features","text":"Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance","title":"Features"},{"location":"index.html#todo","text":"[ ] Pandas dataframe as input of multiple exogenous variables [ ] Parallel grid search [ ] Speed lag creation with numba [ ] Increase unit test coverage","title":"TODO"},{"location":"index.html#tutorials-spanish","text":"Forecasting series temporales con Python y Scikit Learn Predicci\u00f3n (forecasting) de la demanda el\u00e9ctrica con Python","title":"Tutorials (spanish)"},{"location":"index.html#references","text":"Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance","title":"References"},{"location":"index.html#licence","text":"joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Licence"},{"location":"api/ForecasterAutoreg.html","text":"ForecasterAutoreg \u00b6 This class turns a scikit-learn regressor into a recursive autoregressive (multi-step) forecaster. Parameters \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . Attributes \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : 1D np.array Lags used as predictors. max_lag : int Maximum value of lag included in lags. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training. !!! in_sample_residuals \"np.ndarray\" Residuals of the model when predicting training data. Only stored up to 1000 values. !!! out_sample_residuals \"np.ndarray\" Residuals of the model when predicting non training data. Only stored up to 1000 values. __repr__ ( self ) special \u00b6 Information displayed when a ForecasterAutoreg object is printed. Source code in skforecast/ForecasterAutoreg.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoreg object is printed. ''' info = \"=======================\" \\ + \"ForecasterAutoreg\" \\ + \"=======================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Lags: \" + str ( self . lags ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info create_lags ( self , y ) \u00b6 Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. Returns \u00b6 X_data : 2D np.ndarray, shape (samples, len(self.lags)) 2D array with the lag values (predictors). y_data : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of X_data . Source code in skforecast/ForecasterAutoreg.py def create_lags ( self , y : Union [ np . ndarray , pd . Series ]) -> Tuple [ np . ndarray , np . ndarray ]: ''' Transforms a time series into a 2D array and a 1D array where each value of `y` is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. Returns ------- X_data : 2D np.ndarray, shape (samples, len(self.lags)) 2D array with the lag values (predictors). y_data : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of `X_data`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if self . max_lag > len ( y ): raise Exception ( f \"Maximum lag can't be higher than `y` length. \" f \"Got maximum lag= { self . max_lag } and `y` length= { len ( y ) } .\" ) n_splits = len ( y ) - self . max_lag X_data = np . full ( shape = ( n_splits , self . max_lag ), fill_value = np . nan , dtype = float ) y_data = np . full ( shape = ( n_splits , 1 ), fill_value = np . nan , dtype = float ) for i in range ( n_splits ): X_index = np . arange ( i , self . max_lag + i ) y_index = [ self . max_lag + i ] X_data [ i , :] = y [ X_index ] y_data [ i ] = y [ y_index ] X_data = X_data [:, - self . lags ] y_data = y_data . ravel () return X_data , y_data create_train_X_y ( self , y , exog = None ) \u00b6 Create training matrices X, y Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoreg.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_lags ( y = y ) if exog is not None : # The first `self.max_lag` positions have to be removed from exog # since they are not in X_train. X_train = np . column_stack (( X_train , exog [ self . max_lag :,])) return X_train , y_train fit ( self , y , exog = None ) \u00b6 Training ForecasterAutoreg Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 self : ForecasterAutoreg Trained ForecasterAutoreg Source code in skforecast/ForecasterAutoreg.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoreg Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoreg Trained ForecasterAutoreg ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) self . regressor . fit ( X = X_train , y = y_train ) residuals = y_train - self . regressor . predict ( X_train ) if len ( residuals ) > 1000 : # Only up to 1000 residuals are stored residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . in_sample_residuals = residuals # The last time window of training data is stored so that lags needed as # predictors in the first iteration of `predict()` can be calculated. self . last_window = y_train [ - self . max_lag :] . copy () get_coef ( self ) \u00b6 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters \u00b6 self Returns \u00b6 coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoreg.py def get_coef ( self ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- self Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that `coef[i]` is the value associated with `self.lags[i]`. ''' valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressor . coef_ return coef get_feature_importances ( self ) \u00b6 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Parameters \u00b6 self Returns \u00b6 feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoreg.py def get_feature_importances ( self ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- self Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that `feature_importances[i]` is the value associated with `self.lags[i]`. ''' if not isinstance ( self . regressor , ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor )): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressor . feature_importances_ return feature_importances predict ( self , steps , last_window = None , exog = None ) \u00b6 Iterative process in which, each prediction, is used as a predictor for the next step. Parameters \u00b6 steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Returns \u00b6 predictions : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoreg.py def predict ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Returns ------- predictions : 1D np.array, shape (steps,) Values predicted. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = steps , fill_value = np . nan ) for i in range ( steps ): X = last_window [ - self . lags ] . reshape ( 1 , - 1 ) if exog is None : prediction = self . regressor . predict ( X ) else : prediction = self . regressor . predict ( np . column_stack (( X , exog [ i ,] . reshape ( 1 , - 1 ))) ) predictions [ i ] = prediction . ravel ()[ 0 ] # Update `last_window` values. The first position is discarded and # the new prediction is added at the end. last_window = np . append ( last_window [ 1 :], prediction ) return predictions predict_interval ( self , steps , last_window = None , exog = None , interval = [ 5 , 95 ], n_boot = 500 , in_sample_residuals = True ) \u00b6 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters \u00b6 steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns \u00b6 predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes \u00b6 More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/ForecasterAutoreg.py def predict_interval ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. If `False`, out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see `set_out_sample_residuals()`). Returns ------- predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if not in_sample_residuals and self . out_sample_residuals is None : raise Exception ( ( 'out_sample_residuals is empty. In order to estimate prediction ' 'intervals using out of sample residuals, the user shoud have ' 'calculated and stored the residuals within the forecaster (see' '`set_out_sample_residuals()`.' ) ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () # Since during predict() `last_window` and `exog` are modified, the # originals are stored to be used later last_window_original = last_window . copy () if exog is not None : exog_original = exog . copy () else : exog_original = exog predictions = self . predict ( steps = steps , last_window = last_window , exog = exog ) predictions_interval = self . _estimate_boot_interval ( steps = steps , last_window = last_window_original , exog = exog_original , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) predictions = np . column_stack (( predictions , predictions_interval )) return predictions set_lags ( self , lags ) \u00b6 Set new value to the attribute lags . Attribute max_lag is also updated. Parameters \u00b6 lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags . list or np.array : include only lags present in lags . Returns \u00b6 self Source code in skforecast/ForecasterAutoreg.py def set_lags ( self , lags : int ) -> None : ''' Set new value to the attribute `lags`. Attribute `max_lag` is also updated. Parameters ---------- lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. `int`: include lags from 1 to `lags`. `list` or `np.array`: include only lags present in `lags`. Returns ------- self ''' if isinstance ( lags , int ) and lags < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , ( list , range , np . ndarray )) and min ( lags ) < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , int ): self . lags = np . arange ( lags ) + 1 elif isinstance ( lags , ( list , range )): self . lags = np . array ( lags ) elif isinstance ( lags , np . ndarray ): self . lags = lags else : raise Exception ( f \"`lags` argument must be `int`, `1D np.ndarray`, `range` or `list`. \" f \"Got { type ( lags ) } \" ) self . max_lag = max ( self . lags ) set_out_sample_residuals ( self , residuals ) \u00b6 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters \u00b6 params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns \u00b6 self Source code in skforecast/ForecasterAutoreg.py def set_out_sample_residuals ( self , residuals : np . ndarray ) -> None : ''' Set new values to the attribute `out_sample_residuals`. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters ---------- params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns ------- self ''' if not isinstance ( residuals , np . ndarray ): raise Exception ( f \"`residuals` argument must be `1D np.ndarray`. Got { type ( residuals ) } \" ) if len ( residuals ) > 1000 : residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . out_sample_residuals = residuals set_params ( self , ** params ) \u00b6 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. Parameters \u00b6 params : dict Parameters values. Returns \u00b6 self Source code in skforecast/ForecasterAutoreg.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params )","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#forecasterautoreg","text":"This class turns a scikit-learn regressor into a recursive autoregressive (multi-step) forecaster.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg--parameters","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags .","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg--attributes","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : 1D np.array Lags used as predictors. max_lag : int Maximum value of lag included in lags. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training. !!! in_sample_residuals \"np.ndarray\" Residuals of the model when predicting training data. Only stored up to 1000 values. !!! out_sample_residuals \"np.ndarray\" Residuals of the model when predicting non training data. Only stored up to 1000 values.","title":"Attributes"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.__repr__","text":"Information displayed when a ForecasterAutoreg object is printed. Source code in skforecast/ForecasterAutoreg.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoreg object is printed. ''' info = \"=======================\" \\ + \"ForecasterAutoreg\" \\ + \"=======================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Lags: \" + str ( self . lags ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info","title":"__repr__()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_lags","text":"Transforms a time series into a 2D array and a 1D array where each value of y is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.","title":"create_lags()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_lags--parameters","text":"y : 1D np.ndarray, pd.Series Training time series.","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_lags--returns","text":"X_data : 2D np.ndarray, shape (samples, len(self.lags)) 2D array with the lag values (predictors). y_data : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of X_data . Source code in skforecast/ForecasterAutoreg.py def create_lags ( self , y : Union [ np . ndarray , pd . Series ]) -> Tuple [ np . ndarray , np . ndarray ]: ''' Transforms a time series into a 2D array and a 1D array where each value of `y` is associated with the lags that precede it. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. Returns ------- X_data : 2D np.ndarray, shape (samples, len(self.lags)) 2D array with the lag values (predictors). y_data : 1D np.ndarray, shape (n\u00ba observaciones - max(seld.lags),) Values of the time series related to each row of `X_data`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if self . max_lag > len ( y ): raise Exception ( f \"Maximum lag can't be higher than `y` length. \" f \"Got maximum lag= { self . max_lag } and `y` length= { len ( y ) } .\" ) n_splits = len ( y ) - self . max_lag X_data = np . full ( shape = ( n_splits , self . max_lag ), fill_value = np . nan , dtype = float ) y_data = np . full ( shape = ( n_splits , 1 ), fill_value = np . nan , dtype = float ) for i in range ( n_splits ): X_index = np . arange ( i , self . max_lag + i ) y_index = [ self . max_lag + i ] X_data [ i , :] = y [ X_index ] y_data [ i ] = y [ y_index ] X_data = X_data [:, - self . lags ] y_data = y_data . ravel () return X_data , y_data","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y","text":"Create training matrices X, y","title":"create_train_X_y()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.create_train_X_y--returns","text":"X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoreg.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_lags ( y = y ) if exog is not None : # The first `self.max_lag` positions have to be removed from exog # since they are not in X_train. X_train = np . column_stack (( X_train , exog [ self . max_lag :,])) return X_train , y_train","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.fit","text":"Training ForecasterAutoreg","title":"fit()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.fit--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.fit--returns","text":"self : ForecasterAutoreg Trained ForecasterAutoreg Source code in skforecast/ForecasterAutoreg.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoreg Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoreg Trained ForecasterAutoreg ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) self . regressor . fit ( X = X_train , y = y_train ) residuals = y_train - self . regressor . predict ( X_train ) if len ( residuals ) > 1000 : # Only up to 1000 residuals are stored residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . in_sample_residuals = residuals # The last time window of training data is stored so that lags needed as # predictors in the first iteration of `predict()` can be calculated. self . last_window = y_train [ - self . max_lag :] . copy ()","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_coef","text":"Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`.","title":"get_coef()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_coef--parameters","text":"self","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_coef--returns","text":"coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoreg.py def get_coef ( self ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- self Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that `coef[i]` is the value associated with `self.lags[i]`. ''' valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressor . coef_ return coef","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances","text":"Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor .","title":"get_feature_importances()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances--parameters","text":"self","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.get_feature_importances--returns","text":"feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoreg.py def get_feature_importances ( self ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- self Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that `feature_importances[i]` is the value associated with `self.lags[i]`. ''' if not isinstance ( self . regressor , ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor )): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressor . feature_importances_ return feature_importances","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict","text":"Iterative process in which, each prediction, is used as a predictor for the next step.","title":"predict()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict--parameters","text":"steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s.","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict--returns","text":"predictions : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoreg.py def predict ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Returns ------- predictions : 1D np.array, shape (steps,) Values predicted. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = steps , fill_value = np . nan ) for i in range ( steps ): X = last_window [ - self . lags ] . reshape ( 1 , - 1 ) if exog is None : prediction = self . regressor . predict ( X ) else : prediction = self . regressor . predict ( np . column_stack (( X , exog [ i ,] . reshape ( 1 , - 1 ))) ) predictions [ i ] = prediction . ravel ()[ 0 ] # Update `last_window` values. The first position is discarded and # the new prediction is added at the end. last_window = np . append ( last_window [ 1 :], prediction ) return predictions","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict_interval","text":"Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned.","title":"predict_interval()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict_interval--parameters","text":"steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ).","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict_interval--returns","text":"predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.predict_interval--notes","text":"More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/ForecasterAutoreg.py def predict_interval ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. If `False`, out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see `set_out_sample_residuals()`). Returns ------- predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if not in_sample_residuals and self . out_sample_residuals is None : raise Exception ( ( 'out_sample_residuals is empty. In order to estimate prediction ' 'intervals using out of sample residuals, the user shoud have ' 'calculated and stored the residuals within the forecaster (see' '`set_out_sample_residuals()`.' ) ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () # Since during predict() `last_window` and `exog` are modified, the # originals are stored to be used later last_window_original = last_window . copy () if exog is not None : exog_original = exog . copy () else : exog_original = exog predictions = self . predict ( steps = steps , last_window = last_window , exog = exog ) predictions_interval = self . _estimate_boot_interval ( steps = steps , last_window = last_window_original , exog = exog_original , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) predictions = np . column_stack (( predictions , predictions_interval )) return predictions","title":"Notes"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_lags","text":"Set new value to the attribute lags . Attribute max_lag is also updated.","title":"set_lags()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_lags--parameters","text":"lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags . list or np.array : include only lags present in lags .","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_lags--returns","text":"self Source code in skforecast/ForecasterAutoreg.py def set_lags ( self , lags : int ) -> None : ''' Set new value to the attribute `lags`. Attribute `max_lag` is also updated. Parameters ---------- lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. `int`: include lags from 1 to `lags`. `list` or `np.array`: include only lags present in `lags`. Returns ------- self ''' if isinstance ( lags , int ) and lags < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , ( list , range , np . ndarray )) and min ( lags ) < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , int ): self . lags = np . arange ( lags ) + 1 elif isinstance ( lags , ( list , range )): self . lags = np . array ( lags ) elif isinstance ( lags , np . ndarray ): self . lags = lags else : raise Exception ( f \"`lags` argument must be `int`, `1D np.ndarray`, `range` or `list`. \" f \"Got { type ( lags ) } \" ) self . max_lag = max ( self . lags )","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals","text":"Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process.","title":"set_out_sample_residuals()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals--parameters","text":"params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored.","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_out_sample_residuals--returns","text":"self Source code in skforecast/ForecasterAutoreg.py def set_out_sample_residuals ( self , residuals : np . ndarray ) -> None : ''' Set new values to the attribute `out_sample_residuals`. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters ---------- params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns ------- self ''' if not isinstance ( residuals , np . ndarray ): raise Exception ( f \"`residuals` argument must be `1D np.ndarray`. Got { type ( residuals ) } \" ) if len ( residuals ) > 1000 : residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . out_sample_residuals = residuals","title":"Returns"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_params","text":"Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg.","title":"set_params()"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_params--parameters","text":"params : dict Parameters values.","title":"Parameters"},{"location":"api/ForecasterAutoreg.html#skforecast.ForecasterAutoreg.ForecasterAutoreg.set_params--returns","text":"self Source code in skforecast/ForecasterAutoreg.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params )","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html","text":"ForecasterAutoregCustom \u00b6 This class turns a scikit-learn regressor into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. !!! fun_predictors \"Callable\" Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. !!! window_size \"int\" Size of the window needed by fun_predictors to create the predictors. Attributes \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. !!! fun_predictors \"Callable\" Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. !!! window_size \"int\" Size of the window needed by fun_predictors to create the predictors. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the predictors for the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training. !!! in_sample_residuals \"np.ndarray\" Residuals of the model when predicting training data. Only stored up to 1000 values. !!! out_sample_residuals \"np.ndarray\" Residuals of the model when predicting non training data. Only stored up to 1000 values. __repr__ ( self ) special \u00b6 Information displayed when a ForecasterAutoregCustom object is printed. Source code in skforecast/ForecasterAutoregCustom.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoregCustom object is printed. ''' info = \"=======================\" \\ + \"ForecasterAutoregCustom\" \\ + \"=======================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Predictors created with: \" + str ( self . create_predictors . __name__ ) \\ + \" \\n \" \\ + \"Window size: \" + str ( self . window_size ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info create_train_X_y ( self , y , exog = None ) \u00b6 Create training matrices X, y Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregCustom.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) if len ( y ) - self . window_size < 1 : raise Exception ( f '`y` must have as many values as the windows_size needed by { self . create_predictors . __name__ } .' f 'For this Forecaster the minimum lenght is { self . window_size + 1 } ' ) X_train = [] y_train = [] for i in range ( len ( y ) - self . window_size ): train_index = np . arange ( i , self . window_size + i ) test_index = self . window_size + i X_train . append ( self . create_predictors ( y = y [ train_index ])) y_train . append ( y [ test_index ]) X_train = np . vstack ( X_train ) y_train = np . array ( y_train ) if np . isnan ( X_train ) . any (): raise Exception ( f \"`create_predictors()` is returning `NaN` values.\" ) if exog is not None : # The first `self.window_size` positions have to be removed from # exog since they are not in X_train. X_train = np . column_stack (( X_train , exog [ self . window_size :,])) return X_train , y_train fit ( self , y , exog = None ) \u00b6 Training ForecasterAutoregCustom Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 self : ForecasterAutoregCustom Trained ForecasterAutoregCustom Source code in skforecast/ForecasterAutoregCustom.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoregCustom Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoregCustom Trained ForecasterAutoregCustom ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) if len ( y ) - self . window_size < 1 : raise Exception ( f '`y` must have as many values as the windows_size needed by { self . create_predictors . __name__ } .' f 'For this Forecaster the minimum lenght is { self . window_size + 1 } ' ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) self . regressor . fit ( X = X_train , y = y_train ) residuals = y_train - self . regressor . predict ( X_train ) if len ( residuals ) > 1000 : # Only up to 1000 residuals are stored residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . in_sample_residuals = residuals # The last time window of training data is stored so that predictors in # the first iteration of `predict()` can be calculated. self . last_window = y_train . flatten ()[ - self . window_size :] get_coef ( self ) \u00b6 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters \u00b6 self Returns \u00b6 coef : 1D np.ndarray Value of the coefficients associated with each predictor. Coefficients are aligned so that coef[i] is the value associated with predictor i returned by self.create_predictors . Source code in skforecast/ForecasterAutoregCustom.py def get_coef ( self ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- self Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor. Coefficients are aligned so that `coef[i]` is the value associated with predictor i returned by `self.create_predictors`. ''' valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressor . coef_ return coef get_feature_importances ( self ) \u00b6 Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Parameters \u00b6 self Returns \u00b6 feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor. Values are aligned so that feature_importances[i] is the value associated with predictor i returned by self.create_predictors . Source code in skforecast/ForecasterAutoregCustom.py def get_feature_importances ( self ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- self Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor. Values are aligned so that `feature_importances[i]` is the value associated with predictor i returned by `self.create_predictors`. ''' if not isinstance ( self . regressor , ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor )): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressor . feature_importances_ return feature_importances predict ( self , steps , last_window = None , exog = None ) \u00b6 Iterative process in which, each prediction, is used as a predictor for the next step. Parameters \u00b6 steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default None Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Returns \u00b6 predicciones : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoregCustom.py def predict ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default `None` Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Returns ------- predicciones : 1D np.array, shape (steps,) Values predicted. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . window_size : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the predictors ( { self . window_size } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = steps , fill_value = np . nan ) for i in range ( steps ): X = self . create_predictors ( y = last_window ) if np . isnan ( X ) . any (): raise Exception ( f \"`create_predictors()` is returning `NaN` values.\" ) if exog is None : prediction = self . regressor . predict ( X ) else : prediction = self . regressor . predict ( np . column_stack (( X , exog [ i ,] . reshape ( 1 , - 1 ))) ) predictions [ i ] = prediction . ravel ()[ 0 ] # Update `last_window` values. The first position is discarded and # the new prediction is added at the end. last_window = np . append ( last_window [ 1 :], prediction ) return predictions predict_interval ( self , steps , last_window = None , exog = None , interval = [ 5 , 95 ], n_boot = 500 , in_sample_residuals = True ) \u00b6 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters \u00b6 steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default None Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns \u00b6 predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes \u00b6 More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/ForecasterAutoregCustom.py def predict_interval ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default `None` Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. If `False`, out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see `set_out_sample_residuals()`). Returns ------- predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if not in_sample_residuals and self . out_sample_residuals is None : raise Exception ( ( 'out_sample_residuals is empty. In order to estimate prediction ' 'intervals using out of sample residuals, the user shoud have ' 'calculated and stored the residuals within the forecaster (see' '`set_out_sample_residuals()`.' ) ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . window_size : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the predictors ( { self . window_size } ).\" ) else : last_window = self . last_window . copy () # Since during predict() `last_window` and `exog` are modified, the # originals are stored to be used later last_window_original = last_window . copy () if exog is not None : exog_original = exog . copy () else : exog_original = exog predictions = self . predict ( steps = steps , last_window = last_window , exog = exog ) predictions_interval = self . _estimate_boot_interval ( steps = steps , last_window = last_window_original , exog = exog_original , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) predictions = np . column_stack (( predictions , predictions_interval )) return predictions set_out_sample_residuals ( self , residuals ) \u00b6 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters \u00b6 params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns self Source code in skforecast/ForecasterAutoregCustom.py def set_out_sample_residuals ( self , residuals : np . ndarray ) -> None : ''' Set new values to the attribute `out_sample_residuals`. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters ---------- params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns ------- self ''' if not isinstance ( residuals , np . ndarray ): raise Exception ( f \"`residuals` argument must be `1D np.ndarray`. Got { type ( residuals ) } \" ) if len ( residuals ) > 1000 : residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . out_sample_residuals = residuals set_params ( self , ** params ) \u00b6 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. Parameters \u00b6 params : dict Parameters values. Returns \u00b6 self Source code in skforecast/ForecasterAutoregCustom.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params )","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#forecasterautoregcustom","text":"This class turns a scikit-learn regressor into a recursive (multi-step) forecaster with a custom function to create predictors.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom--parameters","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. !!! fun_predictors \"Callable\" Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. !!! window_size \"int\" Size of the window needed by fun_predictors to create the predictors.","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom--attributes","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. !!! fun_predictors \"Callable\" Function that takes a time series window as an argument and returns an np.array with the predictors associated with that window. !!! window_size \"int\" Size of the window needed by fun_predictors to create the predictors. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the predictors for the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training. !!! in_sample_residuals \"np.ndarray\" Residuals of the model when predicting training data. Only stored up to 1000 values. !!! out_sample_residuals \"np.ndarray\" Residuals of the model when predicting non training data. Only stored up to 1000 values.","title":"Attributes"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.__repr__","text":"Information displayed when a ForecasterAutoregCustom object is printed. Source code in skforecast/ForecasterAutoregCustom.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoregCustom object is printed. ''' info = \"=======================\" \\ + \"ForecasterAutoregCustom\" \\ + \"=======================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Predictors created with: \" + str ( self . create_predictors . __name__ ) \\ + \" \\n \" \\ + \"Window size: \" + str ( self . window_size ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info","title":"__repr__()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.create_train_X_y","text":"Create training matrices X, y","title":"create_train_X_y()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.create_train_X_y--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.create_train_X_y--returns","text":"X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregCustom.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags)) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag,) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) if len ( y ) - self . window_size < 1 : raise Exception ( f '`y` must have as many values as the windows_size needed by { self . create_predictors . __name__ } .' f 'For this Forecaster the minimum lenght is { self . window_size + 1 } ' ) X_train = [] y_train = [] for i in range ( len ( y ) - self . window_size ): train_index = np . arange ( i , self . window_size + i ) test_index = self . window_size + i X_train . append ( self . create_predictors ( y = y [ train_index ])) y_train . append ( y [ test_index ]) X_train = np . vstack ( X_train ) y_train = np . array ( y_train ) if np . isnan ( X_train ) . any (): raise Exception ( f \"`create_predictors()` is returning `NaN` values.\" ) if exog is not None : # The first `self.window_size` positions have to be removed from # exog since they are not in X_train. X_train = np . column_stack (( X_train , exog [ self . window_size :,])) return X_train , y_train","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.fit","text":"Training ForecasterAutoregCustom","title":"fit()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.fit--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.fit--returns","text":"self : ForecasterAutoregCustom Trained ForecasterAutoregCustom Source code in skforecast/ForecasterAutoregCustom.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoregCustom Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoregCustom Trained ForecasterAutoregCustom ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) if len ( y ) - self . window_size < 1 : raise Exception ( f '`y` must have as many values as the windows_size needed by { self . create_predictors . __name__ } .' f 'For this Forecaster the minimum lenght is { self . window_size + 1 } ' ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) self . regressor . fit ( X = X_train , y = y_train ) residuals = y_train - self . regressor . predict ( X_train ) if len ( residuals ) > 1000 : # Only up to 1000 residuals are stored residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . in_sample_residuals = residuals # The last time window of training data is stored so that predictors in # the first iteration of `predict()` can be calculated. self . last_window = y_train . flatten ()[ - self . window_size :]","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_coef","text":"Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`.","title":"get_coef()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_coef--parameters","text":"self","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_coef--returns","text":"coef : 1D np.ndarray Value of the coefficients associated with each predictor. Coefficients are aligned so that coef[i] is the value associated with predictor i returned by self.create_predictors . Source code in skforecast/ForecasterAutoregCustom.py def get_coef ( self ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- self Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor. Coefficients are aligned so that `coef[i]` is the value associated with predictor i returned by `self.create_predictors`. ''' valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressor . coef_ return coef","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importances","text":"Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor .","title":"get_feature_importances()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importances--parameters","text":"self","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.get_feature_importances--returns","text":"feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor. Values are aligned so that feature_importances[i] is the value associated with predictor i returned by self.create_predictors . Source code in skforecast/ForecasterAutoregCustom.py def get_feature_importances ( self ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- self Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor. Values are aligned so that `feature_importances[i]` is the value associated with predictor i returned by `self.create_predictors`. ''' if not isinstance ( self . regressor , ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor )): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressor . feature_importances_ return feature_importances","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict","text":"Iterative process in which, each prediction, is used as a predictor for the next step.","title":"predict()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict--parameters","text":"steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default None Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s.","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict--returns","text":"predicciones : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoregCustom.py def predict ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default `None` Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Returns ------- predicciones : 1D np.array, shape (steps,) Values predicted. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . window_size : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the predictors ( { self . window_size } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = steps , fill_value = np . nan ) for i in range ( steps ): X = self . create_predictors ( y = last_window ) if np . isnan ( X ) . any (): raise Exception ( f \"`create_predictors()` is returning `NaN` values.\" ) if exog is None : prediction = self . regressor . predict ( X ) else : prediction = self . regressor . predict ( np . column_stack (( X , exog [ i ,] . reshape ( 1 , - 1 ))) ) predictions [ i ] = prediction . ravel ()[ 0 ] # Update `last_window` values. The first position is discarded and # the new prediction is added at the end. last_window = np . append ( last_window [ 1 :], prediction ) return predictions","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval","text":"Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned.","title":"predict_interval()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval--parameters","text":"steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default None Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ).","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval--returns","text":"predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval","title":"Returns"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.predict_interval--notes","text":"More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/ForecasterAutoregCustom.py def predict_interval ( self , steps : int , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True ) -> np . ndarray : ''' Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters ---------- steps : int Number of future steps predicted. last_window : 1D np.ndarray, pd.Series, default `None` Values of the series used to create the predictors need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. If `False`, out of sample residuals are used. In the latter case, the user shoud have calculated and stored the residuals within the forecaster (see `set_out_sample_residuals()`). Returns ------- predictions : np.array, shape (steps, 3) Values predicted by the forecaster and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' if steps < 1 : raise Exception ( f \"`steps` must be integer greater than 0. Got { steps } .\" ) if not in_sample_residuals and self . out_sample_residuals is None : raise Exception ( ( 'out_sample_residuals is empty. In order to estimate prediction ' 'intervals using out of sample residuals, the user shoud have ' 'calculated and stored the residuals within the forecaster (see' '`set_out_sample_residuals()`.' ) ) if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . window_size : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the predictors ( { self . window_size } ).\" ) else : last_window = self . last_window . copy () # Since during predict() `last_window` and `exog` are modified, the # originals are stored to be used later last_window_original = last_window . copy () if exog is not None : exog_original = exog . copy () else : exog_original = exog predictions = self . predict ( steps = steps , last_window = last_window , exog = exog ) predictions_interval = self . _estimate_boot_interval ( steps = steps , last_window = last_window_original , exog = exog_original , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) predictions = np . column_stack (( predictions , predictions_interval )) return predictions","title":"Notes"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.set_out_sample_residuals","text":"Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process.","title":"set_out_sample_residuals()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.set_out_sample_residuals--parameters","text":"params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns self Source code in skforecast/ForecasterAutoregCustom.py def set_out_sample_residuals ( self , residuals : np . ndarray ) -> None : ''' Set new values to the attribute `out_sample_residuals`. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters ---------- params : 1D np.ndarray Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. Returns ------- self ''' if not isinstance ( residuals , np . ndarray ): raise Exception ( f \"`residuals` argument must be `1D np.ndarray`. Got { type ( residuals ) } \" ) if len ( residuals ) > 1000 : residuals = np . random . choice ( a = residuals , size = 1000 , replace = False ) self . out_sample_residuals = residuals","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.set_params","text":"Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom.","title":"set_params()"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.set_params--parameters","text":"params : dict Parameters values.","title":"Parameters"},{"location":"api/ForecasterAutoregCustom.html#skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom.set_params--returns","text":"self Source code in skforecast/ForecasterAutoregCustom.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params )","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html","text":"ForecasterAutoregMultiOutput \u00b6 This class turns a scikit-learn regressor into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . steps : int Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. Attributes \u00b6 regressor : scikit-learn regressor An instance of a scikit-learn regressor. One instance of this regressor is trainned for each step. All them are stored in slef.regressors_ . regressors_ : dict Dictionary with scikit-learn regressors trained for each step. steps : int Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. lags : 1D np.array Lags used as predictors. max_lag : int Maximum value of lag included in lags. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training. Notes \u00b6 A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hiperparameters. __repr__ ( self ) special \u00b6 Information displayed when a ForecasterAutoreg object is printed. Source code in skforecast/ForecasterAutoregMultiOutput.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoreg object is printed. ''' info = \"============================\" \\ + \"ForecasterAutoregMultiOutput\" \\ + \"============================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Steps: \" + str ( self . steps ) \\ + \" \\n \" \\ + \"Lags: \" + str ( self . lags ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info create_lags ( self , y ) \u00b6 Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. Returns \u00b6 X_data : 2D np.ndarray 2D array with the lag values (predictors). y_data : 2D np.ndarray Values of the time series related to each row of X_data . Source code in skforecast/ForecasterAutoregMultiOutput.py def create_lags ( self , y : Union [ np . ndarray , pd . Series ]) -> Tuple [ np . ndarray , np . ndarray ]: ''' Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. Returns ------- X_data : 2D np.ndarray 2D array with the lag values (predictors). y_data : 2D np.ndarray Values of the time series related to each row of `X_data`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if self . max_lag > len ( y ): raise Exception ( f \"Maximum lag can't be higer than `y` length. \" f \"Got maximum lag= { self . max_lag } and `y` length= { len ( y ) } .\" ) n_splits = len ( y ) - self . max_lag - ( self . steps - 1 ) X_data = np . full ( shape = ( n_splits , self . max_lag ), fill_value = np . nan , dtype = float ) y_data = np . full ( shape = ( n_splits , self . steps ), fill_value = np . nan , dtype = float ) for i in range ( n_splits ): X_index = np . arange ( i , self . max_lag + i ) y_index = np . arange ( self . max_lag + i , self . max_lag + i + self . steps ) X_data [ i , :] = y [ X_index ] y_data [ i , :] = y [ y_index ] X_data = X_data [:, - self . lags ] return X_data , y_data create_train_X_y ( self , y , exog = None ) \u00b6 Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregMultiOutput.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_lags , y_train = self . create_lags ( y = y ) if exog is None : X_train = X_lags else : # Trasform exog to match multi output format X_exog = self . _exog_to_multi_output ( exog = exog ) # The first `self.max_lag` positions have to be removed from X_exog # since they are not in X_lags. X_exog = X_exog [ - X_lags . shape [ 0 ]:, ] X_train = np . column_stack (( X_lags , X_exog )) return X_train , y_train filter_train_X_y_for_step ( self , step , X_train , y_train ) \u00b6 Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() . Parameters \u00b6 step : int step for which columns must be selected selected. Starts at 0. X_train : 2D np.ndarray 2D array with the training values (predictors). y_train : 1D np.ndarray Values (target) of the time series related to each row of X_train . Returns \u00b6 X_train_step : 2D np.ndarray 2D array with the training values (predictors) for step. y_train_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregMultiOutput.py def filter_train_X_y_for_step ( self , step : int , X_train : np . ndarray , y_train : np . ndarray ) -> Tuple [ np . array , np . array ]: ''' Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with `create_train_X_y()`. Parameters ---------- step : int step for which columns must be selected selected. Starts at 0. X_train : 2D np.ndarray 2D array with the training values (predictors). y_train : 1D np.ndarray Values (target) of the time series related to each row of `X_train`. Returns ------- X_train_step : 2D np.ndarray 2D array with the training values (predictors) for step. y_train_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of `X_train`. ''' if step > self . steps - 1 : raise Exception ( f \"Invalid value `step`. For this forecaster, the maximum step is { self . steps - 1 } .\" ) y_train_step = y_train [:, step ] if not self . included_exog : X_train_step = X_train else : idx_columns_lags = np . arange ( len ( self . lags )) idx_columns_exog = np . arange ( X_train . shape [ 1 ])[ len ( self . lags ) + step :: self . steps ] idx_columns = np . hstack (( idx_columns_lags , idx_columns_exog )) X_train_step = X_train [:, idx_columns ] return X_train_step , y_train_step fit ( self , y , exog = None ) \u00b6 Training ForecasterAutoregMultiOutput Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. Returns \u00b6 self : ForecasterAutoregMultiOutput Trained ForecasterAutoregMultiOutput Source code in skforecast/ForecasterAutoregMultiOutput.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoregMultiOutput Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoregMultiOutput Trained ForecasterAutoregMultiOutput ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) # Train one regressor for each step for step in range ( self . steps ): X_train_step , y_train_step = self . filter_train_X_y_for_step ( step = step , X_train = X_train , y_train = y_train ) self . regressors_ [ step ] . fit ( X_train_step , y_train_step ) # The last time window of training data is stored so that lags needed as # predictors in the first iteration of `predict()` can be calculated. if self . steps >= self . max_lag : self . last_window = y_train [ - 1 , - self . max_lag :] else : self . last_window = np . hstack (( y_train [ - ( self . max_lag - self . steps + 1 ): - 1 , 0 ], y_train [ - 1 , :] )) get_coef ( self , step ) \u00b6 Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters \u00b6 step : int Model from which retireve information (a separate model is created for each forecast time step). Returns \u00b6 coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoregMultiOutput.py def get_coef ( self , step ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- step : int Model from which retireve information (a separate model is created for each forecast time step). Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that `coef[i]` is the value associated with `self.lags[i]`. ''' if step > self . steps : raise Exception ( f \"Forecaster traied for { self . steps } steps. Got step= { step } .\" ) valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressors_ [ step - 1 ] . coef_ return coef get_feature_importances ( self , step ) \u00b6 Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor . Parameters \u00b6 step : int Model from which retireve information (a separate model is created for each forecast time step). Returns \u00b6 feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoregMultiOutput.py def get_feature_importances ( self , step ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- step : int Model from which retireve information (a separate model is created for each forecast time step). Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that `feature_importances[i]` is the value associated with `self.lags[i]`. ''' if step > self . steps : raise Exception ( f \"Forecaster traied for { self . steps } steps. Got step= { step } .\" ) valid_instances = ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressors_ [ step - 1 ] . feature_importances_ return feature_importances predict ( self , last_window = None , exog = None , steps = None ) \u00b6 Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. Parameters \u00b6 last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. steps : Ignored Not used, present here for API consistency by convention. Returns \u00b6 predictions : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoregMultiOutput.py def predict ( self , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , steps = None ): ''' Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. Parameters ---------- last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. steps : Ignored Not used, present here for API consistency by convention. Returns ------- predictions : 1D np.array, shape (steps,) Values predicted. ''' if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < self . steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) exog = self . _exog_to_multi_output ( exog = exog ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = self . steps , fill_value = np . nan ) X_lags = last_window [ - self . lags ] . reshape ( 1 , - 1 ) for step in range ( self . steps ): regressor = self . regressors_ [ step ] if exog is None : X = X_lags predictions [ step ] = regressor . predict ( X ) else : # Only columns from exog related with the current step are selected. X = np . hstack ([ X_lags , exog [ 0 ][ step :: self . steps ] . reshape ( 1 , - 1 )]) predictions [ step ] = regressor . predict ( X ) return predictions . reshape ( - 1 ) set_lags ( self , lags ) \u00b6 Set new value to the attribute lags . Attribute max_lag is also updated. Parameters \u00b6 lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags . list or np.array : include only lags present in lags . Returns \u00b6 self Source code in skforecast/ForecasterAutoregMultiOutput.py def set_lags ( self , lags : int ) -> None : ''' Set new value to the attribute `lags`. Attribute `max_lag` is also updated. Parameters ---------- lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. `int`: include lags from 1 to `lags`. `list` or `np.array`: include only lags present in `lags`. Returns ------- self ''' if isinstance ( lags , int ) and lags < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , ( list , range , np . ndarray )) and min ( lags ) < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , int ): self . lags = np . arange ( lags ) + 1 elif isinstance ( lags , ( list , range )): self . lags = np . array ( lags ) elif isinstance ( lags , np . ndarray ): self . lags = lags else : raise Exception ( f \"`lags` argument must be `int`, `1D np.ndarray`, `range` or `list`. \" f \"Got { type ( lags ) } \" ) self . max_lag = max ( self . lags ) set_params ( self , ** params ) \u00b6 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. Parameters \u00b6 params : dict Parameters values. Returns \u00b6 self Source code in skforecast/ForecasterAutoregMultiOutput.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params ) self . regressors_ = { step : clone ( self . regressor ) for step in range ( self . steps )}","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#forecasterautoregmultioutput","text":"This class turns a scikit-learn regressor into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details.","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput--parameters","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list or np.array : include only lags present in lags . steps : int Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training.","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput--attributes","text":"regressor : scikit-learn regressor An instance of a scikit-learn regressor. One instance of this regressor is trainned for each step. All them are stored in slef.regressors_ . regressors_ : dict Dictionary with scikit-learn regressors trained for each step. steps : int Number of future steps the forecaster will predict when using method predict() . Since a diferent model is created for each step, this value should be defined before training. lags : 1D np.array Lags used as predictors. max_lag : int Maximum value of lag included in lags. last_window : 1D np.ndarray Last time window the forecaster has seen when trained. It stores the values needed to calculate the lags used to predict the next step after the training data. included_exog : bool If the forecaster has been trained using exogenous variable/s. exog_type : type Type used for the exogenous variable/s. exog_shape : tuple Shape of exog used in training.","title":"Attributes"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput--notes","text":"A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hiperparameters.","title":"Notes"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.__repr__","text":"Information displayed when a ForecasterAutoreg object is printed. Source code in skforecast/ForecasterAutoregMultiOutput.py def __repr__ ( self ) -> str : ''' Information displayed when a ForecasterAutoreg object is printed. ''' info = \"============================\" \\ + \"ForecasterAutoregMultiOutput\" \\ + \"============================\" \\ + \" \\n \" \\ + \"Regressor: \" + str ( self . regressor ) \\ + \" \\n \" \\ + \"Steps: \" + str ( self . steps ) \\ + \" \\n \" \\ + \"Lags: \" + str ( self . lags ) \\ + \" \\n \" \\ + \"Exogenous variable: \" + str ( self . included_exog ) \\ + \" \\n \" \\ + \"Parameters: \" + str ( self . regressor . get_params ()) return info","title":"__repr__()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_lags","text":"Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.","title":"create_lags()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_lags--parameters","text":"y : 1D np.ndarray, pd.Series Training time series.","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_lags--returns","text":"X_data : 2D np.ndarray 2D array with the lag values (predictors). y_data : 2D np.ndarray Values of the time series related to each row of X_data . Source code in skforecast/ForecasterAutoregMultiOutput.py def create_lags ( self , y : Union [ np . ndarray , pd . Series ]) -> Tuple [ np . ndarray , np . ndarray ]: ''' Transforms a time series into two 2D arrays of pairs predictor-response. Notice that the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. Returns ------- X_data : 2D np.ndarray 2D array with the lag values (predictors). y_data : 2D np.ndarray Values of the time series related to each row of `X_data`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if self . max_lag > len ( y ): raise Exception ( f \"Maximum lag can't be higer than `y` length. \" f \"Got maximum lag= { self . max_lag } and `y` length= { len ( y ) } .\" ) n_splits = len ( y ) - self . max_lag - ( self . steps - 1 ) X_data = np . full ( shape = ( n_splits , self . max_lag ), fill_value = np . nan , dtype = float ) y_data = np . full ( shape = ( n_splits , self . steps ), fill_value = np . nan , dtype = float ) for i in range ( n_splits ): X_index = np . arange ( i , self . max_lag + i ) y_index = np . arange ( self . max_lag + i , self . max_lag + i + self . steps ) X_data [ i , :] = y [ X_index ] y_data [ i , :] = y [ y_index ] X_data = X_data [:, - self . lags ] return X_data , y_data","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_train_X_y","text":"Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step).","title":"create_train_X_y()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_train_X_y--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.create_train_X_y--returns","text":"X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregMultiOutput.py def create_train_X_y ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> Tuple [ np . array , np . array ]: ''' Create training matrices X, y. The created matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- X_train : 2D np.ndarray, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps) 2D array with the training values (predictors). y_train : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of `X_train`. ''' self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_lags , y_train = self . create_lags ( y = y ) if exog is None : X_train = X_lags else : # Trasform exog to match multi output format X_exog = self . _exog_to_multi_output ( exog = exog ) # The first `self.max_lag` positions have to be removed from X_exog # since they are not in X_lags. X_exog = X_exog [ - X_lags . shape [ 0 ]:, ] X_train = np . column_stack (( X_lags , X_exog )) return X_train , y_train","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.filter_train_X_y_for_step","text":"Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with create_train_X_y() .","title":"filter_train_X_y_for_step()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.filter_train_X_y_for_step--parameters","text":"step : int step for which columns must be selected selected. Starts at 0. X_train : 2D np.ndarray 2D array with the training values (predictors). y_train : 1D np.ndarray Values (target) of the time series related to each row of X_train .","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.filter_train_X_y_for_step--returns","text":"X_train_step : 2D np.ndarray 2D array with the training values (predictors) for step. y_train_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . Source code in skforecast/ForecasterAutoregMultiOutput.py def filter_train_X_y_for_step ( self , step : int , X_train : np . ndarray , y_train : np . ndarray ) -> Tuple [ np . array , np . array ]: ''' Select columns needed to train a forcaster for a specific step. The imput matrices should be created with created with `create_train_X_y()`. Parameters ---------- step : int step for which columns must be selected selected. Starts at 0. X_train : 2D np.ndarray 2D array with the training values (predictors). y_train : 1D np.ndarray Values (target) of the time series related to each row of `X_train`. Returns ------- X_train_step : 2D np.ndarray 2D array with the training values (predictors) for step. y_train_step : 1D np.ndarray, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of `X_train`. ''' if step > self . steps - 1 : raise Exception ( f \"Invalid value `step`. For this forecaster, the maximum step is { self . steps - 1 } .\" ) y_train_step = y_train [:, step ] if not self . included_exog : X_train_step = X_train else : idx_columns_lags = np . arange ( len ( self . lags )) idx_columns_exog = np . arange ( X_train . shape [ 1 ])[ len ( self . lags ) + step :: self . steps ] idx_columns = np . hstack (( idx_columns_lags , idx_columns_exog )) X_train_step = X_train [:, idx_columns ] return X_train_step , y_train_step","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.fit","text":"Training ForecasterAutoregMultiOutput","title":"fit()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.fit--parameters","text":"y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i].","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.fit--returns","text":"self : ForecasterAutoregMultiOutput Trained ForecasterAutoregMultiOutput Source code in skforecast/ForecasterAutoregMultiOutput.py def fit ( self , y : Union [ np . ndarray , pd . Series ], exog : Union [ np . ndarray , pd . Series ] = None ) -> None : ''' Training ForecasterAutoregMultiOutput Parameters ---------- y : 1D np.ndarray, pd.Series Training time series. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. Returns ------- self : ForecasterAutoregMultiOutput Trained ForecasterAutoregMultiOutput ''' # Reset values in case the forecaster has already been fitted before. self . included_exog = False self . exog_type = None self . exog_shape = None self . _check_y ( y = y ) y = self . _preproces_y ( y = y ) if exog is not None : self . _check_exog ( exog = exog ) self . exog_type = type ( exog ) exog = self . _preproces_exog ( exog = exog ) self . included_exog = True self . exog_shape = exog . shape if exog . shape [ 0 ] != len ( y ): raise Exception ( f \"`exog` must have same number of samples as `y`\" ) X_train , y_train = self . create_train_X_y ( y = y , exog = exog ) # Train one regressor for each step for step in range ( self . steps ): X_train_step , y_train_step = self . filter_train_X_y_for_step ( step = step , X_train = X_train , y_train = y_train ) self . regressors_ [ step ] . fit ( X_train_step , y_train_step ) # The last time window of training data is stored so that lags needed as # predictors in the first iteration of `predict()` can be calculated. if self . steps >= self . max_lag : self . last_window = y_train [ - 1 , - self . max_lag :] else : self . last_window = np . hstack (( y_train [ - ( self . max_lag - self . steps + 1 ): - 1 , 0 ], y_train [ - 1 , :] ))","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_coef","text":"Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`.","title":"get_coef()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_coef--parameters","text":"step : int Model from which retireve information (a separate model is created for each forecast time step).","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_coef--returns","text":"coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that coef[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoregMultiOutput.py def get_coef ( self , step ) -> np . ndarray : ''' Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using as `regressor: `LinearRegression()`, `Lasso()` or `Ridge()`. Parameters ---------- step : int Model from which retireve information (a separate model is created for each forecast time step). Returns ------- coef : 1D np.ndarray Value of the coefficients associated with each predictor (lag). Coefficients are aligned so that `coef[i]` is the value associated with `self.lags[i]`. ''' if step > self . steps : raise Exception ( f \"Forecaster traied for { self . steps } steps. Got step= { step } .\" ) valid_instances = ( sklearn . linear_model . _base . LinearRegression , sklearn . linear_model . _coordinate_descent . Lasso , sklearn . linear_model . _ridge . Ridge ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor` `LinearRegression()`, ' + ' `Lasso()` or `Ridge()` have coef.' ) ) return else : coef = self . regressors_ [ step - 1 ] . coef_ return coef","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_feature_importances","text":"Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using regressor=GradientBoostingRegressor() or regressor=RandomForestRegressor .","title":"get_feature_importances()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_feature_importances--parameters","text":"step : int Model from which retireve information (a separate model is created for each forecast time step).","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.get_feature_importances--returns","text":"feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that feature_importances[i] is the value associated with self.lags[i] . Source code in skforecast/ForecasterAutoregMultiOutput.py def get_feature_importances ( self , step ) -> np . ndarray : ''' Return impurity-based feature importances of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retireve information. Only valid when the forecaster has been trained using `regressor=GradientBoostingRegressor()` or `regressor=RandomForestRegressor`. Parameters ---------- step : int Model from which retireve information (a separate model is created for each forecast time step). Returns ------- feature_importances : 1D np.ndarray Impurity-based feature importances associated with each predictor (lag). Values are aligned so that `feature_importances[i]` is the value associated with `self.lags[i]`. ''' if step > self . steps : raise Exception ( f \"Forecaster traied for { self . steps } steps. Got step= { step } .\" ) valid_instances = ( sklearn . ensemble . _forest . RandomForestRegressor , sklearn . ensemble . _gb . GradientBoostingRegressor ) if not isinstance ( self . regressor , valid_instances ): warnings . warn ( ( 'Only forecasters with `regressor=GradientBoostingRegressor()` ' 'or `regressor=RandomForestRegressor`.' ) ) return else : feature_importances = self . regressors_ [ step - 1 ] . feature_importances_ return feature_importances","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.predict","text":"Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster.","title":"predict()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.predict--parameters","text":"last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default None Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If ` last_window = None `, the values stored in ` self . last_window ` are used to calculate the initial predictors , and the predictions start right after training data . exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. steps : Ignored Not used, present here for API consistency by convention.","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.predict--returns","text":"predictions : 1D np.array, shape (steps,) Values predicted. Source code in skforecast/ForecasterAutoregMultiOutput.py def predict ( self , last_window : Union [ np . ndarray , pd . Series ] = None , exog : np . ndarray = None , steps = None ): ''' Multi-step prediction. The number of future steps predicted is defined when ininitializing the forecaster. Parameters ---------- last_window : 1D np.ndarray, pd.Series, shape (, max_lag), default `None` Values of the series used to create the predictors (lags) need in the first iteration of predictiont (t + 1). If `last_window = None`, the values stored in` self.last_window` are used to calculate the initial predictors, and the predictions start right after training data. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. steps : Ignored Not used, present here for API consistency by convention. Returns ------- predictions : 1D np.array, shape (steps,) Values predicted. ''' if exog is None and self . included_exog : raise Exception ( f \"Forecaster trained with exogenous variable/s. \" f \"Same variable/s must be provided in `predict()`.\" ) if exog is not None and not self . included_exog : raise Exception ( f \"Forecaster trained without exogenous variable/s. \" f \"`exog` must be `None` in `predict()`.\" ) if exog is not None : self . _check_exog ( exog = exog , ref_type = self . exog_type , ref_shape = self . exog_shape ) exog = self . _preproces_exog ( exog = exog ) if exog . shape [ 0 ] < self . steps : raise Exception ( f \"`exog` must have at least as many values as `steps` predicted.\" ) exog = self . _exog_to_multi_output ( exog = exog ) if last_window is not None : self . _check_last_window ( last_window = last_window ) last_window = self . _preproces_last_window ( last_window = last_window ) if last_window . shape [ 0 ] < self . max_lag : raise Exception ( f \"`last_window` must have as many values as as needed to \" f \"calculate the maximum lag ( { self . max_lag } ).\" ) else : last_window = self . last_window . copy () predictions = np . full ( shape = self . steps , fill_value = np . nan ) X_lags = last_window [ - self . lags ] . reshape ( 1 , - 1 ) for step in range ( self . steps ): regressor = self . regressors_ [ step ] if exog is None : X = X_lags predictions [ step ] = regressor . predict ( X ) else : # Only columns from exog related with the current step are selected. X = np . hstack ([ X_lags , exog [ 0 ][ step :: self . steps ] . reshape ( 1 , - 1 )]) predictions [ step ] = regressor . predict ( X ) return predictions . reshape ( - 1 )","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_lags","text":"Set new value to the attribute lags . Attribute max_lag is also updated.","title":"set_lags()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_lags--parameters","text":"lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags . list or np.array : include only lags present in lags .","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_lags--returns","text":"self Source code in skforecast/ForecasterAutoregMultiOutput.py def set_lags ( self , lags : int ) -> None : ''' Set new value to the attribute `lags`. Attribute `max_lag` is also updated. Parameters ---------- lags : int, list, 1D np.array, range Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. `int`: include lags from 1 to `lags`. `list` or `np.array`: include only lags present in `lags`. Returns ------- self ''' if isinstance ( lags , int ) and lags < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , ( list , range , np . ndarray )) and min ( lags ) < 1 : raise Exception ( 'min value of lags allowed is 1' ) if isinstance ( lags , int ): self . lags = np . arange ( lags ) + 1 elif isinstance ( lags , ( list , range )): self . lags = np . array ( lags ) elif isinstance ( lags , np . ndarray ): self . lags = lags else : raise Exception ( f \"`lags` argument must be `int`, `1D np.ndarray`, `range` or `list`. \" f \"Got { type ( lags ) } \" ) self . max_lag = max ( self . lags )","title":"Returns"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_params","text":"Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters.","title":"set_params()"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_params--parameters","text":"params : dict Parameters values.","title":"Parameters"},{"location":"api/ForecasterAutoregMultiOutput.html#skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput.set_params--returns","text":"self Source code in skforecast/ForecasterAutoregMultiOutput.py def set_params ( self , ** params : dict ) -> None : ''' Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hiperparameters. Parameters ---------- params : dict Parameters values. Returns ------- self ''' self . regressor . set_params ( ** params ) self . regressors_ = { step : clone ( self . regressor ) for step in range ( self . steps )}","title":"Returns"},{"location":"api/model_selection.html","text":"model_selection \u00b6 backtesting_forecaster ( forecaster , y , initial_train_size , steps , metric , exog = None , verbose = False ) \u00b6 Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters \u00b6 forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose : bool, default False Print number of folds used for backtesting. Returns \u00b6 !!! backtest_predictions \"1D np.ndarray\" Value of predictions. !!! metric_value \"np.ndarray shape (1,)\" Value of the metric. Source code in skforecast/model_selection.py def backtesting_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , verbose : bool = False ) -> Tuple [ np . array , np . array ]: ''' Backtesting (validation) of `ForecasterAutoreg`, `ForecasterCustom`, `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. The model is trained only once using the `initial_train_size` first observations. In each iteration, a number of `steps` predictions are evaluated. This evaluation is much faster than `cv_forecaster()` since the model is trained only once. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. verbose : bool, default `False` Print number of folds used for backtesting. Returns ------- backtest_predictions: 1D np.ndarray Value of predictions. metric_value: np.ndarray shape (1,) Value of the metric. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) backtest_predictions = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] if exog is None : forecaster . fit ( y = y [: initial_train_size ]) else : forecaster . fit ( y = y [: initial_train_size ], exog = exog [: initial_train_size ]) folds = ( len ( y ) - initial_train_size ) // steps + 1 remainder = ( len ( y ) - initial_train_size ) % steps window_size = len ( forecaster . last_window ) if isinstance ( forecaster , ForecasterAutoregMultiOutput ) and remainder != 0 : # In ForecasterAutoregMultiOutput predictions are not iterative, # therefore no remainder is allowed. logging . warning ( f \"Backtesting `ForecasterAutoregMultiOutput` only allow completed \" f \"folds. Last { remainder } observations are excluded.\" ) remainder = 0 if verbose : print ( f \"Number of observations used for training: { initial_train_size } \" ) print ( f \"Number of observations used for testing: { len ( y ) - initial_train_size } \" ) print ( f \" Number of folds: { folds - 1 * ( remainder == 0 ) } \" ) print ( f \" Number of steps per fold: { steps } \" ) if remainder != 0 : print ( f \" Last fold only includes { remainder } observations\" ) for i in range ( folds ): last_window_end = initial_train_size + i * steps last_window_start = ( initial_train_size + i * steps ) - window_size last_window = y [ last_window_start : last_window_end ] if i < folds - 1 : if exog is None : pred = forecaster . predict ( steps = steps , last_window = last_window ) else : pred = forecaster . predict ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ] ) elif remainder != 0 : steps = remainder if exog is None : pred = forecaster . predict ( steps = steps , last_window = last_window ) else : pred = forecaster . predict ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ] ) else : continue backtest_predictions . append ( pred ) backtest_predictions = np . concatenate ( backtest_predictions ) metric_value = metric ( y_true = y [ initial_train_size : initial_train_size + len ( backtest_predictions )], y_pred = backtest_predictions ) return np . array ([ metric_value ]), backtest_predictions backtesting_forecaster_intervals ( forecaster , y , initial_train_size , steps , metric , exog = None , interval = [ 5 , 95 ], n_boot = 500 , in_sample_residuals = True , verbose = False ) \u00b6 Backtesting (validation) of ForecasterAutoreg , ForecasterCustom or ForecasterAutoregCustom object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. Both, predictions and intervals, are calculated. This evaluation is much faster than cv_forecaster() since the model is trained only once. Parameters \u00b6 forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom ForecasterAutoreg , ForecasterCustom or ForecasterAutoregCustom object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. verbose : bool, default True Print number of folds used for backtesting. Returns \u00b6 !!! backtest_predictions \"np.ndarray\" If include_intervals=True , 2D np.ndarray shape(steps, 3) with predicted value and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval !!! metric_value \"np.ndarray shape (1,)\" Value of the metric. Notes \u00b6 More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/model_selection.py def backtesting_forecaster_intervals ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True , verbose : bool = False ) -> Tuple [ np . array , np . array ]: ''' Backtesting (validation) of `ForecasterAutoreg`, `ForecasterCustom` or `ForecasterAutoregCustom` object. The model is trained only once using the `initial_train_size` first observations. In each iteration, a number of `steps` predictions are evaluated. Both, predictions and intervals, are calculated. This evaluation is much faster than `cv_forecaster()` since the model is trained only once. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom `ForecasterAutoreg`, `ForecasterCustom` or `ForecasterAutoregCustom` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. verbose : bool, default `True` Print number of folds used for backtesting. Returns ------- backtest_predictions: np.ndarray If `include_intervals=True`, 2D np.ndarray shape(steps, 3) with predicted value and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval metric_value: np.ndarray shape (1,) Value of the metric. Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) backtest_predictions = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] if exog is None : forecaster . fit ( y = y [: initial_train_size ]) else : forecaster . fit ( y = y [: initial_train_size ], exog = exog [: initial_train_size ]) folds = ( len ( y ) - initial_train_size ) // steps + 1 remainder = ( len ( y ) - initial_train_size ) % steps window_size = len ( forecaster . last_window ) if verbose : print ( f \"Number of observations used for training: { initial_train_size } \" ) print ( f \"Number of observations used for testing: { len ( y ) - initial_train_size } \" ) print ( f \" Number of folds: { folds - 1 * ( remainder == 0 ) } \" ) print ( f \" Number of steps per fold: { steps } \" ) if remainder != 0 : print ( f \" Last fold only includes { remainder } observations\" ) for i in range ( folds ): last_window_end = initial_train_size + i * steps last_window_start = ( initial_train_size + i * steps ) - window_size last_window = y [ last_window_start : last_window_end ] if i < folds - 1 : if exog is None : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ], interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) elif remainder != 0 : steps = remainder if exog is None : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ], interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : continue backtest_predictions . append ( pred ) backtest_predictions = np . concatenate ( backtest_predictions ) metric_value = metric ( y_true = y [ initial_train_size :], y_pred = backtest_predictions [:, 0 ] ) return np . array ([ metric_value ]), backtest_predictions cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog = None , allow_incomplete_fold = True , verbose = True ) \u00b6 Cross-validation of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The order of is maintained and the training set increases in each iteration. Parameters \u00b6 forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose : bool, default True Print number of folds used for cross validation. Returns \u00b6 !!! cv_results \"1D np.ndarray\" Value of the metric for each partition. Source code in skforecast/model_selection.py def cv_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , allow_incomplete_fold : bool = True , verbose : bool = True ) -> np . array : ''' Cross-validation of `ForecasterAutoreg`, `ForecasterCustom`, `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. The order of is maintained and the training set increases in each iteration. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. verbose : bool, default `True` Print number of folds used for cross validation. Returns ------- cv_results: 1D np.ndarray Value of the metric for each partition. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) cv_results = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] splits = time_series_spliter ( y = y , initial_train_size = initial_train_size , steps = steps , allow_incomplete_fold = allow_incomplete_fold , verbose = verbose ) for train_index , test_index in splits : if exog is None : forecaster . fit ( y = y [ train_index ]) pred = forecaster . predict ( steps = len ( test_index )) else : forecaster . fit ( y = y [ train_index ], exog = exog [ train_index ]) pred = forecaster . predict ( steps = len ( test_index ), exog = exog [ test_index ]) metric_value = metric ( y_true = y [ test_index ], y_pred = pred ) cv_results . append ( metric_value ) return np . array ( cv_results ) grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog = None , lags_grid = None , method = 'cv' , allow_incomplete_fold = True , return_best = True , verbose = True ) \u00b6 Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting. Parameters \u00b6 forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. param_grid : dict Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid : list of int, lists, np.narray or range. Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . method : {'cv', 'backtesting'} Method used to estimate the metric for each parameter combination. 'cv' for time series crosvalidation and 'backtesting' for simple backtesting. 'backtesting' is much faster since the model is fitted only once. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. return_best : bool Refit the forecaster using the best found parameters on the whole data. verbose : bool, default True Print number of folds used for cv or backtesting. Returns \u00b6 !!! results \"pandas.DataFrame\" Metric value estimated for each combination of parameters. Source code in skforecast/model_selection.py def grid_search_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], param_grid : dict , initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , lags_grid : list = None , method : str = 'cv' , allow_incomplete_fold : bool = True , return_best : bool = True , verbose : bool = True ) -> pd . DataFrame : ''' Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. param_grid : dict Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. lags_grid : list of int, lists, np.narray or range. Lists of `lags` to try. Only used if forecaster is an instance of `ForecasterAutoreg`. method : {'cv', 'backtesting'} Method used to estimate the metric for each parameter combination. 'cv' for time series crosvalidation and 'backtesting' for simple backtesting. 'backtesting' is much faster since the model is fitted only once. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. return_best : bool Refit the `forecaster` using the best found parameters on the whole data. verbose : bool, default `True` Print number of folds used for cv or backtesting. Returns ------- results: pandas.DataFrame Metric value estimated for each combination of parameters. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if isinstance ( forecaster , ( ForecasterCustom , ForecasterAutoregCustom )): if lags_grid is not None : logging . warning ( '`lags_grid` ignored if forecaster is an instance of `ForecasterCustom` or `ForecasterAutoregCustom`.' ) lags_grid = [ 'custom predictors' ] elif lags_grid is None : lags_grid = [ forecaster . lags ] lags_list = [] params_list = [] metric_list = [] param_grid = list ( ParameterGrid ( param_grid )) logging . info ( f \"Number of models to fit: { len ( param_grid ) * len ( lags_grid ) } \" ) for lags in tqdm . tqdm ( lags_grid , desc = 'loop lags_grid' , position = 0 ): if isinstance ( forecaster , ( ForecasterAutoreg , ForecasterAutoregMultiOutput )): forecaster . set_lags ( lags ) lags = forecaster . lags . copy () for params in tqdm . tqdm ( param_grid , desc = 'loop param_grid' , position = 1 , leave = False ): forecaster . set_params ( ** params ) if method == 'cv' : metrics = cv_forecaster ( forecaster = forecaster , y = y , exog = exog , initial_train_size = initial_train_size , steps = steps , metric = metric , allow_incomplete_fold = allow_incomplete_fold , verbose = verbose ) else : metrics = backtesting_forecaster ( forecaster = forecaster , y = y , exog = exog , initial_train_size = initial_train_size , steps = steps , metric = metric , verbose = verbose )[ 0 ] lags_list . append ( lags ) params_list . append ( params ) metric_list . append ( metrics . mean ()) results = pd . DataFrame ({ 'lags' : lags_list , 'params' : params_list , 'metric' : metric_list }) results = results . sort_values ( by = 'metric' , ascending = True ) if return_best : best_lags = results [ 'lags' ] . iloc [ 0 ] best_params = results [ 'params' ] . iloc [ 0 ] logging . info ( f \"Refitting `forecaster` using the best found parameters and the whole data set: \\n \" f \"lags: { best_lags } \\n \" f \"params: { best_params } \\n \" ) if isinstance ( forecaster , ( ForecasterAutoreg , ForecasterAutoregMultiOutput )): forecaster . set_lags ( best_lags ) forecaster . set_params ( ** best_params ) forecaster . fit ( y = y , exog = exog ) return results time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold = True , verbose = True ) \u00b6 Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters \u00b6 y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose : bool, default True Print number of splits created. Yields \u00b6 train : 1D np.ndarray Training indices. test : 1D np.ndarray Test indices. Source code in skforecast/model_selection.py def time_series_spliter ( y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , allow_incomplete_fold : bool = True , verbose : bool = True ): ''' Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. verbose : bool, default `True` Print number of splits created. Yields ------ train : 1D np.ndarray Training indices. test : 1D np.ndarray Test indices. ''' if not isinstance ( y , ( np . ndarray , pd . Series )): raise Exception ( '`y` must be `1D np.ndarray` o `pd.Series`.' ) elif isinstance ( y , np . ndarray ) and y . ndim != 1 : raise Exception ( f \"`y` must be `1D np.ndarray` o `pd.Series`, \" f \"got `np.ndarray` with { y . ndim } dimensions.\" ) if isinstance ( y , pd . Series ): y = y . to_numpy () . copy () folds = ( len ( y ) - initial_train_size ) // steps + 1 # +1 fold is needed to allow including the remainder in the last iteration. remainder = ( len ( y ) - initial_train_size ) % steps if verbose : if folds == 1 : print ( f \"Number of folds: { folds - 1 } \" ) print ( \"Not enought observations in `y` to create to create even a complete fold.\" ) elif remainder == 0 : print ( f \"Number of folds: { folds - 1 } \" ) elif remainder != 0 and allow_incomplete_fold : print ( f \"Number of folds: { folds } \" ) print ( f \"Since `allow_incomplete_fold=True`, \" f \"last fold only includes { remainder } observations instead of { steps } .\" ) print ( 'Incomplete folds with few observations could overestimate or ' , 'underestimate validation metrics.' ) elif remainder != 0 and not allow_incomplete_fold : print ( f \"Number of folds: { folds - 1 } \" ) print ( f \"Since `allow_incomplete_fold=False`, \" f \"last { remainder } observations are descarted.\" ) if folds == 1 : # There are no observations to create even a complete fold return [] for i in range ( folds ): if i < folds - 1 : train_end = initial_train_size + i * steps train_indices = range ( train_end ) test_indices = range ( train_end , train_end + steps ) else : if remainder != 0 and allow_incomplete_fold : train_end = initial_train_size + i * steps train_indices = range ( train_end ) test_indices = range ( train_end , len ( y )) else : break yield train_indices , test_indices","title":"model_selection"},{"location":"api/model_selection.html#model_selection","text":"","title":"model_selection"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster","text":"Backtesting (validation) of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. This evaluation is much faster than cv_forecaster() since the model is trained only once.","title":"backtesting_forecaster()"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster--parameters","text":"forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. verbose : bool, default False Print number of folds used for backtesting.","title":"Parameters"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster--returns","text":"!!! backtest_predictions \"1D np.ndarray\" Value of predictions. !!! metric_value \"np.ndarray shape (1,)\" Value of the metric. Source code in skforecast/model_selection.py def backtesting_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , verbose : bool = False ) -> Tuple [ np . array , np . array ]: ''' Backtesting (validation) of `ForecasterAutoreg`, `ForecasterCustom`, `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. The model is trained only once using the `initial_train_size` first observations. In each iteration, a number of `steps` predictions are evaluated. This evaluation is much faster than `cv_forecaster()` since the model is trained only once. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. verbose : bool, default `False` Print number of folds used for backtesting. Returns ------- backtest_predictions: 1D np.ndarray Value of predictions. metric_value: np.ndarray shape (1,) Value of the metric. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) backtest_predictions = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] if exog is None : forecaster . fit ( y = y [: initial_train_size ]) else : forecaster . fit ( y = y [: initial_train_size ], exog = exog [: initial_train_size ]) folds = ( len ( y ) - initial_train_size ) // steps + 1 remainder = ( len ( y ) - initial_train_size ) % steps window_size = len ( forecaster . last_window ) if isinstance ( forecaster , ForecasterAutoregMultiOutput ) and remainder != 0 : # In ForecasterAutoregMultiOutput predictions are not iterative, # therefore no remainder is allowed. logging . warning ( f \"Backtesting `ForecasterAutoregMultiOutput` only allow completed \" f \"folds. Last { remainder } observations are excluded.\" ) remainder = 0 if verbose : print ( f \"Number of observations used for training: { initial_train_size } \" ) print ( f \"Number of observations used for testing: { len ( y ) - initial_train_size } \" ) print ( f \" Number of folds: { folds - 1 * ( remainder == 0 ) } \" ) print ( f \" Number of steps per fold: { steps } \" ) if remainder != 0 : print ( f \" Last fold only includes { remainder } observations\" ) for i in range ( folds ): last_window_end = initial_train_size + i * steps last_window_start = ( initial_train_size + i * steps ) - window_size last_window = y [ last_window_start : last_window_end ] if i < folds - 1 : if exog is None : pred = forecaster . predict ( steps = steps , last_window = last_window ) else : pred = forecaster . predict ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ] ) elif remainder != 0 : steps = remainder if exog is None : pred = forecaster . predict ( steps = steps , last_window = last_window ) else : pred = forecaster . predict ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ] ) else : continue backtest_predictions . append ( pred ) backtest_predictions = np . concatenate ( backtest_predictions ) metric_value = metric ( y_true = y [ initial_train_size : initial_train_size + len ( backtest_predictions )], y_pred = backtest_predictions ) return np . array ([ metric_value ]), backtest_predictions","title":"Returns"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster_intervals","text":"Backtesting (validation) of ForecasterAutoreg , ForecasterCustom or ForecasterAutoregCustom object. The model is trained only once using the initial_train_size first observations. In each iteration, a number of steps predictions are evaluated. Both, predictions and intervals, are calculated. This evaluation is much faster than cv_forecaster() since the model is trained only once.","title":"backtesting_forecaster_intervals()"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster_intervals--parameters","text":"forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom ForecasterAutoreg , ForecasterCustom or ForecasterAutoregCustom object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. !!! interval \"list, default [5, 100] \" Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. !!! n_boot \"int, default 500 \" Number of bootstrapping iterations used to estimate prediction intervals. !!! in_sample_residuals \"bool, default True \" If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. verbose : bool, default True Print number of folds used for backtesting.","title":"Parameters"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster_intervals--returns","text":"!!! backtest_predictions \"np.ndarray\" If include_intervals=True , 2D np.ndarray shape(steps, 3) with predicted value and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval !!! metric_value \"np.ndarray shape (1,)\" Value of the metric.","title":"Returns"},{"location":"api/model_selection.html#skforecast.model_selection.backtesting_forecaster_intervals--notes","text":"More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. Source code in skforecast/model_selection.py def backtesting_forecaster_intervals ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , interval : list = [ 5 , 95 ], n_boot : int = 500 , in_sample_residuals : bool = True , verbose : bool = False ) -> Tuple [ np . array , np . array ]: ''' Backtesting (validation) of `ForecasterAutoreg`, `ForecasterCustom` or `ForecasterAutoregCustom` object. The model is trained only once using the `initial_train_size` first observations. In each iteration, a number of `steps` predictions are evaluated. Both, predictions and intervals, are calculated. This evaluation is much faster than `cv_forecaster()` since the model is trained only once. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom `ForecasterAutoreg`, `ForecasterCustom` or `ForecasterAutoregCustom` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. interval: list, default `[5, 100]` Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot: int, default `500` Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals: bool, default `True` If `True`, residuals from the training data are used as proxy of prediction error to create prediction intervals. verbose : bool, default `True` Print number of folds used for backtesting. Returns ------- backtest_predictions: np.ndarray If `include_intervals=True`, 2D np.ndarray shape(steps, 3) with predicted value and their estimated interval. Column 0 = predictions Column 1 = lower bound interval Column 2 = upper bound interval metric_value: np.ndarray shape (1,) Value of the metric. Notes ----- More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) backtest_predictions = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] if exog is None : forecaster . fit ( y = y [: initial_train_size ]) else : forecaster . fit ( y = y [: initial_train_size ], exog = exog [: initial_train_size ]) folds = ( len ( y ) - initial_train_size ) // steps + 1 remainder = ( len ( y ) - initial_train_size ) % steps window_size = len ( forecaster . last_window ) if verbose : print ( f \"Number of observations used for training: { initial_train_size } \" ) print ( f \"Number of observations used for testing: { len ( y ) - initial_train_size } \" ) print ( f \" Number of folds: { folds - 1 * ( remainder == 0 ) } \" ) print ( f \" Number of steps per fold: { steps } \" ) if remainder != 0 : print ( f \" Last fold only includes { remainder } observations\" ) for i in range ( folds ): last_window_end = initial_train_size + i * steps last_window_start = ( initial_train_size + i * steps ) - window_size last_window = y [ last_window_start : last_window_end ] if i < folds - 1 : if exog is None : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ], interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) elif remainder != 0 : steps = remainder if exog is None : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : pred = forecaster . predict_interval ( steps = steps , last_window = last_window , exog = exog [ last_window_end : last_window_end + steps ], interval = interval , n_boot = n_boot , in_sample_residuals = in_sample_residuals ) else : continue backtest_predictions . append ( pred ) backtest_predictions = np . concatenate ( backtest_predictions ) metric_value = metric ( y_true = y [ initial_train_size :], y_pred = backtest_predictions [:, 0 ] ) return np . array ([ metric_value ]), backtest_predictions","title":"Notes"},{"location":"api/model_selection.html#skforecast.model_selection.cv_forecaster","text":"Cross-validation of ForecasterAutoreg , ForecasterCustom , ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. The order of is maintained and the training set increases in each iteration.","title":"cv_forecaster()"},{"location":"api/model_selection.html#skforecast.model_selection.cv_forecaster--parameters","text":"forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose : bool, default True Print number of folds used for cross validation.","title":"Parameters"},{"location":"api/model_selection.html#skforecast.model_selection.cv_forecaster--returns","text":"!!! cv_results \"1D np.ndarray\" Value of the metric for each partition. Source code in skforecast/model_selection.py def cv_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , allow_incomplete_fold : bool = True , verbose : bool = True ) -> np . array : ''' Cross-validation of `ForecasterAutoreg`, `ForecasterCustom`, `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. The order of is maintained and the training set increases in each iteration. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. verbose : bool, default `True` Print number of folds used for cross validation. Returns ------- cv_results: 1D np.ndarray Value of the metric for each partition. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if metric not in [ 'mean_squared_error' , 'mean_absolute_error' , 'mean_absolute_percentage_error' ]: raise Exception ( f \"Allowed metrics are: 'mean_squared_error', 'mean_absolute_error' and \" f \"'mean_absolute_percentage_error'. Got { metric } .\" ) cv_results = [] metrics = { 'mean_squared_error' : mean_squared_error , 'mean_absolute_error' : mean_absolute_error , 'mean_absolute_percentage_error' : mean_absolute_percentage_error } metric = metrics [ metric ] splits = time_series_spliter ( y = y , initial_train_size = initial_train_size , steps = steps , allow_incomplete_fold = allow_incomplete_fold , verbose = verbose ) for train_index , test_index in splits : if exog is None : forecaster . fit ( y = y [ train_index ]) pred = forecaster . predict ( steps = len ( test_index )) else : forecaster . fit ( y = y [ train_index ], exog = exog [ train_index ]) pred = forecaster . predict ( steps = len ( test_index ), exog = exog [ test_index ]) metric_value = metric ( y_true = y [ test_index ], y_pred = pred ) cv_results . append ( metric_value ) return np . array ( cv_results )","title":"Returns"},{"location":"api/model_selection.html#skforecast.model_selection.grid_search_forecaster","text":"Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting.","title":"grid_search_forecaster()"},{"location":"api/model_selection.html#skforecast.model_selection.grid_search_forecaster--parameters","text":"forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput ForecasterAutoreg , ForecasterCustom ForecasterAutoregCustom or ForecasterAutoregMultiOutput object. y : 1D np.ndarray, pd.Series Training time series values. param_grid : dict Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default None Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid : list of int, lists, np.narray or range. Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . method : {'cv', 'backtesting'} Method used to estimate the metric for each parameter combination. 'cv' for time series crosvalidation and 'backtesting' for simple backtesting. 'backtesting' is much faster since the model is fitted only once. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. return_best : bool Refit the forecaster using the best found parameters on the whole data. verbose : bool, default True Print number of folds used for cv or backtesting.","title":"Parameters"},{"location":"api/model_selection.html#skforecast.model_selection.grid_search_forecaster--returns","text":"!!! results \"pandas.DataFrame\" Metric value estimated for each combination of parameters. Source code in skforecast/model_selection.py def grid_search_forecaster ( forecaster , y : Union [ np . ndarray , pd . Series ], param_grid : dict , initial_train_size : int , steps : int , metric : str , exog : Union [ np . ndarray , pd . Series ] = None , lags_grid : list = None , method : str = 'cv' , allow_incomplete_fold : bool = True , return_best : bool = True , verbose : bool = True ) -> pd . DataFrame : ''' Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series cross-validation or backtesting. Parameters ---------- forecaster : ForecasterAutoreg, ForecasterCustom, ForecasterAutoregCustom, ForecasterAutoregMultiOutput `ForecasterAutoreg`, `ForecasterCustom` `ForecasterAutoregCustom` or `ForecasterAutoregMultiOutput` object. y : 1D np.ndarray, pd.Series Training time series values. param_grid : dict Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. metric : {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'} Metric used to quantify the goodness of fit of the model. exog : np.ndarray, pd.Series, default `None` Exogenous variable/s included as predictor/s. Must have the same number of observations as `y` and should be aligned so that y[i] is regressed on exog[i]. lags_grid : list of int, lists, np.narray or range. Lists of `lags` to try. Only used if forecaster is an instance of `ForecasterAutoreg`. method : {'cv', 'backtesting'} Method used to estimate the metric for each parameter combination. 'cv' for time series crosvalidation and 'backtesting' for simple backtesting. 'backtesting' is much faster since the model is fitted only once. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. return_best : bool Refit the `forecaster` using the best found parameters on the whole data. verbose : bool, default `True` Print number of folds used for cv or backtesting. Returns ------- results: pandas.DataFrame Metric value estimated for each combination of parameters. ''' forecaster . _check_y ( y = y ) y = forecaster . _preproces_y ( y = y ) if exog is not None : forecaster . _check_exog ( exog = exog ) exog = forecaster . _preproces_exog ( exog = exog ) if isinstance ( forecaster , ( ForecasterCustom , ForecasterAutoregCustom )): if lags_grid is not None : logging . warning ( '`lags_grid` ignored if forecaster is an instance of `ForecasterCustom` or `ForecasterAutoregCustom`.' ) lags_grid = [ 'custom predictors' ] elif lags_grid is None : lags_grid = [ forecaster . lags ] lags_list = [] params_list = [] metric_list = [] param_grid = list ( ParameterGrid ( param_grid )) logging . info ( f \"Number of models to fit: { len ( param_grid ) * len ( lags_grid ) } \" ) for lags in tqdm . tqdm ( lags_grid , desc = 'loop lags_grid' , position = 0 ): if isinstance ( forecaster , ( ForecasterAutoreg , ForecasterAutoregMultiOutput )): forecaster . set_lags ( lags ) lags = forecaster . lags . copy () for params in tqdm . tqdm ( param_grid , desc = 'loop param_grid' , position = 1 , leave = False ): forecaster . set_params ( ** params ) if method == 'cv' : metrics = cv_forecaster ( forecaster = forecaster , y = y , exog = exog , initial_train_size = initial_train_size , steps = steps , metric = metric , allow_incomplete_fold = allow_incomplete_fold , verbose = verbose ) else : metrics = backtesting_forecaster ( forecaster = forecaster , y = y , exog = exog , initial_train_size = initial_train_size , steps = steps , metric = metric , verbose = verbose )[ 0 ] lags_list . append ( lags ) params_list . append ( params ) metric_list . append ( metrics . mean ()) results = pd . DataFrame ({ 'lags' : lags_list , 'params' : params_list , 'metric' : metric_list }) results = results . sort_values ( by = 'metric' , ascending = True ) if return_best : best_lags = results [ 'lags' ] . iloc [ 0 ] best_params = results [ 'params' ] . iloc [ 0 ] logging . info ( f \"Refitting `forecaster` using the best found parameters and the whole data set: \\n \" f \"lags: { best_lags } \\n \" f \"params: { best_params } \\n \" ) if isinstance ( forecaster , ( ForecasterAutoreg , ForecasterAutoregMultiOutput )): forecaster . set_lags ( best_lags ) forecaster . set_params ( ** best_params ) forecaster . fit ( y = y , exog = exog ) return results","title":"Returns"},{"location":"api/model_selection.html#skforecast.model_selection.time_series_spliter","text":"Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration.","title":"time_series_spliter()"},{"location":"api/model_selection.html#skforecast.model_selection.time_series_spliter--parameters","text":"y : 1D np.ndarray, pd.Series Training time series values. !!! initial_train_size \"int \" Number of samples in the initial train split. steps : int Number of steps to predict. allow_incomplete_fold : bool, default True The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose : bool, default True Print number of splits created.","title":"Parameters"},{"location":"api/model_selection.html#skforecast.model_selection.time_series_spliter--yields","text":"train : 1D np.ndarray Training indices. test : 1D np.ndarray Test indices. Source code in skforecast/model_selection.py def time_series_spliter ( y : Union [ np . ndarray , pd . Series ], initial_train_size : int , steps : int , allow_incomplete_fold : bool = True , verbose : bool = True ): ''' Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters ---------- y : 1D np.ndarray, pd.Series Training time series values. initial_train_size: int Number of samples in the initial train split. steps : int Number of steps to predict. allow_incomplete_fold : bool, default `True` The last test set is allowed to be incomplete if it does not reach `steps` observations. Otherwise, the latest observations are discarded. verbose : bool, default `True` Print number of splits created. Yields ------ train : 1D np.ndarray Training indices. test : 1D np.ndarray Test indices. ''' if not isinstance ( y , ( np . ndarray , pd . Series )): raise Exception ( '`y` must be `1D np.ndarray` o `pd.Series`.' ) elif isinstance ( y , np . ndarray ) and y . ndim != 1 : raise Exception ( f \"`y` must be `1D np.ndarray` o `pd.Series`, \" f \"got `np.ndarray` with { y . ndim } dimensions.\" ) if isinstance ( y , pd . Series ): y = y . to_numpy () . copy () folds = ( len ( y ) - initial_train_size ) // steps + 1 # +1 fold is needed to allow including the remainder in the last iteration. remainder = ( len ( y ) - initial_train_size ) % steps if verbose : if folds == 1 : print ( f \"Number of folds: { folds - 1 } \" ) print ( \"Not enought observations in `y` to create to create even a complete fold.\" ) elif remainder == 0 : print ( f \"Number of folds: { folds - 1 } \" ) elif remainder != 0 and allow_incomplete_fold : print ( f \"Number of folds: { folds } \" ) print ( f \"Since `allow_incomplete_fold=True`, \" f \"last fold only includes { remainder } observations instead of { steps } .\" ) print ( 'Incomplete folds with few observations could overestimate or ' , 'underestimate validation metrics.' ) elif remainder != 0 and not allow_incomplete_fold : print ( f \"Number of folds: { folds - 1 } \" ) print ( f \"Since `allow_incomplete_fold=False`, \" f \"last { remainder } observations are descarted.\" ) if folds == 1 : # There are no observations to create even a complete fold return [] for i in range ( folds ): if i < folds - 1 : train_end = initial_train_size + i * steps train_indices = range ( train_end ) test_indices = range ( train_end , train_end + steps ) else : if remainder != 0 and allow_incomplete_fold : train_end = initial_train_size + i * steps train_indices = range ( train_end ) test_indices = range ( train_end , len ( y )) else : break yield train_indices , test_indices","title":"Yields"},{"location":"guides/getting-started.html","text":"Getting Started \u00b6 Introduction \u00b6 A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of a model to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Direct multi-step forecasting This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster. Examples \u00b6 Autoregressive forecaster \u00b6 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import time_series_spliter from skforecast.model_selection import cv_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import backtesting_forecaster_intervals from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) ax . legend (); # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 15 ) forecaster . fit ( y = datos_train ) forecaster ======================= ForecasterAutoreg ======================= Regressor : LinearRegression () Lags : [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ] Exogenous variable : False Parameters : { 'copy_X' : True , 'fit_intercept' : True , 'n_jobs' : None , 'normalize' : False , 'positive' : False } # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = datos_test . index ) # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.011051937043503587 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) results_grid loop lags_grid : 0 %| | 0 / 3 [ 00 : 00 < ? , ? it / s ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.40 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.11 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 02 < 00 : 00 , 1.06 it / s ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.13 s / it ] loop lags_grid : 33 %| \u2588\u2588\u2588\u258e | 1 / 3 [ 00 : 04 < 00 : 08 , 4.28 s / it ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.29 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.20 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 03 < 00 : 01 , 1.03 s / it ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.25 s / it ] loop lags_grid : 67 %| \u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2 / 3 [ 00 : 08 < 00 : 04 , 4.52 s / it ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.38 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.12 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 02 < 00 : 00 , 1.06 it / s ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.14 s / it ] loop lags_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3 / 3 [ 00 : 13 < 00 : 00 , 4.42 s / it ] 2021 - 02 - 25 09 : 51 : 43 , 075 root INFO Refitting `forecaster` using the best found parameters : lags : [ 1 2 3 4 5 6 7 8 9 10 ] params : { 'max_depth' : 10 , 'n_estimators' : 50 } lags params metric 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.023449 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.025417 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.025954 5 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.026003 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.028223 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.030685 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.031385 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.038591 8 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.048428 9 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.049842 10 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.051059 11 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.052205 # Predictors importance # ============================================================================== forecaster . get_feature_importances () [0.58116139 0.12777451 0.04191822 0.03095527 0.02517231 0.02482571 0.04065757 0.01652861 0.02619182 0.08481458] # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = steps , interval = [ 5 , 95 ], n_boot = 1000 ) # Add datetime index to predictions predictions = pd . DataFrame ( data = predictions , index = datos_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) #datos_train.plot(ax=ax, label='train') datos_test . plot ( ax = ax , label = 'test' ) predictions . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions . iloc [:, 1 ], predictions . iloc [:, 2 ], alpha = 0.5 ) ax . legend (); # Backtesting # ============================================================================== n_test = 36 * 3 + 1 datos_train = datos [: - n_test ] datos_test = datos [ - n_test :] steps = 36 regressor = LinearRegression () forecaster = ForecasterAutoreg ( regressor = regressor , lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = datos , initial_train_size = len ( datos_train ), steps = steps , metric = 'mean_squared_error' , verbose = True ) print ( metric ) # Add datetime index to predictions predictions_backtest = pd . Series ( data = predictions_backtest , index = datos_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) #datos_train.plot(ax=ax, label='train') datos_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend (); Number of observations used for training : 95 Number of folds : 4 Last fold only includes 1 observations . [ 0 . 02150972 ] ### Autoregressive forecaster with 1 exogenous predictor # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Exogenous variable # ============================================================================== datos_exog = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 0.5 datos_exog = datos_exog [ 10 :] datos = datos [ 10 :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) datos_exog . plot ( ax = ax , label = 'exogenous variable' ) ax . legend (); # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] datos_exog_train = datos_exog [: - steps ] datos_exog_test = datos_exog [ - steps :] # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 8 ) forecaster . fit ( y = datos_train , exog = datos_exog_train ) # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = datos_exog_test ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = datos_test . index ) # Error prediction # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.020306077140235308 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , exog = datos_exog_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) # Results grid Search # ============================================================================== results_grid ### Autoregressive forecaster with n exogenous predictors # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Exogenous variables # ============================================================================== datos_exog_1 = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 0.5 datos_exog_2 = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 1 datos_exog_1 = datos_exog_1 [ 10 :] datos_exog_2 = datos_exog_2 [ 10 :] datos = datos [ 10 :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) datos_exog_1 . plot ( ax = ax , label = 'exogenous 1' ) datos_exog_2 . plot ( ax = ax , label = 'exogenous 2' ) ax . legend (); # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] datos_exog = np . column_stack (( datos_exog_1 . values , datos_exog_2 . values )) datos_exog_train = datos_exog [: - steps ,] datos_exog_test = datos_exog [ - steps :,] # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 8 ) forecaster . fit ( y = datos_train , exog = datos_exog_train ) # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = datos_exog_test ) # Add datetime index predictions = pd . Series ( data = predictions , index = datos_test . index ) # Error # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.020306077140235298 ### Autoregressive forecaster with custom predictors # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = datos_train ) # Grid search hiperparameters # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , param_grid = param_grid , steps = 36 , metric = 'mean_squared_error' , method = 'cv' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False )","title":"Getting Started"},{"location":"guides/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"guides/getting-started.html#introduction","text":"A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of a model to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Direct multi-step forecasting This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Introduction"},{"location":"guides/getting-started.html#examples","text":"","title":"Examples"},{"location":"guides/getting-started.html#autoregressive-forecaster","text":"# Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import time_series_spliter from skforecast.model_selection import cv_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import backtesting_forecaster_intervals from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) ax . legend (); # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 15 ) forecaster . fit ( y = datos_train ) forecaster ======================= ForecasterAutoreg ======================= Regressor : LinearRegression () Lags : [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ] Exogenous variable : False Parameters : { 'copy_X' : True , 'fit_intercept' : True , 'n_jobs' : None , 'normalize' : False , 'positive' : False } # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = datos_test . index ) # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.011051937043503587 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) results_grid loop lags_grid : 0 %| | 0 / 3 [ 00 : 00 < ? , ? it / s ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.40 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.11 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 02 < 00 : 00 , 1.06 it / s ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.13 s / it ] loop lags_grid : 33 %| \u2588\u2588\u2588\u258e | 1 / 3 [ 00 : 04 < 00 : 08 , 4.28 s / it ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.29 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.20 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 03 < 00 : 01 , 1.03 s / it ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.25 s / it ] loop lags_grid : 67 %| \u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2 / 3 [ 00 : 08 < 00 : 04 , 4.52 s / it ] loop param_grid : 0 %| | 0 / 4 [ 00 : 00 < ? , ? it / s ] loop param_grid : 25 %| \u2588\u2588\u258c | 1 / 4 [ 00 : 00 < 00 : 02 , 1.38 it / s ] loop param_grid : 50 %| \u2588\u2588\u2588\u2588\u2588 | 2 / 4 [ 00 : 02 < 00 : 02 , 1.12 s / it ] loop param_grid : 75 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 3 / 4 [ 00 : 02 < 00 : 00 , 1.06 it / s ] loop param_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 / 4 [ 00 : 04 < 00 : 00 , 1.14 s / it ] loop lags_grid : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 3 / 3 [ 00 : 13 < 00 : 00 , 4.42 s / it ] 2021 - 02 - 25 09 : 51 : 43 , 075 root INFO Refitting `forecaster` using the best found parameters : lags : [ 1 2 3 4 5 6 7 8 9 10 ] params : { 'max_depth' : 10 , 'n_estimators' : 50 } lags params metric 6 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.023449 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.025417 7 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.025954 5 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.026003 1 [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.028223 0 [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.030685 3 [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.031385 2 [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.038591 8 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.048428 9 [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.049842 10 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.051059 11 [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.052205 # Predictors importance # ============================================================================== forecaster . get_feature_importances () [0.58116139 0.12777451 0.04191822 0.03095527 0.02517231 0.02482571 0.04065757 0.01652861 0.02619182 0.08481458] # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = steps , interval = [ 5 , 95 ], n_boot = 1000 ) # Add datetime index to predictions predictions = pd . DataFrame ( data = predictions , index = datos_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) #datos_train.plot(ax=ax, label='train') datos_test . plot ( ax = ax , label = 'test' ) predictions . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions . iloc [:, 1 ], predictions . iloc [:, 2 ], alpha = 0.5 ) ax . legend (); # Backtesting # ============================================================================== n_test = 36 * 3 + 1 datos_train = datos [: - n_test ] datos_test = datos [ - n_test :] steps = 36 regressor = LinearRegression () forecaster = ForecasterAutoreg ( regressor = regressor , lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = datos , initial_train_size = len ( datos_train ), steps = steps , metric = 'mean_squared_error' , verbose = True ) print ( metric ) # Add datetime index to predictions predictions_backtest = pd . Series ( data = predictions_backtest , index = datos_test . index ) fig , ax = plt . subplots ( figsize = ( 9 , 4 )) #datos_train.plot(ax=ax, label='train') datos_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend (); Number of observations used for training : 95 Number of folds : 4 Last fold only includes 1 observations . [ 0 . 02150972 ] ### Autoregressive forecaster with 1 exogenous predictor # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Exogenous variable # ============================================================================== datos_exog = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 0.5 datos_exog = datos_exog [ 10 :] datos = datos [ 10 :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) datos_exog . plot ( ax = ax , label = 'exogenous variable' ) ax . legend (); # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] datos_exog_train = datos_exog [: - steps ] datos_exog_test = datos_exog [ - steps :] # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 8 ) forecaster . fit ( y = datos_train , exog = datos_exog_train ) # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = datos_exog_test ) # Add datetime index to predictions predictions = pd . Series ( data = predictions , index = datos_test . index ) # Error prediction # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.020306077140235308 # Grid search hiperparameters and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , exog = datos_exog_train , param_grid = param_grid , lags_grid = lags_grid , steps = 10 , method = 'cv' , metric = 'mean_squared_error' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False ) # Results grid Search # ============================================================================== results_grid ### Autoregressive forecaster with n exogenous predictors # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Exogenous variables # ============================================================================== datos_exog_1 = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 0.5 datos_exog_2 = datos . rolling ( window = 10 , closed = 'right' ) . mean () + 1 datos_exog_1 = datos_exog_1 [ 10 :] datos_exog_2 = datos_exog_2 [ 10 :] datos = datos [ 10 :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos . plot ( ax = ax , label = 'y' ) datos_exog_1 . plot ( ax = ax , label = 'exogenous 1' ) datos_exog_2 . plot ( ax = ax , label = 'exogenous 2' ) ax . legend (); # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] datos_exog = np . column_stack (( datos_exog_1 . values , datos_exog_2 . values )) datos_exog_train = datos_exog [: - steps ,] datos_exog_test = datos_exog [ - steps :,] # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = LinearRegression (), lags = 8 ) forecaster . fit ( y = datos_train , exog = datos_exog_train ) # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = datos_exog_test ) # Add datetime index predictions = pd . Series ( data = predictions , index = datos_test . index ) # Error # ============================================================================== error_mse = mean_squared_error ( y_true = datos_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) datos_train . plot ( ax = ax , label = 'train' ) datos_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); Test error (mse): 0.020306077140235298 ### Autoregressive forecaster with custom predictors # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) datos = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== datos [ 'fecha' ] = pd . to_datetime ( datos [ 'fecha' ], format = '%Y/%m/ %d ' ) datos = datos . set_index ( 'fecha' ) datos = datos . rename ( columns = { 'x' : 'y' }) datos = datos . asfreq ( 'MS' ) datos = datos [ 'y' ] datos = datos . sort_index () # Split train-test # ============================================================================== steps = 36 datos_train = datos [: - steps ] datos_test = datos [ - steps :] # Custom function to create poredictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = datos_train ) # Grid search hiperparameters # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) # Regressor hiperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 ]} results_grid = grid_search_forecaster ( forecaster = forecaster , y = datos_train , param_grid = param_grid , steps = 36 , metric = 'mean_squared_error' , method = 'cv' , initial_train_size = int ( len ( datos_train ) * 0.5 ), allow_incomplete_fold = False , return_best = True , verbose = False )","title":"Autoregressive forecaster"}]}