{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"skforecast \u00b6 Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (pipelines, CatBoost, LightGBM, XGBoost, Ranger...). Documentation: https://joaquinamatrodrigo.github.io/skforecast/ Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit). Changelog Installation \u00b6 1 pip install skforecast Specific version: 1 pip install skforecast==0.3 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master Dependencies \u00b6 1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=1.0.1 statsmodels>=0.12.2 Features \u00b6 Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance Tutorials \u00b6 English In construction Espa\u00f1ol Forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web References \u00b6 Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance Licence \u00b6 joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Readme"},{"location":"index.html#skforecast","text":"Time series forecasting with scikit-learn regressors. Skforecast is a python library that eases using scikit-learn regressors as multi-step forecasters. It also works with any regressor compatible with the scikit-learn API (pipelines, CatBoost, LightGBM, XGBoost, Ranger...). Documentation: https://joaquinamatrodrigo.github.io/skforecast/ Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas series and dataframes are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit). Changelog","title":"skforecast"},{"location":"index.html#installation","text":"1 pip install skforecast Specific version: 1 pip install skforecast==0.3 Latest (unstable): 1 pip install git+https://github.com/JoaquinAmatRodrigo/skforecast#master","title":"Installation"},{"location":"index.html#dependencies","text":"1 2 3 4 5 6 python>=3.7.1 numpy>=1.20.1 pandas>=1.2.2 tqdm>=4.57.0 scikit-learn>=1.0.1 statsmodels>=0.12.2","title":"Dependencies"},{"location":"index.html#features","text":"Create recursive autoregressive forecasters from any scikit-learn regressor Create multi-output autoregressive forecasters from any scikit-learn regressor Grid search to find optimal hyperparameters Grid search to find optimal lags (predictors) Include exogenous variables as predictors Include custom predictors (rolling mean, rolling variance ...) Backtesting Prediction interval estimated by bootstrapping Get predictor importance","title":"Features"},{"location":"index.html#tutorials","text":"English In construction Espa\u00f1ol Forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web","title":"Tutorials"},{"location":"index.html#references","text":"Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia Time Series Analysis and Forecasting with ADAM Ivan Svetunkov Python for Finance: Mastering Data-Driven Finance","title":"References"},{"location":"index.html#licence","text":"joaquinAmatRodrigo/skforecast is licensed under the MIT License , a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.","title":"Licence"},{"location":"api/ForecasterAutoreg.html","text":"ForecasterAutoreg \u00b6 class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range : include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoreg.html#forecasterautoreg","text":"class skforecast.ForecasterAutoreg.ForecasterAutoreg. ForecasterAutoreg ( regressor , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range : include only lags present in lags . Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoreg object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) \u2014 Create training matrices from univariate time series and exogenous variables. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoreg object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags))) Pandas DataFrame with the training values (predictors). ain : pandas Series, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoreg. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoreg"},{"location":"api/ForecasterAutoregCustom.html","text":"ForecasterAutoregCustom \u00b6 class skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. fun_predictors (Callable) \u2014 Function that takes a time series window as input and returns a numpy ndarray with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). fun_predictors (Callable) \u2014 Function that takes a time series window as input and returns a numpy ndarray with the predictors associated with that window. in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor compatible with the scikit-learn API) \u2014 An instance of a regressor compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame) \u2014 Create training matrices from univariate time series. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. summary ( ) \u2014 Show forecaster information. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame) Pandas DataFrame with the training values (predictors). ain : pandas Series Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregCustom.html#forecasterautoregcustom","text":"class skforecast.ForecasterAutoregCustom.ForecasterAutoregCustom. ForecasterAutoregCustom ( regressor , fun_predictors , window_size ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a recursive (multi-step) forecaster with a custom function to create predictors. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. fun_predictors (Callable) \u2014 Function that takes a time series window as input and returns a numpy ndarray with the predictors associated with that window. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Attributes X_train_col_names (list) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (list) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). fun_predictors (Callable) \u2014 Function that takes a time series window as input and returns a numpy ndarray with the predictors associated with that window. in_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting training data. Only stored up to 1000 values. included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. out_sample_residuals (numpy ndarray) \u2014 Residuals of the model when predicting non training data. Only stored up to 1000 values. regressor (regressor compatible with the scikit-learn API) \u2014 An instance of a regressor compatible with the scikit-learn API. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. training_range (pandas Index) \u2014 First and last values of index of the data used during training. window_size (int) \u2014 Size of the window needed by fun_predictors to create the predictors. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregCustom object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame) \u2014 Create training matrices from univariate time series. fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. get_feature_importance ( ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. predict_interval ( steps , last_window , exog , interval , n_boot , random_state , in_sample_residuals ) (predictions : pandas DataFrame) \u2014 Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_out_sample_residuals ( residuals , append ) (self) \u2014 Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. summary ( ) \u2014 Show forecaster information. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregCustom object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame) Pandas DataFrame with the training values (predictors). ain : pandas Series Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps , last_window=None , exog=None ) Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method predict_interval ( steps , last_window=None , exog=None , interval=[5, 95] , n_boot=500 , random_state=123 , in_sample_residuals=True ) Iterative process in which, each prediction, is used as a predictor for the next step and bootstrapping is used to estimate prediction intervals. Both, predictions and intervals, are returned. Parameters steps (int) \u2014 Number of future steps predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) needed in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. interval (list, default `[5, 95]`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. random_state (int) \u2014 Sets a seed to the random generator, so that boot intervals are always deterministic. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. If False , out of sample residuals are used. In the latter case, the user should have calculated and stored the residuals within the forecaster (see set_out_sample_residuals() ). Returns (predictions : pandas DataFrame) Values predicted by the forecaster and their estimated interval: column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. Notes More information about prediction intervals in forecasting: https://otexts.com/fpp2/prediction-intervals.html Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the ForecasterAutoregCustom. method set_out_sample_residuals ( residuals , append=True ) Set new values to the attribute out_sample_residuals . Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Parameters append (bool, default `True`) \u2014 If True , new residuals are added to the once already stored in the attribute out_sample_residuals . Once the limit of 1000 values is reached, no more values are appended. If False, out_sample_residuals is overwritten with the new residuals. params (1D np.ndarray) \u2014 Values of residuals. If len(residuals) > 1000, only a random sample of 1000 values are stored. method get_coef ( ) Return estimated coefficients for the linear regression model stored in the forecaster. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( ) Return impurity-based feature importance of the model stored in the forecaster. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregCustom"},{"location":"api/ForecasterAutoregMultiOutput.html","text":"ForecasterAutoregMultiOutput \u00b6 class skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. steps (int) \u2014 Maximum number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (tuple) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (tuple) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. One instance of this regressor is trainned for each step. All them are stored in self.regressors_ . regressors_ (dict) \u2014 Dictionary with regressors trained for each step. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. training_range (pandas Index) \u2014 First and last index of samples used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hyperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregMultiOutput object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : pandas DataFrame) \u2014 Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( step ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. get_feature_importance ( step ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregMultiOutput object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) Pandas DataFrame with the training values (predictors) for each step. ain : pd.DataFrame, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train for each step. method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (pandas DataFrame) \u2014 Pandas DataFrame with the training values (predictors). y_train (pandas Series) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : pandas DataFrame) Pandas DataFrame with the training values (predictors) for step. ain_step : pandas Series, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps=None , last_window=None , exog=None ) Predict n steps ahead. Parameters steps (int, None, default `None`) \u2014 Predict n steps ahead. steps must lower or equal to the value of steps defined when initializing the forecaster. If None , as many steps as defined in the initialization are predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method get_coef ( step ) Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( step ) Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregMultiOutput"},{"location":"api/ForecasterAutoregMultiOutput.html#forecasterautoregmultioutput","text":"class skforecast.ForecasterAutoregMultiOutput.ForecasterAutoregMultiOutput. ForecasterAutoregMultiOutput ( regressor , steps , lags ) Bases skforecast.ForecasterBase.ForecasterBase.ForecasterBase This class turns any regressor compatible with the scikit-learn API into a autoregressive multi-output forecaster. A separate model is created for each forecast time step. See Notes for more details. Parameters regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. steps (int) \u2014 Maximum number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. lags (int, list, 1d numpy ndarray, range) \u2014 Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. int : include lags from 1 to lags (included). list , numpy ndarray or range: include only lags present in lags . Attributes X_train_col_names (tuple) \u2014 Names of columns of the matrix created internally for training. creation_date (str) \u2014 Date of creation. exog_col_names (tuple) \u2014 Names of columns of exog if exog used in training was a pandas DataFrame. exog_type (type) \u2014 Type of exogenous variable/s used in training. fit_date (str) \u2014 Date of last fit. fitted (Bool) \u2014 Tag to identify if the regressor has been fitted (trained). included_exog (bool) \u2014 If the forecaster has been trained using exogenous variable/s. index_freq (str) \u2014 Frequency of Index of the input used in training. index_type (type) \u2014 Type of index of the input used in training. lags (numpy ndarray) \u2014 Lags used as predictors. last_window (pandas Series) \u2014 Last window the forecaster has seen during trained. It stores the values needed to predict the next step right after the training data. max_lag (int) \u2014 Maximum value of lag included in lags . regressor (regressor or pipeline compatible with the scikit-learn API) \u2014 An instance of a regressor or pipeline compatible with the scikit-learn API. One instance of this regressor is trainned for each step. All them are stored in self.regressors_ . regressors_ (dict) \u2014 Dictionary with regressors trained for each step. skforcast_version (str) \u2014 Version of skforecast library used to create the forecaster. steps (int) \u2014 Number of future steps the forecaster will predict when using method predict() . Since a different model is created for each step, this value should be defined before training. training_range (pandas Index) \u2014 First and last index of samples used during training. window_size (int) \u2014 Size of the window needed to create the predictors. It is equal to max_lag . Notes A separate model is created for each forecast time step. It is important to note that all models share the same configuration of parameters and hyperparameters. Methods __repr__ ( ) (str) \u2014 Information displayed when a ForecasterAutoregMultiOutput object is printed. create_train_X_y ( y , exog ) (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) \u2014 Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). filter_train_X_y_for_step ( step , X_train , y_train ) (X_train_step : pandas DataFrame) \u2014 Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . fit ( y , exog ) (None) \u2014 Training Forecaster. get_coef ( step ) (coef : pandas DataFrame) \u2014 Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. get_feature_importance ( step ) (feature_importance : pandas DataFrame) \u2014 Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. predict ( steps , last_window , exog ) (predictions : pandas Series) \u2014 Predict n steps ahead. set_lags ( lags ) (self) \u2014 Set new value to the attribute lags . Attributes max_lag and window_size are also updated. set_params ( **params ) (self) \u2014 Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. summary ( ) \u2014 Show forecaster information. method summary ( ) Show forecaster information. method __repr__ ( ) \u2192 str Information displayed when a ForecasterAutoregMultiOutput object is printed. method create_train_X_y ( y , exog=None ) Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the forecaster (one per step). Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned. Returns (X_train : pandas DataFrame, shape (len(y) - self.max_lag, len(self.lags) + exog.shape[1]*steps)) Pandas DataFrame with the training values (predictors) for each step. ain : pd.DataFrame, shape (len(y) - self.max_lag, ) Values (target) of the time series related to each row of X_train for each step. method filter_train_X_y_for_step ( step , X_train , y_train ) Select columns needed to train a forcaster for a specific step. The input matrices should be created with created with create_train_X_y() . Parameters step (int) \u2014 step for which columns must be selected selected. Starts at 0. X_train (pandas DataFrame) \u2014 Pandas DataFrame with the training values (predictors). y_train (pandas Series) \u2014 Values (target) of the time series related to each row of X_train . Returns (X_train_step : pandas DataFrame) Pandas DataFrame with the training values (predictors) for step. ain_step : pandas Series, shape (len(y) - self.max_lag) Values (target) of the time series related to each row of X_train . method fit ( y , exog=None ) Training Forecaster. Parameters y (pandas Series) \u2014 Training time series. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and their indexes must be aligned so that y[i] is regressed on exog[i]. method predict ( steps=None , last_window=None , exog=None ) Predict n steps ahead. Parameters steps (int, None, default `None`) \u2014 Predict n steps ahead. steps must lower or equal to the value of steps defined when initializing the forecaster. If None , as many steps as defined in the initialization are predicted. last_window (pandas Series, default `None`) \u2014 Values of the series used to create the predictors (lags) need in the first iteration of prediction (t + 1). If last_window = None , the values stored in self.last_window are used to calculate the initial predictors, and the predictions start right after training data. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Returns (predictions : pandas Series) Predicted values. method set_params ( **params ) Set new values to the parameters of the scikit learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters. method set_lags ( lags ) Set new value to the attribute lags . Attributes max_lag and window_size are also updated. method get_coef ( step ) Return estimated coefficients for the linear regression model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using as regressor: LinearRegression() , Lasso() or Ridge()`. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). Returns (coef : pandas DataFrame) Value of the coefficients associated with each predictor. method get_feature_importance ( step ) Return impurity-based feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when the forecaster has been trained using GradientBoostingRegressor , RandomForestRegressor or HistGradientBoostingRegressor as regressor. Parameters step (int) \u2014 Model from which retrieve information (a separate model is created for each forecast time step). Returns (feature_importance : pandas DataFrame) Impurity-based feature importance associated with each predictor.","title":"ForecasterAutoregMultiOutput"},{"location":"api/model_selection.html","text":"model_selection \u00b6 function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds and index of training and validation sets used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , set_out_sample_residuals=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection.html#model_selection","text":"function skforecast.model_selection.model_selection. backtesting_forecaster ( forecaster , y , steps , metric , initial_train_size , exog=None , refit=False , interval=None , n_boot=500 , in_sample_residuals=True , set_out_sample_residuals=True , verbose=False ) Backtesting of forecaster model. If refit is False, the model is trained only once using the initial_train_size first observations. If refit is True, the model is trained in each iteration increasing the training set. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. initial_train_size (int, default `None`) \u2014 Number of samples in the initial train split. If None and forecaster is already trained, no initial train is done and all data is used to evaluate the model. However, the first len(forecaster.last_window) observations are needed to create the initial predictors. Therefore, no predictions are calculated for them. None is only allowed when refit is False. exog (panda Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration. interval (list, default `None`) \u2014 Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. If None , no intervals are estimated. Only available for forecaster of type ForecasterAutoreg and ForecasterAutoregCustom. n_boot (int, default `500`) \u2014 Number of bootstrapping iterations used to estimate prediction intervals. in_sample_residuals (bool, default `True`) \u2014 If True , residuals from the training data are used as proxy of prediction error to create prediction intervals. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. Ignored if forecaster is of class ForecasterAutoregMultiOutput . verbose (bool, default `False`) \u2014 Print number of folds and index of training and validation sets used for backtesting. Returns (metric_value: numpy ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Value of predictions and their estimated interval if interval is not None . column pred = predictions. column lower_bound = lower bound of the interval. column upper_bound = upper bound interval of the interval. function skforecast.model_selection.model_selection. cv_forecaster ( forecaster , y , initial_train_size , steps , metric , exog=None , allow_incomplete_fold=True , set_out_sample_residuals=True , verbose=True ) Cross-validation of forecaster. The order of data is maintained and the training set increases in each iteration. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forecaster model. y (pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. allow_incomplete_fold (bool, default `True`) \u2014 The last test partition is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. set_out_sample_residuals (bool, default `True`) \u2014 Save residuals generated during the cross-validation process as out of sample residuals. verbose (bool, default `True`) \u2014 Print number of folds used for cross validation. Returns (cv_metrics: 1d numpy ndarray) Value of the metric for each fold. redictions: pandas DataFrame Predictions. function skforecast.model_selection.model_selection. grid_search_forecaster ( forecaster , y , param_grid , initial_train_size , steps , metric , exog=None , lags_grid=None , refit=False , return_best=True , verbose=True ) Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting. Parameters forecaster (ForecasterAutoreg, ForecasterAutoregCustom, ForecasterAutoregMultiOutput) \u2014 Forcaster model. y (pandas Series) \u2014 Training time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. lags_grid (list of int, lists, np.narray or range.) \u2014 Lists of lags to try. Only used if forecaster is an instance of ForecasterAutoreg . refit (bool, default False) \u2014 Whether to re-fit the forecaster in each iteration of backtesting. return_best (bool) \u2014 Refit the forecaster using the best found parameters on the whole data. verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters. generator skforecast.model_selection.model_selection. time_series_spliter ( y , initial_train_size , steps , allow_incomplete_fold=True , verbose=True ) Split indices of a time series into multiple train-test pairs. The order of is maintained and the training set increases in each iteration. Parameters y (1d numpy ndarray, pandas Series) \u2014 Training time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. allow_incomplete_fold (bool, default `True`) \u2014 The last test set is allowed to be incomplete if it does not reach steps observations. Otherwise, the latest observations are discarded. verbose (bool, default `True`) \u2014 Print number of splits created. Yields (train : 1d numpy ndarray) Training indices. : 1d numpy ndarray Test indices.","title":"model_selection"},{"location":"api/model_selection_statsmodels.html","text":"model_selection_statsmodels \u00b6 function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"api/model_selection_statsmodels.html#model_selection_statsmodels","text":"function skforecast.model_selection_statsmodels.model_selection_statsmodels. backtesting_sarimax ( y , initial_train_size , steps , metric , refit=False , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Backtesting (validation) of SARIMAX model from statsmodels v0.12. The model is trained using the initial_train_size first observations, then, in each iteration, a number of steps predictions are evaluated. If refit is True , the model is re-fitted in each iteration before making predictions. https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX constructor. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for backtesting. Returns (metric_value: np.ndarray shape (1,)) Value of the metric. test_predictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. cv_sarimax ( y , initial_train_size , steps , metric , order=(1, 0, 0) , seasonal_order=(0, 0, 0, 0) , trend=None , alpha=0.05 , exog=None , allow_incomplete_fold=True , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Cross-validation of SARIMAX model from statsmodels v0.12. The order of data is maintained and the training set increases in each iteration. Parameters y (pandas Series) \u2014 Time series values. initial_train_size (int) \u2014 Number of samples in the initial train split. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. order (tuple) \u2014 The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. d must be an integer indicating the integration order of the process, while p and q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. Default is an AR(1) model: (1,0,0). seasonal_order (tuple) \u2014 The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. D must be an integer indicating the integration order of the process, while P and Q may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included) or else iterables giving specific AR and / or MA lags to include. s is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect. trend (str {\u2018n\u2019,\u2019c\u2019,\u2019t\u2019,\u2019ct\u2019}) \u2014 Parameter controlling the deterministic trend polynomial A(t). Can be specified as a string where \u2018c\u2019 indicates a constant (i.e. a degree zero component of the trend polynomial), \u2018t\u2019 indicates a linear trend with time, and \u2018ct\u2019 is both. Can also be specified as an iterable defining the non-zero polynomial exponents to include, in increasing order. For example, [1,1,0,1] denotes a+bt+ct3. Default is to not include a trend component. alpha (float, default 0.05) \u2014 The significance level for the confidence interval. The default alpha = .05 returns a 95% confidence interval. exog (pandas Series, pandas DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. sarimax_kwargs (dict, default {}) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `False`) \u2014 Print number of folds used for cross-validation. Returns (cv_metrics: 1D np.ndarray) Value of the metric for each partition. redictions: pandas DataFrame Values predicted and their estimated interval: column pred = predictions. column lower = lower bound of the interval. column upper = upper bound interval of the interval. function skforecast.model_selection_statsmodels.model_selection_statsmodels. grid_search_sarimax ( y , param_grid , initial_train_size , steps , metric , exog=None , refit=False , sarimax_kwargs={} , fit_kwargs={'disp': 0} , verbose=False ) Exhaustive search over specified parameter values for a SARIMAX model from statsmodels v0.12. Validation is done using time series cross-validation or backtesting. Parameters y (pandas Series) \u2014 Time series values. param_grid (dict) \u2014 Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values. Allowed parameters in the grid are: order, seasonal_order and trend. initial_train_size (int) \u2014 Number of samples used in the initial train. steps (int) \u2014 Number of steps to predict. metric ({'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'}) \u2014 Metric used to quantify the goodness of fit of the model. exog (np.ndarray, pd.Series, pd.DataFrame, default `None`) \u2014 Exogenous variable/s included as predictor/s. Must have the same number of observations as y and should be aligned so that y[i] is regressed on exog[i]. refit (bool, default False) \u2014 Whether to re-fit the model in each iteration. sarimax_kwargs (dict, default `{}`) \u2014 Additional keyword arguments passed to SARIMAX initialization. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX fit_kwargs (dict, default `{'disp':0}`) \u2014 Additional keyword arguments passed to SARIMAX fit. See more in https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html#statsmodels.tsa.statespace.sarimax.SARIMAX.fit verbose (bool, default `True`) \u2014 Print number of folds used for cv or backtesting. Returns (results: pandas DataFrame) Metric value estimated for each combination of parameters.","title":"model_selection_statsmodels"},{"location":"examples/examples.html","text":"Examples \u00b6 Use cases where skforecast library is used for forecasting. English \u00b6 In construction Espa\u00f1ol \u00b6 Forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web","title":"Forecasting web page traffic"},{"location":"examples/examples.html#examples","text":"Use cases where skforecast library is used for forecasting.","title":"Examples"},{"location":"examples/examples.html#english","text":"In construction","title":"English"},{"location":"examples/examples.html#espanol","text":"Forecasting series temporales con Python y Scikit-learn Forecasting de la demanda el\u00e9ctrica Forecasting de las visitas a una p\u00e1gina web","title":"Espa\u00f1ol"},{"location":"guides/autoregresive-forecaster-exogenous.html","text":"Recursive multi-step forecasting with exogenous variables \u00b6 ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i]. Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'datetime' , 'y' , 'exog_1' , 'exog_2' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :] Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-07 21:15:22 Last fit date: 2021-12-07 21:15:22 Skforecast version: 0.4.0 Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they should be provided when predictions. 1 2 3 4 5 6 7 8 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] ) predictions . head ( 3 ) 1 2 3 4 5 datetime 2005-07-01 0.908832 2005-08-01 0.953925 2005-09-01 1.100887 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.004022228812838391 Feature importance \u00b6 1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | feature | importance | |:----------|-------------:| | lag_1 | 0.0133541 | | lag_2 | 0.0611202 | | lag_3 | 0.00908617 | | lag_4 | 0.00272094 | | lag_5 | 0.00247847 | | lag_6 | 0.00315493 | | lag_7 | 0.00217887 | | lag_8 | 0.00815443 | | lag_9 | 0.0103189 | | lag_10 | 0.0205869 | | lag_11 | 0.00703555 | | lag_12 | 0.773389 | | lag_13 | 0.00458297 | | lag_14 | 0.0181272 | | lag_15 | 0.00873237 | | exog_1 | 0.0103638 | | exog_2 | 0.0446156 |","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#recursive-multi-step-forecasting-with-exogenous-variables","text":"ForecasterAutoreg and ForecasterAutoregCustom allow to include exogenous variables as predictors as long as their future values are known, since they must be included during the predict process. When using exogenous variables in recursive multi-step forecasting, their values should be aligned so that y[i] is regressed on exog[i].","title":"Recursive multi-step forecasting with exogenous variables"},{"location":"guides/autoregresive-forecaster-exogenous.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster-exogenous.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o_exog.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'datetime' , 'y' , 'exog_1' , 'exog_2' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ); 1 2 3 4 5 # Split train-test # ============================================================================== steps = 36 data_train = data . iloc [: - steps , :] data_test = data . iloc [ - steps :, :]","title":"Data"},{"location":"guides/autoregresive-forecaster-exogenous.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train [ 'y' ], exog = data_train [[ 'exog_1' , 'exog_2' ]] ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: True Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> Exogenous variables names: ['exog_1', 'exog_2'] Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-07 21:15:22 Last fit date: 2021-12-07 21:15:22 Skforecast version: 0.4.0","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster-exogenous.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they should be provided when predictions. 1 2 3 4 5 6 7 8 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps , exog = data_test [[ 'exog_1' , 'exog_2' ]] ) predictions . head ( 3 ) 1 2 3 4 5 datetime 2005-07-01 0.908832 2005-08-01 0.953925 2005-09-01 1.100887 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train [ 'y' ] . plot ( ax = ax , label = 'train' ) data_test [ 'y' ] . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test [ 'y' ], y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.004022228812838391","title":"Prediction"},{"location":"guides/autoregresive-forecaster-exogenous.html#feature-importance","text":"1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | feature | importance | |:----------|-------------:| | lag_1 | 0.0133541 | | lag_2 | 0.0611202 | | lag_3 | 0.00908617 | | lag_4 | 0.00272094 | | lag_5 | 0.00247847 | | lag_6 | 0.00315493 | | lag_7 | 0.00217887 | | lag_8 | 0.00815443 | | lag_9 | 0.0103189 | | lag_10 | 0.0205869 | | lag_11 | 0.00703555 | | lag_12 | 0.773389 | | lag_13 | 0.00458297 | | lag_14 | 0.0181272 | | lag_15 | 0.00873237 | | exog_1 | 0.0103638 | | exog_2 | 0.0446156 |","title":"Feature importance"},{"location":"guides/autoregresive-forecaster.html","text":"Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend () Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-06 23:22:17 Last fit date: 2021-12-06 23:22:17 Skforecast version: 0.4.0 Prediction \u00b6 1 2 3 4 5 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.916878 2005-08-01 0.946228 2005-09-01 1.107921 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.003811271443540486 Feature importance \u00b6 1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | feature | importance | |:----------|-------------:| | lag_1 | 0.0121745 | | lag_2 | 0.0857522 | | lag_3 | 0.0166948 | | lag_4 | 0.00287025 | | lag_5 | 0.00346738 | | lag_6 | 0.00333398 | | lag_7 | 0.00331932 | | lag_8 | 0.00757049 | | lag_9 | 0.00997129 | | lag_10 | 0.01222 | | lag_11 | 0.0120539 | | lag_12 | 0.794896 | | lag_13 | 0.00438497 | | lag_14 | 0.0161486 | | lag_15 | 0.0151425 | Extract training matrix \u00b6 1 2 3 X , y = forecaster . create_train_X_y ( data_train ) print ( X ) print ( y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | datetime | lag_1 | lag_2 | lag_3 | lag_4 | lag_5 | lag_6 | lag_7 | lag_8 | lag_9 | lag_10 | lag_11 | lag_12 | lag_13 | lag_14 | lag_15 | |:--------------------|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:| | 1992-10-01 00:00:00 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | 0.429795 | | 1992-11-01 00:00:00 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | | 1992-12-01 00:00:00 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | | 1993-01-01 00:00:00 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | | 1993-02-01 00:00:00 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | | datetime | y | |:--------------------|---------:| | 1992-10-01 00:00:00 | 0.568606 | | 1992-11-01 00:00:00 | 0.595223 | | 1992-12-01 00:00:00 | 0.771258 | | 1993-01-01 00:00:00 | 0.751503 | | 1993-02-01 00:00:00 | 0.387554 |","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom .","title":"Recursive multi-step forecasting"},{"location":"guides/autoregresive-forecaster.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/autoregresive-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ()","title":"Data"},{"location":"guides/autoregresive-forecaster.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor (), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor() Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-06 23:22:17 Last fit date: 2021-12-06 23:22:17 Skforecast version: 0.4.0","title":"Create and train forecaster"},{"location":"guides/autoregresive-forecaster.html#prediction","text":"1 2 3 4 5 # Predict # ============================================================================== steps = 36 predictions = forecaster . predict ( steps = steps ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.916878 2005-08-01 0.946228 2005-09-01 1.107921 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend () 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.003811271443540486","title":"Prediction"},{"location":"guides/autoregresive-forecaster.html#feature-importance","text":"1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | feature | importance | |:----------|-------------:| | lag_1 | 0.0121745 | | lag_2 | 0.0857522 | | lag_3 | 0.0166948 | | lag_4 | 0.00287025 | | lag_5 | 0.00346738 | | lag_6 | 0.00333398 | | lag_7 | 0.00331932 | | lag_8 | 0.00757049 | | lag_9 | 0.00997129 | | lag_10 | 0.01222 | | lag_11 | 0.0120539 | | lag_12 | 0.794896 | | lag_13 | 0.00438497 | | lag_14 | 0.0161486 | | lag_15 | 0.0151425 |","title":"Feature importance"},{"location":"guides/autoregresive-forecaster.html#extract-training-matrix","text":"1 2 3 X , y = forecaster . create_train_X_y ( data_train ) print ( X ) print ( y ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | datetime | lag_1 | lag_2 | lag_3 | lag_4 | lag_5 | lag_6 | lag_7 | lag_8 | lag_9 | lag_10 | lag_11 | lag_12 | lag_13 | lag_14 | lag_15 | |:--------------------|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:| | 1992-10-01 00:00:00 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | 0.429795 | | 1992-11-01 00:00:00 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | | 1992-12-01 00:00:00 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | | 1993-01-01 00:00:00 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | | 1993-02-01 00:00:00 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | | datetime | y | |:--------------------|---------:| | 1992-10-01 00:00:00 | 0.568606 | | 1992-11-01 00:00:00 | 0.595223 | | 1992-12-01 00:00:00 | 0.771258 | | 1993-01-01 00:00:00 | 0.751503 | | 1993-02-01 00:00:00 | 0.387554 |","title":"Extract training matrix"},{"location":"guides/backtesting.html","text":"Backtesting \u00b6 Backtesting with refit The model is trained each tieme before making the predictions, in this way, the model use all the information available so far. It is a variation of the standar cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting without refit After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time. Libraries \u00b6 1 2 3 4 5 6 7 8 9 10 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) ax . legend () Backtest \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , verbose = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 print ( f \"Error de backtest: { metric } \" ) 1 Error de backtest: [0.00726621] 1 predictions_backtest . head ( 4 ) 1 2 3 4 5 6 | | pred | |:--------------------|---------:| | 1999-07-01 00:00:00 | 0.712336 | | 1999-08-01 00:00:00 | 0.750542 | | 1999-09-01 00:00:00 | 0.802371 | | 1999-10-01 00:00:00 | 0.806941 | 1 2 3 4 5 6 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend (); Backtest with prediction intervals \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Backtesting"},{"location":"guides/backtesting.html#backtesting","text":"Backtesting with refit The model is trained each tieme before making the predictions, in this way, the model use all the information available so far. It is a variation of the standar cross-validation but, instead of making a random distribution of the observations, the training set is increased sequentially, maintaining the temporal order of the data. Backtesting without refit After an initial train, the model is used sequentially without updating it and following the temporal order of the data. This strategy has the advantage of being much faster since the model is only trained once. However, the model does not incorporate the latest information available so it may lose predictive capacity over time.","title":"Backtesting"},{"location":"guides/backtesting.html#libraries","text":"1 2 3 4 5 6 7 8 9 10 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from skforecast.model_selection import backtesting_forecaster from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/backtesting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' ) # Data preprocessing # ============================================================================== data [ 'fecha' ] = pd . to_datetime ( data [ 'fecha' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'fecha' ) data = data . rename ( columns = { 'x' : 'y' }) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data . plot ( ax = ax ) ax . legend ()","title":"Data"},{"location":"guides/backtesting.html#backtest","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 # Last 9 years are used for backtest data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , verbose = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 print ( f \"Error de backtest: { metric } \" ) 1 Error de backtest: [0.00726621] 1 predictions_backtest . head ( 4 ) 1 2 3 4 5 6 | | pred | |:--------------------|---------:| | 1999-07-01 00:00:00 | 0.712336 | | 1999-08-01 00:00:00 | 0.750542 | | 1999-09-01 00:00:00 | 0.802371 | | 1999-10-01 00:00:00 | 0.806941 | 1 2 3 4 5 6 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . plot ( ax = ax , label = 'predictions' ) ax . legend ();","title":"Backtest"},{"location":"guides/backtesting.html#backtest-with-prediction-intervals","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Backtest forecaster # ============================================================================== n_backtest = 36 * 3 data_train = data [: - n_backtest ] data_test = data [ - n_backtest :] forecaster = ForecasterAutoreg ( regressor = Ridge (), lags = 15 ) metric , predictions_backtest = backtesting_forecaster ( forecaster = forecaster , y = data , initial_train_size = len ( data_train ), steps = 10 , metric = 'mean_squared_error' , refit = True , interval = [ 5 , 95 ], n_boot = 500 , verbose = True ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Information of backtesting process ---------------------------------- Number of observations used for initial training: 96 Number of observations used for backtesting: 108 Number of folds: 11 Number of steps per fold: 10 Last fold only includes 8 observations. Data partition in fold: 0 Training: 1991-07-01 00:00:00 -- 1999-06-01 00:00:00 Validation: 1999-07-01 00:00:00 -- 2000-04-01 00:00:00 Data partition in fold: 1 Training: 1991-07-01 00:00:00 -- 2000-04-01 00:00:00 Validation: 2000-05-01 00:00:00 -- 2001-02-01 00:00:00 Data partition in fold: 2 Training: 1991-07-01 00:00:00 -- 2001-02-01 00:00:00 Validation: 2001-03-01 00:00:00 -- 2001-12-01 00:00:00 Data partition in fold: 3 Training: 1991-07-01 00:00:00 -- 2001-12-01 00:00:00 Validation: 2002-01-01 00:00:00 -- 2002-10-01 00:00:00 Data partition in fold: 4 Training: 1991-07-01 00:00:00 -- 2002-10-01 00:00:00 Validation: 2002-11-01 00:00:00 -- 2003-08-01 00:00:00 Data partition in fold: 5 Training: 1991-07-01 00:00:00 -- 2003-08-01 00:00:00 Validation: 2003-09-01 00:00:00 -- 2004-06-01 00:00:00 Data partition in fold: 6 Training: 1991-07-01 00:00:00 -- 2004-06-01 00:00:00 Validation: 2004-07-01 00:00:00 -- 2005-04-01 00:00:00 Data partition in fold: 7 Training: 1991-07-01 00:00:00 -- 2005-04-01 00:00:00 Validation: 2005-05-01 00:00:00 -- 2006-02-01 00:00:00 Data partition in fold: 8 Training: 1991-07-01 00:00:00 -- 2006-02-01 00:00:00 Validation: 2006-03-01 00:00:00 -- 2006-12-01 00:00:00 Data partition in fold: 9 Training: 1991-07-01 00:00:00 -- 2006-12-01 00:00:00 Validation: 2007-01-01 00:00:00 -- 2007-10-01 00:00:00 Data partition in fold: 10 Training: 1991-07-01 00:00:00 -- 2007-10-01 00:00:00 Validation: 2007-11-01 00:00:00 -- 2008-06-01 00:00:00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot backtest predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions_backtest . iloc [:, 0 ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions_backtest . index , predictions_backtest . iloc [:, 1 ], predictions_backtest . iloc [:, 2 ], color = 'red' , alpha = 0.2 , label = 'prediction interval' ) ax . legend ();","title":"Backtest with prediction intervals"},{"location":"guides/custom-predictors.html","text":"Custom predictors for recursive multi-step forecasting \u00b6 It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors. Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create predictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ======================= ForecasterAutoregCustom ======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with function: create_predictors Window size: 20 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-06 23:30:30 Last fit date: 2021-12-06 23:30:31 Skforecast version: 0.4.0 Prediction \u00b6 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191 Feature importance \u00b6 1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 | feature | importance | |:--------------------|-------------:| | custom_predictor_0 | 0.53972 | | custom_predictor_1 | 0.119097 | | custom_predictor_2 | 0.0464036 | | custom_predictor_3 | 0.0241653 | | custom_predictor_4 | 0.0305667 | | custom_predictor_5 | 0.0151391 | | custom_predictor_6 | 0.0428832 | | custom_predictor_7 | 0.012742 | | custom_predictor_8 | 0.018938 | | custom_predictor_9 | 0.108639 | | custom_predictor_10 | 0.0417066 | Extract training matrix \u00b6 1 2 3 X , y = forecaster . create_train_X_y ( data_train ) print ( X ) print ( y ) 1 2 3 4 5 6 7 | datetime | custom_predictor_0 | custom_predictor_1 | custom_predictor_2 | custom_predictor_3 | custom_predictor_4 | custom_predictor_5 | custom_predictor_6 | custom_predictor_7 | custom_predictor_8 | custom_predictor_9 | custom_predictor_10 | |:--------------------|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|----------------------:| | 1993-03-01 00:00:00 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.496401 | | 1993-04-01 00:00:00 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.496275 | | 1993-05-01 00:00:00 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.496924 | | 1993-06-01 00:00:00 | 0.428859 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.496759 | | 1993-07-01 00:00:00 | 0.470126 | 0.428859 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.495638 | 1 2 3 4 5 6 7 | datetime | y | |:--------------------|---------:| | 1993-03-01 00:00:00 | 0.427283 | | 1993-04-01 00:00:00 | 0.41389 | | 1993-05-01 00:00:00 | 0.428859 | | 1993-06-01 00:00:00 | 0.470126 | | 1993-07-01 00:00:00 | 0.50921 |","title":"Custom predictors"},{"location":"guides/custom-predictors.html#custom-predictors-for-recursive-multi-step-forecasting","text":"It may be interesting to include additional characteristics of the time series in addition to the lags, for example, the moving average of the n last values \u200b\u200bcan be used to capture the trend of the series. The ForecasterAutoregCustom class is very similar to the ForecasterAutoreg class described in the previous section, but it is the user who defines the function used to create the predictors.","title":"Custom predictors for  recursive multi-step forecasting"},{"location":"guides/custom-predictors.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/custom-predictors.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/custom-predictors.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Custom function to create predictors # ============================================================================== def create_predictors ( y ): ''' Create first 10 lags of a time series. Calculate moving average with window 20. ''' X_train = pd . DataFrame ({ 'y' : y . copy ()}) for i in range ( 0 , 10 ): X_train [ f 'lag_ { i + 1 } ' ] = X_train [ 'y' ] . shift ( i ) X_train [ 'moving_avg' ] = X_train [ 'y' ] . rolling ( 20 ) . mean () X_train = X_train . drop ( columns = 'y' ) . tail ( 1 ) . to_numpy () return X_train 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregCustom ( regressor = RandomForestRegressor ( random_state = 123 ), fun_predictors = create_predictors , window_size = 20 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ======================= ForecasterAutoregCustom ======================= Regressor: RandomForestRegressor(random_state=123) Predictors created with function: create_predictors Window size: 20 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-06 23:30:30 Last fit date: 2021-12-06 23:30:31 Skforecast version: 0.4.0","title":"Create and train forecaster"},{"location":"guides/custom-predictors.html#prediction","text":"1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.926598 2005-08-01 0.948202 2005-09-01 1.020947 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.04487765885818191","title":"Prediction"},{"location":"guides/custom-predictors.html#feature-importance","text":"1 forecaster . get_feature_importance () 1 2 3 4 5 6 7 8 9 10 11 12 13 | feature | importance | |:--------------------|-------------:| | custom_predictor_0 | 0.53972 | | custom_predictor_1 | 0.119097 | | custom_predictor_2 | 0.0464036 | | custom_predictor_3 | 0.0241653 | | custom_predictor_4 | 0.0305667 | | custom_predictor_5 | 0.0151391 | | custom_predictor_6 | 0.0428832 | | custom_predictor_7 | 0.012742 | | custom_predictor_8 | 0.018938 | | custom_predictor_9 | 0.108639 | | custom_predictor_10 | 0.0417066 |","title":"Feature importance"},{"location":"guides/custom-predictors.html#extract-training-matrix","text":"1 2 3 X , y = forecaster . create_train_X_y ( data_train ) print ( X ) print ( y ) 1 2 3 4 5 6 7 | datetime | custom_predictor_0 | custom_predictor_1 | custom_predictor_2 | custom_predictor_3 | custom_predictor_4 | custom_predictor_5 | custom_predictor_6 | custom_predictor_7 | custom_predictor_8 | custom_predictor_9 | custom_predictor_10 | |:--------------------|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|---------------------:|----------------------:| | 1993-03-01 00:00:00 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.496401 | | 1993-04-01 00:00:00 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.496275 | | 1993-05-01 00:00:00 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.496924 | | 1993-06-01 00:00:00 | 0.428859 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.496759 | | 1993-07-01 00:00:00 | 0.470126 | 0.428859 | 0.41389 | 0.427283 | 0.387554 | 0.751503 | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.495638 | 1 2 3 4 5 6 7 | datetime | y | |:--------------------|---------:| | 1993-03-01 00:00:00 | 0.427283 | | 1993-04-01 00:00:00 | 0.41389 | | 1993-05-01 00:00:00 | 0.428859 | | 1993-06-01 00:00:00 | 0.470126 | | 1993-07-01 00:00:00 | 0.50921 |","title":"Extract training matrix"},{"location":"guides/direct-multi-step-forecasting.html","text":"Direct multi-step forecaster \u00b6 ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model. Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend (); Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = make_pipeline ( StandardScaler (), Ridge ()), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ============================ ForecasterAutoregMultiOutput ============================ Regressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Maximum steps predicted: 36 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__normalize': 'deprecated', 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.001} Creation date: 2021-12-07 21:30:45 Last fit date: 2021-12-07 21:30:45 Skforecast version: 0.4.0 Prediction \u00b6 If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.965991 2005-08-01 0.973200 2005-09-01 1.144204 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009067941608532212 Feature importance \u00b6 Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 forecaster . get_coef ( step = 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | feature | coef | |:----------|-------------:| | lag_1 | 0.0306858 | | lag_2 | 0.0407212 | | lag_3 | 0.0345991 | | lag_4 | -0.0018438 | | lag_5 | -0.00110815 | | lag_6 | 0.000759624 | | lag_7 | -0.00282664 | | lag_8 | 0.00109809 | | lag_9 | -0.0046271 | | lag_10 | 0.00610878 | | lag_11 | 0.0054867 | | lag_12 | 0.159351 | | lag_13 | -0.0260086 | | lag_14 | -0.0460312 | | lag_15 | -0.0358484 | Extract training matrix \u00b6 Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 8 9 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , ) print ( X_1 . head ( 4 )) print ( y_1 . head ( 4 )) 1 2 3 4 5 6 7 8 9 10 11 12 13 | lag_1 | lag_2 | lag_3 | lag_4 | lag_5 | lag_6 | lag_7 | lag_8 | lag_9 | lag_10 | lag_11 | lag_12 | lag_13 | lag_14 | lag_15 | |---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:| | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | 0.429795 | | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | | y_step_1 | |-----------:| | 0.595223 | | 0.771258 | | 0.751503 | | 0.387554 |","title":"Direct multi-step forecasting"},{"location":"guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","text":"ForecasterAutoreg and ForecasterAutoregCustom models follow a recursive prediction strategy in which, each new prediction, builds on the previous prediction. An alternative is to train a model for each step that has to be predicted. This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the ForecasterAutoregMultiOutput class and can also include one or multiple exogenous variables. In order to train a ForecasterAutoregMultiOutput a different training matrix is created for each model.","title":"Direct multi-step forecaster"},{"location":"guides/direct-multi-step-forecasting.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/direct-multi-step-forecasting.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ();","title":"Data"},{"location":"guides/direct-multi-step-forecasting.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 10 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoregMultiOutput ( regressor = make_pipeline ( StandardScaler (), Ridge ()), steps = 36 , lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ============================ ForecasterAutoregMultiOutput ============================ Regressor: Pipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())]) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Maximum steps predicted: 36 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'ridge__alpha': 1.0, 'ridge__copy_X': True, 'ridge__fit_intercept': True, 'ridge__max_iter': None, 'ridge__normalize': 'deprecated', 'ridge__positive': False, 'ridge__random_state': None, 'ridge__solver': 'auto', 'ridge__tol': 0.001} Creation date: 2021-12-07 21:30:45 Last fit date: 2021-12-07 21:30:45 Skforecast version: 0.4.0","title":"Create and train forecaster"},{"location":"guides/direct-multi-step-forecasting.html#prediction","text":"If the Forecaster has been trained with exogenous variables, they shlud be provided when predictiong. 1 2 3 4 # Predict # ============================================================================== predictions = forecaster . predict ( steps = 36 ) predictions . head ( 3 ) 1 2 3 4 2005-07-01 0.965991 2005-08-01 0.973200 2005-09-01 1.144204 Freq: MS, Name: pred, dtype: float64 1 2 3 4 5 6 7 # Plot predictions # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) predictions . plot ( ax = ax , label = 'predictions' ) ax . legend (); 1 2 3 4 5 6 7 # Prediction error # ============================================================================== error_mse = mean_squared_error ( y_true = data_test , y_pred = predictions ) print ( f \"Test error (mse): { error_mse } \" ) 1 Test error (mse): 0.009067941608532212","title":"Prediction"},{"location":"guides/direct-multi-step-forecasting.html#feature-importance","text":"Since ForecasterAutoregMultiOutput fits one model per step,it is necessary to specify from which model retrieve its feature importance. 1 forecaster . get_coef ( step = 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | feature | coef | |:----------|-------------:| | lag_1 | 0.0306858 | | lag_2 | 0.0407212 | | lag_3 | 0.0345991 | | lag_4 | -0.0018438 | | lag_5 | -0.00110815 | | lag_6 | 0.000759624 | | lag_7 | -0.00282664 | | lag_8 | 0.00109809 | | lag_9 | -0.0046271 | | lag_10 | 0.00610878 | | lag_11 | 0.0054867 | | lag_12 | 0.159351 | | lag_13 | -0.0260086 | | lag_14 | -0.0460312 | | lag_15 | -0.0358484 |","title":"Feature importance"},{"location":"guides/direct-multi-step-forecasting.html#extract-training-matrix","text":"Two steps are needed. One to create the whole training matrix and a second one to subset the data needed for each model (step). 1 2 3 4 5 6 7 8 9 X , y = forecaster . create_train_X_y ( data_train ) # X and y to train model for step 1 X_1 , y_1 = forecaster . filter_train_X_y_for_step ( step = 1 , X_train = X , y_train = y , ) print ( X_1 . head ( 4 )) print ( y_1 . head ( 4 )) 1 2 3 4 5 6 7 8 9 10 11 12 13 | lag_1 | lag_2 | lag_3 | lag_4 | lag_5 | lag_6 | lag_7 | lag_8 | lag_9 | lag_10 | lag_11 | lag_12 | lag_13 | lag_14 | lag_15 | |---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:| | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | 0.429795 | | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | 0.400906 | | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | 0.432159 | | 0.771258 | 0.595223 | 0.568606 | 0.534761 | 0.475463 | 0.483389 | 0.410534 | 0.361801 | 0.379808 | 0.351348 | 0.33622 | 0.660119 | 0.602652 | 0.502369 | 0.492543 | | y_step_1 | |-----------:| | 0.595223 | | 0.771258 | | 0.751503 | | 0.387554 |","title":"Extract training matrix"},{"location":"guides/introduction-forecasting.html","text":"Introduction to forcasting \u00b6 Time Series and forecasting \u00b6 A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions. Recursive multi-step forecasting \u00b6 Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable. Direct multi-step forecasting \u00b6 This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#introduction-to-forcasting","text":"","title":"Introduction to forcasting"},{"location":"guides/introduction-forecasting.html#time-series-and-forecasting","text":"A time series is a sequence of data arranged chronologically, in principle, equally spaced in time. Time series forecasting is the use of models to predict future values based on previously observed values, with the option of also including other external variables. When working with time series, it is seldom needed to predict only the next element in the series ( t+1 ). Instead, the most common goal is to predict a whole future interval ( t+1, ..., t+n ) or a far point in time ( t+n ). There are several strategies that allow generating this type of multiple predictions.","title":"Time Series and forecasting"},{"location":"guides/introduction-forecasting.html#recursive-multi-step-forecasting","text":"Since the value of t(n) is required to predict the point t(n-1) , and t(n-1) is unknown, it is necessary to make recursive predictions in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting. The main challenge when using scikit-learn models for recursive multi-step forecasting is transforming the time series in an matrix where, each value of the series, is related to the time window (lags) that precedes it. This forecasting strategy can be easily generated with the classes ForecasterAutoreg and ForecasterAutoregCustom . Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. Time series transformation including an exogenous variable.","title":"Recursive multi-step forecasting"},{"location":"guides/introduction-forecasting.html#direct-multi-step-forecasting","text":"This strategy consists of training a different model for each step. For example, to predict the next 5 values of a time series, 5 different models are trainded, one for each step. As a result, the predictions are independent of each other. This forecasting strategy can be easily generated with the ForecasterAutoregMultiOutput class (changed in version 0.1.9). Time series transformation into the matrices needed to train a direct multi-step forecaster.","title":"Direct multi-step forecasting"},{"location":"guides/prediction-intervals.html","text":"Prediction intervals \u00b6 Libraries \u00b6 1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend () Create and train forecaster \u00b6 1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-08 00:27:29 Last fit date: 2021-12-08 00:27:29 Skforecast version: 0.4.0 Prediction intervals \u00b6 1 2 3 4 5 6 7 8 9 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = 36 , interval = [ 5 , 95 ], n_boot = 500 ) predictions . head ( 4 ) 1 2 3 4 5 6 | | pred | lower_bound | upper_bound | |:--------------------|---------:|--------------:|--------------:| | 2005-07-01 00:00:00 | 0.899283 | 0.832109 | 0.977063 | | 2005-08-01 00:00:00 | 0.954796 | 0.881798 | 1.02203 | | 2005-09-01 00:00:00 | 1.06672 | 0.964466 | 1.13247 | | 2005-10-01 00:00:00 | 1.1022 | 1.01777 | 1.17421 | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions [ 'pred' ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions [ 'lower_bound' ], predictions [ 'upper_bound' ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#prediction-intervals","text":"","title":"Prediction intervals"},{"location":"guides/prediction-intervals.html#libraries","text":"1 2 3 4 5 6 7 8 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/prediction-intervals.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 36 data_train = data [: - steps ] data_test = data [ - steps :] # Plot # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_train . plot ( ax = ax , label = 'train' ) data_test . plot ( ax = ax , label = 'test' ) ax . legend ()","title":"Data"},{"location":"guides/prediction-intervals.html#create-and-train-forecaster","text":"1 2 3 4 5 6 7 8 9 # Create and fit forecaster # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 15 ) forecaster . fit ( y = data_train ) forecaster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ================= ForecasterAutoreg ================= Regressor: RandomForestRegressor(random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Window size: 15 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')] Training index type: DatetimeIndex Training index frequency: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False} Creation date: 2021-12-08 00:27:29 Last fit date: 2021-12-08 00:27:29 Skforecast version: 0.4.0","title":"Create and train forecaster"},{"location":"guides/prediction-intervals.html#prediction-intervals_1","text":"1 2 3 4 5 6 7 8 9 # Prediction intervals # ============================================================================== predictions = forecaster . predict_interval ( steps = 36 , interval = [ 5 , 95 ], n_boot = 500 ) predictions . head ( 4 ) 1 2 3 4 5 6 | | pred | lower_bound | upper_bound | |:--------------------|---------:|--------------:|--------------:| | 2005-07-01 00:00:00 | 0.899283 | 0.832109 | 0.977063 | | 2005-08-01 00:00:00 | 0.954796 | 0.881798 | 1.02203 | | 2005-09-01 00:00:00 | 1.06672 | 0.964466 | 1.13247 | | 2005-10-01 00:00:00 | 1.1022 | 1.01777 | 1.17421 | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Plot predictions and interval # ============================================================================== fig , ax = plt . subplots ( figsize = ( 9 , 4 )) data_test . plot ( ax = ax , label = 'test' ) predictions [ 'pred' ] . plot ( ax = ax , label = 'predictions' ) ax . fill_between ( predictions . index , predictions [ 'lower_bound' ], predictions [ 'upper_bound' ], color = 'red' , alpha = 0.2 , label = 'prediction_interval' ) ax . legend ( loc = 'upper right' );","title":"Prediction intervals"},{"location":"guides/tuning-forecaster.html","text":"Tuning forecaster \u00b6 Skforecast library allows to combine grid search strategy with backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediction performance. Libraries \u00b6 1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error Data \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :] Grid search \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Grid search hyperparameter and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hyperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 , 15 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data . loc [: '2006-01-01' ], param_grid = param_grid , lags_grid = lags_grid , steps = 12 , refit = True , metric = 'mean_squared_error' , initial_train_size = len ( data_train ), return_best = True , verbose = False ) 1 results_grid 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | lags | params | metric | max_depth | n_estimators | |---------------------------------|----------------------------------------|-----------|-------------|----------------| | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 5, 'n_estimators': 50} | 0.0334486 | 5 | 50 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 10, 'n_estimators': 50} | 0.0392212 | 10 | 50 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 15, 'n_estimators': 100} | 0.0392658 | 15 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 5, 'n_estimators': 100} | 0.0395258 | 5 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 10, 'n_estimators': 100} | 0.0402408 | 10 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 15, 'n_estimators': 50} | 0.0407645 | 15 | 50 | | [ 1 2 3 20] | {'max_depth': 15, 'n_estimators': 100} | 0.0439092 | 15 | 100 | | [ 1 2 3 20] | {'max_depth': 5, 'n_estimators': 100} | 0.0449923 | 5 | 100 | | [ 1 2 3 20] | {'max_depth': 5, 'n_estimators': 50} | 0.0462237 | 5 | 50 | | [1 2 3] | {'max_depth': 5, 'n_estimators': 50} | 0.0486662 | 5 | 50 | | [ 1 2 3 20] | {'max_depth': 10, 'n_estimators': 100} | 0.0489914 | 10 | 100 | | [ 1 2 3 20] | {'max_depth': 10, 'n_estimators': 50} | 0.0501932 | 10 | 50 | | [1 2 3] | {'max_depth': 15, 'n_estimators': 100} | 0.0505563 | 15 | 100 | | [ 1 2 3 20] | {'max_depth': 15, 'n_estimators': 50} | 0.0512172 | 15 | 50 | | [1 2 3] | {'max_depth': 5, 'n_estimators': 100} | 0.0531229 | 5 | 100 | | [1 2 3] | {'max_depth': 15, 'n_estimators': 50} | 0.0602604 | 15 | 50 | | [1 2 3] | {'max_depth': 10, 'n_estimators': 50} | 0.0609513 | 10 | 50 | | [1 2 3] | {'max_depth': 10, 'n_estimators': 100} | 0.0673343 | 10 | 100 | 1 skforecast 1 2 3 4 5 6 7 8 9 10 11 12 13 ========================================================================== <class 'skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg'> ========================================================================== Regressor: RandomForestRegressor(max_depth=5, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] Training index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'> Training index frequancy: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#tuning-forecaster","text":"Skforecast library allows to combine grid search strategy with backtesting in order to identify the combination of lags and hyperparameters that achieve the best prediction performance.","title":"Tuning forecaster"},{"location":"guides/tuning-forecaster.html#libraries","text":"1 2 3 4 5 6 7 8 9 # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.ForecasterAutoreg import ForecasterAutoreg from sklearn.ensembre import Ridge from sklearn.metrics import mean_squared_error","title":"Libraries"},{"location":"guides/tuning-forecaster.html#data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Download data # ============================================================================== url = ( 'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv' ) data = pd . read_csv ( url , sep = ',' , header = 0 , names = [ 'y' , 'datetime' ]) # Data preprocessing # ============================================================================== data [ 'datetime' ] = pd . to_datetime ( data [ 'datetime' ], format = '%Y/%m/ %d ' ) data = data . set_index ( 'datetime' ) data = data . asfreq ( 'MS' ) data = data [ 'y' ] data = data . sort_index () # Split train-test # ============================================================================== steps = 24 data_train = data . loc [: '2001-01-01' ] data_val = data . loc [ '2001-01-01' : '2006-01-01' ] data_test = data . loc [ '2006-01-01' :]","title":"Data"},{"location":"guides/tuning-forecaster.html#grid-search","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Grid search hyperparameter and lags # ============================================================================== forecaster = ForecasterAutoreg ( regressor = RandomForestRegressor ( random_state = 123 ), lags = 12 # Placeholder, the value will be overwritten ) # Regressor hyperparameters param_grid = { 'n_estimators' : [ 50 , 100 ], 'max_depth' : [ 5 , 10 , 15 ]} # Lags used as predictors lags_grid = [ 3 , 10 , [ 1 , 2 , 3 , 20 ]] results_grid = grid_search_forecaster ( forecaster = forecaster , y = data . loc [: '2006-01-01' ], param_grid = param_grid , lags_grid = lags_grid , steps = 12 , refit = True , metric = 'mean_squared_error' , initial_train_size = len ( data_train ), return_best = True , verbose = False ) 1 results_grid 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | lags | params | metric | max_depth | n_estimators | |---------------------------------|----------------------------------------|-----------|-------------|----------------| | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 5, 'n_estimators': 50} | 0.0334486 | 5 | 50 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 10, 'n_estimators': 50} | 0.0392212 | 10 | 50 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 15, 'n_estimators': 100} | 0.0392658 | 15 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 5, 'n_estimators': 100} | 0.0395258 | 5 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 10, 'n_estimators': 100} | 0.0402408 | 10 | 100 | | [ 1 2 3 4 5 6 7 8 9 10] | {'max_depth': 15, 'n_estimators': 50} | 0.0407645 | 15 | 50 | | [ 1 2 3 20] | {'max_depth': 15, 'n_estimators': 100} | 0.0439092 | 15 | 100 | | [ 1 2 3 20] | {'max_depth': 5, 'n_estimators': 100} | 0.0449923 | 5 | 100 | | [ 1 2 3 20] | {'max_depth': 5, 'n_estimators': 50} | 0.0462237 | 5 | 50 | | [1 2 3] | {'max_depth': 5, 'n_estimators': 50} | 0.0486662 | 5 | 50 | | [ 1 2 3 20] | {'max_depth': 10, 'n_estimators': 100} | 0.0489914 | 10 | 100 | | [ 1 2 3 20] | {'max_depth': 10, 'n_estimators': 50} | 0.0501932 | 10 | 50 | | [1 2 3] | {'max_depth': 15, 'n_estimators': 100} | 0.0505563 | 15 | 100 | | [ 1 2 3 20] | {'max_depth': 15, 'n_estimators': 50} | 0.0512172 | 15 | 50 | | [1 2 3] | {'max_depth': 5, 'n_estimators': 100} | 0.0531229 | 5 | 100 | | [1 2 3] | {'max_depth': 15, 'n_estimators': 50} | 0.0602604 | 15 | 50 | | [1 2 3] | {'max_depth': 10, 'n_estimators': 50} | 0.0609513 | 10 | 50 | | [1 2 3] | {'max_depth': 10, 'n_estimators': 100} | 0.0673343 | 10 | 100 | 1 skforecast 1 2 3 4 5 6 7 8 9 10 11 12 13 ========================================================================== <class 'skforecast.ForecasterAutoreg.ForecasterAutoreg.ForecasterAutoreg'> ========================================================================== Regressor: RandomForestRegressor(max_depth=5, n_estimators=50, random_state=123) Lags: [ 1 2 3 4 5 6 7 8 9 10] Window size: 10 Included exogenous: False Type of exogenous variable: None Exogenous variables names: None Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')] Training index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'> Training index frequancy: MS Regressor parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}","title":"Grid search"},{"location":"releases/releases.html","text":"Releases \u00b6 [0.3.0] - [2021-09-01] \u00b6 Added \u00b6 New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels Changed \u00b6 cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements Fixed \u00b6 [0.2.0] - [2021-08-26] \u00b6 Added \u00b6 Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Fixed \u00b6 [0.1.9] - 2121-07-27 \u00b6 Added \u00b6 Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog . Changed \u00b6 New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric. Fixed \u00b6 Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster . [0.1.8.1] - 2021-05-17 \u00b6 Added \u00b6 set_out_sample_residuals method to store or update out of sample residuals used by predict_interval . Changed \u00b6 backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals . Fixed \u00b6 Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput . [0.1.8] - 2021-04-02 \u00b6 Added \u00b6 Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals. Changed \u00b6 Fixed \u00b6 [0.1.7] - 2021-03-19 \u00b6 Added \u00b6 Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors. Changed \u00b6 grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg . Fixed \u00b6 [0.1.6] - 2021-03-14 \u00b6 Added \u00b6 Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection . Changed \u00b6 Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster . Fixed \u00b6 [0.1.4] - 2021-02-15 \u00b6 Added \u00b6 Method get_coef to skforecast.ForecasterAutoreg . Changed \u00b6 Fixed \u00b6","title":"Releases"},{"location":"releases/releases.html#releases","text":"","title":"Releases"},{"location":"releases/releases.html#030-2021-09-01","text":"","title":"[0.3.0] - [2021-09-01]"},{"location":"releases/releases.html#added","text":"New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library: backtesting_autoreg_statsmodels cv_autoreg_statsmodels backtesting_sarimax_statsmodels cv_sarimax_statsmodels grid_search_sarimax_statsmodels","title":"Added"},{"location":"releases/releases.html#changed","text":"cv_forecaster returns cross-validation metrics and cross-validation predictions. Added an extra column for each parameter in the dataframe returned by grid_search_forecaster . statsmodels 0.12.2 added to requirements","title":"Changed"},{"location":"releases/releases.html#fixed","text":"","title":"Fixed"},{"location":"releases/releases.html#020-2021-08-26","text":"","title":"[0.2.0] - [2021-08-26]"},{"location":"releases/releases.html#added_1","text":"Multiple exogenous variables can be passed as pandas DataFrame. Documentation at https://joaquinamatrodrigo.github.io/skforecast/ New unit test Increased typing","title":"Added"},{"location":"releases/releases.html#changed_1","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput .","title":"Changed"},{"location":"releases/releases.html#fixed_1","text":"","title":"Fixed"},{"location":"releases/releases.html#019-2121-07-27","text":"","title":"[0.1.9] - 2121-07-27"},{"location":"releases/releases.html#added_2","text":"Logging total number of models to fit in grid_search_forecaster . Class ForecasterAutoregCustom . Method create_train_X_y to facilitate access to the training data matrix created from y and exog .","title":"Added"},{"location":"releases/releases.html#changed_2","text":"New implementation of ForecasterAutoregMultiOutput . The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old versi\u00f3n can be acces with skforecast.deprecated.ForecasterAutoregMultiOutput . Class ForecasterCustom has been renamed to ForecasterAutoregCustom . However, ForecasterCustom will still remain to keep backward compatibility. Argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. Check if argument metric in cv_forecaster , backtesting_forecaster , grid_search_forecaster and backtesting_forecaster_intervals is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'. time_series_spliter doesn't include the remaining observations in the last complete fold but in a new one when allow_incomplete_fold=True . Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.","title":"Changed"},{"location":"releases/releases.html#fixed_2","text":"Update lags of ForecasterAutoregMultiOutput after grid_search_forecaster .","title":"Fixed"},{"location":"releases/releases.html#0181-2021-05-17","text":"","title":"[0.1.8.1] - 2021-05-17"},{"location":"releases/releases.html#added_3","text":"set_out_sample_residuals method to store or update out of sample residuals used by predict_interval .","title":"Added"},{"location":"releases/releases.html#changed_3","text":"backtesting_forecaster_intervals and backtesting_forecaster print number of steps per fold. Only stored up to 1000 residuals. Improved verbose in backtesting_forecaster_intervals .","title":"Changed"},{"location":"releases/releases.html#fixed_3","text":"Warning of inclompleted folds when using backtesting_forecast with a ForecasterAutoregMultiOutput . ForecasterAutoregMultiOutput.predict allow exog data longer than needed (steps). backtesting_forecast prints correctly the number of folds when remainder observations are cero. Removed named argument X in self.regressor.predict(X) to allow using XGBoost regressor. Values stored in self.last_window when training ForecasterAutoregMultiOutput .","title":"Fixed"},{"location":"releases/releases.html#018-2021-04-02","text":"","title":"[0.1.8] - 2021-04-02"},{"location":"releases/releases.html#added_4","text":"Class ForecasterAutoregMultiOutput.py : forecaster with direct multi-step predictions. Method ForecasterCustom.predict_interval and ForecasterAutoreg.predict_interval : estimate prediction interval using bootstrapping. skforecast.model_selection.backtesting_forecaster_intervals perform backtesting and return prediction intervals.","title":"Added"},{"location":"releases/releases.html#changed_4","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_4","text":"","title":"Fixed"},{"location":"releases/releases.html#017-2021-03-19","text":"","title":"[0.1.7] - 2021-03-19"},{"location":"releases/releases.html#added_5","text":"Class ForecasterCustom : same functionalities as ForecasterAutoreg but allows custom definition of predictors.","title":"Added"},{"location":"releases/releases.html#changed_5","text":"grid_search forecaster adapted to work with objects ForecasterCustom in addition to ForecasterAutoreg .","title":"Changed"},{"location":"releases/releases.html#fixed_5","text":"","title":"Fixed"},{"location":"releases/releases.html#016-2021-03-14","text":"","title":"[0.1.6] - 2021-03-14"},{"location":"releases/releases.html#added_6","text":"Method get_feature_importances to skforecast.ForecasterAutoreg . Added backtesting strategy in grid_search_forecaster . Added backtesting_forecast to skforecast.model_selection .","title":"Added"},{"location":"releases/releases.html#changed_6","text":"Method create_lags return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor. Renamed argument X to last_window in method predict . Renamed ts_cv_forecaster to cv_forecaster .","title":"Changed"},{"location":"releases/releases.html#fixed_6","text":"","title":"Fixed"},{"location":"releases/releases.html#014-2021-02-15","text":"","title":"[0.1.4] - 2021-02-15"},{"location":"releases/releases.html#added_7","text":"Method get_coef to skforecast.ForecasterAutoreg .","title":"Added"},{"location":"releases/releases.html#changed_7","text":"","title":"Changed"},{"location":"releases/releases.html#fixed_7","text":"","title":"Fixed"}]}